,created_date,created_timestamp,subreddit,title,id,author,author_created_utc,full_link,score,num_comments,num_crossposts,subreddit_subscribers,post
0,2015-02-06 20:41:55,1423248115.0,dataengineering,Welcome!,2v0djt,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/2v0djt/welcome/,5.0,0.0,,,Thought it's about time to start a subreddit on Data Engineering.
1,2015-02-08 18:45:08,1423413908.0,dataengineering,How to kick-start Apache Spark development on IntelliJ IDEA,2v7bjp,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/2v7bjp/how_to_kickstart_apache_spark_development_on/,1.0,0.0,,,
2,2015-02-27 16:58:10,1425049090.0,dataengineering,The Principia Data,2xctqx,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/2xctqx/the_principia_data/,1.0,0.0,,,
3,2015-03-07 07:45:52,1425707152.0,dataengineering,"The Value of Data, Part 1: Using Data as a Competitive Advantage · Coding VC",2y7tpr,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/2y7tpr/the_value_of_data_part_1_using_data_as_a/,1.0,0.0,,,
4,2015-05-28 19:47:18,1432831638.0,dataengineering,Insight Data Engineering Fellows Program Expands to New York!,37m2hs,abc-jap,1432831456.0,https://www.reddit.com/r/dataengineering/comments/37m2hs/insight_data_engineering_fellows_program_expands/,1.0,0.0,,,
5,2015-07-01 18:08:56,1435763336.0,dataengineering,Data Lake,3brcyh,vsupalov,1426286778.0,https://www.reddit.com/r/dataengineering/comments/3brcyh/data_lake/,3.0,0.0,,,
6,2015-08-03 16:17:31,1438607851.0,dataengineering,Data messes | DBMS 2 : DataBase Management System Services,3fm7a5,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/3fm7a5/data_messes_dbms_2_database_management_system/,1.0,0.0,,,
7,2015-08-06 11:30:49,1438849849.0,dataengineering,"Apache Kafka, Samza, and the Unix Philosophy of Distributed Data",3fzdc8,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/3fzdc8/apache_kafka_samza_and_the_unix_philosophy_of/,1.0,0.0,,,
8,2015-11-27 15:42:38,1448631758.0,dataengineering,"Compression Benchmarks: brotli, gzip, xz, bz2",3ugr6g,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/3ugr6g/compression_benchmarks_brotli_gzip_xz_bz2/,1.0,0.0,,,
9,2015-12-01 12:24:58,1448965498.0,dataengineering,Try Luigi with Vagrant,3uze0t,vsupalov,1426286778.0,https://www.reddit.com/r/dataengineering/comments/3uze0t/try_luigi_with_vagrant/,2.0,0.0,,,
10,2016-01-07 08:07:50,1452146870.0,dataengineering,HIRING ANONYMOUS FULL STACK DEVELOPERS,3zu76v,stinghire,1452093379.0,https://www.reddit.com/r/dataengineering/comments/3zu76v/hiring_anonymous_full_stack_developers/,2.0,0.0,,,"Full-time and part-time positions available for highly experienced full-stack developers
who are interested in working anonymously on various dark net projects:


Please ONLY submit your applications via THE BOX secure messaging system:

http://theboxmmvl6zg3wi.onion/?ID=57739907578e
using the following guidelines :


a) Provide 2 (two) alternate secure contact options such as onion-based jabber, bitmessage, etc.

  NO CLEARNET/EMAIL OR OTHER CLEARNET-BASED SERVICES

b) Provide your PGP public key. 1024/2048-bit keys are NOT ACCEPTED

c) Provide a reference/link to the original job posting

d) Provide a description of the position you're applying for

e) Provide a brief resume (work experience/key skills information), personal/identifying information should be omitted.

f) Provide salary expectations per hour/week/month

g) Provide information on the number of hours you can allocate to the job on a weekly basis


ALL of the above information, including your PGP public key must be encrypted using the key attached below.
All unencrypted or partially encrypted messages will be ignored and automatically discarded.


PLEASE SAVE YOUR ACCESS URL FOR THE BOX SECURE MESSAGING SYSTEM!!!
WE WILL ONLY REPLY USING THE BOX. ALTERNATE CONTACT OPTIONS WILL NOT BE USED.

http://humanresources.i2p.xyz/



-----BEGIN PGP PUBLIC KEY BLOCK-----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=zC0C
-----END PGP PUBLIC KEY BLOCK-----"
11,2016-03-01 14:49:50,1456836590.0,dataengineering,"Easy going person, nice, good educated,a lot of work in business, at the same time ZgWXRz",48g6km,myapovand,1456836493.0,https://www.reddit.com/r/dataengineering/comments/48g6km/easy_going_person_nice_good_educateda_lot_of_work/,1.0,0.0,,,
12,2016-03-02 11:13:56,1456910036.0,dataengineering,Outsource Medical Forms Scanning Services India,48leh5,outsourcingdata2,1442637764.0,https://www.reddit.com/r/dataengineering/comments/48leh5/outsource_medical_forms_scanning_services_india/,1.0,0.0,,,
13,2016-03-21 19:53:08,1458582788.0,dataengineering,Engineers Shouldn’t Write ETL: A Guide to Building a High Functioning Data Science Department,4bddfy,vsupalov,1426286778.0,https://www.reddit.com/r/dataengineering/comments/4bddfy/engineers_shouldnt_write_etl_a_guide_to_building/,1.0,0.0,,,
14,2016-03-31 01:08:07,1459375687.0,dataengineering,Apache Kafka + Kafka Connect: A Guide to building connectors,4cn30p,geopsis,1397471213.0,https://www.reddit.com/r/dataengineering/comments/4cn30p/apache_kafka_kafka_connect_a_guide_to_building/,1.0,0.0,,,
15,2016-04-03 01:53:37,1459637617.0,dataengineering,5 CDO Steps for Month 1,4d3qkf,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4d3qkf/5_cdo_steps_for_month_1/,1.0,0.0,,,"TODAY IS OUR first day. We’re the Chief Data Officer, the CDO here to help you, the organization, better leverage your data. Provide you with better customer insights, more opportunities, and more management support for your growing needs for data…

Doesn’t this sound like a wobbly job description for an executive? Though what I described is usually not written down, this is how a lot of organizations treat their first CDO. With poor core objectives, limited budgetary support, and poor political wrangling the CDO is often re-casted into a CIO-clone or the COO’s technical counterpart. Because the CDO is so new, organizations don’t have the culture or org structure to support a CDO and this is a setup for failure.

Here are the 5 steps that we as new CDOs need to do to be successful.

http://bizcatalyst360.com/5-techniques-for-the-innovation-culture"
16,2016-04-05 13:48:28,1459853308.0,dataengineering,Outsource Indexing Service India,4dfvmc,outsourcingdata2,1442637764.0,https://www.reddit.com/r/dataengineering/comments/4dfvmc/outsource_indexing_service_india/,1.0,0.0,,,
17,2016-04-05 19:50:51,1459875051.0,dataengineering,5 Techniques for the Innovation Culture,4dhb08,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4dhb08/5_techniques_for_the_innovation_culture/,1.0,0.0,,,"WHAT IS INNOVATION? Just how much is innovation worth to us? How do we get innovation to work here, in our culture?

It’s hard to get a handle on innovation. It’s hard to understand it. It’s hard to practice it. It’s even hard to support it. Innovation is not a set of skills, methodologies, or knowledge. Innovation is a set of behaviors and habits that allow us to “connect unconnected things”. Most of us aren’t prepared for innovation or to be leaders for innovation.

To get us prepared and become the leaders we need to be here are the 6 techniques to establish the innovation culture.

http://bizcatalyst360.com/5-techniques-for-the-innovation-culture
"
18,2016-04-06 07:45:09,1459917909.0,dataengineering,Data Engineering at QA InfoTech,4dkb1g,QAThoughtLeaders,1330062120.0,https://www.reddit.com/r/dataengineering/comments/4dkb1g/data_engineering_at_qa_infotech/,1.0,0.0,,,
19,2016-04-07 18:07:02,1460041622.0,dataengineering,Practical Capacity Scheduling with YARN,4drg5x,nieuweyork,1340374968.0,https://www.reddit.com/r/dataengineering/comments/4drg5x/practical_capacity_scheduling_with_yarn/,1.0,0.0,,,
20,2016-04-13 06:56:18,1460519778.0,dataengineering,3 Big Data Areas For Non-Profits,4ejvru,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4ejvru/3_big_data_areas_for_nonprofits/,1.0,0.0,,,"WHEN YOU SAY Big Data, do you think of charities, donations, and non-profits? Why not? We non-profits have to go through quite a few hoops to get our donations. And to get them, we need to spend our donors’ money in a way the each individual donor feels the most comfortable with. And with a large number of donors we really need to be on our game. And Big Data can help.

Here are the three main areas where Big Data best helps us non-profits.

http://bizcatalyst360.com/3-big-data-areas-for-non-profits"
21,2016-04-18 21:35:31,1461004531.0,dataengineering,Big Data Builds Stellar Leaders,4fdb6b,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4fdb6b/big_data_builds_stellar_leaders/,0.0,0.0,,,"WHAT WAS YOUR FIRST team like? The first corporate team I was on was a team of leaders. Each leader had a different area of expertise. When expertise was needed the appropriate leader led and the rest of us followed. It was drilled into each of us… to be good leaders we must also be great followers.

But when you think about it, leader or not, we all follow somebody. Becoming stellar leaders require us to model ourselves after our favorite leaders. But in doing so we have to be cautious with how we fit ourselves into our leaders’ molds. Leadership is an old concept with a lot of historic baggage that just doesn’t apply in today’s knowledge economy. To be stellar leaders we need to start with an unbiased foundation and then build ourselves up from there. Here are the things we need to understand and do to mold ourselves into being stellar leaders.

http://bizcatalyst360.com/big-data-builds-stellar-leaders"
22,2016-04-27 06:49:08,1461728948.0,dataengineering,"10 Algorithm Categories for A.I., Big Data, and Data Science",4gmraw,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4gmraw/10_algorithm_categories_for_ai_big_data_and_data/,1.0,0.0,,,"ARE ALGORITHMS taking over our jobs? Yes, yes they are… and that a good thing.

An algorithm is a series of steps with rules that help us solve problems and accomplish goals. And when we structure these steps and rules the right way we can automate the algorithm to establish Artificial Intelligence (A.I.). And it is this A.I. that helps us do our analytical heavy lifting so we can focus our time on doing the things that we’re good at… the things we were hired to do.

A.I. is changing our jobs, our work styles, and our business cultures. A.I. helps us discover and focus on the key subject matter expertise that makes our human capital good, really good at what they do. But using A.I. in the work place does get complicated. It gets complicated because there are different levels of algorithms used to implement A.I., each varying in their use and impact. To better balance our human capital with our A.I. capital, here are the top 10 algorithm categories used to implement A.I., Big Data, and Data Science.

http://bizcatalyst360.com/10-algorithm-categories-for-a-i-big-data-and-data-science
"
23,2016-05-11 18:33:53,1462980833.0,dataengineering,Data Dithering: How White Noise Can Improve Data Importing,4ivh0x,buttercupsmom,1440431601.0,https://www.reddit.com/r/dataengineering/comments/4ivh0x/data_dithering_how_white_noise_can_improve_data/,4.0,0.0,,,
24,2016-05-12 02:16:04,1463008564.0,dataengineering,Is Data The New Capital? 4 Paradigms Needed,4ixr01,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4ixr01/is_data_the_new_capital_4_paradigms_needed/,1.0,2.0,,,"DATA’S IMPACT has gone far beyond operational efficiencies. Data is now capital, a financial resource that is convertible to cash and accounts receivable. Not only that, data capital protects and maximizes revenue, profit, and cash flow by supporting the right risk management, right business planning, right corporate strategies, and the right leadership development. Like having the right executives, the right data capital too is a force multiplier that multiplies our returns on our investments. Data capital multiplies our impact, our productivity rates, and our revenue and revenue growth. Data is no longer just information flowing through our wires. Data is now a strategic cornerstone to our organization. To make data work as our capital, to make data work as our force multiplier, we must establish four fundamental paradigms.

http://bizcatalyst360.com/is-data-the-new-capital-4-paradigms-needed"
25,2016-05-22 00:28:38,1463866118.0,dataengineering,Do Multipliers Trump Big Data Analytics?,4kfkcw,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4kfkcw/do_multipliers_trump_big_data_analytics/,1.0,0.0,,,"DO MULTIPLIERS TRUMP Big Data analytics? A multiplier is a factor used to estimate the impact an input has to the total end-result. Multipliers are useful tools for understanding, planning, and forecasting. They are used in risk management, business planning, and business development; specifically returns on investment, productivity, cash flow, and revenue growth. Analytics, on the other hand, are automated analyses on data and statistics.

Analytics are used as inputs to our decision-making and just like multipliers, analytics are useful for understanding, planning, and forecasting. Because of their similarity, multipliers and Big Data analytics are tightly integrated. Multipliers feed into and improve the accuracy of our analytics. Analytics feed into and improve the accuracy of our multipliers.

Because of their tight integration multipliers and analytics should be used together at all levels of the organization. The challenge is that their use changes based on the level they’re applied.

http://bizcatalyst360.com/do-multipliers-trump-big-data-analytics
"
26,2016-05-25 19:17:23,1464193043.0,dataengineering,Open Sourcing Twitter Heron | Twitter Blogs,4l0fsm,mhausenblas,1221723173.0,https://www.reddit.com/r/dataengineering/comments/4l0fsm/open_sourcing_twitter_heron_twitter_blogs/,1.0,0.0,,,
27,2016-05-31 19:38:34,1464712714.0,dataengineering,Introducing Pegasus: One does not simply pip install hadoop,4lw4lf,mwakanosya,1310055254.0,https://www.reddit.com/r/dataengineering/comments/4lw4lf/introducing_pegasus_one_does_not_simply_pip/,2.0,0.0,,,
28,2016-06-01 19:52:13,1464799933.0,dataengineering,Strategic Change: How Much Art Do We Need In Data Science? | C-SUITE DATA,4m287z,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4m287z/strategic_change_how_much_art_do_we_need_in_data/,1.0,0.0,,,"It’s our moment. There were twenty of us, a mixture of executives, consultants, and senior directors sitting in the conference room. We’re there to present the new direction we as a company are taking. We weren’t starting off on a good foot. A lot has happened recently. We got our lumps from those market analysts. We’re going through a massive layoff. And a well-respected executive resigned. Many in our audience aren’t coming from a good place. Who could blame them?

We were ready. To back up our narrative we got everyone we needed in the room. I opened up the conference bridge. Over three hundred from across the country chimed in to listen to what we had to say. For six hours we presented the financial and strategic benefits for our new direction and what we must do to realize those benefits. With our due diligence, we walked through the evidence. We were prepared; and we have Data Science to thank.

http://bizcatalyst360.com/strategic-change-how-much-art-do-we-need-in-data-science/
"
29,2016-06-02 21:39:39,1464892779.0,dataengineering,What is it like to be a Data Engineer at Mozilla?,4m8nms,redblackbit,1393015775.0,https://www.reddit.com/r/dataengineering/comments/4m8nms/what_is_it_like_to_be_a_data_engineer_at_mozilla/,2.0,0.0,,,
30,2016-06-10 23:11:17,1465589477.0,dataengineering,Order-preserving stream event processors,4ni7de,[deleted],,https://www.reddit.com/r/dataengineering/comments/4ni7de/orderpreserving_stream_event_processors/,1.0,0.0,,,
31,2016-06-14 11:00:14,1465891214.0,dataengineering,The Syntax of Semi-Structured Data | Panoply.io,4o0e2i,buttercupsmom,1440431601.0,https://www.reddit.com/r/dataengineering/comments/4o0e2i/the_syntax_of_semistructured_data_panoplyio/,1.0,0.0,,,
32,2016-06-15 03:25:45,1465950345.0,dataengineering,Training and Education: Can Big Data Help Us Compete With What the Web Gives Away for Free? | C-SUITE DATA,4o4m6a,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4o4m6a/training_and_education_can_big_data_help_us/,1.0,0.0,,,"Do you remember this expression?

“ Give a man a fish and you feed him for a day.
Teach a man to fish and you feed him for a lifetime.”

I sure do. It’s an expression that spoke well on how we teach and educate. But today this expression doesn’t speak well at all. This expression needs an update:

“Teach a man [how to teach himself] to fish and you feed him for a lifetime.”

Training and education has changed. Rather than students being immersed in books and lectures, they’re now looking at how-to-do videos and blog posts building their foundational skills for free. Because we charge our students for their education, to stay in business we need to better compete with these free web resources. To do this we need to become pure learning organizations.

See more at: http://bizcatalyst360.com/training-and-education-can-big-data-help-us-compete-with-what-the-web-gives-away-for-free

"
33,2016-06-20 23:32:34,1466454754.0,dataengineering,Insight Partners with IAVA to Help Veterans Transition to the Tech Industry,4p0typ,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/4p0typ/insight_partners_with_iava_to_help_veterans/,1.0,0.0,,,
34,2016-06-21 20:44:29,1466531069.0,dataengineering,[SURVEY] How do you interact with data at work?,4p5w6l,talameetsbetty,1375653075.0,https://www.reddit.com/r/dataengineering/comments/4p5w6l/survey_how_do_you_interact_with_data_at_work/,1.0,0.0,,,"Hello fellow data workers!
Lately I’ve been getting rather frustrated with some things at work, and was wondering if this was endemic to just my workplace, or to the field as a whole. Like a good statistician, I’m reaching out to all of you in the hopes that you’ll answer a 5 minute (okay, so far it takes the average responder 6.5 minutes to finish), 16 question survey, but like a bad statistician, the input text fields are free form. For every person who fills out the survey, I’ll donate $1 to CodeNow, a non-profit that helps inner city kids learn to program (up to $1000).

[Survey here. Thanks in advance for the help!](https://docs.google.com/forms/d/1dT4FuUgYTPblxCGFSWs1kmMVC28ZxDjEsY0kCMV4mNQ/viewform)



Sorry for formatting; on mobile. "
35,2016-06-23 00:15:44,1466630144.0,dataengineering,Big Data? Data-Driven? Think Even Bigger!,4pd2lk,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4pd2lk/big_data_datadriven_think_even_bigger/,0.0,0.0,,,"We were thrown together to define and frame the new strategic change program. There were a few of us management consultants; a few folk from sales and marketing; some from operations and IT; and even legal and change management were there. What brought us together were the hemorrhaging costs. We wanted profit. We wanted revenue growth. We believed Big Data can help. From there we thought even bigger. We talked about what it would take to build a data-driven organization.

See more at:
http://bizcatalyst360.com/big-data-data-driven-think-even-bigger"
36,2016-06-30 22:08:19,1467313699.0,dataengineering,Anatomy of an Elasticsearch Cluster: Part I,4qneqy,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/4qneqy/anatomy_of_an_elasticsearch_cluster_part_i/,3.0,0.0,,,
37,2016-07-05 01:12:56,1467670376.0,dataengineering,Leadership: Data Odyssey For The Data Officer,4r9k4p,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4r9k4p/leadership_data_odyssey_for_the_data_officer/,2.0,0.0,,,"“Look at our executive team. Do you see our A-team? Do you see their exhaustion from balancing the day-to-day with the ten year vision? Do you see them as political warriors with battle scars, scars that could tear the whole team apart?”

See more at:
http://bizcatalyst360.com/leadership-data-odyssey-for-the-data-officer"
38,2016-07-05 20:36:55,1467740215.0,dataengineering,Do You Have What It Takes To Build Your Data Office? | C-SUITE DATA,4rdv7q,cpehura,1457823785.0,https://www.reddit.com/r/dataengineering/comments/4rdv7q/do_you_have_what_it_takes_to_build_your_data/,1.0,0.0,,,"“Having a data office means executives accept data’s strategic value; that data is an executive priority for our organization. Having a data office means executives politically back our CDO, our data programs, and our data activities. But our data office is more than just about data. When done right, our data office increases our organization’s competency to prioritize, forecast, plan, and execute all our business activities across the organization. Our data office doesn’t just focus on business opportunities. Our data office is an integral part to the ongoing success of our corporate governance and our executive board.”

See more at:
http://bizcatalyst360.com/do-you-have-what-it-takes-to-build-your-data-office
"
39,2016-07-07 22:14:26,1467918866.0,dataengineering,Anatomy of an Elasticsearch Cluster: Part II,4rqd1s,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/4rqd1s/anatomy_of_an_elasticsearch_cluster_part_ii/,2.0,0.0,,,
40,2016-08-03 21:55:11,1470250511.0,dataengineering,Full-Stack Analytical Infrastructure Platform for Developers,4w05ni,buttercupsmom,1440431601.0,https://www.reddit.com/r/dataengineering/comments/4w05ni/fullstack_analytical_infrastructure_platform_for/,2.0,0.0,,,
41,2016-08-04 08:27:22,1470288442.0,dataengineering,Yelp's Real-time Data Pipeline (series of posts),4w2z8v,gpsis13,1465601515.0,https://www.reddit.com/r/dataengineering/comments/4w2z8v/yelps_realtime_data_pipeline_series_of_posts/,3.0,0.0,,,
42,2016-08-05 01:47:11,1470350831.0,dataengineering,Anatomy of an Elasticsearch Cluster: Part III,4w7aov,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/4w7aov/anatomy_of_an_elasticsearch_cluster_part_iii/,2.0,0.0,,,
43,2016-08-15 14:51:39,1471261899.0,dataengineering,Castles in the Cloud: Tips for Cloud Infrastructure,4xt6s2,buttercupsmom,1440431601.0,https://www.reddit.com/r/dataengineering/comments/4xt6s2/castles_in_the_cloud_tips_for_cloud_infrastructure/,1.0,0.0,,,
44,2016-09-06 12:44:19,1473155059.0,dataengineering,Benefits of Data Entry Outsourcing services,51efr8,RayvatBPO,1393410513.0,https://www.reddit.com/r/dataengineering/comments/51efr8/benefits_of_data_entry_outsourcing_services/,1.0,0.0,,,
45,2016-09-06 23:43:15,1473194595.0,dataengineering,Fast approximate k-nearest neighbors for Apache Flink using Z-values,51hckn,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/51hckn/fast_approximate_knearest_neighbors_for_apache/,2.0,0.0,,,
46,2016-10-03 20:49:48,1475516988.0,dataengineering,Monitoring Flask applications on Kubernetes with Prometheus,55owve,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/55owve/monitoring_flask_applications_on_kubernetes_with/,1.0,0.0,,,
47,2016-10-06 20:39:13,1475775553.0,dataengineering,Introducing “A day in the life of a Data Engineer” blog series,566m8l,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/566m8l/introducing_a_day_in_the_life_of_a_data_engineer/,2.0,0.0,,,
48,2016-10-07 00:46:08,1475790368.0,dataengineering,Freeing the Law at Casetext,567y3x,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/567y3x/freeing_the_law_at_casetext/,2.0,0.0,,,
49,2016-10-10 18:23:16,1476112996.0,dataengineering,Complex Data Pipelines with Spotify's Luigi,56shmq,promptworks,1469805456.0,https://www.reddit.com/r/dataengineering/comments/56shmq/complex_data_pipelines_with_spotifys_luigi/,2.0,0.0,,,
50,2016-10-11 21:47:50,1476211670.0,dataengineering,"Tutorial on AWS serverless architecture: Kinesis, DynamoDB and Twitter data",56zn0z,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/56zn0z/tutorial_on_aws_serverless_architecture_kinesis/,2.0,0.0,,,
51,2016-10-12 15:08:04,1476274084.0,dataengineering,"Tweezer, the friendly twitter data stream collector",573pn6,nischalhp,1359995373.0,https://www.reddit.com/r/dataengineering/comments/573pn6/tweezer_the_friendly_twitter_data_stream_collector/,1.0,0.0,,,
52,2016-10-13 01:49:13,1476312553.0,dataengineering,Insight Data Engineering Fellows Program Applications Due November 3,5771lj,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/5771lj/insight_data_engineering_fellows_program/,1.0,2.0,,,
53,2016-10-18 19:51:04,1476809464.0,dataengineering,Im a passionate noob for Data Engineering,584v2h,IVIascitelli,1461363489.0,https://www.reddit.com/r/dataengineering/comments/584v2h/im_a_passionate_noob_for_data_engineering/,2.0,1.0,,,"Sup yall

So after my 1st co op ive become extremely passionate about Data Engineering. I wanted to collect some thoughts on something...

1) Ive broken down Data Engineering as a combination of ETL, Databasing, and Distributed System (and of course, the optional Cloud migration). Generally, is there any major component that I seem to be missing? 

2) What are some good Data Eng projects that I could do? Every time I ask people for ideas I get a response like ""find an open dataset and find some trends within it"", which is Data Science and Stats, which im not interested in at this time. 
I have a cool idea for a (semi) real-time system with ElasticSearch that I will definitely be posting here once I get some time to work on it. 

Thanks!"
54,2016-10-19 16:28:29,1476883709.0,dataengineering,data engineer - work from home?,58a99r,johnt8989,1453441691.0,https://www.reddit.com/r/dataengineering/comments/58a99r/data_engineer_work_from_home/,0.0,0.0,,,"I have been in IT for over 10 years and now have an interest in studying data engineering thru a bootcamp/workshop etc.

My question is how flexible are most companies with data engineers. Do any of you get to work from home fully or partially?

Also if anyone can recommend a good bootcamp/training/workshop etc. in Toronto area or online that would be great.

Thanks!"
55,2016-10-20 20:30:22,1476984622.0,dataengineering,Making it Scale at DoubleVerify,58i5dt,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/58i5dt/making_it_scale_at_doubleverify/,1.0,0.0,,,
56,2016-10-20 21:16:55,1476987415.0,dataengineering,The State of Data Engineering,58ifak,hitruncatch,1450405341.0,https://www.reddit.com/r/dataengineering/comments/58ifak/the_state_of_data_engineering/,1.0,0.0,,,
57,2016-11-08 04:32:21,1478572341.0,dataengineering,What is San Francisco on a Data Engineering Hiring Spree?,5bqnl6,thundergolfer,1414715187.0,https://www.reddit.com/r/dataengineering/comments/5bqnl6/what_is_san_francisco_on_a_data_engineering/,1.0,0.0,,,
58,2016-11-17 10:42:28,1479372148.0,dataengineering,"Are there important or valuable ""Data Engineer"" certificates?",5det56,Smiffsten,1385562390.0,https://www.reddit.com/r/dataengineering/comments/5det56/are_there_important_or_valuable_data_engineer/,3.0,4.0,,,As title states. Curious about certificates that have helped you in your career.
59,2016-11-22 01:12:37,1479769957.0,dataengineering,[Hiring] Experienced Data Engineers in NYC,5e79fm,nieuweyork,1340374968.0,https://www.reddit.com/r/dataengineering/comments/5e79fm/hiring_experienced_data_engineers_in_nyc/,1.0,0.0,,,"I'm looking to hire experienced data engineers to join me in collecting and fusing all the office space listings in the US. 

Whole listing below. Apply at https://thesquarefoot.workable.com/jobs/381355

----

About You

You like data engineering. You have opinions on data quality, schemas, data formats, data stores, and operating the infrastructure that supports data. You like delivering on projects. You work well with others, you enjoy working through solutions with your colleagues, and you know how to give and get help when needed to maintain your and your colleagues' velocity.



About Our Challenges

Our first project is getting comprehensive data on all the office buildings in our target markets, and updating our listings (correctly!) as soon as new information is available. We'll be building data curation tools, building new data pipelines, designing elasticsearch schemas, pumping data into elasticsearch, and synthesizing dirty data into coherent listing information.

About Us

TheSquareFoot is a next generation online platform to make office leasing easier. Our team combines deep industry expertise with innovative web and mobile-enabled technology to provide clients with a best-in-class commercial leasing experience. TheSquareFoot’s New York-based brokerage is a central hub where businesses can search for available space while getting hands-on support from our team and landlords and listing agents have a full suite of tools for managing properties online. Our team is committed to providing our customers with better data, convenience, and increased efficiency across the board.

Requirements

At least some experience of data engineering
Overall comfort with software development, basics of working with infrastructure; probably 3+ years of experience; highly experienced engineers more than welcome
Gets things done
Gets things done with their team
Enthusiasm for data
Please use the ""Cover Letter"" box to describe an idea that has recently been interesting to you
Benefits

TheSquareFoot believes in creating a workspace that challenges, empowers, and engages our teams. Market rate compensation. Joining us starts with competitive health coverage, 401(K), pre-tax commuter benefits, shiny Apple products when you start, and that’s just the tip of the iceberg!

"
60,2016-12-07 12:06:35,1481105195.0,dataengineering,"Benchmarking between Redshift, Bigquery, Spark, Presto etc.",5gz4m7,bocosan,1458646240.0,https://www.reddit.com/r/dataengineering/comments/5gz4m7/benchmarking_between_redshift_bigquery_spark/,1.0,0.0,,,
61,2016-12-12 08:45:42,1481525142.0,dataengineering,Paper on the SciLuigi Flow-based inspired API for simplifying complex Luigi workflows,5hv7m5,samuellampa,1368273786.0,https://www.reddit.com/r/dataengineering/comments/5hv7m5/paper_on_the_sciluigi_flowbased_inspired_api_for/,3.0,0.0,,,
62,2016-12-12 19:40:14,1481564414.0,dataengineering,Data pipelining and versioning with Pachyderm and Go,5hxy49,dwhitena,1453762419.0,https://www.reddit.com/r/dataengineering/comments/5hxy49/data_pipelining_and_versioning_with_pachyderm_and/,2.0,0.0,,,
63,2016-12-15 20:59:16,1481828356.0,dataengineering,Scaling Scala: Evaluating the state and development of Scala from a data engineering perspective,5ijcae,DataScienceInc,1481147653.0,https://www.reddit.com/r/dataengineering/comments/5ijcae/scaling_scala_evaluating_the_state_and/,2.0,0.0,,,
64,2016-12-16 04:39:06,1481855946.0,dataengineering,10 Unique Gift Ideas for Data Scientists and Engineers,5iluf7,estherhakka,1478565893.0,https://www.reddit.com/r/dataengineering/comments/5iluf7/10_unique_gift_ideas_for_data_scientists_and/,1.0,0.0,,,
65,2016-12-20 04:32:28,1482201148.0,dataengineering,Want to speak at the next DataEngConf data science and engineering event?,5jaq3a,estherhakka,1478565893.0,https://www.reddit.com/r/dataengineering/comments/5jaq3a/want_to_speak_at_the_next_dataengconf_data/,2.0,0.0,,,
66,2016-12-28 20:47:20,1482950840.0,dataengineering,What 3 open source data technology do you want to learn more about in 2017?,5kru2o,estherhakka,1478565893.0,https://www.reddit.com/r/dataengineering/comments/5kru2o/what_3_open_source_data_technology_do_you_want_to/,2.0,0.0,,,
67,2017-01-02 20:53:06,1483383186.0,dataengineering,Opinions on Organizing Niche Scientific File Formats in a Filesystem for Data Analysis,5lmuoj,neuromantik8086,1413900068.0,https://www.reddit.com/r/dataengineering/comments/5lmuoj/opinions_on_organizing_niche_scientific_file/,1.0,0.0,,,"Hello data engineers!

I'm research assistant at neuroimaging lab that is behind a large-scale data sharing project.  Recently, we've been organizing our data (brain images stored in a modified version of a medical imaging file format) on a filesystem according to a standard proposed within our field (the Brain Imaging Data Structure / BIDS; http://bids.neuroimaging.io/).  Essentially, the way this standard works is that file names are composed of multiple key-value pairs.  The key and value are separated by a dash, and distinct key-value pairs are delimited by an underscore.  So the file name: 'sub-01_ses-01_task-stroop_bold.nii.gz' would indicate that the file contained task-related data from the first session of subject 1 where the subject performed a stroop task in the MRI scanner ('.nii.gz' is the format, NifTI compressed w/ gzip and 'bold' describes the type of MRI sequence used for imaging).  This file would be nested within several directories- the hierarchy would be composed of some of the key-value pairs as well as a directory name that indicates whether the scan was task-related or just a single volume anatomical image (so the file in the above example might be stored at 'basedirectory/sub-01/ses-01/func/sub-01_ses-01_task-stroop_bold.nii.gz').  Additional metadata for the file would be stored within a JSON file with a filename that is identical to the file's except for the filename extension.

Since my background isn't in either comp sci or stats originally, I was wondering what people from these fields (or who've successfully migrated to data engineering from an unrelated field) might think of this manner of dealing with data.  I'm also wondering if there isn't a more generalized scheme for organizing datasets on filesystems / dealing with proprietary or niche formats.

One thing that I've come across is that many people are opposed to working with data entirely on a filesystem, preferring instead that it be in a database of some flavor (NoSQL, SQL, etc) due to increased performance.  Would this preference still hold for niche scientific data formats?  If so, what kind of database would be ideal for this kind of problem?  I was looking at Elasticsearch / other document-based databases, but I'm still a little new to this and was wondering how a more experienced data engineer might approach this problem."
68,2017-01-17 03:49:03,1484617743.0,dataengineering,"An interview about Pachyderm, the platform for versioned data and containerized analysis",5of4y9,blarghmatey,1429832609.0,https://www.reddit.com/r/dataengineering/comments/5of4y9/an_interview_about_pachyderm_the_platform_for/,3.0,0.0,,,
69,2017-01-25 23:33:07,1485379987.0,dataengineering,A Majority of Data Scientists are Likely to Work within Data Science Platforms in the Near Future,5q62cr,DataScienceInc,1481147653.0,https://www.reddit.com/r/dataengineering/comments/5q62cr/a_majority_of_data_scientists_are_likely_to_work/,6.0,0.0,,,
70,2017-01-30 09:50:18,1485762618.0,dataengineering,The Rise of the Data Engineer,5qzfeu,rororo-your-boat,1470778556.0,https://www.reddit.com/r/dataengineering/comments/5qzfeu/the_rise_of_the_data_engineer/,6.0,0.0,,,
71,2017-01-30 22:51:35,1485809495.0,dataengineering,Embarrassingly easy distributed joins!,5r3gnj,dwhitena,1453762419.0,https://www.reddit.com/r/dataengineering/comments/5r3gnj/embarrassingly_easy_distributed_joins/,3.0,0.0,,,
72,2017-01-31 11:53:08,1485856388.0,dataengineering,New open sourced luigi-based workflows,5r7a9y,benyuel,1485856255.0,https://www.reddit.com/r/dataengineering/comments/5r7a9y/new_open_sourced_luigibased_workflows/,3.0,0.0,,,https://github.com/groupon/luigi-warehouse
73,2017-01-31 23:15:16,1485897316.0,dataengineering,Not sure if I am in the right spot but hey worth a shot.,5rb1jm,Sfpkt,1402696420.0,https://www.reddit.com/r/dataengineering/comments/5rb1jm/not_sure_if_i_am_in_the_right_spot_but_hey_worth/,3.0,4.0,,,"A quick background. Made a career transition at 30 from Chemist to Software Engineer. As my first position at the start up I was working at I was employed as an Associate Data engineer. We weren't working with any big data as of yet. Because it was a small start up I wore many hats. I worked in node and postgres. I built apis, implemented unit tests, and automated build processes. 6 months into the position the entire team imploded and engineering was laid off.

Since then I have found a new position, but not as a data engineer. Ideally, I would like to be employed as a data engineer, but then again I would also like it to rain doughnuts...I have browsed around on linkedin to check on the minimum qualifications and it seems that I am FAR OFF from the minimum time required to be hired as a data engineer.

Seeing how I have only 6 months under my belt and mostly self taught I am confused if becoming a data engineer is something that is an attainable goal at this point in time.

I have checked out other paths such learning front end, make my own projects all the way through to be able to put on my git hub, so I can become employed as a Full Stack. For some reason I have this impression that if I get hired into a company it would be a foot in the door to be able to reach that goal of becoming a data engineer. All the learning data engineering and building my portfolio to complement that.

Tldr; I want to do data engineering professionally but can't due to lack of experience. No one will even give me an interview. I have to start projects that complement data engineering skills to put on my git. Thinking about trying to get a job as a full stack to get my foot in the door at a company that has data engineering role available so later down the line I can jockey myself for a position as a data engineer or just focus on data engineering projects and screw fullstack? I am pretty sure that I am over thinking this and should just learn it all"
74,2017-02-01 09:17:32,1485933452.0,dataengineering,Data Appending - Turning a deficient address list to great deal of ROI,5re57y,Info_Andy,1485931697.0,https://www.reddit.com/r/dataengineering/comments/5re57y/data_appending_turning_a_deficient_address_list/,1.0,0.0,,,
75,2017-02-13 21:27:08,1487014028.0,dataengineering,NoSql Data Modelling with Hypertable,5tuxba,kalkaseer,1487012581.0,https://www.reddit.com/r/dataengineering/comments/5tuxba/nosql_data_modelling_with_hypertable/,1.0,0.0,,,
76,2017-02-15 11:05:31,1487149531.0,dataengineering,Crossing the Streams – Joins in Apache Kafka,5u6dpp,krallistic,1398527267.0,https://www.reddit.com/r/dataengineering/comments/5u6dpp/crossing_the_streams_joins_in_apache_kafka/,1.0,0.0,,,
77,2017-02-17 02:02:40,1487289760.0,dataengineering,"Building machine learning systems, or, How to keep your data scientists and engineers from killing each other",5uix8m,kkwteh,1417548740.0,https://www.reddit.com/r/dataengineering/comments/5uix8m/building_machine_learning_systems_or_how_to_keep/,3.0,1.0,,,
78,2017-02-26 14:11:24,1488111084.0,dataengineering,Minipipe: a minimal end-to-end toy data pipeline,5w9uat,[deleted],,https://www.reddit.com/r/dataengineering/comments/5w9uat/minipipe_a_minimal_endtoend_toy_data_pipeline/,1.0,0.0,,,
79,2017-03-14 15:04:09,1489496649.0,dataengineering,F# for Data Engineering,5zbz8d,insulanian,1444068874.0,https://www.reddit.com/r/dataengineering/comments/5zbz8d/f_for_data_engineering/,7.0,9.0,,,"I'm shifting focus to data engineering lately, and I'm doing everything with SQL and F#. However, I do notice that F# is rarely mentioned in data engineering circles - it's mostly Python and Scala.

Will I make my life easier if I switch to Scala or Python as I go deeper into the topic, or I should stick with what I'm comfortable with?"
80,2017-03-14 23:48:45,1489528125.0,dataengineering,Ansible playbooks for Kafka and Zookeeper,5zf74x,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/5zf74x/ansible_playbooks_for_kafka_and_zookeeper/,2.0,0.0,,,
81,2017-03-21 22:49:57,1490129397.0,dataengineering,Preparing for the Transition to Data Engineering,60q88q,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/60q88q/preparing_for_the_transition_to_data_engineering/,7.0,0.0,,,
82,2017-03-22 19:02:27,1490202147.0,dataengineering,Data Cleansing Solution: Ensuring your database is up-to-date,60vvg5,pearljoyo,1443629871.0,https://www.reddit.com/r/dataengineering/comments/60vvg5/data_cleansing_solution_ensuring_your_database_is/,1.0,0.0,,,
83,2017-03-23 17:42:01,1490283721.0,dataengineering,The Road to Streaming Graph Analysis - an Insight Data Engineering story,612m8w,Insightmegan,1452886649.0,https://www.reddit.com/r/dataengineering/comments/612m8w/the_road_to_streaming_graph_analysis_an_insight/,1.0,0.0,,,
84,2017-03-28 23:12:29,1490731949.0,dataengineering,New release from Hadoop killer Pachyderm!,621vjb,dwhitena,1453762419.0,https://www.reddit.com/r/dataengineering/comments/621vjb/new_release_from_hadoop_killer_pachyderm/,3.0,0.0,,,
85,2017-04-23 22:14:42,1492974882.0,dataengineering,Stream REST API into kakfa,67434s,geoheil,1342725415.0,https://www.reddit.com/r/dataengineering/comments/67434s/stream_rest_api_into_kakfa/,2.0,0.0,,,"I want to build a streaming system which takes the bitcoin courses from different exchange platforms and compares courses in real time.
For the latter part I probably would want to use flink (maybe beam). But the first part of how to ingest the data is not really clear to me.

 - akka stream (maybe with http://developer.lightbend.com/docs/alpakka/current/) with quartz timer to connect to apis
 - flume to poll the rest apis of the exchanges
 - direct flink (stream processor) connection
 - Kafka connect and some java code
 - nifi with Kafka sink (latency?, GUI,  really easy to setup)

To connect to an index once using https://github.com/timmolter/XChange is demonstrated in the snippet below

    val currencyPair = new CurrencyPair(Currency.XMR, Currency.BTC)
      CertHelper.trustAllCerts()
      val poloniex = ExchangeFactory.INSTANCE.createExchange(classOf[PoloniexExchange].getName)
      val dataService = poloniex.getMarketDataService
    
      generic(dataService)
      raw(dataService.asInstanceOf[PoloniexMarketDataServiceRaw])
    System.out.println(dataService.getTicker(currencyPair))

I am really unsure how to create a steam from it which would allow low latency processing (and an overall simple architecture). So far I have seen http://stackoverflow.com/questions/39857922/apache-flume-vs-apache-flink-difference which suggests to not use flink to ingest/access the data.

Probably the overall architecture would look like `X -&gt; Kafka -&gt; stream processor`.

Which path to move forward would you suggest?"
86,2017-05-04 20:54:32,1493920472.0,dataengineering,How we developed our own ETL pipeline for processing large amounts of data on a single machine using Clojure,6994fa,sunshinewyin,1441300257.0,https://www.reddit.com/r/dataengineering/comments/6994fa/how_we_developed_our_own_etl_pipeline_for/,9.0,0.0,,,
87,2017-05-23 04:11:24,1495501884.0,dataengineering,Ergonomics in Data Engineering,6crhgl,jstuartmill,1393380481.0,https://www.reddit.com/r/dataengineering/comments/6crhgl/ergonomics_in_data_engineering/,2.0,0.0,,,
88,2017-05-23 22:55:55,1495569355.0,dataengineering,To learn Java 8 or Scala,6cx3ok,SecretAgentZeroNine,1334520146.0,https://www.reddit.com/r/dataengineering/comments/6cx3ok/to_learn_java_8_or_scala/,4.0,0.0,,,
89,2017-06-01 21:29:32,1496341772.0,dataengineering,Five Dysfunctions of a Data Engineering Team,6eopkc,eljefe6a,1314155429.0,https://www.reddit.com/r/dataengineering/comments/6eopkc/five_dysfunctions_of_a_data_engineering_team/,1.0,5.0,,,
90,2017-06-05 16:53:47,1496670827.0,dataengineering,The Promise and Expense of Distributed In-Memory Data Processing,6feahs,jstuartmill,1393380481.0,https://www.reddit.com/r/dataengineering/comments/6feahs/the_promise_and_expense_of_distributed_inmemory/,3.0,0.0,,,
91,2017-06-19 21:10:39,1497895839.0,dataengineering,Rebuilding Yelp's Data Pipeline (Interview),6i87gx,blarghmatey,1429832609.0,https://www.reddit.com/r/dataengineering/comments/6i87gx/rebuilding_yelps_data_pipeline_interview/,5.0,0.0,,,
92,2017-06-27 13:48:36,1498560516.0,dataengineering,The data engineering ecosystem in 2017 – Insight Data,6jrvfr,luba_belokon,1487164661.0,https://www.reddit.com/r/dataengineering/comments/6jrvfr/the_data_engineering_ecosystem_in_2017_insight/,2.0,0.0,,,
93,2017-06-29 22:50:35,1498765835.0,dataengineering,New Quality of Database Documentation – Dataedo,6kaux7,piotr79,1484749888.0,https://www.reddit.com/r/dataengineering/comments/6kaux7/new_quality_of_database_documentation_dataedo/,4.0,0.0,,,
94,2017-07-10 16:00:54,1499691654.0,dataengineering,Data Entry Service Provider Company in New Delhi - vaishnaviassociates.in/,6meacd,dvtplweb,1454653957.0,https://www.reddit.com/r/dataengineering/comments/6meacd/data_entry_service_provider_company_in_new_delhi/,1.0,0.0,,,
95,2017-07-26 14:04:56,1501067096.0,dataengineering,http://www.liveinternet.ru/users/outsource_dataworks/,6pnav2,OutsourceDatawork,,https://www.reddit.com/r/dataengineering/comments/6pnav2/httpwwwliveinternetruusersoutsource_dataworks/,1.0,0.0,,,
96,2017-07-28 08:17:51,1501219071.0,dataengineering,Using the web as a tool for market research,6q1pzx,OutsourceDatawork,,https://www.reddit.com/r/dataengineering/comments/6q1pzx/using_the_web_as_a_tool_for_market_research/,1.0,0.0,,,
97,2017-07-31 04:57:16,1501466236.0,dataengineering,Virtual Assistant Services India,6qlfvl,librawebsolutions,1498482387.0,https://www.reddit.com/r/dataengineering/comments/6qlfvl/virtual_assistant_services_india/,1.0,0.0,,,
98,2017-08-04 16:09:23,1501852163.0,dataengineering,Validating loglines against a predetermined schema.,6rkctf,rajenur,,https://www.reddit.com/r/dataengineering/comments/6rkctf/validating_loglines_against_a_predetermined_schema/,1.0,0.0,,,"Hey folks,


I was wondering if I could get some direction/feedback/advice. Has anyone used or heard of a catalog or repository with a GUI where an analyst(or anyone else) can define some analytic loglines for an application? Oftentimes when we are building a feature, especially across platforms, product engineers can write loglines which can mess up our data-pipelines. For example, if in a log category common convention is to call a field ""errorMessage"" and someone writes ""errormessage"" or ""erormessage"" our whole data-pipeline can be messed up. 

My idea was if we have a repository where we define the loglines, we could either write a small script to generate the code to write the loglines... and/or have a tool for the engineers to validate/test the analytic loglines they write against the pre-defined schema. 

All of our loglines are JSON structured. 

Has anyone run into a similar issue? How did you deal with it?

Thank you!"
99,2017-08-07 11:51:21,1502095881.0,dataengineering,Concentrate your own business after outsource the additional data entry works to us,6s4cap,OutsourceDatawork,,https://www.reddit.com/r/dataengineering/comments/6s4cap/concentrate_your_own_business_after_outsource_the/,1.0,0.0,,,
100,2017-08-14 23:34:59,1502742899.0,dataengineering,Make your app dev life easy with a single database for relational and JSON based unstructured data -- plus a free EMEA workshop to learn it all!,6tp97i,cmrussell99,,https://www.reddit.com/r/dataengineering/comments/6tp97i/make_your_app_dev_life_easy_with_a_single/,1.0,0.0,,,
101,2017-08-15 09:49:31,1502779771.0,dataengineering,data entry services,6tsjku,teqtindia,,https://www.reddit.com/r/dataengineering/comments/6tsjku/data_entry_services/,1.0,1.0,,,
102,2017-08-15 22:24:00,1502825040.0,dataengineering,From 1 to N: Distributed Data Processing with Airflow,6twhk4,kathleenyanolatos,,https://www.reddit.com/r/dataengineering/comments/6twhk4/from_1_to_n_distributed_data_processing_with/,6.0,0.0,,,
103,2017-08-21 08:02:07,1503291727.0,dataengineering,How to get started? Only 7 months of learning code.,6v0tek,abovexbeyondtv,,https://www.reddit.com/r/dataengineering/comments/6v0tek/how_to_get_started_only_7_months_of_learning_code/,3.0,3.0,,,"Hi guys, I'm really interested in becoming a data engineer. There aren't many resources to help with my learning. Or content in general. Most articles are about data scientists.

Could somebody please point me in a particular direction? Right now I am currently doing MIT's intro to comp sci and python on edx. Will learn SQL after that."
104,2017-09-05 00:18:57,1504559937.0,dataengineering,What's the best way to get into data engineering as a back end developer ?,6y39jx,tritech05,,https://www.reddit.com/r/dataengineering/comments/6y39jx/whats_the_best_way_to_get_into_data_engineering/,1.0,1.0,0.0,,"Hi,

I'm really interested in  getting into data engineering but am unsure as to the best way to do this.

I work as a back end developer in ruby working with graph databases (this is what really sparked my interest in focusing more on data engineering). I've signed up to the Big Data Coursera Specialization and am improving my Python and have been applying for data engineering roles but have been told by one recruiter that I lack experience.

What other things would you suggest I do to demonstrate expertise to potential employers ?"
105,2017-09-05 05:01:31,1504576891.0,dataengineering,"Didn't know this sub existed, now that i do, can you take a survey?",6y4wa4,Altern8_Reality,,https://www.reddit.com/r/dataengineering/comments/6y4wa4/didnt_know_this_sub_existed_now_that_i_do_can_you/,3.0,3.0,0.0,,"So my company is going through a transformation right now and i have been asked to help with coming up with a solution on how to train everyone(shitty i know) but this is my first step, let me know what you guys think, take it if you want

https://goo.gl/forms/ohPgeuB4AxHU46Cx1"
106,2017-09-10 20:36:31,1505064991.0,dataengineering,Jathena: An Open Source Amazon Athena,6z9m06,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/6z9m06/jathena_an_open_source_amazon_athena/,0.0,0.0,0.0,,
107,2017-09-14 00:13:26,1505337206.0,dataengineering,data cleansing filters that you can use for free(upto 500MB). Would love to find what you think,6zxm91,araina72374,,https://www.reddit.com/r/dataengineering/comments/6zxm91/data_cleansing_filters_that_you_can_use_for/,0.0,0.0,0.0,,
108,2017-09-23 11:47:03,1506156423.0,dataengineering,Is Alteryx used in Data Engineering ?,71x7qj,tritech05,,https://www.reddit.com/r/dataengineering/comments/71x7qj/is_alteryx_used_in_data_engineering/,2.0,3.0,0.0,,"Hi Reddit,
I'm looking at data engineering roles and am interested to find if Alteryx is used in this field? I ask because, judging from some casual research, it looks like something that is used by Data Analysts so looking for feedback from Data Engineers or people who use it currently and for what task.
Thanks"
109,2017-09-26 20:04:56,1506445496.0,dataengineering,"How are you all handilng quick, semi-custom, on-demand reporting? (cross-post)",72lrvu,dasjestyr,,https://www.reddit.com/r/dataengineering/comments/72lrvu/how_are_you_all_handilng_quick_semicustom/,3.0,3.0,0.0,,"Apologize for the cross-post, I just realized it would probably be better asked in this sub. I am application architect being forced into filling the solution architect role...

So I have a number of ideas of my own, but I want to know what you all are already going that works. In terms of data pipeline, what are some of you doing to be able to support live reporting? For example, basic data warehousing (lol ""basic"") solves most of the problems of dealing with reports. You transform operational data into informational data and either precompile these reports on a schedule, or at least store it in a denormalized informational format that will make the query as straight forward and performant as possible. But a new problem arises if that data is needed on demand, like when you have live reporting widgets on a dashboard application and the business expects that the data will somewhat live (vs. batch processing it on a nightly schedule).
In other words, if a full pipline looks like this...

Collect Data -&gt; predict/store/explore -&gt; Visualize

... then we're talking about the middle area.

That's some explanation, but lemme give you the actual use case:
We have a dashboard with probably at least 10 widgets that display a lot of statistical reporting. All of the widgets on the page can be filtered by things like date range, tags, enterprise hierarchy, etc. The requirements are:
All widgets update when the filters are updated

The widgets should update quickly (no more than a second or two)
Now to make the first part happen, we would probably take one of a couple different approaches. For statistic-type reports where the parameters are known (e.g. 30/60/90 day views), we'd run them on a schedule, but for statistic-type reports where the filter parameters are unknown, they basically have to be calculated on demand. After the first run, we can cache the result (to deal with page reloads, and reforming of the same data, etc.), but when you have thousands of users, that's thousands of first runs, and that can put a lot of load on the data warehouse. Right now (brace yourselves), we don't even have a data warehouse. We're performing reports directly out of our operational/oltp databases and it's really starting to hurt as our data is growing very quickly (we work in social media but also integrate with ingesting transactions from CRMs). We've done a lot of work to try and find new indexes, optimize queries, etc., but it's really starting to slug. So I want to start with the warehouse, of course.
For the second requirement... I think this is where it gets more tricky because some of these data sets used to calculate and already compute-heavy operation (aggregation), are millions of records deep. I've known a number of products out there that can do this, but most of them are built by businesses that focus on this technology -- big data, as it were.

I know the data warehouse is made for dealing with reports, primarily, but I feel like driving dashboard widgets might be pushing it a bit far? I dunno... something doesn't quite feel right about it and BigQuery charges by query, or rather by the size of the data being returned. I feel like driving widgets would push that number up fairly quickly... but perhaps not, if we're at least doing some high level caching at the endpoints.

We are in a stage of development where being data intensive should be a core competency, but isn't. We do not even employ data engineers of any type, but we have lots of bright developers that can at least get the ball rolling while we continue to make the business case to hire those data engineers. At the moment, I am looking into at least getting the early workings of a data pipeline put together by getting hooked up with an ETL service such as Alooma to shuttle our normalized data from about a dozen different databases into Google BigQuery in various denormalized formats. I'm also looking at introducing a data lake to deal with (as a first step) all of the raw data that we ingest from about a dozen different services.
As far as what we're working with in terms of the platform architecture, we're in the middle of a rebuild. The old platform is a monolith being run on SQL Server. The new platform is somewhat of a microservice architecture (early stage coarse-grained SOA that will evolve into fine grained services). CQRS model, and event sourced. Eventually consistent. MongoDB is the main database for denormalized data from the event store. We also have ElasticSearch and Neo4j running as specialized databases to deal with search and well... graph stuff -- but obviously the source of truth is the event store. Here is the basic architecture (the lake, bi tools, report services, BigQuery and Alooma are all just speculative right now) --&gt; https://i.imgur.com/gR8pdjI.png

So what I'm asking for is some discussion on the topic, or even just some plain old advice. What are you doing to drive reports on demand? I realize there are some pretty damn complex data strategies out there, and I want to be exposed to more of them. What does your pipeline look like? What is the purpose of each layer? Aside from examples, I want to talk more about the concepts than the products you use to accomplish them unless some product offers some significant feature to solve a real problem. In other words, I'd rather it not turn into a product debate of Redshift vs. BigQuery vs. Snowflake etc., unless it actually moves the conversation forward.

Also, what I have placed on the table so far:

- Start with the warehouse. Figure out ETL to get stuff out of the operational databases and into BigQuery.
- Precompile aggregations where the parameters are pre-determined
- Convince the business unit that we're simply incapable of performant on-demand reports like this right now, and then wait to hire experienced data engineers to carve this out."
110,2017-09-26 22:17:01,1506453421.0,dataengineering,How do you export data from a MS SQL db on a Windows machine to a POSTGRESQL db on a Mac ?,72mpa8,tritech05,,https://www.reddit.com/r/dataengineering/comments/72mpa8/how_do_you_export_data_from_a_ms_sql_db_on_a/,1.0,1.0,0.0,,"Hi Redditers,

Has any one had any experience of exporting data from a MS SQL db on a Windows machine to a POSTGRESQL db on a Mac ? If so can someone point me to a process that would work ?

Thanks

"
111,2017-09-28 18:42:55,1506613375.0,dataengineering,[Hiring] Senior Data Engineer role,730x2e,3rdkulturekyd,,https://www.reddit.com/r/dataengineering/comments/730x2e/hiring_senior_data_engineer_role/,0.0,2.0,0.0,,"Any Scala Engineers looking to move over to a more Data focused role? This very cool company in Dublin have just that role...see below:

https://functional.works-hub.com/job-board/Senior-Data-Engineer-Dublin-Ireland-Sep-2017-5577e?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=walker&amp;utm_content=job"
112,2017-09-30 22:33:01,1506799981.0,dataengineering,Getting started with Spark on AWS,73gynv,mwakanosya,,https://www.reddit.com/r/dataengineering/comments/73gynv/getting_started_with_spark_on_aws/,2.0,0.0,0.0,,
113,2017-10-05 16:34:34,1507210474.0,dataengineering,https://www.udemy.com/hadoop-querying-tool-hive-to-advance-hivereal-time-usage/?couponCode=INSTUDEM3,74fz6z,jiveshgarg1,,https://www.reddit.com/r/dataengineering/comments/74fz6z/httpswwwudemycomhadoopqueryingtoolhivetoadvancehiv/,0.0,0.0,0.0,,
114,2017-10-05 18:42:16,1507218136.0,dataengineering,How to empower data analytics / ops at my organization?,74gs7w,ryntck,,https://www.reddit.com/r/dataengineering/comments/74gs7w/how_to_empower_data_analytics_ops_at_my/,2.0,0.0,0.0,,"I'm part of a 4-person data engineering team at my company. We support about 100 analysts / ops people across multiple departments. Over the past year I've been focusing on making data engineering less of a bottleneck for data work requests by leading technical trainings and making as many resources and documentation available for people to solve their own problems.

Some fun stuff we've rolled out:

- Technical training for SQL, python, and R
- Internally-available servers for jupyter notebooks, Rstudio, and SQL clients and databases
- BigQuery (big hit with analysts)
- Developer access within our Looker (BI) instance
- Python and R libraries for data transformations / accessing common internal data sources
- Overpowered windows instances for running excessively large excel workbooks

Some ideas for the near future:

- chatbot interfaces for running data pipelines
- internal zapier-like system for basic workflows

I'm really interested in hearing how others solve the problem of analytics empowerment, or whether you've seen great examples of simple tooling / methods that have really transformed analytics at an organization. The dark side of democratized data (definition drift, etc) is also really interesting to me!

Note: I've posted this in r/businessintelligence also, but wanted to post this same discussion here to focus more on technical implementation and support of these systems."
115,2017-10-08 17:28:18,1507472898.0,dataengineering,Would you recommend this book?,751yqp,bluesufi,,https://www.reddit.com/r/dataengineering/comments/751yqp/would_you_recommend_this_book/,3.0,5.0,0.0,,
116,2017-10-09 08:46:03,1507527963.0,dataengineering,Building data infrastructure in Coursera,75743t,zhangzhaojun,,https://www.reddit.com/r/dataengineering/comments/75743t/building_data_infrastructure_in_coursera/,2.0,0.0,0.0,,
117,2017-10-12 18:03:44,1507820624.0,dataengineering,How to organize Data Engineering teams,75xk8r,ssola,,https://www.reddit.com/r/dataengineering/comments/75xk8r/how_to_organize_data_engineering_teams/,5.0,2.0,0.0,,"I am currently working in a company where the data engineering team is growing pretty fast. I would like to know experiences and possible team structures among Data Engineers + Data Scientists + Data Analysts.

I would like to ask for your experience, and if you know any good book/article related to this topic.

I can find lot of articles talking about team structures, but not specifically related to Data Engineering."
118,2017-10-12 19:31:26,1507825886.0,dataengineering,Interest in ETL course/bootcamp?,75y5nz,bookshelf11,,https://www.reddit.com/r/dataengineering/comments/75y5nz/interest_in_etl_coursebootcamp/,1.0,1.0,0.0,,"Hi all, I've found myself in a position recently at work learning/doing a lot of ETL work. There seems to be a huge demand for it here in NY. Everyone wants to be a data scientist, but there's tons of money in ETL/running environments for the data scientists.
I was wondering, do you guys think I would be able to get any traction with an ETL course focusing on Hive, Python, SQL, and PySpark?
FYI this is cross posted from /r/datascience."
119,2017-10-12 22:05:04,1507835104.0,dataengineering,Best skills for data engineer working in that capacity,75z76m,Zedmor,,https://www.reddit.com/r/dataengineering/comments/75z76m/best_skills_for_data_engineer_working_in_that/,3.0,1.0,0.0,,"Posted to /r/cscareerquestions - did not got traction there. Maybe here I can get more help? :) Thank you.

Hi! I am interested in meta question in regard to what skills are most valuable for a data engineer to learn.

 I work in that capacity in a startup in Philadelphia, in the nutshell, my job is writing microservices for samza framework (java/hadoop/messages running through kafka), various ETL scripts, QA scripts + deploy/monitoring. We have dedicated ops so keeping servers running is not my job - I am 90% dev side.

I am pretty good with python, ops side (Linux/not to point ot deploy my own cluster thought - should I learn how to do that? I can shadow our ops), I know  SQL/reddis/druid.io.
I do understand our stack pretty good. My Java is not that great but I am learning. 

Therefore the question is: which skills I should learn to progress better/become more valuable as an engineer? I enjoy working with data/databases.

My goal is to become an expert in this field.

 What should I do on my free time? What activity would get me the best bang for a buck?

I see several opportunities:

1. Get as good as possible in java (design patterns/general aptitude in java)

2. Other JVM language popular for streaming/data processing (scala/kotlin)

3. Non JVM language (Go?)

4. Contribute to Apache projects? Or some other opensource?

If you are data engineer or had similar experience could you share? Especially which OS project is worth contributing to."
120,2017-10-13 12:43:59,1507887839.0,dataengineering,2Captcha তে ১০০০ ক্যাপচা এন্টির টাইপ করার জন্য ১ ডলার দেওয়া হবে,763qdb,mdjobayer68,,https://www.reddit.com/r/dataengineering/comments/763qdb/2captcha_ত_১০০০_কযপচ_এনটর_টইপ_করর_জনয_১_ডলর_দওয়_হব/,1.0,0.0,0.0,,
121,2017-10-13 13:41:10,1507891270.0,dataengineering,MS needed for Data Engineering?,763yoh,AdrenalineSpark,,https://www.reddit.com/r/dataengineering/comments/763yoh/ms_needed_for_data_engineering/,5.0,5.0,0.0,,"Hey guys,

I know data science positions typically require Master's degrees [(88% of data scientists have at least an MS degree).](https://datasciencedegree.wisconsin.edu/data-science/what-do-data-scientists-do/) However, I was wondering if data engineering positions have a similar breakdown in terms of academic credentials, or is a Bachelor's degree good enough to pursue a career in data engineering (just like how a Bachelor's is just fine for traditional software engineering positions)?"
122,2017-10-15 11:16:42,1508055402.0,dataengineering,Issue: Unstructured text processing,76htuu,vij__s,,https://www.reddit.com/r/dataengineering/comments/76htuu/issue_unstructured_text_processing/,0.0,2.0,0.0,,"We receive a lot of text file (Let's say they are all song lyrics in English ) You need to build a data structure and pipeline to pre-process and store these data. Then provide the set of these tasks.

API: Search (read)

User send a list of N words and a percentage number p% in the request, Our API should return the list of file names. In which each of the file contains p% of the N words.

API 1 example :

User request is [""banana"" , ""apple"",""orange"" ,""watermalon""] and 60% Our API should find the 3 of those 4 words appeared in the file.

API2 : Blacklist update [write]

User sends a list of N words and percentage P% in the request, Our api should update whatever dat store you have such that

**IF a file contains &lt;p% of the N words, marks these word with *** sign in place.**

**IF a file contains &gt;=p% of the N words, FLAG the file to hide it from API.**
API 2 example :

User request is [""kill"", ""hate"", ""blood"" ] and 80%. And you have two files with contents like these: file1: ""I kill banana hate apple and kill orange"" file2: ""I hate banana apple orange and I kill for blood"" Your API 2 should update accordingly, such that the API 1 request example above should receive only file1 in response, and file 1 content will be ""I * banana * apple and *** orange"" In this case, even if file2 match API 1 requirement, it been flagged and hid by API 2, thus not retrieved from API 1.

**Production Scenario**

There are 3 scenarios we need to handle in our data-pipeline and API designs

1, we will continuously receive more files (hundreds of millions of files)

2, the list of N words in both API requests could be very big (e.g. N &gt; 10,000)

3, in terms of frequency, API 1 will be called all the time (let say 100 qps), API 2 will be called once a while (let say once a day)

**Questions**
Q1:

* Illustrate our data-pipeline, data structure, and API with pseudocode, explain your design and how it solves the problem under the requirement.
* If drawing diagrams help to explain, feel free to put them in

Q2: design thinking

* Explain the time complexity of your pre-processing pipeline.
* Explain the time complexity of your API in each of the task.
* Do you think your solutions are the optimal solutions for performance? If so, explain why. If not, explain where do you think needs major improvement.
Q3: architecture

* When is the product going to production, which parts need to consider scalability problems?
* Which system/platform/framework are the most suitable to solve the scalability problems? (you can pick anything from Apache, cloud services like AWS/Azure/GCP, etc.)
Q4: software engineering best practice

* Give 2 test cases to test the performance (edge/boundary cases are better).
* Give 2 test cases to check the data integrity in the pipeline.

**Extra**
What I hope to see from your response

-Pseudocode that gives a clear explanation of your design. Correct use of big O notation to explain the time complexity of each part of the design.

-Put time performance as the highest priority. Y - Please Don’t give me full code, pseudocode has a purpose. please Answer in simple sentences.

Any help on it would be much appreciated?"
123,2017-10-17 00:40:17,1508190017.0,dataengineering,Transition from traditional ETL/Data Warehouse to Data Engineering?,76turq,ClemsonLaxer,,https://www.reddit.com/r/dataengineering/comments/76turq/transition_from_traditional_etldata_warehouse_to/,3.0,2.0,0.0,,"Have any of you made the leap from the more traditional ETL/Data Warehous roles into data engineering?

I've worked in the traditional batch-oriented area for about seven years now (Using IBM Datastage with Oracle/Teradata, shell scripts, Autosys) building out data marts for reporting purposes, data quality/data governance, and dabbled some in data visualization with Tableau. I'm trying to figure out how to break into the data engineering roles that seem more interesting. Any tips would be greatly appreciated!

Thanks."
124,2017-10-23 22:53:10,1508788390.0,dataengineering,[PyData Warshaw 2017] From a model to production like a Pro: Software-engineering best-practices,78acru,mkrcah,,https://www.reddit.com/r/dataengineering/comments/78acru/pydata_warshaw_2017_from_a_model_to_production/,1.0,1.0,0.0,,
125,2017-10-26 02:18:05,1508973485.0,dataengineering,Anyone use Airflow... on windows... Or at least with windows executors and destinations?,78rl7z,pretenddev,,https://www.reddit.com/r/dataengineering/comments/78rl7z/anyone_use_airflow_on_windows_or_at_least_with/,2.0,0.0,0.0,,any advice here would be great
126,2017-10-26 19:02:03,1509033723.0,dataengineering,Data Lake: Dump your whole historical raw data for analytics purpose but don't use it for collaboration between operational systems [X-Post r/programming],78wiyt,bluesufi,,https://www.reddit.com/r/dataengineering/comments/78wiyt/data_lake_dump_your_whole_historical_raw_data_for/,2.0,0.0,0.0,,
127,2017-10-29 05:01:23,1509246083.0,dataengineering,Feature engineering,79eboc,saurabh502,,https://www.reddit.com/r/dataengineering/comments/79eboc/feature_engineering/,0.0,2.0,0.0,,"Hi All,

I am trying to find good resource for learning feature engineering. Your help will be highly appreciated. Thanks in advance!!"
128,2017-11-06 13:59:17,1509969557.0,dataengineering,You might want to hire a Data Engineer instead of a Data Scientist,7b4ol8,mkrcah,,https://www.reddit.com/r/dataengineering/comments/7b4ol8/you_might_want_to_hire_a_data_engineer_instead_of/,1.0,0.0,0.0,,
129,2017-11-06 17:23:58,1509981838.0,dataengineering,"Difficult reporting situation at work, not certain how to proceed.",7b5spx,mobastar,,https://www.reddit.com/r/dataengineering/comments/7b5spx/difficult_reporting_situation_at_work_not_certain/,2.0,2.0,0.0,,"We have very old record keeping systems at work, and serve hundreds of clients.  Over time we've gotten to the point of providing loads of custom reporting (both internally and externally) that are a Frankenstein'd together mash of outputs from various systems.  There is no central source, not even close.  I've been tasked with helping to rectify the daunting situation, and I'm having trouble with my approach other than re-creating a database using large outputs from the other systems, and from here create large, meaningful outputs that give all requesting parties everything they could possibly want.  This seems incorrect though as 1) it's essentially recreating a yearly/quarterly review we complete - i.e. way too much data and 2) I'm not really satisfying the requests as I'm trying to show everyone how wild and unique the requests are.


I'm really struggling to see the solution here.  Any thoughts on a proven process to resolve the situation and move forward?  Heck, I'm even starting to question my overall role and purpose as a reporting analyst as currently my day to day is feeding requests that feel like scavenger hunts while stressing over a potential solution to make reporting easier, faster, and more effective for everyone.


*EDIT* To clarify I'm not involved with the DB team whatsoever, nor technology.  I'm a business-side analyst that has a knack for reporting, programming, etc. and often live in the area where work needs to be done but there isn't enough time to wait for the ticket/request to work its way through technology.

*EDIT 2* The yearly/quarterly report item isn't totally off the table.  If I could come up with a way to refresh a dozen or so visuals via an Excel export that could help.  We're talking 20+ tabs of data that would dynamically change in size depending on the client, but would fulfill the same visuals for each.  This is likely an entirely different conversation...just something else on my mind."
130,2017-11-08 17:00:28,1510153228.0,dataengineering,What problem should I solve to establish myself as a Data Engineer?,7bltu3,simple-helper,,https://www.reddit.com/r/dataengineering/comments/7bltu3/what_problem_should_i_solve_to_establish_myself/,5.0,4.0,0.0,,"I have built web backends extensively. I want to move towards data engineering which (in my opinion) means solving problems that involve high data volumes and building scalable data pipelines.
Can you suggest an end to end problem that I can solve as a side project to claim that title?

[for context, some stuff I built in recent past: https://medium.com/@hrshasan]"
131,2017-11-11 23:13:02,1510434782.0,dataengineering,"Implementing Lambda Architecture using Kafka, Spark Streaming, Redshift, S3 - Hands-on",7cb1w4,svejed123,,https://www.reddit.com/r/dataengineering/comments/7cb1w4/implementing_lambda_architecture_using_kafka/,8.0,1.0,0.0,,
132,2017-11-15 19:02:26,1510765346.0,dataengineering,Insight Data Engineering Fellows Program Expands to Boston,7d5gdf,mwakanosya,,https://www.reddit.com/r/dataengineering/comments/7d5gdf/insight_data_engineering_fellows_program_expands/,2.0,0.0,0.0,,
133,2017-11-20 07:26:00,1511155560.0,dataengineering,"Becoming a proficient data engineer, coming from non-traditional background (non CS/EE/software)?",7e6ld3,oarabbus,,https://www.reddit.com/r/dataengineering/comments/7e6ld3/becoming_a_proficient_data_engineer_coming_from/,3.0,4.0,0.0,,"My undergraduate degree was in biomedical engineering, with little coding outside of MATlab scripts. During my Master's, I worked in a Robotics lab, performing research on robots for neuroscience, and getting at least rudimentary experience with using linux and the shell/terminal.

Professionally, in the medical device industry, I'd used Python, SQL, Excel and visualization softwares for analytics/reporting/dashboarding. It just so happened that recently, I switched out of med dev and into Tech, and leaving behind the 5 years of domain knowledge I'd built up for another field in which I'm starting out knowing less than 99% of people in the industry has been both challenging and rewarding. 

There's now an opportunity in my current role to work to be interacting with data engineers, or alternatively, providing data engineering professional services to clients who do not have any currently in their company.

I'm looking for suggestions for what I should learn in order to become competent in the field. Keep in mind that while I can tell you how the heart works and how to develop a microfluidic incubator to grow heart tissue in-vivo, or how to develop fMRI-compatible robotics, I have almost zero domain knowledge in the data engineering space, and I have never been a software engineer (although I'm currently learning programming as well). Any list of required readings or subjects would be greatly appreciated. I'm aware that expertise won't come overnight (or even after a year) but I want to begin building up my knowledge now. "
134,2017-11-22 18:05:28,1511366728.0,dataengineering,How To Choose A Data Interchange Format (Interview),7erzkq,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7erzkq/how_to_choose_a_data_interchange_format_interview/,5.0,0.0,0.0,,
135,2017-11-27 20:55:27,1511808927.0,dataengineering,Where are the best places to find out about new data eng tools?,7fxgek,justinontheshore,,https://www.reddit.com/r/dataengineering/comments/7fxgek/where_are_the_best_places_to_find_out_about_new/,4.0,3.0,0.0,,Curious to know how people hear about interesting new data eng tools. Hacker News? Conferences? Colleagues? This subreddit...? I'm trying to be more productive and make sure I'm always using the right tool for the job.
136,2017-11-29 09:54:02,1511942042.0,dataengineering,Setting up an end to end data flow architecture,7gbmen,iceman_dfw,,https://www.reddit.com/r/dataengineering/comments/7gbmen/setting_up_an_end_to_end_data_flow_architecture/,1.0,1.0,0.0,,We need to set up and end to end data pipeline on an opensource stack. Please point me to the best resources on the internet for the same
137,2017-12-01 00:55:30,1512082530.0,dataengineering,A newbie's guide to Scala and why it's used for distributed computing,7gqf5x,fayezor,,https://www.reddit.com/r/dataengineering/comments/7gqf5x/a_newbies_guide_to_scala_and_why_its_used_for/,4.0,0.0,0.0,,
138,2017-12-04 03:52:07,1512352327.0,dataengineering,Complex Event Processing realtime Twitter data with Apache Flink,7he4xx,svpadd2,,https://www.reddit.com/r/dataengineering/comments/7he4xx/complex_event_processing_realtime_twitter_data/,2.0,0.0,0.0,,
139,2017-12-04 19:14:01,1512407641.0,dataengineering,Data.World: The Platform For The Web Of Linked Data (Interview),7hip6f,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7hip6f/dataworld_the_platform_for_the_web_of_linked_data/,2.0,0.0,0.0,,
140,2017-12-05 10:04:49,1512461089.0,dataengineering,Switching from public health data analyst to data engineer? Possible for me?,7hoamk,squirrelinstinct,,https://www.reddit.com/r/dataengineering/comments/7hoamk/switching_from_public_health_data_analyst_to_data/,1.0,4.0,0.0,,"Hi friends, I have been working for 5 years now with rdbms, data analysis and general data management. I am getting bored with what's possible in my field as there are too many restrictions to, well everything. 

I want to switch career and work not only in the corporate field but also in tech, I want to focus on database management, quality assurance, database architecture as well as some data analysis (minus more complex analysis than something like a chi square and OR's). 

The methods I use now are very different from the field I try to get into. I use SAS almost exclusively for everything but from what I see I need to switch to Python or R?
(I worked with both very limited and definitely prefer Python)
I have experience in SQL scripting but never worked with an sql database like MySQL, Sql Server or Oracle. Though I have worked with Postgres. 
(I also picked up some javascript, ruby, node.js and other generals of web development because, well, why not)

I obviously heard of hadoop and spark and kafka but that's the content of my knowledge already and I was hoping to learn certain specifics on the job. I learn fast but I only learn by doing and all these online tutorials seem to teach the wrong thing. (They always focus on the syntax while that is the easiest part, the hard part is the context of interconnectedness and it never teaches that.)

Can you give me feedback on how much I am still missing and what I should focus on? I have no frame of reference here. "
141,2017-12-12 14:57:25,1513083445.0,dataengineering,How Centralized Schemas Help Tame Distributed Streaming Analytics (Interview),7jajre,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7jajre/how_centralized_schemas_help_tame_distributed/,1.0,0.0,0.0,,
142,2017-12-17 15:30:18,1513517418.0,dataengineering,Apache Kafka - And the Other Way is Wrong,7kdrxy,bluesufi,,https://www.reddit.com/r/dataengineering/comments/7kdrxy/apache_kafka_and_the_other_way_is_wrong/,0.0,0.0,0.0,,
143,2017-12-24 02:32:05,1514075525.0,dataengineering,Advice for Experienced Traditional Data warehouse / ETL Engineer looking to transition to modern Data Work,7ls4zd,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/7ls4zd/advice_for_experienced_traditional_data_warehouse/,1.0,2.0,0.0,,"Hi Guys,

I have 6+ years of experience data working in traditional data warehouses as an ETL developer, Data Analyst and a Data Architect. I've worked on 3nF data warehouses and built and enhanced my Kimball style dimensional data marts. Past couple years I have been doing more solution design and data modeling than ETL development.

Here is the current technology I am experienced with:
*Relational Databases : Can write complex sql as well as plsql scripts, store procedures, database triggers etc. Most of my work has been with oracle databases but I have also worked with other vendors throughout my career.

*Informatica: This has been the ETL tool I am the most familiar with. I no longer do any hands on ETL development but I work closely with developers as I give them the specifications on what to build. I also get in there and debug from time to time.

*Business Objects: I have never been a report developer and have worked mainly on the database side. However, BO has been the reporting tool that all the teams I have worked on have used so I am familiar with it.

*Erwin: This is the data modeling tool I use.

Now that you know where I am coming from maybe you can give me some advice on how best to pick up ""modern"" data engineering skills. I am aware of what is in demand in the market and have been proactive do self study, creating a list of technology to pickup. This is what I have picked up so far as well as plan to learn in the near future but because I don't use these on a daily basis I consider my level to be be that of a beginner:

*Python: I know the syntax and the fundamentals of the language. I've also spent time with some popular libraries like Pandas and Numpy. What should I really focus on specific to data engineering and maybe data science? Ay resources you can point me to?

*AWS: I've signed up for the free tier and have gone through the majore services (s3, dynamodb, ec2 etc.) What services should I really focus on here and can you point me to some good resources? I already use acloudguru.

*NoSQL: I've done a course and played around with DynamoDB so I understand key/value and documented oriented databases. Should I spend time learning MongoDB or do the concepts transfer over easily? Any other tips you can share? I also plan to play with a column oriented database.

*Spark: I have no experience with spark. This is next on my list. I plan to do some courses that leverage Databricks. Any tips or resources I can use?

*Redshift: I haven't yet played with it but intend to. Any tips? I assume most of learning this is conceptual coming from row oriented databases and I should be able to pick this up fairly fast. Any thoughts?

*Tableau: I've messed around with the community edition. This is really low on my list.

So what do you think of my approach to pick some of these skills up. I'm using whatever resources I find online (udemy, pluralsight etc) to learn this tech and practicing on my local machine or in the cloud when possible.

In my experience I think self learning can only go so far and there is no substitute for working on actual deliverables with a team. My aim is to get to a point where I won't be deadweight and then just apply for jobs that utilize these newer skill sets.
"
144,2018-01-14 05:06:18,1515899178.0,dataengineering,"Just got a job in Financial Data Engineering, any advice for a new grad to get far in industry?",7q9czl,KalEl1191,,https://www.reddit.com/r/dataengineering/comments/7q9czl/just_got_a_job_in_financial_data_engineering_any/,2.0,5.0,0.0,,
145,2018-01-16 19:45:03,1516124703.0,dataengineering,A Beginner’s Guide to Data Engineering — Part I,7qu0si,zemcunha,,https://www.reddit.com/r/dataengineering/comments/7qu0si/a_beginners_guide_to_data_engineering_part_i/,5.0,1.0,0.0,,
146,2018-01-16 22:05:52,1516133152.0,dataengineering,What is Data Entry?,7qv3e5,mdjobayer68,,https://www.reddit.com/r/dataengineering/comments/7qv3e5/what_is_data_entry/,1.0,0.0,0.0,,
147,2018-01-17 22:39:58,1516221598.0,dataengineering,Rise of the Data Engineering: Questions,7r43a7,jaywalker76,,https://www.reddit.com/r/dataengineering/comments/7r43a7/rise_of_the_data_engineering_questions/,4.0,1.0,0.0,,"Hey guys,
after coming across this blog:
https://medium.freecodecamp.org/the-rise-of-the-data-engineer-91be18f1e603
I was left with some questions, related to these two points in particular:
 1 - blobs: modern databases have a growing support for blobs through native types and functions. This opens new moves in the data modeler’s playbook, and can allow for fact tables to store multiple grains at once when needed
2 - dynamic schemas: since the advent of map reduce, with the growing popularity of document stores and with support for blobs in databases, it’s becoming easier to evolve database schemas without executing DML. This makes it easier to have an iterative approach to warehousing, and removes the need to get full consensus and buy-in prior to development.

Do any of you guys have any insight as to how this could be accomplished, specially the blob example, where you could store multiple grains at once? Or any pointers as to where to look for answers?

Thanks"
148,2018-01-22 00:59:45,1516575585.0,dataengineering,"Should I study Data Science minor if it takes me an extra semester to complete my Business Info Sys major degree? If take the minor, I will graduate in May 2020 If not, December 2019. It also cost me extra $10k for tuition and living expense for that semester.",7s1ajc,vStevenPhan,,https://www.reddit.com/r/dataengineering/comments/7s1ajc/should_i_study_data_science_minor_if_it_takes_me/,3.0,3.0,0.0,,"Should I study Data Science minor if it takes me an extra semester to complete my Business Info Sys major degree? If take the minor, I will graduate in May 2020 If not, December 2019. It also cost me extra $10k for tuition and living expense for that semester."
149,2018-01-23 15:53:44,1516715624.0,dataengineering,Snorkel: Extracting Value From Dark Data With Python (Interview),7seifl,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7seifl/snorkel_extracting_value_from_dark_data_with/,2.0,0.0,0.0,,
150,2018-01-24 19:49:40,1516816180.0,dataengineering,SQL report writer to data engineer,7sovgr,darna2017,,https://www.reddit.com/r/dataengineering/comments/7sovgr/sql_report_writer_to_data_engineer/,3.0,5.0,0.0,,"Basically what I do for work is build Oracle SQL queries based on client requirements, create a layout and produce report and client will then do whatever they want to the data generated by the report. I want to work on the other side where I just don't generate report but will have the ability to do more and I am thinking about switching to data engineer. Except for database administration my Oracle SQL skills is intermediate to advanced. I am also trying to learn Python on my own. Will I have a chance? I found a lot of data analyst positions but not a lot of data engineer in healthcare industry. What similar positions are being posted for data engineer? Any insight, help, advise would be really appreciated. Thanks!
 "
151,2018-01-29 14:12:26,1517227946.0,dataengineering,Global Earthquake Magnitude Data Analysis with R,7trvws,engineeringbigdata,,https://www.reddit.com/r/dataengineering/comments/7trvws/global_earthquake_magnitude_data_analysis_with_r/,2.0,0.0,0.0,,
152,2018-01-31 14:15:38,1517400938.0,dataengineering,Free Lectures playlist for ADVANCE HADOOP Concepts,7u9hbs,jiveshgarg1,,https://www.reddit.com/r/dataengineering/comments/7u9hbs/free_lectures_playlist_for_advance_hadoop_concepts/,1.0,0.0,0.0,,
153,2018-02-03 23:41:48,1517694108.0,dataengineering,Stored procedures?,7v2bec,clojureyourmouth,,https://www.reddit.com/r/dataengineering/comments/7v2bec/stored_procedures/,1.0,1.0,0.0,,Are stored procedures in the data warehouses a good idea? 
154,2018-02-04 17:55:06,1517759706.0,dataengineering,Request for quick feedback on resume,7v7grz,neothemaster,,https://www.reddit.com/r/dataengineering/comments/7v7grz/request_for_quick_feedback_on_resume/,1.0,0.0,0.0,,"Hi, https://docs.google.com/document/d/17snbB9lMqHj58wp9p6hTlecgmVHBE3WcJMkE1DF8OO4/edit?usp=sharing Here is my resume. Sorry for this but I'm reaaly confused on the best way to structure my resume for companies. I want to showcase my data engineering skills with basic foray in ML. Have I structured this right? Also I have a publication on medium and am a top writer on Medium. Should I highlight that? Thanks for your time :)

I wanna apply as a data engineer."
155,2018-02-06 18:35:31,1517934931.0,dataengineering,The Airflow Podcast: Origins of Airflow with Maxime Beauchemin,7vojak,ben_astronomer,,https://www.reddit.com/r/dataengineering/comments/7vojak/the_airflow_podcast_origins_of_airflow_with/,9.0,0.0,1.0,,
156,2018-02-06 20:30:22,1517941822.0,dataengineering,How to Become a Data Engineer,7vpeyh,fstewart86,,https://www.reddit.com/r/dataengineering/comments/7vpeyh/how_to_become_a_data_engineer/,2.0,11.0,0.0,,I love data and building the necessary infrastructure to process and analyze it. How do I become a Data Engineer? 
157,2018-02-07 11:07:58,1517994478.0,dataengineering,Fraud Data Entry Company की पहचान कैसे करे ?,7vuv09,mahaveer96,,https://www.reddit.com/r/dataengineering/comments/7vuv09/fraud_data_entry_company_क_पहचन_कस_कर/,1.0,0.0,0.0,,
158,2018-02-07 23:11:55,1518037915.0,dataengineering,"[Help] Creating an automated pipeline with python, BigQuery, and App Engine",7vzgn4,jdb441,,https://www.reddit.com/r/dataengineering/comments/7vzgn4/help_creating_an_automated_pipeline_with_python/,1.0,4.0,0.0,,"Hello.

I am working on a reporting automation project that pulls data from the Google AdWords API using python and stores it in BigQuery. 

I work at a two man digital marketing startup and we are researching services where we can deploy our scripts and schedule them.

**Is Google App Engine an appropriate tool to deploy python scripts and schedule using cron?** 

**Is there another service or platform besides App Engine that would better suit our needs?** We will not generate a lot of data but we want to build this with a scalable foundation for other clients

**Are there any caveats here that I am not considering?**

If you need more details please let me know. I would really appreciate any discussion/feedback.

Thank you.

"
159,2018-02-09 23:14:08,1518210848.0,dataengineering,"Fast, Globally Scalable Data Streaming with Pulsar (Interview)",7wgobe,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7wgobe/fast_globally_scalable_data_streaming_with_pulsar/,2.0,0.0,0.0,,
160,2018-02-10 11:53:50,1518256430.0,dataengineering,What can Apache Spark(not spark streaming) do that regular Hive queries can't? Or is it simply faster/more efficient to use?,7wklyc,GoodJobMate,,https://www.reddit.com/r/dataengineering/comments/7wklyc/what_can_apache_sparknot_spark_streaming_do_that/,3.0,2.0,1.0,,What can Apache Spark(not spark streaming) do that regular Hive (configured to use Spark as its engine)queries can't? Or is it simply faster/more efficient to use? 
161,2018-02-12 16:17:42,1518445062.0,dataengineering,TimescaleDB: Fast and Scalable Timeseries On PostGreSQL (Interview),7x103r,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7x103r/timescaledb_fast_and_scalable_timeseries_on/,1.0,0.0,0.0,,
162,2018-02-19 14:27:02,1519043222.0,dataengineering,Data Processing in the Social Media Platform,7ymawc,OutsourceDataworks,,https://www.reddit.com/r/dataengineering/comments/7ymawc/data_processing_in_the_social_media_platform/,1.0,0.0,0.0,,
163,2018-02-20 13:01:52,1519124512.0,dataengineering,How Data Teams Work Together (Interview),7yv2qh,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/7yv2qh/how_data_teams_work_together_interview/,2.0,0.0,0.0,,
164,2018-02-22 21:59:14,1519329554.0,dataengineering,Highway MPG Data Set Graphical Analysis with R,7zhtsd,engineeringbigdata,,https://www.reddit.com/r/dataengineering/comments/7zhtsd/highway_mpg_data_set_graphical_analysis_with_r/,1.0,0.0,0.0,,
165,2018-02-23 18:12:56,1519402376.0,dataengineering,"The Airflow Podcast: Use Cases with core contributors Bolke de Bruin, Chris Riccomini, and more.",7zp5oe,ben_astronomer,,https://www.reddit.com/r/dataengineering/comments/7zp5oe/the_airflow_podcast_use_cases_with_core/,3.0,0.0,0.0,,
166,2018-02-26 12:53:32,1519642412.0,dataengineering,books for data engineering?,80c7le,bluesufi,,https://www.reddit.com/r/dataengineering/comments/80c7le/books_for_data_engineering/,2.0,4.0,0.0,,
167,2018-02-26 16:14:23,1519654463.0,dataengineering,Event Data Infrastructure at Honeycomb.io (Interview),80dabg,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/80dabg/event_data_infrastructure_at_honeycombio_interview/,3.0,0.0,0.0,,
168,2018-02-28 04:52:05,1519786325.0,dataengineering,"ETL pipeline for small data size, but 100 various sources - what would you choose?",80smns,whiskeyfox_,,https://www.reddit.com/r/dataengineering/comments/80smns/etl_pipeline_for_small_data_size_but_100_various/,2.0,5.0,0.0,,"I'm leaning toward NiFi fetching everything file-based, then spitting it into a distributed file system for storage. 

Another NiFi leg outputs via Avro conversion into Sexy Kafka (it's like normal Kafka but this is my first time using it).

From Kafka --&gt; data warehouse/various frontends

I'm not sure if Kafka/Kappa architecture is right for the hybrid datatypes I'm working with...

---

FTP - NiFi fetch and process

Bulk data delivery - upload files for NiFi

Scheduled API calls direct to Kafka (or NiFi just for consistency?)

Scrapers - direct to Kafka

Clickstreams - direct to Kafka"
169,2018-03-01 17:51:50,1519919510.0,dataengineering,Training/Conferences for a Data Engineer/ETL Developer,8167pn,LordCommanderStannis,,https://www.reddit.com/r/dataengineering/comments/8167pn/trainingconferences_for_a_data_engineeretl/,3.0,4.0,0.0,,"So I just graduated college and I'm working full time as a data engineer/etl developer. my manager told me if there are any training seminars or conferences I want to go to, I can go and it'll be fully paid by my company. Do you guys recommend any type of training for a data engineer/etl developer?

FYI I have an ok amount of knowledge using SSIS, no knowledge of Informatica, below average programming skills and average SQL skills. We are also going to be switching to Azure this year, so I was interested in those training/seminars as well"
170,2018-03-05 14:52:54,1520254374.0,dataengineering,Data Liquidity In The AI Economy (Interview),825prj,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/825prj/data_liquidity_in_the_ai_economy_interview/,1.0,1.0,0.0,,
171,2018-03-05 17:48:05,1520264885.0,dataengineering,Twitter Heron's git history visualized,826uhi,ksyucs,,https://www.reddit.com/r/dataengineering/comments/826uhi/twitter_herons_git_history_visualized/,3.0,0.0,0.0,,
172,2018-03-07 23:18:23,1520457503.0,dataengineering,Easily run Apache Airflow and deploy DAGs in Kubernetes,82rx5i,rywalker,,https://www.reddit.com/r/dataengineering/comments/82rx5i/easily_run_apache_airflow_and_deploy_dags_in/,0.0,0.0,0.0,,
173,2018-03-08 00:41:18,1520462478.0,dataengineering,[Question] Having trouble finding work as a data engineer. Suggestions?,82skox,BvckR0g3rs,,https://www.reddit.com/r/dataengineering/comments/82skox/question_having_trouble_finding_work_as_a_data/,5.0,6.0,0.0,,"So I've been applying for mid-level data engineering positions for about 6 months with no luck. Not sure if I have some glaring gaps in my knowledge/skill set that I'm not seeing, if its just the local market (King of Prussia/Philadelphia) being wonky, or a combination of both. Looking to reddit for suggestions on places to improve so I can land a data engineering job.

&amp;nbsp;


Here is a list of my current skills, years of experience, and level of competency.  


*Postgres: 6 years, Senior
&amp;nbsp;

*MS SQL Server: 6 years, Senior
&amp;nbsp;

*Redshift: 4 years, Senior
&amp;nbsp;

*SQL: 6 years, Expert (seriously, there isn't much I can't do in a RDBMS with SQL)
&amp;nbsp;

*Python: 6 years, this is a weird one. I've done mostly scripting in Python for ETL, but not a whole lot of ""application"" development. So mid-level?
&amp;nbsp;

*Spark: ~1 year total, a few of PoCs but nothing thats seen a Prod environment though, entry-level
&amp;nbsp;
 
*Storm: 2 years, supporting/modifying an existing production system written in Clojure, mid-level
&amp;nbsp;

*Looker: 2 years, mid-level
&amp;nbsp;

*Drake: 2 years
&amp;nbsp;

*Jenkins: 2 years
&amp;nbsp;



*Other stuff that I've used at some point, but either have less than a year or haven't used in a while: Aurora(MySQL), MemSQL, Kafka, C#, Java
&amp;nbsp;



*Education: BS in Management of IT
&amp;nbsp;



*Current job: Redshift/MS SQL DBA
&amp;nbsp;

*Past: Data Engineer/BI Dev 2 years, Data Engineer 2 years, MS SQL DBA 2 Years . 
&amp;nbsp;
&amp;nbsp;

Summary of work experience: Started out as an intern and was hired as a DBA after a few months. Later moved onto a company doing ETL for Machine Learning and Predictive Analytics (had a lot of data science exposure) until it got bought by a large corporation. Went to work for another small company doing both ETL and BI work. Company drama caused me to leave. Now I'm doing 24/7 production support DBA work, not coding, and hating it.
&amp;nbsp;
 


Everything that has come my way so far is either a recruiter wanting me to interview for MS SQL Server stuff (which I really don't want to do anymore), or companies that are looking for unicorns that have 5+ years of app dev experience that decided to they wanted to be data engineers instead. I've been told I wouldn't be interviewed because I didn't have a CompSci degree by a few places as well, although I've never had a problem keeping up with other developers and have been often praised by my peers for having good mix of coding and ""data guy"" skills.
&amp;nbsp;



Another issue is that I am currently slightly north of 100K/yr and don't want to take too much of a pay cut, if possible.
&amp;nbsp;



So far my plans consist of working on personal projects to build a data engineering centric portfolio, trying to sharpen my python/CS algorithm skills so I can ace coding interviews, and possibly going back to school for a MS in Applied Data Science. All while applying for everything in a commutable distance.
&amp;nbsp;



Am I being reasonable by assuming I can land a mid-level role, or am I setting my sights too high for my background, education, and hopeful pay grade?
&amp;nbsp;



TL;DR: I have 6 years of RDBMS, ETL, python scripting experience, but lack a CompSci degree. Is it reasonable to assume I can land a mid-level Data Engineer position?
&amp;nbsp;


EDIT: Longtime lurker, first time posted. Edited to fix formatting. 
EDIT EDIT: Grammar..."
174,2018-03-11 22:30:46,1520800246.0,dataengineering,"Lighthouse, a library to help you build and manage data lakes on Apache Spark. Tutorial in comments.",83pdvi,[deleted],,https://www.reddit.com/r/dataengineering/comments/83pdvi/lighthouse_a_library_to_help_you_build_and_manage/,1.0,1.0,0.0,,
175,2018-03-11 22:40:57,1520800857.0,dataengineering,"Lighthouse, a library to build and manage a data lake on top of Apache Spark",83pgb0,datamindedbe,,https://www.reddit.com/r/dataengineering/comments/83pgb0/lighthouse_a_library_to_build_and_manage_a_data/,2.0,1.0,0.0,,
176,2018-03-12 15:42:33,1520862153.0,dataengineering,Evolutionary Database Design and Refactoring (Interview),83v128,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/83v128/evolutionary_database_design_and_refactoring/,5.0,0.0,0.0,,
177,2018-03-14 05:31:28,1520998288.0,dataengineering,Interning as a Data Engineer: What to pick up beforehand?,84abiu,brweird,,https://www.reddit.com/r/dataengineering/comments/84abiu/interning_as_a_data_engineer_what_to_pick_up/,3.0,5.0,0.0,908.0,"Hi all, I'm a student in a vocational institute with a background in data analytics. For my compulsory internship, I got a position as a data engineer.


My job scope requires me to construct ETL pipelines using scripting languages (python, unix bash). However, I'm only familiar with ETL using Visual Studio data tools and some basic python. 


Since I'm not very familiar with programming (only learned basic java and python), I was wondering what should I pick up beforehand to familiarise myself with the required skills? I'm currently in the process of learning the numpy library.


Thanks in advance!"
178,2018-03-14 18:37:53,1521045473.0,dataengineering,a managed Apache Airflow service,84esjm,rywalker,,https://www.reddit.com/r/dataengineering/comments/84esjm/a_managed_apache_airflow_service/,7.0,0.0,0.0,908.0,
179,2018-03-14 19:07:43,1521047263.0,dataengineering,Spam or Ham Data Set Analysis with Word Cloud in R,84f1hf,engineeringbigdata,,https://www.reddit.com/r/dataengineering/comments/84f1hf/spam_or_ham_data_set_analysis_with_word_cloud_in_r/,1.0,0.0,0.0,908.0,
180,2018-03-16 19:35:02,1521221702.0,dataengineering,Github - Example Airflow DAGs,84x8be,ben_astronomer,,https://www.reddit.com/r/dataengineering/comments/84x8be/github_example_airflow_dags/,7.0,0.0,0.0,911.0,
181,2018-03-16 23:34:03,1521236043.0,dataengineering,Machine Learning: The High-Interest Credit Card of Technical Debt,84z0z6,[deleted],,https://www.reddit.com/r/dataengineering/comments/84z0z6/machine_learning_the_highinterest_credit_card_of/,2.0,0.0,0.0,911.0,
182,2018-03-17 13:12:28,1521285148.0,dataengineering,Machine Learning: The High Interest Credit Card of Technical Debt,85344c,magicsrb,,https://www.reddit.com/r/dataengineering/comments/85344c/machine_learning_the_high_interest_credit_card_of/,7.0,0.0,0.0,911.0,
183,2018-03-18 19:05:26,1521392726.0,dataengineering,What is your integration nightmare?,85ciar,anonymous_pepper,,https://www.reddit.com/r/dataengineering/comments/85ciar/what_is_your_integration_nightmare/,3.0,2.0,0.0,915.0,"Hello data engineers!

FTP - I'm a data scientist and trying to give integration estimates for a couple of client databases. 

Right now I'm wondering what 'worst case scenarios' look like . . . nightmare admins, non existent IT . . . et cetera, whatever made your life hell for that particular project.

Please share your stories!

Thanks!

"
184,2018-03-19 03:51:08,1521424268.0,dataengineering,Exploring The Elastic Stack: From Text Search To Metrics Platform (Interview),85g0ed,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/85g0ed/exploring_the_elastic_stack_from_text_search_to/,2.0,0.0,0.0,917.0,
185,2018-03-19 06:27:51,1521433671.0,dataengineering,Ia this data engineering?,85gvto,Jachtheripper,,https://www.reddit.com/r/dataengineering/comments/85gvto/ia_this_data_engineering/,2.0,2.0,0.0,917.0,"Hi all,
I work at big firm firm as a quantitative engineer/big data engineer/big data analytics (confused myself). I have a bachelor's in CS and Masters in CS focused on ML.

What i do revolves around the following

1. Getting data from various data sources , mostly structured  from RDBMs using sqoop  into HDFS and write Java map reduce to put it into Hbase cluster using protobufs. We also get data from other large databases where they dump data on to our HDFS. These are our data pipelines which runs in a very large Hadoop cluster~ 40TB memory 3000 cores and currently data size is close to 3 PB

2. Use the data in Hbase to do analytics based on use cases. They involve outlier detection, trend analysis and other statisticcal methods. . These analytics projects are also written in Java map reduce and pushed into production which run as periodic jobs in the same cluster. The data used for these analytics projects run on about 50-100 TB of data.

Since we don't use any RDBMS other than sourcing , we don't have things like facts, dimensions,data Mart's quret engine, data streaming and such, I am not sure  if this is data engineering. At the same time I do statistical modelling on the data. 

From what I've seen this looks like a combo of data engineer and data scientist but I'm not sure.

Can anyone help me out here? 

EDIT 1: Typo in title. Not sure how to edit it in Mobile
EDIT 2: forgot to mention that no one else uses our data other than us.we build pipelines to facilitate our models."
186,2018-03-22 21:09:27,1521745767.0,dataengineering,Linear Regression Algorithm | Machine Learning Algorithm,86e5q9,engineeringbigdata,,https://www.reddit.com/r/dataengineering/comments/86e5q9/linear_regression_algorithm_machine_learning/,1.0,0.0,0.0,932.0,
187,2018-03-23 15:15:17,1521810917.0,dataengineering,Help for FB interview prep!,86kj5x,jaybastin,,https://www.reddit.com/r/dataengineering/comments/86kj5x/help_for_fb_interview_prep/,1.0,1.0,0.0,933.0,
188,2018-03-26 15:37:54,1522067874.0,dataengineering,Redshift tips and tricks - part 1,878rgj,[deleted],,https://www.reddit.com/r/dataengineering/comments/878rgj/redshift_tips_and_tricks_part_1/,1.0,0.0,0.0,940.0,
189,2018-03-26 15:48:06,1522068486.0,dataengineering,Redshift tips and tricks - part 1,878tr6,parudod,,https://www.reddit.com/r/dataengineering/comments/878tr6/redshift_tips_and_tricks_part_1/,8.0,0.0,0.0,940.0,
190,2018-03-26 16:13:41,1522070021.0,dataengineering,Fast and Scalable Financial Timeseries Dataframes with MarketStore (Interview),878zot,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/878zot/fast_and_scalable_financial_timeseries_dataframes/,3.0,0.0,0.0,940.0,
191,2018-03-26 22:19:03,1522091943.0,dataengineering,Data Engineer future job?,87bu8u,Chr0nomaton,,https://www.reddit.com/r/dataengineering/comments/87bu8u/data_engineer_future_job/,8.0,12.0,0.0,941.0,"I'm currently a data engineer at a tech company. We are a relatively new team (about a year old). Right now, the work the data engineers do (team of 4) is close to what a devops role does. I guess it's technically dataops. I still write a lot of ETL jobs, work on scaling out our infrastructure, data architecture, and adding new systems to better support our data science initiatives (e.g. implementing graphDBs, GraphQL for apis, etc). What we don't have is massive scale. We collect from a lot of sources, but we don't have that much. I would say roughly under 50TB. We also have most of our infrastructure in python. Should I attempt to learn Scala in the meantime? How can I still marketing part of my skills, and still be a worthwhile candidate for some of the really popular data engineering firms right now even though we don't process that much data?"
192,2018-03-28 22:24:15,1522265055.0,dataengineering,Decision Tree Analysis with Credit Data in R | Part 1,87uqus,engineeringbigdata,,https://www.reddit.com/r/dataengineering/comments/87uqus/decision_tree_analysis_with_credit_data_in_r_part/,0.0,0.0,0.0,946.0,
193,2018-03-29 22:18:35,1522351115.0,dataengineering,Help for building infrastructure,8843aj,christogil,,https://www.reddit.com/r/dataengineering/comments/8843aj/help_for_building_infrastructure/,3.0,5.0,0.0,949.0,"I'm looking for advice in order to set up a project in order to improve my skills in data engineering and web development.Im planning to build a hackernews like and store the data on hadoop cluster to run analysis and recommendation system. I have intermediate skills in python/flask/hdfs but i lack skills in the devops part. What do you recommend as a general stack or solution  in order to transfer my data between the operational data store of my Web app and my hadoop cluster? Im thinking of 2 private server.one running the Web app and database and the second running hadoop.
Thank you"
194,2018-04-02 18:06:29,1522681589.0,dataengineering,Using Anomaly Detection To Secure Your Cloud with ThreatStack (Interview),88zzhb,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/88zzhb/using_anomaly_detection_to_secure_your_cloud_with/,3.0,0.0,0.0,957.0,
195,2018-04-03 11:46:53,1522745213.0,dataengineering,"Analytics pipeline to give meaning to 100 billion events a day (Kafka, Dataflow and BigQuery)",89bmxk,benjamindavy,,https://www.reddit.com/r/dataengineering/comments/89bmxk/analytics_pipeline_to_give_meaning_to_100_billion/,6.0,0.0,0.0,963.0,
196,2018-04-05 05:45:23,1522896323.0,dataengineering,[Question] Just graduated college. Is Google Cloud Data Engineering certification meaningful for me?,89w4qe,jdb441,,https://www.reddit.com/r/dataengineering/comments/89w4qe/question_just_graduated_college_is_google_cloud/,10.0,0.0,0.0,970.0,"Hi guys. 

Is the Data Engineering certification on Google Cloud a good investment of my time? 

I just graduated with a degree in Finance &amp; minor in Data Science. I really developed passion in my DSCI &amp; CS courses which has led me to pursue personal projects in my free time involving Python, Linux, and development in general.

I have a years experience so far working as a web analyst. In my current role, I am doing all the development work to automate reporting for all of our clients using Python, Bash, Google Cloud, and the APIs of the platforms we use. 

In an effort to both learn and build credibility, my employer and I have been researching the Google Cloud certifications.

I fear that I could learn just as much by pursuing my current projects and study materials. The thing is that I have no other professional development credentials and see this as a certification that could be a tool to build credibility for our businesses services while learning a lot of applicable knowledge.

If any of you have this certification, do you think it was a positive journey? Do you agree that it would be a good tool for me to develop a credential in this field?"
197,2018-04-09 16:10:18,1523279418.0,dataengineering,Building Better Analytics Using DataOps (Interview),8ayaat,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8ayaat/building_better_analytics_using_dataops_interview/,4.0,2.0,0.0,989.0,
198,2018-04-09 18:56:50,1523289410.0,dataengineering,Data Engineering Podcast about DataOps,8azi64,botswana99,,https://www.reddit.com/r/dataengineering/comments/8azi64/data_engineering_podcast_about_dataops/,3.0,0.0,0.0,989.0,
199,2018-04-10 04:46:09,1523324769.0,dataengineering,What language do you wish your Data Science colleagues knew?,8b3yd1,SecretAgentZeroNine,,https://www.reddit.com/r/dataengineering/comments/8b3yd1/what_language_do_you_wish_your_data_science/,3.0,6.0,0.0,992.0,"1. Java
2. Scala
3. C++
4. PHP
5. Ruby
6. Python"
200,2018-04-12 02:01:45,1523487705.0,dataengineering,[Question] Current best practices around running multiple queries to multiple sources that run for hours at a time?,8blgqe,db_w,,https://www.reddit.com/r/dataengineering/comments/8blgqe/question_current_best_practices_around_running/,2.0,0.0,0.0,999.0,"Hey Everyone

I have a project that I need to get out the door soon and I am immediately looking at Airflow right now for this, but I was just curious, is there anything else I need to be taking into consideration? I plan on taking Airflow and running it on a local k8's cluster, basically an Airflow container per business area. The queries that I have are already built, I just need to orchestrate them and make sure data is flowing efficiently and in order because right now it's a monolithic manual process that my Data Scientist has to run.

Also - what's the best way to do something like this on AWS or GCP? Just build an AMI for Airflow and throw it up on an instance where you need it?

I'm a fresh new DE coming from a Python / Big Data / DevOps background so this might be a dumb question. sorry :x

Thanks!"
201,2018-04-12 11:24:28,1523521468.0,dataengineering,Transition to blockchain?,8bolsi,christogil,,https://www.reddit.com/r/dataengineering/comments/8bolsi/transition_to_blockchain/,1.0,3.0,0.0,1002.0,"Hi
Do you think it is a good career move for a data engineer to develop skills in blockchain file system and databases  (ipfs,bigchaindb...)?"
202,2018-04-14 14:01:00,1523703660.0,dataengineering,"How much understanding of the business side of the data should the data engineers and scientists have, in your opinion?",8c6v9w,GoodJobMate,,https://www.reddit.com/r/dataengineering/comments/8c6v9w/how_much_understanding_of_the_business_side_of/,3.0,12.0,1.0,1009.0,"Hi everybody. I started working as a data engineer at a large telecom company about 2 months ago. One thing I've noticed is my manager expects me to have a deep understanding of what kind of story the data we're working with can tell actually us.

For example, let's say we have a ""business process""(I think that's what we would call it?..) of somebody buying a sim card and activating it using a particular device. She thinks I should have enough knowledge to translate these words into an SQL query that would give us stuff like where the sim card was bought, when the activation occurred, who bought it, where it was bought, etc etc. The data warehouse we're working with has numerous problems, from the absence of strong naming conventions, documentation and so on and so forth...so constructing a query such as this can take me a really, really long time. But I also think that I shouldn't even be the one who cares about this stuff. I'm supposed to work on data infrastructure and pipelines, not actually understanding the whole business side. Am I wrong?"
203,2018-04-15 08:47:29,1523771249.0,dataengineering,Keeping Up With The Data Engineering industry (Interview),8cdbxy,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8cdbxy/keeping_up_with_the_data_engineering_industry/,2.0,0.0,0.0,1020.0,
204,2018-04-17 11:21:25,1523953285.0,dataengineering,Who are the most influential and knowledgeable people working as Data engineers?,8cuz53,neothemaster,,https://www.reddit.com/r/dataengineering/comments/8cuz53/who_are_the_most_influential_and_knowledgeable/,5.0,8.0,0.0,1025.0,"Hi,
I was just interested in knowing who all are considered as people worth following in this field. Anytime I search, I find Data scientists and Analysts. Now this field is a little more closely tied to software engineering and its a little hard to differentiate between the two, but I wanted to know about people who have worked a lot on Big Data and Distributes systems. Eg- Martin Kleppman, Jay Kreps, Neha Narkhede etc.
Names or links to blogs would do.
Thanks"
205,2018-04-20 12:59:06,1524218346.0,dataengineering,Best Home Based Data Entry Jobs Online,8dmfdz,sharonalfred,,https://www.reddit.com/r/dataengineering/comments/8dmfdz/best_home_based_data_entry_jobs_online/,0.0,2.0,0.0,1034.0,
206,2018-04-20 16:22:56,1524230576.0,dataengineering,Data Warehouse or DataLake?,8dniuj,jerrie86,,https://www.reddit.com/r/dataengineering/comments/8dniuj/data_warehouse_or_datalake/,1.0,10.0,0.0,1034.0,"Hey guys,

This is my first post here and I am not sure if this is the right sub for it. So I recently joined an organization and one of their goals for next few years is to create a central repository for data from different databases.

Currently, we have 15 different databases and a badly designed datawarehouse where there are no conformed dimensions and hence, not able to cross query and users being able to get the data they way they would like.

One step would be to recreate the EDW and gather the requirements from users again and start with the grain and move on from there.

Second option would be to create a datalake and get all the data in there and transform and use it the way we want it.

regarding this technique, I have just started reading about it and not sure about the resources it would take to accomplish this task.

The data is less than 100 TB and growth is minimal.

I would really appreciate if there is a path we could take besides from these two or if we chose the second option, what resources it would take and how much time frame should we look at to get this taken care of?

Note: I am new to this and would like to come with ideas before my next meeting with the team.

We currently have SQL 2012 And We got 3 SQL BI developers and 2 DBA's.

Thanks guys and happy weekend!"
207,2018-04-23 04:09:09,1524445749.0,dataengineering,Octopai Managed Metadata Service For Better Business Intelligence (Interview),8e7pqf,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8e7pqf/octopai_managed_metadata_service_for_better/,0.0,5.0,0.0,1041.0,
208,2018-04-26 01:15:05,1524694505.0,dataengineering,Are any of you doing AI / ML?,8exmj0,db_w,,https://www.reddit.com/r/dataengineering/comments/8exmj0/are_any_of_you_doing_ai_ml/,3.0,4.0,0.0,1057.0,"I'm a brand new DE and I'm helping establish / define what DE is at my org, but the problem is, I'm also being lumped into doing AI projects which I don't necessarily think is the right thing to do. I'm a software / systems engineer historically, not a Data Scientist or someone with a degree in Mathematics. So I feel like I don't have the right knack to do the job properly and it's pretty much keeping me up at night. I can learn it no problem, but I don't think it's the right structure to my future self / team. Thoughts?"
209,2018-04-26 10:44:30,1524728670.0,dataengineering,Mesos or YARN for Spark Jobs?,8f0woh,neothemaster,,https://www.reddit.com/r/dataengineering/comments/8f0woh/mesos_or_yarn_for_spark_jobs/,5.0,3.0,0.0,1059.0,"Hi,
We want to use a resource scheduler for our Spark jobs. I came across Mesos and Yarn but am unable to decide which one to use.
The primary goal is ease of setup, parallelization of jobs and better resource utilization. I read a lot on the differences but can't find any opinion on what to use.
For now the use case is Spark but we would like to extend the resource pooling to other services too, though that will be quite later in the future.
Please help me out on this. Thanks "
210,2018-04-29 22:22:41,1525029761.0,dataengineering,"Moved from software engineering into data engineering, getting ready to interview elsewhere...",8ftlbq,junjk,,https://www.reddit.com/r/dataengineering/comments/8ftlbq/moved_from_software_engineering_into_data/,2.0,2.0,0.0,1065.0,"I have about ten years professional experience, with plenty of Python application development and web application development.  I started doing data analytics and about two years ago started filling a more data engineering oriented role, picking up Scala and Spark as well as Hadoop and related scheduling and graphdb tools.  Most of my work was still very application oriented and I didn't do a lot of ""building pipelines"" as the architecture was already there.

Anyway, long story short, now I'm starting to interview for data engineer roles elsewhere and I'm not sure what I might be expected to know in an on\-site interview.  I'm used to programming interviews where you whiteboard some stuff about data structures or whatever, show that you know how to use sets and iterate recursively and so on.  What will the interview structure be like?  What kinds of technical questions might I get?  When it comes to my day to day, I do much more functional development than pipeline management or whatever so I'm used to having to look up how to do a lot of stuff there and I don't know if I could whiteboard a data pipeline solution much past

\-\-\-\-\&lt;\-\- hdfs? aws? \&lt;\-\-\-\-\-\-\-\-\-ingestion \(csv? raw?\) 

|                  \^

|                  |

|                  |

\-\-\-\-\-\-\-\-\-\-\-\- scala,spark,maybe oozie?\-\-\-\-\&gt; logs \(yarn?\)

So yeah... I feel that I can speak to a general idea of how a data engineer should approach a problem, work with their data science team, and so on.  I can certainly roughly code a map reduce or a transform using rdds or whatever.

I'm not really even sure what questions I should be asking here because I have so little idea what to expect.  Any advice is very welcome.  I am interviewing next week with a large financial company as well as with a small pharmaceutical testing company.  Thanks in advance!"
211,2018-04-30 04:12:24,1525050744.0,dataengineering,Self Service Business Intelligence For Everyone With Metabase (Interview),8fvv2b,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8fvv2b/self_service_business_intelligence_for_everyone/,3.0,0.0,0.0,1065.0,
212,2018-04-30 20:48:22,1525110502.0,dataengineering,Has anyone taken Jesse Anderson's online database engineer courses?,8g1d6e,Sexychocolate42o,,https://www.reddit.com/r/dataengineering/comments/8g1d6e/has_anyone_taken_jesse_andersons_online_database/,8.0,14.0,0.0,1065.0,I'm curious because i'm considering purchasing it and i'm wondering if anyone here has gone through with it.
213,2018-05-02 01:48:48,1525214928.0,dataengineering,Crosspost: Data Engineers Vs. Data Scientists And Why These Differences Are So Important,8gcn6u,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/8gcn6u/crosspost_data_engineers_vs_data_scientists_and/,7.0,2.0,0.0,1070.0,
214,2018-05-02 18:29:51,1525274991.0,dataengineering,Astronomer Airflow v0.2.0 – A fully open source alternative to Google Composer,8gia1p,ben_astronomer,,https://www.reddit.com/r/dataengineering/comments/8gia1p/astronomer_airflow_v020_a_fully_open_source/,3.0,0.0,0.0,1071.0,
215,2018-05-02 21:24:38,1525285478.0,dataengineering,"Google cloud composer is a managed service, combine the strengths of Google Cloud Platform with Airflow.",8gjnda,prabeeshk,,https://www.reddit.com/r/dataengineering/comments/8gjnda/google_cloud_composer_is_a_managed_service/,1.0,0.0,0.0,1071.0,
216,2018-05-02 21:35:09,1525286109.0,dataengineering,"Cloud Composer is now in beta: build and run practical workflows with minimal effort. Google cloud composer is a managed service, combine the strengths of Google Cloud Platform with Airflow.",8gjqf5,prabeeshk,,https://www.reddit.com/r/dataengineering/comments/8gjqf5/cloud_composer_is_now_in_beta_build_and_run/,1.0,0.0,0.0,1071.0,
217,2018-05-04 01:48:05,1525387685.0,dataengineering,"The best article I’ve read about the modern data engineer, by the creator of Airflow",8guih3,davedotwav,,https://www.reddit.com/r/dataengineering/comments/8guih3/the_best_article_ive_read_about_the_modern_data/,18.0,11.0,0.0,1079.0,
218,2018-05-04 20:44:12,1525455852.0,dataengineering,Advice on the HOW,8h14ex,sum1udontknow,,https://www.reddit.com/r/dataengineering/comments/8h14ex/advice_on_the_how/,2.0,1.0,0.0,1082.0,"I have been tasked with managing our data which will be used for various machine learning work by our data scientist. Most of my experiences have been around development so I am little lost as the best path to take. We have about 300K word documents that are parsed and text extracted. It is growing at a steady rate. To all those with a million times more experiences than me, how would you approach this problem? Any advices is greatly appreciated! "
219,2018-05-05 02:45:25,1525477525.0,dataengineering,"As someone who's still learning data engineering, I recently attempted to get into a data engineering program by performing a data engineering challenge. I was able to write all of the data cleaning/preparation code correctly, but didn't know how to do the rest. What else do I need to learn?",8h3npu,HAL9000000,,https://www.reddit.com/r/dataengineering/comments/8h3npu/as_someone_whos_still_learning_data_engineering_i/,5.0,4.0,0.0,1083.0,"I started out learning to write code for data science last year but I'm also trying to learn data engineering. Thus far, the classes I've taken have all focused on just writing code to mine and clean data (lots of Pandas), visualize it (Matplotlib and Seaborn), perform statistical analysis (Numpy/Scikit-learn). And I generally have used either Jupyter Notebooks or Spyder IDEs as my coding environments with no other software that data engineers use (for example, I'm aware of things like Apache Spark and Hadoop but I don't yet know how to use them or why I would use them other than that I know there's necessary for managing scalable projects).

So with this data engineering challenge, the data cleaning/wrangling was doable as I've learned Python for data science at at least an intermediate level. But the challenge asked me to do things I just haven't learned how to do, such as:

1) Arranging my Python file within the proper directory structure to allow scaling for large amounts of data, 

2) Loading my directory structure for the file to Github, 

3) Have the directory corrected arranged to allow the use of unit tests, etc...

Basically, I know there's a set of software that data engineers would use to manage their work and prepare it for scaling and testing but I don't know where to go to learn this stuff.

1) Is there like one piece of IDE software or something else that I should use for preparing my code? Or should I be using something like Django or Spark?

2) Are there good tutorials I can check out to learn what I need to learn?"
220,2018-05-08 12:21:51,1525771311.0,dataengineering,Brief Conversations On Data Engineering From The Open Data Science Conference,8hvc0s,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8hvc0s/brief_conversations_on_data_engineering_from_the/,3.0,0.0,0.0,1098.0,
221,2018-05-08 22:37:07,1525808227.0,dataengineering,What do you do in a situation where your internal customer wants to own the pipeline you build?,8hzlu9,dez_pak,,https://www.reddit.com/r/dataengineering/comments/8hzlu9/what_do_you_do_in_a_situation_where_your_internal/,4.0,3.0,0.0,1099.0,"I should first ask: Do you guys own the pipelines after you build them? Or do you typically hand them off?

I have a pipeline (with not many dependencies) I need to create for a different department and they want their BI dev to own the whole thing after I'm done. But I don't want them having access to my Airflow instances which are running other processes, and having them needing to learn Python etc to manage it. I'm using Apache Nifi for some other things and was thinking about throwing it on there. Thoughts? How do you guys handle situations like this? Is my approach here sensible?"
222,2018-05-09 10:44:52,1525851892.0,dataengineering,Curious to know about real world cases where an always-on cluster setup has proven more useful than cloud storage like S3 for storing all your data,8i46a0,sharky993,,https://www.reddit.com/r/dataengineering/comments/8i46a0/curious_to_know_about_real_world_cases_where_an/,4.0,2.0,0.0,1102.0,"Also hybrid setup?

For the uninitiated - https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html"
223,2018-05-15 00:22:48,1526332968.0,dataengineering,Getting Data to Data Lake from Microservices — Part 2: The Logs or Clickstream,8jg0kh,kiarash-irandoust,,https://www.reddit.com/r/dataengineering/comments/8jg0kh/getting_data_to_data_lake_from_microservices_part/,4.0,0.0,0.0,1118.0,
224,2018-05-15 16:15:00,1526390100.0,dataengineering,A Brief Look At Geospatial Data And Graph Databases (Interview),8jlh3r,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8jlh3r/a_brief_look_at_geospatial_data_and_graph/,2.0,0.0,0.0,1121.0,
225,2018-05-15 16:15:17,1526390117.0,dataengineering,Testing Data Pipelines With Great Expectations (Interview),8jlh6n,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8jlh6n/testing_data_pipelines_with_great_expectations/,3.0,2.0,0.0,1121.0,
226,2018-05-19 17:28:08,1526740088.0,dataengineering,Java in Data Engineering,8kll3e,GoodJobMate,,https://www.reddit.com/r/dataengineering/comments/8kll3e/java_in_data_engineering/,3.0,8.0,0.0,1127.0,"Hi everyone! 

I've done some searches for Data Engineering on various sites and it seems that at least some of those jobs require 3\+ years of experience with enterprise Java. In your experience, is it hard to get a big data dev job without a lot of experience of Java? And what is it used for\(why not Scala/Python, in other words\)?"
227,2018-05-21 05:28:32,1526869712.0,dataengineering,"One ETL, one partition rule",8kxji7,Chr0nomaton,,https://www.reddit.com/r/dataengineering/comments/8kxji7/one_etl_one_partition_rule/,2.0,2.0,0.0,1133.0,"Maxime has talked about a ""one ETL, one partition"" guideline in this post: https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a. Can anyone shed some light on this? At first, I thought this meant one ETL should output one HDFS partition. However, HDFS has a normal block size (default is 128mb). Does that mean all ETLs should return results less than the HDFS partition size? "
228,2018-05-22 05:04:23,1526954663.0,dataengineering,Analyzing Your Data Lake With PrestoDB (Interview),8l6bv5,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8l6bv5/analyzing_your_data_lake_with_prestodb_interview/,2.0,0.0,0.0,1139.0,
229,2018-05-23 00:45:20,1527025520.0,dataengineering,Data Warehouse Concept Question,8ldwvi,jwdatascience,,https://www.reddit.com/r/dataengineering/comments/8ldwvi/data_warehouse_concept_question/,2.0,10.0,0.0,1145.0,"Hello,

Newb question here. So if I use an ETL tool like [Stitch](https://www.stitchdata.com/) and load all these data tables into my Data Warehouse, is the best practice to create a data mart from this warehouse for my specific reporting needs? If so, how would I refresh this data mart given the data from the original data warehouse is being refreshed daily? Would I have to write a bunch of SQL scripts to do this? Obviously that sounds like not a good idea, so there must be some technologies I am unaware of."
230,2018-05-26 16:16:34,1527340594.0,dataengineering,Convert scraping script into live data stream?,8ma2y2,dbsopinion,,https://www.reddit.com/r/dataengineering/comments/8ma2y2/convert_scraping_script_into_live_data_stream/,0.0,3.0,0.0,1184.0,"When I want to test an hypothesis, I know how the steps to take to scrape for the data, clean it. However, I find it challanging to then turn those steps into a program that adds new rows to the dataset. I can manage it, but it feels like I'm missing some basic tool that I don't know about.

I feel that way because, after some time has past, and I did my initial analysis, the data is already stale, and I need to scrape it from the date of the scrape to the present, then continue to stream it from that date without data loss.

the resulting code requires changes to the structure of the code that is quite different from the structure of code in the original script. Which is tedious and unproductive to do for every hypothesis raised. What is a better workflow/tool that suits these situations than coding these rules by hand in python?"
231,2018-05-27 11:25:21,1527409521.0,dataengineering,Data Engineering Project Ideas,8mgivy,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/8mgivy/data_engineering_project_ideas/,5.0,16.0,0.0,1184.0,"I'm looking for suggestions for a data engineering related project idea.. something that I could dig my heels into and then push on github to help with my job search. I think my lack of github projects has hurt my employability and no one wants to interview me even though I feel like I have 75% of the experience. I'm hoping to spend the next month to solely just work on this project. Hopefully that'll help!

Thanks in advance!"
232,2018-05-28 12:45:40,1527500740.0,dataengineering,The Alooma Cloud Data Pipeline Deep Dive (Interview),8mowau,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8mowau/the_alooma_cloud_data_pipeline_deep_dive_interview/,3.0,0.0,0.0,1185.0,
233,2018-05-31 18:34:29,1527780869.0,dataengineering,Resources for Data UAT Best Practices,8njd8n,Strider_A,,https://www.reddit.com/r/dataengineering/comments/8njd8n/resources_for_data_uat_best_practices/,4.0,0.0,0.0,1196.0,"My company is building a new data platform. We're consolidating data that had lived in different databases and other repositories, which includes completely re-working many of our base tables. We've moved into the user acceptance testing stage for tables in the new platform, but our process is pretty haphazard. So I volunteered to craft UAT guidelines that both the techy people and business users can refer to. 

The problem is, I'm only finding suggestions for software UAT. Can anyone point me towards best practices for data-ralated UAT? I'd want to ensure that both our base tables are accurate and eventually that the reports in our BI tool are also correct."
234,2018-06-02 19:10:44,1527955844.0,dataengineering,Some help in terms of which classes to take to go towards data engineering?,8o1l7w,letmelickyoutwice,,https://www.reddit.com/r/dataengineering/comments/8o1l7w/some_help_in_terms_of_which_classes_to_take_to_go/,1.0,12.0,0.0,1205.0,"Hello! I am a Master’s student in CS (bachelors was not in CS) and I want to get towards a career in Data Engineering. The program is around 2 years and I am currently midway through the 1st year. 

My major allows me to take 4 classes from a specific discipline (SE, Data Science, Database Systems, AI, Theory, or Software and Systems Development) and then 4 more classes from any of those listed disciplines. I’m not sure which classes would be most beneficial in terms of the data engineering field and obviously you guys won’t know what classes these are exactly but the names might help. Here are classes in the three main disciplines that I’m assuming I should be concerned with: 

Database Systems:

- Database Programming
- Database Administration and Management
- Spatial Databases and Geographic IS
- Database System Implementation
- Distributed Database Systems
- Advance Database Concepts
- Advance Database Management 
- Mining Big Data
- Intelligent Information Retrieval 

Data Science:

- Data Analysis and Regression
- Intro to Image Processing 
- Applied Image Analysis
- Computer Logic Design 
- Neural Networks and Deep Learning
(Left some classes out here)

Software Engineering (general SE courses there’s a whole lot to type)

Thanks so much in advance!"
235,2018-06-03 10:16:58,1528010218.0,dataengineering,Choosing between jobs,8o7002,_data_scientist_,,https://www.reddit.com/r/dataengineering/comments/8o7002/choosing_between_jobs/,0.0,2.0,0.0,1208.0,"I am interviewing for these two roles:

Junior IT Developer \- [https://capsconnections.ualberta.ca/caplet/Job/Detail/17678?fromsearch=True](https://capsconnections.ualberta.ca/caplet/Job/Detail/17678?fromsearch=True)

Data Scientist \- [https://group.bnpparibas/en/careers/offers\-world/standard\-permanent/data\-scientist\-9/amp?utm\_campaign=google\_jobs\_apply&amp;utm\_source=google\_jobs\_apply&amp;utm\_medium=organic](https://group.bnpparibas/en/careers/offers-world/standard-permanent/data-scientist-9/amp?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic)

Which one do you think would give me more salary and more opportunity to grow?"
236,2018-06-03 11:17:44,1528013864.0,dataengineering,Anyone like Nepusz Tamas in the world? he made this,8o78ds,Burning_Pikachu,,https://www.reddit.com/r/dataengineering/comments/8o78ds/anyone_like_nepusz_tamas_in_the_world_he_made_this/,2.0,2.0,0.0,1208.0,
237,2018-06-04 08:50:36,1528091436.0,dataengineering,Big Break in Big Data: Sapient Talent Hunt for Data Engineers,8oeze3,diipiika,,https://www.reddit.com/r/dataengineering/comments/8oeze3/big_break_in_big_data_sapient_talent_hunt_for/,1.0,0.0,0.0,1211.0,
238,2018-06-04 16:06:06,1528117566.0,dataengineering,"Fast, Scalable, and Flexible Data Storage with ArangoDB (Interview)",8oh3rd,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8oh3rd/fast_scalable_and_flexible_data_storage_with/,3.0,0.0,0.0,1212.0,
239,2018-06-05 19:17:51,1528215471.0,dataengineering,Data Engineering Weekly Digest,8os4fg,adilkhash,,https://www.reddit.com/r/dataengineering/comments/8os4fg/data_engineering_weekly_digest/,2.0,0.0,0.0,1214.0,
240,2018-06-06 07:59:28,1528261168.0,dataengineering,The Math behind Face Recognition,8oxwxz,johnsnowlab,,https://www.reddit.com/r/dataengineering/comments/8oxwxz/the_math_behind_face_recognition/,1.0,0.0,0.0,1215.0,
241,2018-06-06 18:50:24,1528300224.0,dataengineering,New site that ranks the most popular data science tools,8p1pk9,justinontheshore,,https://www.reddit.com/r/dataengineering/comments/8p1pk9/new_site_that_ranks_the_most_popular_data_science/,3.0,0.0,0.0,1215.0,
242,2018-06-09 12:29:44,1528536584.0,dataengineering,Bistro: a radically new approach to data processing (alternative to MapReduce),8prphd,asavinov,,https://www.reddit.com/r/dataengineering/comments/8prphd/bistro_a_radically_new_approach_to_data/,4.0,2.0,0.0,1224.0,
243,2018-06-11 14:41:49,1528717309.0,dataengineering,CockroachDB In Depth with Peter Mattis - Episode 35,8q8v3j,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8q8v3j/cockroachdb_in_depth_with_peter_mattis_episode_35/,1.0,0.0,0.0,1233.0,
244,2018-06-11 22:35:31,1528745731.0,dataengineering,Electrical Engineer Entering the World or Data Engineering,8qcamj,AutoElectric,,https://www.reddit.com/r/dataengineering/comments/8qcamj/electrical_engineer_entering_the_world_or_data/,2.0,2.0,0.0,1233.0,"Hi, I am an 28 years old EE graduate who is currently working  as a controls/automation engineer in a automotive testing company. I have always had a strong passion to get into the world or big data. 

I have this plan to take online courses and build projects using exisiting data of our automotive test measurements. But my concern is that without a mentor at work (All my co workers are mech engineers) can i atually make something useful to put on my resume so i can break into data engineering/data sciense as an intermediate level developer? Or should i just apply for a junior position now as a data analyst. 

Which way would be the best to get into big data. Any answer or advice will be really appreciated 

My skill set now: 2 years mssql
5 years control systems and UI/HMI programming using plc ladder logic, C# and web app development with JavaScript and react 
Intermediate level python and spark "
245,2018-06-12 12:28:01,1528795681.0,dataengineering,Suggestions for a small personal project,8qhkrq,daguito81,,https://www.reddit.com/r/dataengineering/comments/8qhkrq/suggestions_for_a_small_personal_project/,2.0,3.0,0.0,1235.0,"Hey guys, so I'm making a small project which is basically as follows:  I have a python script running through crontab in a VM in AWS pulling data from an API every minute, changing a bit of the data and then sending it to a PSQL instance also in AWS. 

It's been working five for days (planning on leaving it for months to compile data), but i've been thinking about redundancy.

Now I'm really new at this and mostly deal with the business side of things but have wanted to learn more about the technical side of things. 

My original idea was to use docker to create 2 containers running the data aquisition service, and then have a 3rd container doing a reconciliation service where it will grab the data from both containers and kind of create 1 dataframe taking only 1 of the data so if at 00:00 both containers send data, it will pick up only one copy, but if 1 container fails to collec and the other one does , the data goes in.  And this reconciliation (if you can even call it that) service then pushes the dataframe to PSQL that's a different instance. maybe push it to 2 different databases in different services like 1 PSQL and 1 MongoDB. 

The problem is I have no idea how to do that 3rd service that grabs the data from both and reconciles the data between both of them. I guess my main problem is dealing with the pipeline as I don't know the best way to send the data between containers until the end. My original Idea was to spin up a service and a psql instance on the reconciliation container and have both services push into those psql instances and then have a python script collect the data from the database, remove duplicates and then send it to the final database. But that sounds needlessly complicated and those databases will grow big quick and i don't know enough to do some kind of ""streaming service"" where only the newest data is appended. 

Either way to stop ranting about it I wanted to ask if you guys had any suggestions of what I could do or how bad my line of thinking about this was. 

Thank you for your time."
246,2018-06-13 00:35:53,1528839353.0,dataengineering,"I am completing a data engineering assignment for a class, and it was my plan to use Pandas for the assignment. But the instructions specifically say do not use Pandas because it's apparently not scalable for the kind of problem I'm working on. What's a better alternative for Python?",8qmvz6,HAL9000000,,https://www.reddit.com/r/dataengineering/comments/8qmvz6/i_am_completing_a_data_engineering_assignment_for/,2.0,18.0,0.0,1237.0,"I will acknowledge that I know next to nothing about scalability, but I honestly thought that Pandas was better for scalability than just using a regular Python script with built-in functions.

Am I actually better off just using regular Python with built-ins rather than Pandas for scalability, or is there some other trick within Python that I'm not aware of?"
247,2018-06-17 19:00:54,1529251254.0,dataengineering,Suggestions for a data pipeline that sometimes has to correct old outputs?,8rrycz,joshlemer,,https://www.reddit.com/r/dataengineering/comments/8rrycz/suggestions_for_a_data_pipeline_that_sometimes/,4.0,1.0,0.0,1256.0,"I am working on a project that works basically like so:

* New files of protobuf messages are dropped into an S3 bucket, and correspond to 1 hour of event data
* Each of these files must be read, translated to apache parquet and written to a different S3 bucket, and separated into directories according to day (so, all data for 2018-06-17 goes together, all the data for 2018-06-18 goes together, etc)  
  
* Sometimes, a new proto file will appear in the source bucket, which corresponds to an hour which has already been processed. This is because upstream producer of the data may detect that it needs to reprocess a time interval, and write a new file with the more correct data. In the case that one of these files occurs, it's important to delete the old data for that hour, and write the new data in its place.  

Other info: 
  * max acceptable delay in writing output is 4 hours  
  * these ""corrections"" in point 3 can happen to any data that is up to 7 days old  
  

The first two points are very easy to me, I've set up a nice apache flink application which reads proto in, and writes parquet to a bucket, bucketing output by day. That's fine. But these streaming frameworks like Spark and Flink don't seem to cover the case of going in and deleting old data that has already been written. Are streaming frameworks the wrong tool for this job? What would you use?  
  
Thanks!"
248,2018-06-18 10:31:31,1529307091.0,dataengineering,How to master the basics of Data Engineering,8rxnpk,softdevlife,,https://www.reddit.com/r/dataengineering/comments/8rxnpk/how_to_master_the_basics_of_data_engineering/,11.0,0.0,0.0,1258.0,
249,2018-06-18 14:32:39,1529321559.0,dataengineering,Heap's Data Infrastructure In Depth (Interview),8rysl5,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8rysl5/heaps_data_infrastructure_in_depth_interview/,2.0,0.0,0.0,1259.0,
250,2018-06-19 13:23:04,1529403784.0,dataengineering,How to implement consistent hashing efficiently,8s7rr2,atanasovd,,https://www.reddit.com/r/dataengineering/comments/8s7rr2/how_to_implement_consistent_hashing_efficiently/,4.0,0.0,0.0,1264.0,
251,2018-06-20 11:51:55,1529484715.0,dataengineering,"Moving from DevOps into Big Data, too much or achievable?",8sgrw9,ujjain,,https://www.reddit.com/r/dataengineering/comments/8sgrw9/moving_from_devops_into_big_data_too_much_or/,6.0,7.0,5.0,1268.0,"I am considering 2 job offers. 1 as a traditional DevOps Linux infrastructure, which is my background.

big data stack: big data pipelines with Spark, Scala on AWS EMR including Redshift, Snowplow and Airflow.

kinesis kafka cluster Kubernetes cluster python data bridge data-iq scala packer s3 kickstream redshift ec2 elasticsearch aws terraform docker aws-eks (nieuwe eks)

I like learning open source technologies, but I haven't done any kinesis, kubernetes, kafka, data bridge, data-iq, kickstream, redshift, emr, snowplow, airflow, spark before. They know I don't know any scala/java, just normal scripting and that was fine.

Could this be fun or does it seem too much to learn? "
252,2018-06-21 04:38:27,1529545107.0,dataengineering,Deploying machine learning models to production an overview,8snwg8,svpadd2,,https://www.reddit.com/r/dataengineering/comments/8snwg8/deploying_machine_learning_models_to_production/,3.0,0.0,0.0,1273.0,
253,2018-06-25 13:29:02,1529922542.0,dataengineering,Quilt: The Package Manager And Repository For Your Data (Interview),8tpfkf,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8tpfkf/quilt_the_package_manager_and_repository_for_your/,2.0,0.0,0.0,1282.0,
254,2018-06-26 01:14:18,1529964858.0,dataengineering,How to partition by event processing time in Airflow?,8tuoxp,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/8tuoxp/how_to_partition_by_event_processing_time_in/,3.0,7.0,0.0,1283.0,"In this article by the creator of Airflow (https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a) it is mentioned that data should be partitioned by event processing time to land immutable blocks of data. How is this implemented?

For an example, if I have a system with events entering some stream (e.g. Kafka or Kinesis) and then periodically the data is written to storage (e.g. S3 or other) which are then batch processed on some schedule (e.g. airflow), then there are multiple 'time' values to consider.

t1 -&gt; time of the event occurring t2 -&gt; time of the event entering the stream t3 -&gt; time of persisting batch of events to storage t4 -&gt; time of batch run (airflow) for further processing

What is considered the ""event processing"" time in this case? How is a partition generated so that at immutable block of data can be landed predictably? Presumably there must be some deterministic pattern for generating batch runs so that time partitions are immutable and so that backfill tasks can be generated."
255,2018-06-27 01:24:44,1530051884.0,dataengineering,A Beginner's Guide to Data Engineering - The Series Finale,8u4bbj,robert_chang,,https://www.reddit.com/r/dataengineering/comments/8u4bbj/a_beginners_guide_to_data_engineering_the_series/,21.0,3.0,0.0,1285.0,"Hi all,

Data engineering is a very important adjacent discipline to data science, but it is new, often under-appreciated, and rarely discussed in details relative to its close cousin Data Science. I still remembered the first time I was trying to learn Luigi, an open-sourced project from Spotify for ETL, and it was really difficult to find well presented materials out there.

Having worked at Airbnb for a few years, I was really fortunate to learn a few things about data engineering from some of the best people in the industry. As I am acutely aware of this knowledge gap, I hope to contribute to sharing and disseminating knowledge with all of you who are still learning.

You can find my post on Medium: https://medium.com/@rchang/a-beginners-guide-to-data-engineering-the-series-finale-2cc92ff14b0, focusing on data engineering frameworks. If you are completely new to DE, you might consider reading Part I &amp; Part II of the series.

Your feedback, comments, and suggestions are always helpful and welcome!"
256,2018-06-27 22:25:53,1530127553.0,dataengineering,The Two Types of Data Engineering,8ucjx2,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/8ucjx2/the_two_types_of_data_engineering/,7.0,11.0,0.0,1294.0,
257,2018-07-02 02:36:39,1530488199.0,dataengineering,Airflow ETL for Google Sheets and PostgreSQL,8ve2x3,pacunar,,https://www.reddit.com/r/dataengineering/comments/8ve2x3/airflow_etl_for_google_sheets_and_postgresql/,8.0,0.0,0.0,1313.0,
258,2018-07-02 15:24:54,1530534294.0,dataengineering,Integrating Crowd Scale Human Intelligence In AI Projects (Interview),8vi7q3,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8vi7q3/integrating_crowd_scale_human_intelligence_in_ai/,2.0,0.0,0.0,1315.0,
259,2018-07-09 07:10:00,1531109400.0,dataengineering,"""Building Data Flows In Apache NiFi With Kevin Doran and Andy Lopresto - Episode 39"" (https://www.dataengineeringpodcast.com/nifi-with-kevin-doran-and-andy-lopresto-episode-39/)",8x7z8k,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/8x7z8k/building_data_flows_in_apache_nifi_with_kevin/,3.0,0.0,0.0,1337.0,
260,2018-07-09 18:40:27,1531150827.0,dataengineering,New Skills for data engineer [MICROSOFT],8xct84,imba22,,https://www.reddit.com/r/dataengineering/comments/8xct84/new_skills_for_data_engineer_microsoft/,4.0,25.0,0.0,1338.0,"Hey I am a software engineer with expertise in building ETL pipelines using SQL and SSIS. I also code in C#

to write webservices off the datawarehouse which are used by analytics reporting in different parts of our product.

Since I have expertise in SQL, data modelling and programming in general, I wanted to make a shift in data 

engineer role. Can someone suggest what new skills I should pick up and after that how can I model my resume 

to get interview calls? 

FYI: We are primarily a Microsoft technology stack. So please keep the suggestion related to Microsoft (if that's possible)"
261,2018-07-10 05:12:22,1531188742.0,dataengineering,How to learn Data Engineering knowledge and get an internship? • r/cscareerquestions,8xjpi3,danish1234567890,,https://www.reddit.com/r/dataengineering/comments/8xjpi3/how_to_learn_data_engineering_knowledge_and_get/,6.0,0.0,0.0,1338.0,
262,2018-07-10 13:33:13,1531218793.0,dataengineering,Self Service Data Flows With Apache NiFi (Interview),8xns66,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8xns66/self_service_data_flows_with_apache_nifi_interview/,4.0,0.0,0.0,1340.0,
263,2018-07-12 18:03:57,1531407837.0,dataengineering,What should I add to my profile to begin a data engineering carreer (as a software engineer) ?,8yajel,skini26,,https://www.reddit.com/r/dataengineering/comments/8yajel/what_should_i_add_to_my_profile_to_begin_a_data/,8.0,0.0,0.0,1341.0,"Hi everyone,

I'm a software engineer, working mainly on web backend systems that recently developped a new passion about data engineering without knowing that it was an already existing and ""standardized"" field.  


I worked on multiple personal projects where I built data pipelines and analytics systems using technologies such as AWS Kinesis Stream and Data Analytics, Azure Stream Analytics and EventHub and many other related techs such as S3, Azure Blob Storage and MongoDB.

The programming language I use to extract ""transform"" and send the data is Node.js (using Typescript). Many people think that Node.js is not fit at all for this but when you look at it closely, it's really a perfect fit for those use cases.

Node.js is perfect for handling I/O in a fast and efficient way especially when you use streams (it's the best feature of Node.js but also the less known and used).

I also program in Java but I mainly use Node.js nowadays, way easier and faster to create I/O handling programs/scripts.

My main concern is that my knowledge of Node.js will not be recognized in this new field and that people/companies would favor Python. I feel that Python is one of the missing pieces in my profile but I don't feel I really need it, especially if I just focus on data engineering and not data science.

There's also all the hadoop ecosystem + apache spark that I think are really important to know. For now I learned (and still learning) to use managed services such as Kinesis Stream and Kinesis Data Analytics instead of Spark + Storm/Flink for example.

Do you think this is enough to start or is there an important missing piece that I should add before diving in this field ?

Thanks !"
264,2018-07-13 02:18:56,1531437536.0,dataengineering,What Are The Best Books / Resources On Data Engineering?,8yervw,SimpleSwim,,https://www.reddit.com/r/dataengineering/comments/8yervw/what_are_the_best_books_resources_on_data/,9.0,8.0,0.0,1342.0,"I am becoming quite interested in the data engineering world, and there are some roles in my company for this type of work. I want to improve my depth of knowledge on a few things, and there is enough data/project here to get this type of work. Can someone give examples or / clarify actual projects which could cover this criteria (hypothetical projects are fine, it is just to get some idea)? 

1. **Code complex algorithms that traverse multiple data structure types in highly optimized manner to be performant in a big data environment.** What would be an example algorithm that would traverse data structures? 
2. **Data modeling skills for both upstream operational systems and data warehouse fact / dimensional data modeling.** Data modeling = database design? e.g. distribution keys, sort keys etc. 
3. **Distributed systems processing in the area of data ingestion and data distribution.** Using technologies like EC2/EMR? Or how your databases are distributed / how often they update? 
4. **Working with either a Map Reduce or a MPP system on any size/scale.** Again, using technologies like EMR? I use Zeppelin on EMR, but mainly for running Spark and doing ""data science"" work (but have developed scripts, then run them on spark-shell). 

I want to be the one to suggest improvements to the current data structures / pipelines etc, I have strong skills in Python and SQL mainly, so want to ramp up on the theory enough to feel confident and suggest improvements. Therefore, any book / resource recommendations that cover the above and/or more would be great. 

Thanks in advance!"
265,2018-07-16 15:38:20,1531744700.0,dataengineering,"Using Ceph For Highly Available, Scalable, And Flexible File Storage (Interview)",8zak7r,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/8zak7r/using_ceph_for_highly_available_scalable_and/,1.0,0.0,0.0,1356.0,
266,2018-07-16 16:40:30,1531748430.0,dataengineering,Free Lectures playlist for ADVANCE HADOOP Concepts,8zazpb,jiveshgarg1,,https://www.reddit.com/r/dataengineering/comments/8zazpb/free_lectures_playlist_for_advance_hadoop_concepts/,2.0,0.0,0.0,1356.0,
267,2018-07-16 21:07:22,1531764442.0,dataengineering,A typical data engineering project - Netflix perspective,8zd86c,hz265,,https://www.reddit.com/r/dataengineering/comments/8zd86c/a_typical_data_engineering_project_netflix/,27.0,0.0,1.0,1361.0,"Hi all, I just started sharing learninga from DE meetups in the Bay Area and conversations with experienced data engineers in the Bay area. Sharing the first post here: “Personalized Learning Platform For All Technical…” https://medium.com/hasbrain
"
268,2018-07-18 23:43:45,1531946625.0,dataengineering,Small data engineer?,8zz8v1,Chr0nomaton,,https://www.reddit.com/r/dataengineering/comments/8zz8v1/small_data_engineer/,8.0,7.0,0.0,1394.0,"Just saw this post by Jesse Anderson: [http://www.jesse-anderson.com/2018/07/saying-you-have-small-data-isnt-belittling-your-use-case/](http://www.jesse-anderson.com/2018/07/saying-you-have-small-data-isnt-belittling-your-use-case/). I think it's really good, and I agree with the idea.

However, it makes me wonder what the validity behind calling yourself a data engineer, if you don't have a big data use case? "
269,2018-07-20 01:18:42,1532038722.0,dataengineering,Developing and Testing Airflow,90acuk,suhprano,,https://www.reddit.com/r/dataengineering/comments/90acuk/developing_and_testing_airflow/,9.0,0.0,0.0,1397.0,
270,2018-07-22 17:32:51,1532269971.0,dataengineering,Most requested technologies for Data Engineering Jobs,90y4d3,pacunar,,https://www.reddit.com/r/dataengineering/comments/90y4d3/most_requested_technologies_for_data_engineering/,4.0,2.0,0.0,1405.0,
271,2018-07-22 20:12:41,1532279561.0,dataengineering,Architecture and operations of data engineering project in Azure,90z9lr,imba22,,https://www.reddit.com/r/dataengineering/comments/90z9lr/architecture_and_operations_of_data_engineering/,2.0,0.0,0.0,1406.0,"Hey do you guys know if theres any company blog explainging how they have used microsoft azure (and microsoft stack), to perform ETL. I am trying to learn technologies like azure data factory, azure sql db etc. and I was 

wondering how does the operations works. Few question are below:

1. How does source control works in azure data factory.

2. How does releases happen in azure data factory?"
272,2018-07-23 14:13:15,1532344395.0,dataengineering,The future of Data Engineering,9165of,hashtagbrexit,,https://www.reddit.com/r/dataengineering/comments/9165of/the_future_of_data_engineering/,4.0,1.0,0.0,1410.0,"I am looking to hold an event in London next month focusing on the future of data engineering, and would be really keen to get everyone’s input. What are your thoughts? What should be spoken about? What is everyone’s views on what the future holds for data engineering? 

Thanks! "
273,2018-07-24 11:08:31,1532419711.0,dataengineering,Ingest tweets using Kafka and parse in Real Time with Spark Streaming,91fhll,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/91fhll/ingest_tweets_using_kafka_and_parse_in_real_time/,4.0,0.0,0.0,1414.0,
274,2018-07-25 16:45:14,1532526314.0,dataengineering,Data Analysis in the Age of Cambridge Analytica,91ryf8,engineeringbigdata,,https://www.reddit.com/r/dataengineering/comments/91ryf8/data_analysis_in_the_age_of_cambridge_analytica/,2.0,0.0,0.0,1421.0,
275,2018-07-27 00:19:18,1532639958.0,dataengineering,How to architect the perfect Data Warehouse,926bkk,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/926bkk/how_to_architect_the_perfect_data_warehouse/,11.0,5.0,0.0,1427.0,
276,2018-07-27 16:34:19,1532698459.0,dataengineering,What Are Good (Media) Data Engineering + Machine Learning Project Ideas?,92cjvs,SimpleSwim,,https://www.reddit.com/r/dataengineering/comments/92cjvs/what_are_good_media_data_engineering_machine/,4.0,2.0,0.0,1443.0,"I have been given some freedom to create some useful big data sets and publish them for business insights / downstream use. 

I will primarily work on 'video' data (e.g. movies, TV shows) but there is some music and other 'media' data available also. 

There is a tonne of customer data (subscriptions, purchases, etc) and a reasonably robust taxonomy of data on videos (e.g. what category a video is labelled under) already; I want to push the team forward and therefore explore something fresh but still useful.

I would like to derive some project that mixes machine learning in the data engineering space and/or requires map reduce in Python and/or both to also learn something new and upskill. As I am quite junior, I am struggling to find an 'end goal' to work towards. I have asked the business but there isn't much insight provided given this area is a black hole. 

Can someone give me a few ideas, then I can figure out how to make it work technically? 

I would also be interested in resources (books, talks, etc) that cover this space and what cool stuff has been done in the space! It is an exciting area so I feel there is a tonne of untapped potential, just need ideas! "
277,2018-07-27 17:40:17,1532702417.0,dataengineering,Oak Ridge NL is hiring a Data Engineer - Come help us build cool stuff!,92d2sn,jpiburn,,https://www.reddit.com/r/dataengineering/comments/92d2sn/oak_ridge_nl_is_hiring_a_data_engineer_come_help/,0.0,1.0,0.0,1447.0,
278,2018-07-28 23:17:20,1532809040.0,dataengineering,Earn money to promote this link,92p3gy,7183210158,,https://www.reddit.com/r/dataengineering/comments/92p3gy/earn_money_to_promote_this_link/,1.0,0.0,0.0,1455.0,
279,2018-07-29 11:15:23,1532852123.0,dataengineering,Need help in finding best Hadoop hands on tutorial,92tcu5,VividDrama,,https://www.reddit.com/r/dataengineering/comments/92tcu5/need_help_in_finding_best_hadoop_hands_on_tutorial/,12.0,0.0,0.0,1457.0,I am beginner in the field of data engineering. I know some concepts in hadoop and big data. But I dont have much of hands on experience. So can you please suggest some free online resources where I can get some hands on experience.
280,2018-07-31 06:41:18,1533008478.0,dataengineering,Collecting And Analysing Data At Human Scale With Ona And Canopy (Interview),93b1ks,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/93b1ks/collecting_and_analysing_data_at_human_scale_with/,0.0,0.0,0.0,1470.0,
281,2018-07-31 20:31:56,1533058316.0,dataengineering,Interview resources for data engineer interviews,93gjhp,imba22,,https://www.reddit.com/r/dataengineering/comments/93gjhp/interview_resources_for_data_engineer_interviews/,10.0,6.0,0.0,1476.0,"Hey 

1. I have a data engineer interview coming up with Snapchat, I have done all the free question related to sql (under database category) on leetcode. Will this be enough for the interview?
2. I have done some 50 question on leetcode related to arrays and strings, and can handle easy level of leetcode. 
3. I still struggle with DP related question, even medium and easy at leetcode? Should I expect them to be in data engineer interview?

Does someone has any suggestion for the interview?"
282,2018-08-01 17:04:12,1533132252.0,dataengineering,How adopting a distributed rate limiting helps scale your platform,93p88b,atanasovd,,https://www.reddit.com/r/dataengineering/comments/93p88b/how_adopting_a_distributed_rate_limiting_helps/,2.0,0.0,0.0,1477.0,
283,2018-08-01 18:07:59,1533136079.0,dataengineering,What is a Data Pipeline?,93prrw,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/93prrw/what_is_a_data_pipeline/,9.0,1.0,0.0,1477.0,
284,2018-08-02 23:03:26,1533240206.0,dataengineering,What can I do to make my data engineering processing run more efficiently?,942p5x,anonymous-man,,https://www.reddit.com/r/dataengineering/comments/942p5x/what_can_i_do_to_make_my_data_engineering/,1.0,8.0,0.0,1482.0,"So I'm doing a data engineering challenge and it's very straightforward. I have written a Python script (using only built-in modules) that successfully processes small datasets, but I don't yet know how to properly process large datasets and I need to learn this. I'm using Linux, with an i7 processor and 4 GB of RAM.

Basically, here's the project: 

- Read in a .txt file with two columns of data. 

- One column is the name of a product, a second column is the price of the product from some source. The product appears potentially many times, always with a different price. Like this:

    data_input = [['product_name', 'price'], ['bed', '600'], ['chair', '100'], ['bed', '800'], 'chair', '140'], ['fridge', '1000'], ['fridge', '2000'], ['knives', '180']]


- The output data should be 3 columns, where each product appears on one row, with the product name in one column, the total price in a second column, and the total count of products in a third column.

    data_output = [['product_name', 'total_cost', 'total_count'], ['bed', '1400', 2], ['chair', '240', 2], ['fridge', '3000', 2], [['product_name', 'price', 'count'], ['bed', '600', 1], ['chair', '100', 1], ['bed', '800', 1], 'chair', '140', 1], ['fridge', '1000', 1], ['knives', '180', 1]]

So, like I said, I've already written a Python script that does this for a small dataset like above. But when I expand the input data file so it's in excess of 1 GB and my computer lags with millions of rows, the computer just stalls and won't process the dataset.

What software or other language or Python technique or other method do I need to use to make this work so my code will successfully process massive datasets? And where can I look to learn how to do this?"
285,2018-08-04 15:30:18,1533385818.0,dataengineering,Assignment on graph processing using GraphX in Apache Spark,94isfa,sobitan,,https://www.reddit.com/r/dataengineering/comments/94isfa/assignment_on_graph_processing_using_graphx_in/,8.0,0.0,0.0,1485.0,"Graph processing using GraphX in Apache Spark. While working on a simple issue of combining two data-sets by finding and using the common neighbors to generate a new data-set with these values and the similarity measure I realized that we need alot more articles on Spark GraphX problems and solutions. Sure it was fun researching, pair-coding as well as reaching-out to seek advise and clarification, which in turn made me realize that such a great tool should have more published and easily accessible examples online."
286,2018-08-06 13:53:10,1533552790.0,dataengineering,A Whirlwind Tour Of The PostgreSQL Database (Interview),950a8o,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/950a8o/a_whirlwind_tour_of_the_postgresql_database/,1.0,0.0,0.0,1491.0,
287,2018-08-08 12:35:57,1533720957.0,dataengineering,Monitoring Airflow with Prometheus,95kjze,elephantum,,https://www.reddit.com/r/dataengineering/comments/95kjze/monitoring_airflow_with_prometheus/,6.0,0.0,0.0,1503.0,
288,2018-08-10 02:25:36,1533857136.0,dataengineering,15 Proven Steps to Become a Data Engineer | Cracking Hadoop,961ma1,vinibone,,https://www.reddit.com/r/dataengineering/comments/961ma1/15_proven_steps_to_become_a_data_engineer/,1.0,0.0,0.0,1509.0,
289,2018-08-11 04:06:15,1533949575.0,dataengineering,Anybody using AWS Kinesis Data Firehose Data Transformation?,96cn93,[deleted],,https://www.reddit.com/r/dataengineering/comments/96cn93/anybody_using_aws_kinesis_data_firehose_data/,1.0,0.0,0.0,16159.0,
290,2018-08-11 04:16:42,1533950202.0,dataengineering,Anybody using Record Format conversion in AWS Kinesis Data Firehose?,96cpq6,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/96cpq6/anybody_using_record_format_conversion_in_aws/,4.0,0.0,0.0,14336.0,"Anybody using this feature of Kinesis Firehose and care to share their experience? It looks like a convenient way to write event data directly in Orc/Parquet format into S3 for downstream processing.

[https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html](https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html)"
291,2018-08-13 19:40:15,1534178415.0,dataengineering,Lessons Learned While Building A Data Science Platform With Airflow (Interview),96zuz8,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/96zuz8/lessons_learned_while_building_a_data_science/,10.0,0.0,0.0,1526.0,
292,2018-08-15 01:46:31,1534286791.0,dataengineering,Creating Readable Spark Jobs,97czl9,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/97czl9/creating_readable_spark_jobs/,8.0,1.0,0.0,1527.0,
293,2018-08-16 14:39:48,1534419588.0,dataengineering,Why data engineering jobs require backend experience?,97rj8e,sheldonzy,,https://www.reddit.com/r/dataengineering/comments/97rj8e/why_data_engineering_jobs_require_backend/,4.0,4.0,0.0,1540.0,"I'm a 3rd year CS student. Long story short I love data and programming, and from what I understood data engineering is the way to combine both.

Pretty much all of the entry level data engineer job posts I see, require 2+ years in backend development. 

I know that backend is a part of web development, that focuses on the servers, logic, and many of the big stuff. I can see the resemblance due to the communication with databases, but I was wondering if there is any connection to web development in general.

If someone could put in order all the bullshit I just wrote, it would be great :)"
294,2018-08-16 14:54:50,1534420490.0,dataengineering,Anyone did data engineering course in dataquest.io ?,97rme8,sheldonzy,,https://www.reddit.com/r/dataengineering/comments/97rme8/anyone_did_data_engineering_course_in_dataquestio/,1.0,3.0,0.0,1540.0,"Anyone did data engineering course in [https://www.dataquest.io](https://www.dataquest.io) ?

It seems great, but 50$ a month is a lot.

I'm a CS student without any knowledge in databases, and  I want to learn some stuff in this subject throughout my summer."
295,2018-08-16 16:31:21,1534426281.0,dataengineering,Big Data Weekly Newsletter – Big Data News Weekly,97saqi,Veerans,,https://www.reddit.com/r/dataengineering/comments/97saqi/big_data_weekly_newsletter_big_data_news_weekly/,0.0,0.0,0.0,1540.0,
296,2018-08-17 00:00:13,1534453213.0,dataengineering,Why I’m sick of ETL tools and what I use instead (hint: it’s a different ETL tool),97w4ip,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/97w4ip/why_im_sick_of_etl_tools_and_what_i_use_instead/,4.0,0.0,0.0,1543.0,
297,2018-08-17 20:11:55,1534525915.0,dataengineering,Data validation for new data source,9842w2,Chr0nomaton,,https://www.reddit.com/r/dataengineering/comments/9842w2/data_validation_for_new_data_source/,2.0,2.0,0.0,1548.0,"How does everyone here do data validation for new data sources? What are things that you look out for? I'm very new to the field, so I have some really basic things that I look for like amount of data returned on average and when that goes down I say something, but there have been a few instances where I missed something that was really bad. In the last case I broke a lot of stuff because there was a field that was completely null except for when the data scientists pulled them, and it's still unclear to me where they got the fields from. "
298,2018-08-19 01:10:20,1534630220.0,dataengineering,Data Engineer vs. DBA salary / career prospects.,98fen8,_Zer0_Cool_,,https://www.reddit.com/r/dataengineering/comments/98fen8/data_engineer_vs_dba_salary_career_prospects/,2.0,6.0,0.0,1555.0,"Current Data Engineer here,

At one point I wanted to go down the DBA track for my love of databases, data modeling, and in-DB development, but my breadth of skills with general programming etc.. landed me firmly in data engineering role. 

Sometimes, I still fantasize about being semi vendor-specific DBA but in a polyglot persistence environment (Oracle, PostgreSQL, MSSQL, MySQL, etc..).

My questions are as follows --

- Has anyone here converted from Data Engineer to DBA or vice versa? 
- If so, what are the salary differences? 
- Any other relevant experiences?

(Also, for salary it looks I'm already making more than the average DBA, but I'm not sure how accurate Payscale.com and other websites are).

P.S. I already do a lot of DBA tasks, but I envy the autonomy and solo-ish responsibilities of the DBAs I've met. In my experience, Data Engineers typically work in very cohesive teams, but I dislike pair programming and prefer solo work.
"
299,2018-08-19 21:52:05,1534704725.0,dataengineering,ETL from individual JSONs to RDBS: language/framework/tool choice?,98mjyn,slavetoastory,,https://www.reddit.com/r/dataengineering/comments/98mjyn/etl_from_individual_jsons_to_rdbs/,8.0,6.0,0.0,1560.0,"Hey everyone. At my company we have an inventory of API repsonse data, each response sitting in a separate JSON file. I'm responsible for the data pipeline that extracts useful analytics from this data, and as a first stage I extract all data from each JSON file and flatten it for insertion into an RDBS so that raw data can easily be compared.

Although the main language within our team is Python, I decided to go with Go for this stage of the pipeline. The way I go about it is basically: I define a struct that represents the JSON schema, another struct that represents the record to be inserted, and a function that maps one onto the other.

However, there are issues due to the schema not being consistent at all times. There was a moment where we switched endpoints at which point the schema changed for example. Both schemas are not separated at storage, so due to the rigidity of Go I have to do some convoluted things to figure out which is which and use the appropriate struct. Another case was that a long time ago, one value ended up being stored as a float (100.0) as opposed to the int (100) it's been in more recent times. When Go unmarshals the JSON, it throws an error saying it can't map 100.0 to an int, and sets it to zero. Something a language like Python would happily let through. I have found a way using JSON specific types in Go to avoid this issue, however, then it deviates from the elegant bliss I hoped it would be again.

So I do like the idea of clearly defining a struct in Go and having it throw errors as soon as things deviate. On the other hand, I would like things to be a little more flexible, especially in terms of cleanly defining a schema and functionality that decides which one is supposed to be applied. Now I'm a bit overwhelmed and thinking whether it's best to stick with Go implementing solutions for all edge cases is the solution, or perhaps it's best to have another look at using Python and one of the many schema frameworks. Alternatively, maybe I'm missing a really obvious solution here due to my juniorness, as I assume there have to be ready-made solutions for this kind of task?

Any help, advice, criticism, greatly appreciated :) Thanks in advance!"
300,2018-08-20 13:37:09,1534761429.0,dataengineering,"DGraph: A Fast, Distributed, Transactional Graph Database Built For Scale (Interview)",98shr6,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/98shr6/dgraph_a_fast_distributed_transactional_graph/,2.0,1.0,0.0,1566.0,
301,2018-08-23 10:41:20,1535010080.0,dataengineering,Practical Machine Learning basics with The Simpsons,99ldx3,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/99ldx3/practical_machine_learning_basics_with_the/,4.0,1.0,0.0,1578.0,
302,2018-08-25 08:10:15,1535173815.0,dataengineering,Fresher computer science graduate looking to build a strong base for a long future in data engineering field,9a4ffe,MavSidharth,,https://www.reddit.com/r/dataengineering/comments/9a4ffe/fresher_computer_science_graduate_looking_to/,3.0,12.0,0.0,1586.0,"I completed my computer science graduate this year and am looking to build a skill set, along with a portfolio that I can showcase for interviews. 

I am practicing SQL right now, and getting basics of object oriented programming, as I learnt these are the most basic things for a long career in data engineering. 

What should be my next step after this?
What technologies should I learn after these so that I can start applying for jobs? 

The options that I have come across are 

1) learn some NoSQL technologies like Cassandra, MongoDB
2) focus on cloud based technologies, cloud is the future
3) focus on learning Hadoop, Hadoop is the future
4) learn how to use ETL based tools

I am willing to know what my next step should be, is it something from the above mentioned points, or something else. 
"
303,2018-08-26 02:22:42,1535239362.0,dataengineering,apache-airflow 1.9.0 docker based ready to use container.,9ab4di,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/9ab4di/apacheairflow_190_docker_based_ready_to_use/,1.0,0.0,0.0,1588.0,
304,2018-08-26 02:36:06,1535240166.0,dataengineering,apache-airflow 1.9.0 docker based container.,9ab7ju,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/9ab7ju/apacheairflow_190_docker_based_container/,9.0,0.0,0.0,1588.0,
305,2018-08-27 22:30:49,1535398249.0,dataengineering,Using Homomorphic Encryption In Production With Enveil (Interview),9as004,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9as004/using_homomorphic_encryption_in_production_with/,0.0,0.0,0.0,1595.0,
306,2018-08-29 15:01:53,1535544113.0,dataengineering,Best Hadoop blogs/tutorial portals,9b8nie,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/9b8nie/best_hadoop_blogstutorial_portals/,8.0,1.0,0.0,1605.0,What are the best Hadoop blogs/tutorial portals for learning the internals of the system?
307,2018-08-29 22:33:56,1535571236.0,dataengineering,Cross-post: Creating Work Queues with Apache Kafka and Apache Pulsar,9bcg6g,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/9bcg6g/crosspost_creating_work_queues_with_apache_kafka/,1.0,0.0,0.0,1606.0,
308,2018-08-29 22:58:49,1535572729.0,dataengineering,Anyone ever worked with Nexla?,9bco60,nieuweyork,,https://www.reddit.com/r/dataengineering/comments/9bco60/anyone_ever_worked_with_nexla/,2.0,0.0,0.0,1606.0,"These people: https://www.nexla.com/

If you've worked with them, I'd be interested to hear what you think of them, and how comprehensive they are. I'm afraid that they do the easy 80% of the work."
309,2018-08-30 04:22:55,1535592175.0,dataengineering,Data system opens its doors to all Liners : LINE Engineering Blog,9bf8y3,kjmrknsn,,https://www.reddit.com/r/dataengineering/comments/9bf8y3/data_system_opens_its_doors_to_all_liners_line/,0.0,0.0,0.0,1607.0,
310,2018-08-30 14:23:21,1535628201.0,dataengineering,Data engineering position in Madrid Spain |On-site|EU citizenship required|,9biraf,Krzysztof_jankowski,,https://www.reddit.com/r/dataengineering/comments/9biraf/data_engineering_position_in_madrid_spain/,1.0,3.0,0.0,1608.0,"Hey!

&amp;#x200B;

If you are a data engineer who makes his ETL with Python, and have some experience send me your CV at [Christopher@akuaro.com](mailto:Christopher@akuaro.com) and we can chat there :)"
311,2018-09-03 23:18:35,1536005915.0,dataengineering,Building A Master Data Catalog Using Machine Learning (Interview),9cou1g,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9cou1g/building_a_master_data_catalog_using_machine/,1.0,0.0,0.0,1621.0,
312,2018-09-06 07:53:33,1536209613.0,dataengineering,SLAs on failed ETL jobs and backfilling?,9dey0p,Chr0nomaton,,https://www.reddit.com/r/dataengineering/comments/9dey0p/slas_on_failed_etl_jobs_and_backfilling/,6.0,6.0,0.0,1631.0,"Our pipelines are relatively robust and fault tolerant, but issues can still come up. We're facing an issue where, when a job fails, the data scientists both need to know when it happened, and when a backfill will occur. Does anyone have any recommendations on methods to accomplish? I immediately mentally jump to introducing SLAs but I'm not sure those actually make sense for ETL jobs."
313,2018-09-06 19:29:11,1536251351.0,dataengineering,Mirroring an FTP Using lftp and cron,9dkag1,randyzwitch,,https://www.reddit.com/r/dataengineering/comments/9dkag1/mirroring_an_ftp_using_lftp_and_cron/,2.0,0.0,0.0,1632.0,
314,2018-09-07 21:44:07,1536345847.0,dataengineering,"Any advice would be helpful, please!",9dx9aw,mkhan935,,https://www.reddit.com/r/dataengineering/comments/9dx9aw/any_advice_would_be_helpful_please/,4.0,3.0,0.0,1632.0,"Hi, I recently graduated from college in 2017 with a B.S in comp sci. Have spent the last year working professionally as a backend java developer. I have been learning spark and hadoop for the past 6 months, also been learning aws big data stack. How can i switch from backend to a big data developer? how do i show companies I know this stuff and that I know it well? Also i'm a little confused with the all the positions, are data analysts like step 1 before becoming a data engineer?"
315,2018-09-08 10:13:54,1536390834.0,dataengineering,In which case use sparksql vs hive ?,9e2g6m,christogil,,https://www.reddit.com/r/dataengineering/comments/9e2g6m/in_which_case_use_sparksql_vs_hive/,3.0,5.0,0.0,1636.0,"I know both can perform sql queries and be used in ETL processes.
In which case should i use one instead of the other assuming we are in hadoop environment :
structured vs unstructured data? performance ? 

"
316,2018-09-10 05:42:52,1536547372.0,dataengineering,Using Chaos Search To Make Long Term Log Storage Affordable And Useful (Interview),9eiycp,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9eiycp/using_chaos_search_to_make_long_term_log_storage/,3.0,0.0,0.0,1644.0,
317,2018-09-10 18:48:34,1536594514.0,dataengineering,"Building a Real-Time Bike-Share Data Pipeline with StreamSets, Kafka and MapD",9eobyp,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/9eobyp/building_a_realtime_bikeshare_data_pipeline_with/,8.0,0.0,0.0,1647.0,
318,2018-09-13 03:51:44,1536799904.0,dataengineering,Guided Online Projects/ Labs for introduction to Data Engineering?,9fdg8z,SportsballPlayer,,https://www.reddit.com/r/dataengineering/comments/9fdg8z/guided_online_projects_labs_for_introduction_to/,11.0,4.0,0.0,1666.0,"Hello! Forgive me if this is not the right place to post this, but I've been exploring some additonal degree options beyond my current occupation in IT. One of these options is Data Engineering and it seems rather interesting. Is there any good resources/ projects one might be able to do to get a taste of what a data engineer does on a day-to-day basis? "
319,2018-09-14 03:51:04,1536886264.0,dataengineering,Resources on Data management best practices?,9fnkna,dondon492,,https://www.reddit.com/r/dataengineering/comments/9fnkna/resources_on_data_management_best_practices/,8.0,0.0,0.0,1669.0,"Hi everyone, I am sorry if this is the wrong subreddit but I wanted to know if anyone could point me in the direction of data management best practices? Specifically I am looking at information for the ETL process. What are some resources or concepts that you can point me to regarding data management when integrating, normalizing, and storing data into a data warehouse or similar?"
320,2018-09-17 04:09:01,1537146541.0,dataengineering,Which of the following (must have) features would compel you to buy a Machine Learning platform?,9gfy6p,hyper_ex,,https://www.reddit.com/r/dataengineering/comments/9gfy6p/which_of_the_following_must_have_features_would/,3.0,0.0,0.0,1681.0,"Hey everyone! I am a student and am doing a research project for Stanford GSB on machine learning platforms (like Cloudera, Databricks, etc) and wanted to get some feedback from the community. I am performing an analysis to understand the most important features required for these machine learning platforms. Let me know if you want me to add any other options. Will be sharing the results from this analysis at the end.

[https://goo.gl/forms/4sMwflF4L58lUh9j2](https://goo.gl/forms/4sMwflF4L58lUh9j2)"
321,2018-09-17 14:50:37,1537185037.0,dataengineering,Taking Ownership Of Your Web Analytics With Snowplow (Interview),9gjmbx,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9gjmbx/taking_ownership_of_your_web_analytics_with/,1.0,0.0,0.0,1681.0,
322,2018-09-19 00:19:53,1537305593.0,dataengineering,How much would you say the skills you learned was from your job?,9gywm3,plumfu,,https://www.reddit.com/r/dataengineering/comments/9gywm3/how_much_would_you_say_the_skills_you_learned_was/,3.0,3.0,0.0,1689.0,"I’m have a year left of grad school (Masters in CS) and I’m getting so stressed and lost on which career path I should be taking. I’ve been splitting up my courses to see if I can get X skills by the time I’m out and working, but I’m scared because of requirements and whether I’m taking the right courses or not and everything. Do you normally learn Data engineering skills while on the job? Should I be taking data science classes rather than database classes, since I don’t want to be more of a Database Admin role? Right now I’ve been taking both software engineering classes and data science classes, and with planning my schedule I’m just very muddled. Does anyone have any insight on their journey through everything? Thanks for reading."
323,2018-09-21 04:32:03,1537493523.0,dataengineering,AWS vs Azure culture and career path,9hkzco,_Zer0_Cool_,,https://www.reddit.com/r/dataengineering/comments/9hkzco/aws_vs_azure_culture_and_career_path/,5.0,5.0,0.0,1703.0,"Junior (contacted) Data Engineer here,

[NOTE: This is a request for 100% subjective opinions and experiences]

So far I've only worked with Azure as a Data Engineer.

However, I've always been an open source-y, cross-platform type of fellow and....I feel torn.

I'm a Pythonista and I love Linux, PostgreSQL, Docker, and other open source tools. So naturally I'm drawn towards that spectrum. Also, I find myself in the company of AWS fanboys alot at local Meetups etc.. 

Microsoft has done a wonderful job winning back my affections recently with its Python support and it's open sourcing of .NET and PowerShell and with its SQL Server Linux support, but I still feel torn....

In my job, although I get to use Python, I'm constantly on Windows VMs, Docker is nowhere to be seen, and we are hard-pressed to be able to use Linux VMs at all which bums me out. MS might have embraced Linux, but MS-shop developers don't use or understand Linux and don't necessarily like non-MS open source stuff. There's even some (slight) resistance to Python.

Additionally... While I like SQL Server, I don't think T-SQL is as sexy as Postgres' PL/pgSQL or even Oracle's PL/SQL. 

I'd like to work with open source technologies and databases on Linux.

Here are my questions to more experienced Data Engineers... 

Is the grass really greener in AWS-land? 

Do you find that you get to use the technologies that you love? -- If so, which technologies and tech stacks?

Do you feel less restricted by your company's development culture than I do?

P.S. I'd like to hear from folks on both sides, and especially those who have ""played ball"" for both teams. 


"
324,2018-09-21 18:31:22,1537543882.0,dataengineering,What do data engineers do? I’m having trouble knowing if what I do falls into that realm.,9hr1wi,MsCardeno,,https://www.reddit.com/r/dataengineering/comments/9hr1wi/what_do_data_engineers_do_im_having_trouble/,5.0,14.0,0.0,1705.0,"Hi all,

I apologize if this doesn’t belong here but I have a question that has been bugging me for quite some time. I found this sub and I’m hoping it may shed some light for me.

So I work at a large F100 company. My job title is “business analyst” but we are always referred to as “hybrids” and we work in data process management.

Basically our team (about 50 people) transform data and load up Oracle SQL tables that produce other feeds that out actuaries (or other business partners) use. 

Essentially, we work with the BPs and gather the requirements. We work with the developer to set up our Oracle SQL tables and make table updates. I use SQL to query our databases to validate the correct changes were made. And we configure and run all of our processes in our very complex tool kit.

One project I have been working on is taking this feed (3 million+ records) and transforming and parsing this file and loading into several SQL tables I created with the help of a developer. I created the whole process and designed the whole architecture. I am going to be doing this again for many other large feeds. My team also uses Oracle BI a lot but I haven’t been exposed much yet. 

Is this considered data engineering? Or is the simple data analysis? I see jobs for “data engineers” but I’m hesitant to ever apply bc I’m unsure if it’s above my playing field. They also usually look for CS degrees and my degree is in Marketing but have 3 years experience doing what I do.

Any insight is greatly appreciated! Thanks everyone! "
325,2018-09-21 22:52:08,1537559528.0,dataengineering,Working as a Data Engineer and Reporting Analyst?,9htfsd,magpie_killer,,https://www.reddit.com/r/dataengineering/comments/9htfsd/working_as_a_data_engineer_and_reporting_analyst/,2.0,4.0,0.0,1705.0,"Who here has a role that requires them to do both data engineering and business intelligence (data analysis, report building, web analytics, etc) ? Are you enjoying doing both/all the roles? What does your tech stack look like and what would you change if you could?  Are you doing multiple roles because you want to or because your org is short-staffed?"
326,2018-09-22 06:53:24,1537588404.0,dataengineering,Do you need Java or scala for next generation data engineering?,9hwtad,sasikanth88,,https://www.reddit.com/r/dataengineering/comments/9hwtad/do_you_need_java_or_scala_for_next_generation/,2.0,8.0,0.0,1707.0,
327,2018-09-22 19:06:44,1537632404.0,dataengineering,Striking out on your own?,9i0sky,rolkien29,,https://www.reddit.com/r/dataengineering/comments/9i0sky/striking_out_on_your_own/,5.0,5.0,0.0,1709.0,"I am a data analyst thinking about trying to make the leap into data engineering. As data engineers, how easy or hard is it to take those skill sets and strike out on your own, as a consultant or some sort of startup?"
328,2018-09-23 21:09:42,1537726182.0,dataengineering,Seeking advice on data analysis workflow,9iah27,sgallagher89,,https://www.reddit.com/r/dataengineering/comments/9iah27/seeking_advice_on_data_analysis_workflow/,1.0,3.0,0.0,1712.0,"Hello, I'm working on a data analysis platform. My data source is compressed log files in an AWS S3, they are a proprietary binary format (which adds its pains, I have a custom Python app for decoding them) each around 30mins worth of data, the data contains around 6000 channels logged every 0.5s.

I have been getting by on local processing on my machine by downloading the files. However as the data has grown I've found this approach is lacking. The bottlenecks are (unsurprisingly) downloading the files and decompressing them.

My idea is to scale this out to a multi node cluster, using docker to contain the analysis apps. I'm getting stuck on how to split up the work. The workflow is as follows:

Download file -&gt; decompress -&gt; memory map -&gt; load relevant data -&gt; run several aggregating functions -&gt; store results in DB

Theres obviously lots of ways I could do this, here are some ideas:

1. A single docker app running the full process, spinup lots to get the work done
2. Smaller micro service apps covering each part of the workflow, sending relevant parts between (is it silly to try to send the file between containers)
3. A larger data platform, like Spark, that can access the files in the S3 (would this require plugins to open the custom files?)

Thats as far as I've got. I'm not the most experienced in this, but I'd like to set off on the right path to get the work done properly. Thanks in advance to anyone willing to offer thier advice.

&amp;#x200B;"
329,2018-09-24 00:45:32,1537739132.0,dataengineering,"If you have a year and a half left at college to graduate, what should you do to be prepared to apply for a data engineer position?",9ic8z7,ASamir,,https://www.reddit.com/r/dataengineering/comments/9ic8z7/if_you_have_a_year_and_a_half_left_at_college_to/,3.0,5.0,0.0,1713.0,I have a fair background in AI. I completed the Deep Learning nanodegree program. I've been doing competitive programming for almost 3 years now. But I'm more interested in working with data specifically. Building the architecture and pipelines for it. But I have no idea where to start. Any help?
330,2018-09-24 05:42:15,1537756935.0,dataengineering,Big Data Curation Strategies (Interview),9iecrm,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9iecrm/big_data_curation_strategies_interview/,2.0,0.0,0.0,1716.0,
331,2018-09-24 18:12:10,1537801930.0,dataengineering,The Case of the Broken Lambda,9iiqsj,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/9iiqsj/the_case_of_the_broken_lambda/,8.0,0.0,0.0,1719.0,
332,2018-09-25 15:09:38,1537877378.0,dataengineering,Getting started with Apache Airflow,9irk0d,pknerd,,https://www.reddit.com/r/dataengineering/comments/9irk0d/getting_started_with_apache_airflow/,23.0,0.0,0.0,1725.0,
333,2018-09-27 07:05:57,1538021157.0,dataengineering,Where to learn more in-demand DE technologies?,9j9n4z,Blarglephish,,https://www.reddit.com/r/dataengineering/comments/9j9n4z/where_to_learn_more_indemand_de_technologies/,11.0,14.0,0.0,1733.0,"Where can I learn and practice with more in-demand DE technologies? (Hadoop, Hive, AWS, Snowflake, Scala, Spark, Sqoop, Airflow, Kinesis, Kafka, Spark Streaming, Flink, Nifi, DynamoDB, Jenkins, Circle CI, etc.)

I know this is a long list, but I see these technologies pop up a lot. Google searching seems like a good place to start, but I wonder if it is necessarily the best option as most of these technologies appear to be suited to the enterprise. Would I be able to easily set up my own AWS or Hadoop environment on my laptop, and learn from there?

I ask because, after four years working in a DE+Data Science role at a large tech company, I have found that I am familiar with many of the common tasks and workflows of data engineering, but my company has favored proprietary or in-house tools over more standard and/or open-source tools. I'm just now discovering how un-marketable this makes me when searching for other DE oriented jobs. However, it seems like I would need to work at a large enterprise if I were to learn some of these tools, since these are more suited for large enterprises.

Any good resources that could help me out?"
334,2018-09-30 18:32:46,1538321566.0,dataengineering,Want to learn commands of linux command line for data science/data engineering jobs,9k77st,MavSidharth,,https://www.reddit.com/r/dataengineering/comments/9k77st/want_to_learn_commands_of_linux_command_line_for/,3.0,4.0,0.0,1747.0,"I graduated my bachelors in Computer Science this year and want to become a data engineer in the future, will be applying for basic data analyst/ETL/ data engineer jobs soon enough.

It has come to my knowledge that being good at Linux command line can help me go a long way

Any help regarding how to learn these commands, by tutorials, or online videos, or any book, anything will be appreciated

Thank you :)"
335,2018-10-01 23:52:21,1538427141.0,dataengineering,Positive and Negative Data Engineering,9kk8j2,sweml,,https://www.reddit.com/r/dataengineering/comments/9kk8j2/positive_and_negative_data_engineering/,1.0,0.0,0.0,1751.0,
336,2018-10-02 03:40:58,1538440858.0,dataengineering,Which tool to use in today's day and age for Star Schema ETL?,9km2e6,jwdatascience,,https://www.reddit.com/r/dataengineering/comments/9km2e6/which_tool_to_use_in_todays_day_and_age_for_star/,1.0,8.0,0.0,1754.0,"Hello,

I was wondering if anyone had experience with my situation:

* I have a Redshift cluster which stores Stitch data from a couple of our CRM/Sales systems
* We will continue to store other system databases in this redshift warehouse in the future

My goal is to start creating star schemas for reporting given all these transactional schemas living in Redshift. However, this brings up a couple of questions:

1. How expensive is it to both store and do transformations all within Redshift? Is my suggestion an efficient way to do things or is there a better way?
2. What tool(s) should be used to even create star schema transformations on top of Redshift? Hive/spark come to mind as buzzwords but I really have no idea which one is optimal for my use case.
3. Are there any guides/tools/articles to guide me that you know of? 

Thanks a ton. I appreciate any/all help."
337,2018-10-02 04:59:49,1538445589.0,dataengineering,The Data Engineering Behind A Real-World Knowledge Graph (Interview),9kmnma,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9kmnma/the_data_engineering_behind_a_realworld_knowledge/,1.0,0.0,0.0,1754.0,
338,2018-10-03 00:15:54,1538514954.0,dataengineering,Setting up Redshift with PostgreSQL in a VPC,9kuq5n,will_nowak,,https://www.reddit.com/r/dataengineering/comments/9kuq5n/setting_up_redshift_with_postgresql_in_a_vpc/,1.0,0.0,0.0,1761.0,"I found this process to be a real pain, so I wrote an explicit step by step tutorial of how to set up a Redshift database using EC2 instances (and a VPC (Virtual Private Cloud)).

[https://medium.com/@williampnowak/amazon-redshift-setup-tutorial-for-use-with-aws-vpc-ec2-and-postgresql-30281622e3b9](https://medium.com/@williampnowak/amazon-redshift-setup-tutorial-for-use-with-aws-vpc-ec2-and-postgresql-30281622e3b9)

Hope this can save someone the effort I spent to get this basic setup running. Share freely!"
339,2018-10-03 17:10:36,1538575836.0,dataengineering,I’m at a fork in the road in choosing between two completely different career paths post graduation. Can someone help me understand a little more about my opportunities in data engineering before I make a decision?,9l1i2d,cheezenutz43,,https://www.reddit.com/r/dataengineering/comments/9l1i2d/im_at_a_fork_in_the_road_in_choosing_between_two/,1.0,2.0,0.0,1761.0,"I’ll graduate with a Bachelor of Science in Accounting and Finance. I got really lucky in this data engineering offer, but I’m afraid I’m going to limit myself with my lack of knowledge on the industry and just get used as a “pawn” for a big corporation with no room for upward mobility.

First off, I have two job offers/potential career paths.

**Offer 1:** auditor for a public accounting firm (smaller mid size): $57,000. There are 4 promotion levels before reaching partner. After 5 years I’m expecting to make 65-75k After 10 years I’ll expect to make 80-$120k. Partners make (estimated) between $170-$400k  and that will be after 15-20 years. And after a while with the firm I can make equity partner and earn much more. 

- Pro’s: I’ve worked as an intern for 11 months total, they love me. I feel like my work is valued and I contribute a lot. I’ll be able to get my CPA to differentiate myself and allow for further job opportunities down the road.

- Con’s: work life balance is not good (50-70 hours a week, and it increases slightly with each promotion, until you reach partner) neither are benefits. 

**option 2:** Data engineer: $70K starting salary. No idea where it goes from there, all I know is that I’ll be getting paid more to start, I’ll have a better work life balance, and better benefits. I know the next promotion level is senior and that’s about it.

**Questions:**

- is my idea that I might just get used as a pawn with no room upward mobility a possibility? I find it kind of odd that I was able to get this position without any knowledge of engineering or computer science. It is in their business sector, so it makes sense, but I’m worried about what my career path looks like.

- what is my salary potential? What are he different career paths I can take? What will be my expectations in regards to further education? 

- am I hurting myself by not getting my CPA? What are other certifications I can get in the field to differentiate myself in the industry?

- is there anything else I should know?


My position as a data engineer will be strictly related to the business sector. I know big data is changing the world, in fact it’s changing my current profession entirely. I do have a strong interest in data and analyzing data, and making decisions based on data presented (I know that wouldn’t be my first year role, but it’s something I’m interested in for the future). The career itself is more appealing to me than an auditor because I feel like I am actually bringing value to the business. 

I have decent knowledge of what I’ll be doing as an employee, and I’m not worried about learning the software, methodology, etc. because I know they have a rigorous training process. 
"
340,2018-10-04 08:47:58,1538632078.0,dataengineering,Schedule web scrapers with Apache Airflow,9l9784,pknerd,,https://www.reddit.com/r/dataengineering/comments/9l9784/schedule_web_scrapers_with_apache_airflow/,1.0,0.0,0.0,1763.0,
341,2018-10-04 19:43:41,1538671421.0,dataengineering,DWH Team name,9ldl1u,serkef-,,https://www.reddit.com/r/dataengineering/comments/9ldl1u/dwh_team_name/,1.0,4.0,0.0,1764.0,"Looking for some inspiration, we are looking for a code name for our data warehouse team.

Any ideas? "
342,2018-10-05 00:18:39,1538687919.0,dataengineering,What are your thoughts on recruiters?,9lg11q,mardavarot93,,https://www.reddit.com/r/dataengineering/comments/9lg11q/what_are_your_thoughts_on_recruiters/,1.0,6.0,0.0,1767.0,"Just curious about your perception on recruiters working at companies like Google, Facebook or Netflix.

Do they reach out to you? Do you hate them? Do you enjoy seeing what opportunities are out there?

Yes I'm a recruiter, not trying to recruit anyone. Just trying to learn more about the market and how people feel."
343,2018-10-05 07:13:00,1538712780.0,dataengineering,Dr. Martin Loetzsch - ETL Patterns with Postgres,9lj2vc,pacunar,,https://www.reddit.com/r/dataengineering/comments/9lj2vc/dr_martin_loetzsch_etl_patterns_with_postgres/,1.0,0.0,0.0,1768.0,
344,2018-10-05 21:46:12,1538765172.0,dataengineering,Building a warehouse from the ground up,9lp1k9,lady_junior,,https://www.reddit.com/r/dataengineering/comments/9lp1k9/building_a_warehouse_from_the_ground_up/,1.0,9.0,0.0,1772.0,"I'm new to the data engineering role and I've started at a company that is just entering into the 
""move/store"" level of the [AI hierarchy](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007).

Currently everything is stored in a MySQL database on a Heroku server or in an ElasticSearch database.
My primary focus is on the MySQL data. Before I got here, someone decided to do monthly aggregations of 
the most minimally useful data and use that for reporting. This has been fine for bare bones situations, 
however, with the hiring of a data scientist and myself, the reporting requests are getting more complicated 
and we don't have access to the data we need so we've been pushing for the chance to build a data warehouse for
analytics and eventual machine learning projects.

I've been doing a lot of reading and I've read the Kimball Data Warehouse book and I know about OLTP vs OLAP
and SQL vs NoSQL and all that jazz, but what I'd like to know: if you were asked to build a data warehouse 
right now and start turning out basic metrics quickly, what would be your first couple of steps?

My gut reaction is to create a replica of our MySQL app database in a Postgres one without PII data and as we
start building out our reports, I can figure out the best way to normalize the data for better performance
and better insight, but that doesn't seem to sit well with the person who would give me access to the MySQL
database because it won't have ""business logic"" applied to it. I personally feel like the data _shouldn't_ have business logic applied to, but I don't know enough to argue.

I'd love to know about other peoples experiences and what steps they took to get the ball rolling."
345,2018-10-08 04:55:54,1538963754.0,dataengineering,Serverless Data Engineering in Azure,9mass4,edholland,,https://www.reddit.com/r/dataengineering/comments/9mass4/serverless_data_engineering_in_azure/,1.0,0.0,0.0,1787.0,
346,2018-10-08 08:39:17,1538977157.0,dataengineering,xkcd: Data Pipeline,9mc7o3,flipstables,,https://www.reddit.com/r/dataengineering/comments/9mc7o3/xkcd_data_pipeline/,1.0,1.0,0.0,1788.0,
347,2018-10-09 15:21:05,1539087665.0,dataengineering,"Fast, Scalable, and Flexible Data For Applications And Analytics On MemSQL (Interview)",9monpj,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9monpj/fast_scalable_and_flexible_data_for_applications/,1.0,0.0,0.0,1792.0,
348,2018-10-09 20:55:42,1539107742.0,dataengineering,Insight Data Engineering Fellowship coming to Seattle,9mrf65,reporterhoa,,https://www.reddit.com/r/dataengineering/comments/9mrf65/insight_data_engineering_fellowship_coming_to/,1.0,0.0,0.0,1793.0,"Insight Data Engineering Fellowship program [is coming to Seattle](https://blog.insightdatascience.com/insight-data-engineering-devops-engineering-land-in-seattle-3826bc52b4de). After a year running the [Insight Data Science Fellowship out of Seattle](https://blog.insightdatascience.com/seattle-insight-data-science-one-year-anniversary-9e51153d3779), we are adding two new offerings in Seattle -- a Data Engineering Fellowship and DevOps Engineering Fellowship. (NYC [also will get](https://blog.insightdatascience.com/insight-devops-engineering-expands-to-nyc-2723ddd574c4) a DevOps program). Applications are now open for all programs."
349,2018-10-10 15:41:12,1539175272.0,dataengineering,Flink Vs Spark | Apache Flink is successor to Hadoop and Spark,9mz3eq,jiveshgarg1,,https://www.reddit.com/r/dataengineering/comments/9mz3eq/flink_vs_spark_apache_flink_is_successor_to/,1.0,0.0,0.0,1796.0,
350,2018-10-11 16:41:21,1539265281.0,dataengineering,Best of Linux Academy,9na29t,MavSidharth,,https://www.reddit.com/r/dataengineering/comments/9na29t/best_of_linux_academy/,1.0,6.0,0.0,1801.0,"Recently I asked a question about learning Linux command line and the reddit community gave me some amazing suggestions **&lt;3**. One of those suggestions was Linux Academy and turns out this website has 2 months of free trial period and a lot more content which actually I was looking for other than Linux command line for data science ( data engineering to be specific ) 

Anyone here who has used or using Linux Academy at the moment? 

If yes, 

**What was the best course you came across on Linux Academy ( purely your own personal experience and opinion )**

I am asking this because I am starting the 2 month free trial period , and really want to make the optimal use of it !! "
351,2018-10-11 17:03:59,1539266639.0,dataengineering,Looking for recommendations of Kafka architectures for Machine Learning Practitioners,9na8qx,tzoom,,https://www.reddit.com/r/dataengineering/comments/9na8qx/looking_for_recommendations_of_kafka/,1.0,6.0,0.0,1802.0,"I'm trying to get ramped on Kafka and particularly how to deploy ML models connected to it. I'm a ML practitioner with experience coding, building ML models, stats. However, I'm new to most distributed architectures and Kafka as a whole.

Do you all have any recommended slides, documents, or videos that walk through how ML models can be deployed in Kafka? To simplify discussion, assume the ML model is built from scikit-learn (i.e. no Spark, Tensorflow, etc.) Some key topics I'm interested include:

* How to architect a distributed feature store that a ML model can query?
* How to architect a consumer that wraps/calls a ML model, particularly if the input data comes from outside the topic or consumer?
* Other information related to passing data in the distributed Kafka manner and getting it to a single ML model to consume and act on."
352,2018-10-15 14:45:45,1539603945.0,dataengineering,Iceberg: Improving The Utility Of Cloud-Native Big Data At Netflix (Interview),9obxqc,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9obxqc/iceberg_improving_the_utility_of_cloudnative_big/,1.0,0.0,0.0,1814.0,
353,2018-10-15 17:52:16,1539615136.0,dataengineering,Pipeline Pitfalls,9odcw4,sweml,,https://www.reddit.com/r/dataengineering/comments/9odcw4/pipeline_pitfalls/,1.0,0.0,0.0,1814.0,
354,2018-10-16 00:09:01,1539637741.0,dataengineering,"Example ""Case Studies"" of ML model management and feature stores in the cloud",9oguwx,tzoom,,https://www.reddit.com/r/dataengineering/comments/9oguwx/example_case_studies_of_ml_model_management_and/,1.0,3.0,0.0,1814.0,"I was wondering if anyone can recommend articles from industry on managing deployment and updating of ML models and feature stores in a scaled environment. 

For example,

1. What distributed databases they used for real time features needed for the ML model predictions.

2. What workflow was used to identify when to retrain, how it's triggered, and how to gracefully redeploy.

3. Logging and version control management for ML model versions, particularly in A/B testing or for when failover conditions execute.

Etc.

Thank you!"
355,2018-10-17 01:44:57,1539729897.0,dataengineering,Resources for Interview Preparation,9osmt3,sharadov,,https://www.reddit.com/r/dataengineering/comments/9osmt3/resources_for_interview_preparation/,1.0,2.0,0.0,1823.0,"I have about 12 years experience as a Database Developer/Administrator based in the Bay Area. Looking for a new gig and wanted to know what are the best resources  out there for interview preparation?

What should I be focussing on?"
356,2018-10-17 19:53:00,1539795180.0,dataengineering,Airflow 101: Automating your workflow for complex data pipelines,9p0f51,mwakanosya,,https://www.reddit.com/r/dataengineering/comments/9p0f51/airflow_101_automating_your_workflow_for_complex/,1.0,0.0,0.0,1826.0,
357,2018-10-18 13:09:34,1539857374.0,dataengineering,Why physical storage of your database tables might matter,9p7wtj,apoorva_ag29,,https://www.reddit.com/r/dataengineering/comments/9p7wtj/why_physical_storage_of_your_database_tables/,1.0,3.0,0.0,1830.0,
358,2018-10-19 00:03:37,1539896617.0,dataengineering,Finding My Way to Insight DevOps,9pd955,rastorbean,,https://www.reddit.com/r/dataengineering/comments/9pd955/finding_my_way_to_insight_devops/,1.0,1.0,0.0,1832.0,"Career transitions are always challenging. Program Director John Taylor, who recently finished Insight's first DevOps Program, shares his journey from being an traditional infrastructure engineer to adopting DevOps culture. [https://blog.insightdatascience.com/finding-my-way-to-insight-devops-42a194ff7ab](https://blog.insightdatascience.com/finding-my-way-to-insight-devops-42a194ff7ab)"
359,2018-10-19 17:23:41,1539959021.0,dataengineering,DataCamp is hiring course instructors,9pka4p,emm-kay,,https://www.reddit.com/r/dataengineering/comments/9pka4p/datacamp_is_hiring_course_instructors/,1.0,0.0,0.0,1835.0,
360,2018-10-25 00:35:49,1540416949.0,dataengineering,Why You Can’t Do All of Your Data Engineering with SQL,9r3z8y,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/9r3z8y/why_you_cant_do_all_of_your_data_engineering_with/,1.0,0.0,0.0,1903.0,
361,2018-10-25 06:01:26,1540436486.0,dataengineering,Beginner needing some advices on the DE carrer,9r6hrw,dharpuia,,https://www.reddit.com/r/dataengineering/comments/9r6hrw/beginner_needing_some_advices_on_the_de_carrer/,1.0,4.0,0.0,1905.0,"Hello folks. Not really sure this subreddit is a good place for posting this type of thread, but I'm going to risk it.  


I came here so I can share my experience as a Jr. Software Analyst and a data engineering enthusiast. My goal is to share my experiences and see/read somebody else's opinions about what I've been going through. The reason that I'm doing this is that I don't have any personal contact with anyone on the analytics/data engineering field, so I would like to read other people experiences.  
Well, I have one year worth of professional experience with background on BI and relational databases. Example of tasks that I would commonly work on: database maintenence, query optimizations, data modelling, development of ETL processes, etc.  


The first project that I happened to participate at my company was a BI project. It was a project with a few team members and I was the only one in the team with an IT tech background. I've got a really nice feedback from my superiors, but I had a lot of flexibility with the architectural decisions that I had to make in the project, since no one in the team had an IT background as me.   


This project's contract unfortunatelly was not renewed and then I got reallocated on another project. The new project has a team fully composed of IT professionals. As days gone by, my co-workers became aware of how inexperienced I am, and now I feel like they don't fully trust any decisions that I make. They are mostly developers and have different views compared to me, who has been working mostly on the database side. My co-workers work at a different branches (not in the same physical place) and I believe this make it even more difficult to propose any solution, since they are always backing each other up and not agreeing with me.   


Even though I'm inexperienced, I'm currently responsible for the ""data aspect"" of the project (collecting, loading, modelling optimizing access, integrating, collecting ""data requirements"", etc.). I've been doing well with it (most of the time, that is). What makes me really worried is that I need to propose a solution to a data engineering ETL problem in a Cloud environment. I could make all sorts of architectural decisions to solve the problem, but such decision, once made, could be vital for the whole project. 

&amp;#x200B;

In my professional life, I don't have any reference (or co-worker) in the data engineering field.  With a year of experience, I feel so pressured to come up with a solution that will scale well. I almost quit my job once because I was feeling I was not going anywhere. I feel like, even if I come up with a good solution, my colleagues are probably not going to accept it that easily. My inexperience make me feel so insecure, but I need to be strong to stand by what I've been searching/proposing, and this is really difficult when you're all alone.  


I believe there are few people in the world working as a data engineers. It is a new carreer after all with all sorts of tools and architectural solutions. Most of you may also know/work with few team members in the DE field. If anybody could give me some sort of advice or share your experiences. I would be grateful!  


(Sorry for my english, I'm not really a native speaker)  
"
362,2018-10-25 22:26:52,1540495612.0,dataengineering,Big Data Engineers Job Outlook,9rdfpb,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/9rdfpb/big_data_engineers_job_outlook/,1.0,11.0,0.0,1910.0,"A few questions about outlook (maybe 3-4 years). Want to make sure I am consciously improving and staying ahead of the curve and doing stuff on the side to work on my career.

* Curious about what the outlook is for big data engineers. Does it scale 1 for 1 along with Data Scientists? 
* How likely could Data Engineering be outsourced outside of US?  
* What specific DE skills are currently highly valued and any insight on how that might change in a few years?

Background: I just started a somewhat entry-level Data Engineering job (or so I think). Been a python engineer for 2-3 years prior. Currently, I feel like I just write a lot of python scripts to move data into different places (data warehouses, aggregate/roll up data, clean data, importing code into Spark and run jobs) and do a bunch of ETLs. Learning a lot of Big Data technologies in the meantime(Spark, Pig, Hadoop).  Not sure if this even Data Engineering tbh."
363,2018-10-29 15:40:25,1540820425.0,dataengineering,How Netflix Is Using Jupyter Notebooks In Production (Interview),9sd687,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9sd687/how_netflix_is_using_jupyter_notebooks_in/,1.0,0.0,0.0,1928.0,
364,2018-10-30 16:41:59,1540910519.0,dataengineering,Apache Airflow best deployed on Qubole (Managed BigData Platform) or with EKS/k8s ?,9soj06,sulabhc,,https://www.reddit.com/r/dataengineering/comments/9soj06/apache_airflow_best_deployed_on_qubole_managed/,1.0,2.0,0.0,1935.0,
365,2018-10-30 21:27:12,1540927632.0,dataengineering,"ETL framework for Python, need suggestion.",9sr5zi,imba22,,https://www.reddit.com/r/dataengineering/comments/9sr5zi/etl_framework_for_python_need_suggestion/,1.0,10.0,0.0,1936.0,"What is the best ETL framework for doing ETL using Python. We have been using SSIS for it, and sources of our dimensions and fact tables are strored procedure in sql server. Now as a team we are fed up of SSIS

because it is difficult to maintain it with GIT, it is slower and just difficult to work with. I was thinking since we have all the sources in sql server it should be relatively easy for us to use Python to do the ETL.

After building this, my intentions is to deploy it on azure.

With my situation what would be best ETL framework for python ? Recommendations?"
366,2018-11-04 16:42:26,1541342546.0,dataengineering,CS degree,9u3tno,rolkien29,,https://www.reddit.com/r/dataengineering/comments/9u3tno/cs_degree/,1.0,7.0,0.0,1964.0,Do you think a CS degree is beneficial to breaking into data engineering?  I have an MBA and a liberal arts bachelors. I have been a data analyst for a few years and want to grow into data engineering. I see most data engineer job posts ask for a bachelor's in CS but mostly want experience. Is it a good idea to get a Master's in CS to get that foundation of knowledge/ get my first data engineering job?
367,2018-11-04 19:22:32,1541352152.0,dataengineering,Apache Spark and or Kafka classes,9u54ff,RadiantPancakr,,https://www.reddit.com/r/dataengineering/comments/9u54ff/apache_spark_and_or_kafka_classes/,1.0,5.0,0.0,1967.0,"I'm currently a BI Analyst that has transitioned into a data engineering role. I mostly build data sets, create SSIS jobs, and am currently helping build a new data warehouse. My boss would like me to learn Apache spark and or Kafka. What are some good courses to take or open source project to work on in order to leant these skills. "
368,2018-11-05 16:37:54,1541428674.0,dataengineering,Easy And Powerful Self Service Business Intelligence With Looker (Interview),9ue0hh,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9ue0hh/easy_and_powerful_self_service_business/,1.0,0.0,0.0,1967.0,
369,2018-11-05 20:07:49,1541441269.0,dataengineering,Six Rules for Deploying your Machine Learning Models Faster,9ufwvn,mike_from_statefarm,,https://www.reddit.com/r/dataengineering/comments/9ufwvn/six_rules_for_deploying_your_machine_learning/,1.0,0.0,0.0,1973.0,
370,2018-11-06 17:25:47,1541517947.0,dataengineering,Any good resources for learning the anatomy of a Data Pipeline?,9up88z,Conjoined_Triangles,,https://www.reddit.com/r/dataengineering/comments/9up88z/any_good_resources_for_learning_the_anatomy_of_a/,1.0,5.0,0.0,1979.0,"There's many different tools I can use in a data pipeline and I'm able to find resources on comparisons of those tools, but I'm having a hard time trying to find the proper anatomy and the decision making process of creating a data pipeline. I feel that my knowledge of data engineering is at the point in which I know use cases of several interchangeable technologies, but I don't know how to glue any of the compatible ones together to form a data pipeline."
371,2018-11-08 19:41:32,1541698892.0,dataengineering,"Why did you want to become a data engineer? Also, what are surprising aspects of the job that you like but that you weren't expecting? And what are some things you like most about the job?",9vc41v,HAL9000000,,https://www.reddit.com/r/dataengineering/comments/9vc41v/why_did_you_want_to_become_a_data_engineer_also/,1.0,9.0,0.0,2046.0,
372,2018-11-08 23:06:50,1541711210.0,dataengineering,Creating a Data Engineering Culture,9vdyal,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/9vdyal/creating_a_data_engineering_culture/,1.0,0.0,0.0,2046.0,
373,2018-11-09 07:47:22,1541742442.0,dataengineering,Contacted for data engineer role amazon. Advice?,9vhwme,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/9vhwme/contacted_for_data_engineer_role_amazon_advice/,1.0,13.0,0.0,2048.0,"I just had a baby so have a lot going on right now so I don't know if I'll even pursue it but if I do I want to know what to expect in terms of:

1. What is the interview process like?
2. Will they give me prep material for it - I have heard they do
3. How long will they give me to prep?
4. What will be the main areas they will test me?

**My current situation**
I am data architect / technical lead working for a financial institution. Have ~6 yrs exp.

Our Data Stack is what's considered traditional - RDBMS is Oracle/Postgres/SQL server, ETL is Informatica, linux server and reporting/visualizations is Business Objects and Tableau.

My SQL and database knowledge is pretty high. I have a good understanding and experience with ETL patterns/data warehousing as well. However, when it comes to ""modern"" data skills I've taught myself enough to know whats out there and know how to use it on a superficial level - spark, nosql, Hadoop, emu etc.

Since we use informatica we don't use a general purpose programming language for our ETL. But I do know python and the popular libraries for data ""stuff"" - pandas, numpy etc. 

My ability to solve algorithms leetcode style is poor. I'm working on it.

my current job is comfortable - good boss, decent pay, always get my 20% bonus, work from home, full benefits, 6 weeks pto, 6 weeks paternity. I don't work more than 40 hrs a week. Only complaint is I feel like im not growing/learning at work. 

Right now with the newborn this setup is awesome + I still need to take my paternity. So, I feel like the opportunities from ""tech firms"" although normally enticing aren't coming at a good time.

If you have perspective to share regarding going for it anyway or just staying put please share as well.




"
374,2018-11-11 00:23:34,1541888614.0,dataengineering,Dimensions in a data lake?,9vyhfv,alexisprince,,https://www.reddit.com/r/dataengineering/comments/9vyhfv/dimensions_in_a_data_lake/,1.0,1.0,0.0,2057.0,"Hi All,

As a background, this is going to be my first time implementing a data lake, and we're going to be using S3. However, I've been a part of implementing a typical data warehouse project, so only the data lake portion is new. I've read up a lot on typical architecture of organizing partitions, file formats, tooling, etc, but the one thing missing from pretty much everything I've read is how dimensions (particularly slowly changing) are handled within this architecture. Since pretty much everything gets stored in parquet now-a-days for data lake consumption, do you just attach all dimensional information to each incoming fact record? Do you store versioned copies of the entire source tables and take care to join only the correct version during analysis?

Any insight or relevant reading would be much appreciated!

Thank you!"
375,2018-11-11 18:26:07,1541953567.0,dataengineering,Data Engineering Pun,9w52g7,phillylovesdata,,https://www.reddit.com/r/dataengineering/comments/9w52g7/data_engineering_pun/,1.0,8.0,0.0,2062.0,"Hi there,

I am a Data Scientist/Data Engineer currently setting up a ""knowledge group"" in our company together with some other colleagues.

However we've got a tremendous problem....the name...preferably something punny.

Any suggestions? 

&amp;#x200B;

I'm looking for a short name that contains some sort of pun but still contains either 'data', 'engineer' or both. Maybe you can help :D

&amp;#x200B;

&amp;#x200B;"
376,2018-11-12 16:21:42,1542032502.0,dataengineering,Google Cloud Platform Data Engineering Exam,9weejf,ethanenglish,,https://www.reddit.com/r/dataengineering/comments/9weejf/google_cloud_platform_data_engineering_exam/,1.0,4.0,0.0,2065.0,"I'm preparing to take the [GCP Data Engineering Exam](https://cloud.google.com/certification/data-engineer) in January and wondering the best approach to studying from people that have passed. I've been working in GCP for the last year and passed the [Coursera GCP Data Engineering Specialization](https://www.coursera.org/specializations/gcp-data-machine-learning). I took the practice exam and got a 60% -- not great so I started studying.

&amp;#x200B;

The [exam case studies](https://cloud.google.com/certification/guides/data-engineer/#sample-case-study) seem important and I've been told to read the documentation for Google's various products. Initially, I was reading through the core concepts of Storage, and created a massive pile of notes. Not sure that's sustainable for each product. 

&amp;#x200B;

I'm trying a different approach: going through the [case studies](https://cloud.google.com/certification/guides/data-engineer/#sample-case-study), understanding each of them, *then* going through each product and to figure out how, as a GCP Data Engineer, would help in each scenario.

&amp;#x200B;

Does that align other people's study habits? What did you do to pass? Detail would be very helpful. 

&amp;#x200B;

Thank you!

&amp;#x200B;"
377,2018-11-12 18:01:11,1542038471.0,dataengineering,Postgres vs Redshift,9wf7in,lady_junior,,https://www.reddit.com/r/dataengineering/comments/9wf7in/postgres_vs_redshift/,1.0,4.0,0.0,2065.0,"The data team is finally getting access to data! 

So while we wait for our replica, we're trying to decide how the data is going to be store on our end. We have a MySQL and an Elasticsearch replica that are going to be dumped in a separate AWS account. From there we have to decide if we're going to go MySQL, Postgres, or Redshift. 

For the most part, we're torn between Postgres (because MySQL would have to be 5.6 and I don't want to deal with that) and Redshift. We're being pressured to turn out results immediately so I'm leaning toward Postgres because everyone on the team knows it and none of us have extensive experience with using Redshift, let alone setting it up.

Is there any reason we shouldn't set up Postgres now and incorporate Redshift later? Would it be dumb to have a Postgres database and a Redshift database? Currently, our business data is at half a terabyte, but this doesn't include event data. The elasticsearch database is about 2 terabytes. To me, it doesn't feel imperative to go Redshift now, but I'm having a hard time convincing others."
378,2018-11-13 15:04:01,1542114241.0,dataengineering,"Is there a recommended learning path to make the move from software development to data engineering? Or just general required skills that one could focus on learning, one by one?",9wovg0,irrelevant_banana,,https://www.reddit.com/r/dataengineering/comments/9wovg0/is_there_a_recommended_learning_path_to_make_the/,1.0,6.0,0.0,2067.0,"I'm currently a software developer and will make the switch to data engineering. Was wondering if there is a recommended learning path, or a list of skills you need or something.   


Thanks!

&amp;#x200B;"
379,2018-11-13 15:10:51,1542114651.0,dataengineering,Building A Data Lake Platform In The Cloud At Upsolver (Interview),9wox5r,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9wox5r/building_a_data_lake_platform_in_the_cloud_at/,1.0,0.0,0.0,2067.0,
380,2018-11-13 23:15:17,1542143717.0,dataengineering,[Hiring] 2 Positions: Business Intelligence Data Analyst And Data Engineer (No Visa Sponsorship),9wt7lh,jensyao,,https://www.reddit.com/r/dataengineering/comments/9wt7lh/hiring_2_positions_business_intelligence_data/,1.0,0.0,0.0,2069.0,"https://recruiting.paylocity.com/recruiting/jobs/Details/68787/Viral-Launch-Inc/Data-Analyst Master's in Business Analytics preferred

https://recruiting.paylocity.com/recruiting/jobs/Details/67014/Viral-Launch-Inc/Data-Engineer MIS with some job experience preferred"
381,2018-11-14 04:39:23,1542163163.0,dataengineering,Built to Scale: Running Highly-Concurrent ETL with Apache Airflow (part 1),9wvxip,mejakethomas,,https://www.reddit.com/r/dataengineering/comments/9wvxip/built_to_scale_running_highlyconcurrent_etl_with/,1.0,1.0,0.0,2071.0,
382,2018-11-15 04:22:43,1542248563.0,dataengineering,Meeting of backend dev with data engineering.,9x6uqp,alex-the-alright,,https://www.reddit.com/r/dataengineering/comments/9x6uqp/meeting_of_backend_dev_with_data_engineering/,1.0,5.0,0.0,2104.0,"Can anyone clear up the differences in back end operations and data engineering. Log files, user actions, etc will be dealt with by the back end team, no? When does the DE team come in?

I ask because I've picked up enough python, SQL, stats to do my job and help out with other departments in the creation of small pipelines. Making a move to a more robust role means dedicating time to serious study. Should I hang out with the back end team and pick up some javascript and node? 

Last, briefly, how are distributed computing systems implemented. I understand the concept but don't have a grasp on the actual connection between something like amazon S3 and the script that logs data points. What language/tool is used here?"
383,2018-11-16 11:18:18,1542359898.0,dataengineering,Incomes of experienced data engineers,9xkn8r,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/9xkn8r/incomes_of_experienced_data_engineers/,1.0,26.0,0.0,2110.0,"I'm in LA and I see jobs with advertised rates for experienced roles in the 120k - 190k range. This is cash compensation I am assuming (base and maybe a cash bonus). The higher end end of the range tends to be more ""modern"" data engineering roles using python/scala/java/kafka/spark etc while the lower end tends to be ""traditional"" tech  - SSIS/Informatica/Oracle etc.

The company I work is more traditional tech heavy and the range for cash+bonus comp appears to be 120 k to 160k.

Is this generally the range? What do you think the average would be? Of course I know this is heavily dependent on location but share your experience please."
384,2018-11-17 18:57:59,1542473879.0,dataengineering,Data Eng/Architect Masters in Canada?,9xxx09,beaverhair,,https://www.reddit.com/r/dataengineering/comments/9xxx09/data_engarchitect_masters_in_canada/,1.0,0.0,0.0,2139.0,
385,2018-11-18 13:20:43,1542540043.0,dataengineering,"I’m looking into advancing my skills as a data engineer. Can you recommend material (books, courses) that can do that?",9y59ep,kimokono15,,https://www.reddit.com/r/dataengineering/comments/9y59ep/im_looking_into_advancing_my_skills_as_a_data/,1.0,9.0,0.0,2148.0,Most material I find talks about the tools. I’m looking for more of an architecture prospective. Looking into different use cases and the pro/cons of those to solve data and scalability problems. 
386,2018-11-18 19:06:36,1542560796.0,dataengineering,Synthesizing big data frameworks and deep learning,9y7mfb,svpadd2,,https://www.reddit.com/r/dataengineering/comments/9y7mfb/synthesizing_big_data_frameworks_and_deep_learning/,1.0,1.0,0.0,2151.0,
387,2018-11-19 13:39:14,1542627554.0,dataengineering,Scalable and Stateful Streaming Data With Apache Flink (Interview),9yg0o5,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/9yg0o5/scalable_and_stateful_streaming_data_with_apache/,1.0,2.0,0.0,2156.0,
388,2018-11-19 15:22:33,1542633753.0,dataengineering,A simple practical example of data governance in action,9ygpgv,sqatas,,https://www.reddit.com/r/dataengineering/comments/9ygpgv/a_simple_practical_example_of_data_governance_in/,1.0,1.0,0.0,2156.0,"I'm reading Ladley's  *Data Governance: How to Design, Deploy and Sustain an Effective Data Governance Program,* and I think ""get"" the essence of it.

&amp;#x200B;

But I'd really appreciate it if someone could give a simple practical example of data governance in action (in your work place or even your daily life!).  "
389,2018-11-19 20:12:53,1542651173.0,dataengineering,Using Terraform and Packer to autoscale data pipelines,9yj9zf,mwakanosya,,https://www.reddit.com/r/dataengineering/comments/9yj9zf/using_terraform_and_packer_to_autoscale_data/,1.0,0.0,0.0,2156.0,
390,2018-11-21 01:06:51,1542755211.0,dataengineering,Automatic type inference from delimited files for creating schemas...,9yxfzy,Syneirex,,https://www.reddit.com/r/dataengineering/comments/9yxfzy/automatic_type_inference_from_delimited_files_for/,1.0,1.0,0.0,2162.0,"Hi all,

My data science team has asked me to research the feasibility to automatically infer types and generate create table scripts for Snowflake.

They’re familiar with Pandas and use it extensively in their work and would like to create schemata from a dataframe but my impression is that it isn’t very sophisticated in how it infers data types—seeming to frequently turn columns containing only integers/blanks into floats or objects instead of integers—and chop off leading zeroes in columns like ZIP, FIPS or IDs.

In the past I’ve manually determined types when a data dictionary with type/length isn’t available, curious if there a reliable or accepted way to approach this and accurately infer types for all columns in a CSV versus specifying them or manually defining them when a data dictionary isn’t available."
391,2018-11-21 15:15:23,1542806123.0,dataengineering,Airflow at Drivy,9z34ba,mickeyben,,https://www.reddit.com/r/dataengineering/comments/9z34ba/airflow_at_drivy/,1.0,0.0,0.0,2167.0,
392,2018-11-21 19:04:21,1542819861.0,dataengineering,Client-side instrumentation for under $1 per month. No servers necessary.,9z51e5,mejakethomas,,https://www.reddit.com/r/dataengineering/comments/9z51e5/clientside_instrumentation_for_under_1_per_month/,1.0,0.0,0.0,2171.0,
393,2018-11-21 21:38:32,1542829112.0,dataengineering,Azure HDInsight Operators For Apache Airflow,9z6itr,alikemalocalan,,https://www.reddit.com/r/dataengineering/comments/9z6itr/azure_hdinsight_operators_for_apache_airflow/,1.0,0.0,0.0,2171.0,
394,2018-11-21 22:32:01,1542832321.0,dataengineering,Fully Managed Services (GCP) vs Open Sourced,9z712i,ethanenglish,,https://www.reddit.com/r/dataengineering/comments/9z712i/fully_managed_services_gcp_vs_open_sourced/,1.0,5.0,0.0,2172.0,"I attended DataEngConfNYC recently and was surprised to not hear GCP discussed once. 

A lot of the talks were technical and I loved them but I don't come into problems with let's say back pressure in Kafka. Pub/Sub manages it for me. Dataflow manages Apache Beam, Composer Apache Airflow, Dataproc Hadoop clusters as a few examples. And authentication and security is baked in. 

My hunch is there are two main issues: 

1. cost of migrating from an older system to fully managed 
2. ability to hyper tune / optimize servers.

I'm curious if I attend the conference in 2 years, will everyone still be talking about Kafka and Spark, or will  people be using fully managed services?

Is that the case or am I missing something? Would like to get your thoughts."
395,2018-11-24 18:14:39,1543076079.0,dataengineering,Looking to do some sort of schooling in Data Science/Engineering [Looking for suggestions],9zzods,itzmebigd,,https://www.reddit.com/r/dataengineering/comments/9zzods/looking_to_do_some_sort_of_schooling_in_data/,1.0,0.0,0.0,2179.0,"I am a petroleum engineer going to work for a very data driven company and thus, it’s probably a good idea to get a good background in data science. I have been shopping around for textbooks on R and Python for data science but wanted to get some input. Have y’all taken any edX/coursera courses that you thought were really good? Do y’all recommend one of the online masters programs at an engineering school (I will be in Midland for most of a 2 year rotation and will have TONS of time)?

Hit me with some suggestions that might be outside of those questions as well, I’ve just started dabbling in working in data science as a side hobby and am super interested in it!"
396,2018-11-26 13:15:23,1543230923.0,dataengineering,Building The Dremio Open Source Data-as-a-Service Platform (Interview),a0igrp,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/a0igrp/building_the_dremio_open_source_dataasaservice/,1.0,0.0,0.0,2189.0,
397,2018-11-26 21:09:36,1543259376.0,dataengineering,Newbie's Experience Building an Airflow Data Pipeline,a0me4w,jwdatascience,,https://www.reddit.com/r/dataengineering/comments/a0me4w/newbies_experience_building_an_airflow_data/,1.0,5.0,0.0,2192.0,"Wrote a post about building a data pipeline in Airflow. It's very basic and I believe a good intro to people wanting to learn data pipelines. Feedback is definitely welcome.

[https://josephwibowo.github.io/Meetup\_Analytics/](https://josephwibowo.github.io/Meetup_Analytics/)"
398,2018-11-27 21:52:23,1543348343.0,dataengineering,Building our data science platform with Spark and Jupyter,a0yc1y,rhyswes,,https://www.reddit.com/r/dataengineering/comments/a0yc1y/building_our_data_science_platform_with_spark_and/,1.0,0.0,0.0,2203.0,
399,2018-12-01 19:10:40,1543684240.0,dataengineering,Turbine: an AWS Airflow stack easily deployed with CloudFormation,a24qwg,villasv,,https://www.reddit.com/r/dataengineering/comments/a24qwg/turbine_an_aws_airflow_stack_easily_deployed_with/,1.0,3.0,0.0,2225.0,
400,2018-12-02 17:51:36,1543765896.0,dataengineering,Some might want to grab this data bundle. Ends in about 24 hours,a2e9cl,confuego116,,https://www.reddit.com/r/dataengineering/comments/a2e9cl/some_might_want_to_grab_this_data_bundle_ends_in/,1.0,0.0,0.0,2229.0,"[Source](https://twitter.com/fwpekfjwipefj/status/1067806126635331584)

&amp;#x200B;

I just got it a few days ago and reminded of reminding others to get it too."
401,2018-12-03 06:01:04,1543809664.0,dataengineering,An Honest Review of AWS Managed Apache Kafka: Amazon MSK,a2kx9t,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/a2kx9t/an_honest_review_of_aws_managed_apache_kafka/,1.0,0.0,0.0,2233.0,
402,2018-12-03 14:44:57,1543841097.0,dataengineering,Building Distributed Systems On Top Of Apache Zookeeper (Interview),a2obhm,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/a2obhm/building_distributed_systems_on_top_of_apache/,1.0,0.0,0.0,2238.0,
403,2018-12-05 17:26:40,1544023600.0,dataengineering,Lars Albertsson – Top 10 data engineering mistakes (Berlin Buzzwords 2018),a3d9it,newthinkingevents,,https://www.reddit.com/r/dataengineering/comments/a3d9it/lars_albertsson_top_10_data_engineering_mistakes/,1.0,0.0,0.0,2259.0,
404,2018-12-05 17:42:03,1544024523.0,dataengineering,Lars Albertsson – Top 10 data engineering mistakes (Berlin Buzzwords 2018),a3dei7,newthinkingevents,,https://www.reddit.com/r/dataengineering/comments/a3dei7/lars_albertsson_top_10_data_engineering_mistakes/,1.0,1.0,0.0,2259.0,
405,2018-12-07 15:16:00,1544188560.0,dataengineering,Apache Spark troubleshooting from the trenches,a3zrkw,benjamindavy,,https://www.reddit.com/r/dataengineering/comments/a3zrkw/apache_spark_troubleshooting_from_the_trenches/,1.0,0.0,0.0,2272.0,
406,2018-12-09 17:16:30,1544368590.0,dataengineering,Need tips on how to (ideally) integrate Spark on a local Hadoop cluster,a4ln38,vinit144,,https://www.reddit.com/r/dataengineering/comments/a4ln38/need_tips_on_how_to_ideally_integrate_spark_on_a/,1.0,7.0,0.0,2288.0,"Hi.

I am an Engineering student and am currently working on a Big Data project. I am doing the project one of the Computer Labs in my College (5 PCs - 1 Master, 4 Slaves - Ubuntu 18.04 - Hadoop 2.7.7).

For the first part of the project, I successfully configured the Multi-node cluster and carried out the required tasks (Analyzing a [data set](https://archive.ics.uci.edu/ml/datasets/Poker+Hand) using a [K-Nearest Neighbor Algorithm](https://github.com/vinitS101/knn/blob/master/KnnPokerhand.java) in Java).

For this I used this [blog](http://kishorer747.blogspot.com/2014/10/setting-up-hadoop-241-multi-node.html) as a reference.  (Additionally, if you think this isn't an ideal configuration, I would love to get some tips and feedback so that my Hadoop cluster works better).

&amp;#x200B;

Now, I plan to carry out the same analysis but with Spark on the same Hadoop Cluster. I tried to follow this [guide](https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/) to set it up (Note: I used the appropriate spark version) and it kind of worked. Spark does seem to start but there are numerous errors.

I am a beginner with very basic knowledge in this. It took me about 2-3 weeks to fully understand, configure and run my Hadoop cluster.

I would really appreciate if you could:

1. Help point out the issues with the resources I linked and how can I rectify them.
2. If you have any better tutorials, blogs or guides that I can read/watch and understand this better please share the links.
3. Am I even doing this right? I have had no actual courses on these topics in College and I am just doing this because the field of Big Data, Machine Learning, etc interest me.
4. How does Cloudera,, Hortonworks etc actually work? I have full systems with Ubuntu and not VMs and I have been unable to find a guide or blog that could help me setup either of those services for my cluster of 5 Computers. If these are any good, I would really appreciate it if you could explain it a little and/or link some blogs, guides or tutorials I could refer to.

I have been stuck on this for a few weeks now. Trying to figure out how to run it. Haven't had a lot of success. Any help would be really appreciated!

&amp;#x200B;

Thanks. :)

&amp;#x200B;

P.S: I am, what they call, a n00b when it comes to most of this. I have  done Python Scripting, web dev, etc that a usual CS grad does. This is  completely new though and for the most part I remain clueless. :D"
407,2018-12-10 15:04:55,1544447095.0,dataengineering,Tackling Apache Spark From The Data Engineer's Perspective (Interview),a4vhi2,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/a4vhi2/tackling_apache_spark_from_the_data_engineers/,1.0,0.0,0.0,2328.0,"Apache Spark is a popular and widely used tool for a variety of data oriented projects. With the large array of capabilities, and the complexity of the underlying system, it can be difficult to understand how to get started using it. Jean George Perrin has been so impressed by the versatility of Spark that he is writing a book for data engineers to hit the ground running. In this episode he helps to make sense of what Spark is, how it works, and the various ways that you can use it. He also discusses what you need to know to get it deployed and keep it running in a production environment and how it fits into the overall data ecosystem.

https://www.dataengineeringpodcast.com/putting-apache-spark-into-action-with-jean-georges-perrin-episode-60/"
408,2018-12-10 20:05:09,1544465109.0,dataengineering,Creating a Data Engineering Culture (Talk - Video),a4y1oj,ScottSkillsMatter,,https://www.reddit.com/r/dataengineering/comments/a4y1oj/creating_a_data_engineering_culture_talk_video/,1.0,3.0,0.0,2330.0,
409,2018-12-11 02:31:53,1544488313.0,dataengineering,Apache Superset in production environment,a51mx9,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/a51mx9/apache_superset_in_production_environment/,1.0,3.0,0.0,2335.0,
410,2018-12-12 16:46:08,1544625968.0,dataengineering,Publish Data Outside Your Data Lake With a Spark Connector,a5iqyx,pierremarcenac,,https://www.reddit.com/r/dataengineering/comments/a5iqyx/publish_data_outside_your_data_lake_with_a_spark/,1.0,0.0,0.0,2341.0,
411,2018-12-14 08:43:39,1544769819.0,dataengineering,"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends",a62e6r,sainathkapil,,https://www.reddit.com/r/dataengineering/comments/a62e6r/global_big_data_and_data_engineering_services/,1.0,0.0,0.0,2346.0,
412,2018-12-14 18:36:58,1544805418.0,dataengineering,What's your local stack? (and why?),a66e3s,Cosack,,https://www.reddit.com/r/dataengineering/comments/a66e3s/whats_your_local_stack_and_why/,1.0,2.0,0.0,2348.0,"I'm an ops side data scientist getting a new workstation. With such a convenient opportunity to start over, I'd like to take DataOps to the fullest I can on a local machine. Before anyone says it, I know ""local,"" ""ops,"" and ""fullest"" don't necessarily belong in the same sentence, but humor me.  


So, what are the gurus here running locally, and if you're allowed to share, why?"
413,2018-12-15 09:09:32,1544857772.0,dataengineering,Tips for a spark engineer (early in career) starting a new job,a6dahd,Blayzovich,,https://www.reddit.com/r/dataengineering/comments/a6dahd/tips_for_a_spark_engineer_early_in_career/,1.0,8.0,0.0,2351.0,"Hi all! I just accepted a position in a different state for a large company as a spark data engineer. I will be writing a ton of spark for data ETL/processing in scala (I know Python, so they know I'll need to get up to speed on scala spark). I'm so excited for this opportunity but have only a few years of experience under my belt. 

I'm wondering what I should focus on learning/doing in the next month prior to starting at the new position. I will be catching myself up on scala and scala spark, but any good resources that you recommend (ideally free) would be immensely helpful. Thanks for your help!"
414,2018-12-15 13:33:28,1544873608.0,dataengineering,[Question] What is the most efficient/cheap way to store huge data sets (X TBs) + streams of data,a6ekyt,__Julia,,https://www.reddit.com/r/dataengineering/comments/a6ekyt/question_what_is_the_most_efficientcheap_way_to/,1.0,11.0,0.0,2351.0,"I am building a pipeline to analyze huge datasets of text and images (&gt; 2TB). My end goal is to store the data I have, and continuously add streams of data to it. The data is unstructured data (nested json files).

Right now, I am using Kafka to handle streams of data, Spark to run data processing tasks, and Tensorflow to train my models. However, after few weeks, my data is growing very fast (&gt; 2TB), I am trying to choose another infrastructure to store my data. I am using MongoDB right now, sometimes it crashes for not being able to perform I/O operations, and writing streams of data.

What do you advise ? I am using EC2. Do you recommend using Google BigQuery in these cases ? Do you recommend any other (cheap) way to store data. What is the most efficient NoSQL DB to handle nested json files. Is HBase/Cassandra a good option in here ? 

I don't wanna use S3/ or any blob as it is slow to read/write.  
"
415,2018-12-16 19:14:52,1544980492.0,dataengineering,r/ETL x-post. Airflow question on setting up dependent DAGs on different refresh intervals.,a6qrjz,Busenheimer,,https://www.reddit.com/r/dataengineering/comments/a6qrjz/retl_xpost_airflow_question_on_setting_up/,1.0,0.0,0.0,2353.0,
416,2018-12-17 04:58:57,1545015537.0,dataengineering,Getting into data engineering,a6vt9l,papa-sg,,https://www.reddit.com/r/dataengineering/comments/a6vt9l/getting_into_data_engineering/,1.0,6.0,0.0,2355.0,"**TL/DR: How do I break into data engineering and what does a data engineer do?**

&amp;#x200B;

I am a 3rd year university student studying Business and Computer Science, and I recently took a relational databases courses where I studied ER models, logical database design and normalization, Relational Algebra, Datalog, SQL, and data warehouses (star vs snowflake schema, apirori algorithm,etc.). I have become very interested in working with data and building data pipelines, the ETL process and hence started to look into data engineering. However, I am having a hard time finding a clear distinction between the role of a Data engineer vs a data scientist. Here are a few questions i have:

&amp;#x200B;

1) Who performs the ETL process? the data engineering or the data scientist? (every resource I read gives me a different answer

2) Why is the ETL process so difficult? Does data cleaning really take that much time?

3) What are the main responsibilities of a data engineer besides building data pipeline and ETL? For an established company like Google for example with major data pipelines and an established infrastructure, what does the role of a data engineer look like in such a massive company?

4) Do data engineers perform any data analysis? (I am very interested in the business side of things as well, and I am hoping to put the business knowledge i gain from school into use as well, however I am not very interested in heavy mathematical/statistical models used for things like Machine Learning,  but I am willing to learn if necessary)

5) Can an employee perform both the tasks of a data engineer and data scientist?

&amp;#x200B;

Finally, any recommended resources or types of personal projects to practice data engineering and break into the field? I have some experience with web and app development, but I have no idea on how to get started on data engineering and if there are any recommended courses online I can take (on things like Hadoop which I have no idea about) to get a better look on the field.

&amp;#x200B;"
417,2018-12-17 09:50:20,1545033020.0,dataengineering,"If you were a server, which data center would you choose?",a6xt64,LifeAtUngleich,,https://www.reddit.com/r/dataengineering/comments/a6xt64/if_you_were_a_server_which_data_center_would_you/,1.0,0.0,0.0,2358.0,
418,2018-12-17 11:24:38,1545038678.0,dataengineering,[Hiring] Data Engineer - Singapore,a6ybcs,consultant_start,,https://www.reddit.com/r/dataengineering/comments/a6ybcs/hiring_data_engineer_singapore/,1.0,0.0,0.0,2364.0,[https://stackoverflow.com/jobs/217855/data-engineer-streaming-video-hooq?so=i&amp;pg=1&amp;offset=2](https://stackoverflow.com/jobs/217855/data-engineer-streaming-video-hooq?so=i&amp;pg=1&amp;offset=2)
419,2018-12-17 14:58:38,1545051518.0,dataengineering,The Evolution Of ETL As A Function Of Business Growth (Interview),a6zlg4,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/a6zlg4/the_evolution_of_etl_as_a_function_of_business/,1.0,0.0,0.0,2364.0,
420,2018-12-18 07:43:30,1545111810.0,dataengineering,Has Anyone Created a Data Pipeline from Teradata?,a788nk,rekon32,,https://www.reddit.com/r/dataengineering/comments/a788nk/has_anyone_created_a_data_pipeline_from_teradata/,1.0,7.0,0.0,2373.0,"Long time lurker on here. My team and I have a project to create an ETL process that brings in over 30M rows of data from Teradata and loads them into SQL Server.  All of the manipulation has been done on the Teradata side so it's mostly a straight shot to SQL Server. I built a simple data flow in SSIS but it's taking over 12 hours to import all 30M rows. Any ideas on a faster approach? My next approach was to export via PyODBC into a flat file from Teradata but I have little hope that it will be significantly faster. 

I'm interested to know if anyone has worked with exporting large amounts of data from Teradata and what your approach was to do so."
421,2018-12-19 12:19:45,1545214785.0,dataengineering,Looking to use airflow at work however our current infrastructure is 100% SAS based,a7l7g4,montrex,,https://www.reddit.com/r/dataengineering/comments/a7l7g4/looking_to_use_airflow_at_work_however_our/,1.0,10.0,0.0,2383.0,"I'm looking to help improve our data workflow at work and from what I can see airflow is looking pretty nice.

Our ""ETL"" process is pretty much 100% based in SAS at the moment and it's unlikely to change anytime soon. My team is more analyst focused than what might be considered a more traditional data engineering background.

No one has any experience with any new technology like airflow, and the current plan is basically writing our own scheduler, meta data processes, and web interface dashboard.

I'd like to start exploring solutions like airflow and others, but due to IT security constraints like not having access to docker I'm not sure if I can even start testing simple SAS data flows on Windows machines.

Does anyone have any experience with airflow (or an alternative) and working with SAS, or windows environments with heavy security, and is able to give me advice or suggestions? 

Haven't been able to find much online regarding SAS integration, but I'm guessing I can run batch files which can run the SAS code."
422,2018-12-21 03:46:25,1545356785.0,dataengineering,Does anyone have a different background?,a853uq,Firm_Bit,,https://www.reddit.com/r/dataengineering/comments/a853uq/does_anyone_have_a_different_background/,1.0,18.0,0.0,2389.0,"Are data engineers exclusively from software and CS backgrounds?

I'm not interested in building ML models or performing analysis. I am very interested in building infrastructure for data pipelines and data intensive applications and projects. 

I have a MSc in mechanical engineering but experience with embedded systems. I'm confident in my ability to learn, but less so in my ability to pass screens for jobs looking for CS backgrounds or software experience

I'm in no rush, and I don't mind taking junior or intern positions so long as I'm learning big data paradigms. 

Are there any data engineers from other backgrounds?"
423,2018-12-24 09:29:48,1545636588.0,dataengineering,Real-Time Analysis Of Time-Series Data In PostgreSQL With PipelineDB (Interview),a92ybm,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/a92ybm/realtime_analysis_of_timeseries_data_in/,1.0,0.0,0.0,2403.0,
424,2018-12-24 16:42:25,1545662545.0,dataengineering,Question about database design.,a95hsa,confused_data_monkey,,https://www.reddit.com/r/dataengineering/comments/a95hsa/question_about_database_design/,1.0,0.0,0.0,2403.0,"Hi,

&amp;#x200B;

I'm working on a side project as a consultant where I'm confronted to the following database design:

\- 1 big table including all events. An event can be anything that happens in the application (login, purchase, delivery, click on product, page visited, etc.)

\- table includes numerous columns, some are straightforward (timestamp, country of customer), others are json/dict columns ({key:value} pairs that can also change over time, a good example of that is a count by product id of what has been bought by a user: product\_bought: {'clothing1': 3, 'clothing4': 2, ...} so this can evolve over time as more products are added to the catalog).

\- in parallel, I have access to a bunch of JSON files that describe merchandise items (for instance a  'sneaker' JSON including all sneaker IDs and different parameters like color, available size, etc.)

&amp;#x200B;

Now I'm just an product guy who can analyze data and not a DBA/SWE by any mean, but I was wondering if this is the best option to organize the DB this way. My newbie mind is thinking that splitting this big monolithic DB into different tables would be better and also feed it these JSON files for easier querying.

&amp;#x200B;

Advice? Thanks!"
425,2018-12-26 09:47:58,1545810478.0,dataengineering,"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends",a9mq2n,Harshvrdhan,,https://www.reddit.com/r/dataengineering/comments/a9mq2n/global_big_data_and_data_engineering_services/,1.0,0.0,0.0,2411.0," Global big data and data engineering services market size was valued at $29.72 billion in 2017 and is estimated to reach $110.66 billion by 2025 with the CAGR of 17.86% during 2019-2025.  

Request  a sampler eoprt @  https://www.envisioninteligence.com/industry-report/global-big-data-and-data-engineering-services-market-size-outlook-trends-and-forecasts-2019-2025/?utm\_source=redit-anusha "
426,2018-12-30 04:10:30,1546135830.0,dataengineering,"Having Trouble Installing Airflow: ""python setup.py egg_info"" failed",aarnyf,redsquall,,https://www.reddit.com/r/dataengineering/comments/aarnyf/having_trouble_installing_airflow_python_setuppy/,1.0,13.0,0.0,2433.0,"# Issue

I want to try and experiment with Airflow on my local machine, but I haven't been able to get past the first step. I've been following instructions on the [official documentation](https://airflow.incubator.apache.org/installation.html). I receive the following error when using pip3: 

    &gt; pip3 install apache-airflow
    
    Command ""python setup.py egg_info"" failed with error code 1 in /private/var/folders/yp/gw_kk_h15tx5b9dptzk5rfmr0000gn/T/pip-install-4xu47h_9/apache-airflow/

OS: Mac OS X 

Python Versions Tried: 3.7.2, 3.6.5

# Troubleshooting Steps

* Installing various versions of Python (from python.org). Initially I started with Python 3.7, however, I read there was an issue with [reserved keywords](https://stackoverflow.com/questions/53176846/command-python-setup-py-egg-info-failed). I then tried 3.6.5 but still received the same error. 
* Upgrading pip3 and setup tools as per this [Github thread](https://github.com/facebook/prophet/issues/418)
* Installing python via brew

Any help would be greatly appreciated!

&amp;#x200B;

&amp;#x200B;"
427,2018-12-31 20:17:51,1546280271.0,dataengineering,Feature Store: the missing data layer in ML pipelines?,ab9nqy,limmen,,https://www.reddit.com/r/dataengineering/comments/ab9nqy/feature_store_the_missing_data_layer_in_ml/,1.0,0.0,0.0,2440.0,
428,2018-12-31 21:44:46,1546285486.0,dataengineering,Stream-Native Storage For Unbounded Data With Pravega (Interview),abai2u,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/abai2u/streamnative_storage_for_unbounded_data_with/,1.0,0.0,0.0,2440.0,
429,2019-01-01 22:57:40,1546376260.0,dataengineering,Making S3A Hadoop connector workable with Apache-Druid,ablwt6,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/ablwt6/making_s3a_hadoop_connector_workable_with/,1.0,2.0,0.0,2445.0,"Just published the article on how to make S3A connector workable with Apache-Druid. For using S3 as a deep-storage and ingestion of parquet format data,  hadoop\_indexd which in-case of S3N connector explicitly requires AWS Secret Key, however with S3A connector AWS IAM profile credentials can be used. Have a look on the article [https://medium.com/@abhioncbr/making-s3a-hadoop-connector-workable-with-druid-35e4df4bd444](https://medium.com/@abhioncbr/making-s3a-hadoop-connector-workable-with-druid-35e4df4bd444)"
430,2019-01-02 09:38:44,1546414724.0,dataengineering,"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends",abr85n,terrible_read,,https://www.reddit.com/r/dataengineering/comments/abr85n/global_big_data_and_data_engineering_services/,1.0,0.0,0.0,2448.0," 

Global big data and data engineering services market size was valued at $29.72 billion in 2017 and is estimated to reach $110.66 billion by 2025 with the CAGR of 17.86% during 2019-2025.

Request a sample report @ [https://www.envisioninteligence.com/industry-report/global-big-data-and-data-engineering-services-market-size-outlook-trends-and-forecasts-2019-2025/?utm\_source=reddit-Bindu](https://www.envisioninteligence.com/industry-report/global-big-data-and-data-engineering-services-market-size-outlook-trends-and-forecasts-2019-2025/?utm_source=reddit-Bindu)"
431,2019-01-02 15:26:07,1546435567.0,dataengineering,Career Advice Needed: Moving from Backend Engineer to Data Engineer,abtdud,karna007,,https://www.reddit.com/r/dataengineering/comments/abtdud/career_advice_needed_moving_from_backend_engineer/,1.0,9.0,0.0,2451.0,"Hi Guys,

&amp;#x200B;

I have 9+ years of experience as a backend developer (with a moderate frontend experience on React which I have used for my side projects). Lately, I've been thinking of pursuing data engineering as a career and I'm looking for any advice out there that could be thrown at me by the community. In the 9 years of experience, I did work on developing an ETL system for about 2 years in a startup and that was developed in-house using tools like Scribe, Vertica, Highcharts etc. I've browsed over the internet and came across articles explaining data engineering as an entity but did not come across anything useful regarding a career shift and so the reason for this post.

&amp;#x200B;

What I'd like to know is

1. Is it really worth to move from a regular backend engineer working at application level to a data engineer at this point of my career? Are there companies out there who might be interested in candidates such as myself?
2. If it is worth moving, are there any recommended courses out there for me to get on board. Again here, I came across few courses but they only got me confused. I'm okay to chip in few bucks if you say that the course is really useful. 

&amp;#x200B;

Thanks in advance folks!

&amp;#x200B;"
432,2019-01-03 04:39:30,1546483170.0,dataengineering,"SQL developer background, how to data engineer?",ac0y1h,smokinggun46,,https://www.reddit.com/r/dataengineering/comments/ac0y1h/sql_developer_background_how_to_data_engineer/,1.0,5.0,0.0,2462.0,Get MS certification? Go DBA? 
433,2019-01-03 10:41:35,1546504895.0,dataengineering,1.1 Billion Taxi Rides: Spark 2.4.0 versus Presto 0.214,ac3u2u,marklit,,https://www.reddit.com/r/dataengineering/comments/ac3u2u/11_billion_taxi_rides_spark_240_versus_presto_0214/,1.0,1.0,0.0,2462.0,
434,2019-01-03 13:12:39,1546513959.0,dataengineering,Would you consider this infographic a good learning path for a college student aspiring to become a data engineer?,ac4rgn,fut-13,,https://www.reddit.com/r/dataengineering/comments/ac4rgn/would_you_consider_this_infographic_a_good/,1.0,17.0,0.0,2463.0,
435,2019-01-03 17:55:19,1546530919.0,dataengineering,data warehouse schema newbie question,ac6yjq,LDNDataNoob,,https://www.reddit.com/r/dataengineering/comments/ac6yjq/data_warehouse_schema_newbie_question/,1.0,14.0,0.0,2465.0,"I am new to the world of data science and I have just recently found myself exploring data engineering. To be more specific, data warehousing and data pipelines.

The main questions I have right now are 

* how do you migrate the data warehouse schema? is that done within the ETL process or is that done somewhere separately?
* Does anybody use the alembic python library to do this?"
436,2019-01-04 01:06:08,1546556768.0,dataengineering,Best resources for getting familiar with Docker?,acbgsb,inlatitude,,https://www.reddit.com/r/dataengineering/comments/acbgsb/best_resources_for_getting_familiar_with_docker/,1.0,7.0,0.0,2468.0,"Hey all, I am a data engineer working in a software department at a predominantly hardware company.  We're all on premise and on Hadoop and I use Spark to do most of our pipeline and analysis work.  It's pretty low volume stuff in the grand scheme of what's around us in Silicon Valley, but one thing I think we could really benefit from is Docker.  

I was wondering if anyone could recommend good tutorials/workshops for getting familiar with using Docker, aside from the official documentation?  Things seem to ""stick"" better for me if I can implement them in a mini project, so anything like that would be much appreciated!  Thanks :)"
437,2019-01-04 06:12:11,1546575131.0,dataengineering,New Data Engineer: Tips on Strengthening My Knowledge?,aceb6o,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/aceb6o/new_data_engineer_tips_on_strengthening_my/,1.0,1.0,0.0,2470.0,"Hi all,

I recently got a full-time job as a data engineer with a startup that I had been interning with for a couple of months (my background is in healthcare with minimal programming experience), and I wanted to garner some advice from you all on how to strengthen my knowledge/skillset as I don't want to plateau or possibly pigeon-hole myself as a data engineer with useless skills. On a daily basis I touch the following applications/languages: Snowflake (cloud data warehouse), Looker, AWS (S3, Fargate, IAM, Lambda, etc.), Python functions, Docker, Ansible, and a few others. Given that, I have a couple of questions:

* Is it valuable to be ""fluent"" in the aforementioned applications/languages?
* Do you have advice on how I can strengthen my expertise for any of the aforementioned applications/languages outside of my job (e.g. AWS certification)?
* What applications/languages should I learn during my free time outside of the ones I use for work? I've heard a lot about Airflow, Apache Spark, and Hadoop, but I'm unsure if it's worth (or possible) building small projects that utilize any of those modules.

My goal is to accelerate my professional growth and build confidence that I get what I'm doing, ha. Thanks in advance!

Side note: I should mention that I'm also doing a master degree in computer science with a specialization in machine learning, so there will be opportunity for me to learn about big data concepts and whatnot."
438,2019-01-05 15:30:24,1546695024.0,dataengineering,Building a workstation for ETL and Data Analysis - what are your thoughts?,acu5qk,friendlyimposter,,https://www.reddit.com/r/dataengineering/comments/acu5qk/building_a_workstation_for_etl_and_data_analysis/,1.0,1.0,0.0,2475.0,
439,2019-01-06 21:12:55,1546801975.0,dataengineering,Pachyderm vs Airflow,ad8o02,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/ad8o02/pachyderm_vs_airflow/,1.0,3.0,0.0,2483.0,
440,2019-01-07 16:25:47,1546871147.0,dataengineering,Bringing Fast Data To The Hadoop Ecosystem With Kudu (Interview),adi4w7,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/adi4w7/bringing_fast_data_to_the_hadoop_ecosystem_with/,1.0,0.0,0.0,2486.0,
441,2019-01-07 17:29:08,1546874948.0,dataengineering,Does this subreddit cover personal data management?,adipps,hxr,,https://www.reddit.com/r/dataengineering/comments/adipps/does_this_subreddit_cover_personal_data_management/,1.0,0.0,0.0,2486.0," I.e. syncthing, perkeep or some other open-source cloud softwares? I'm mainly on syncthing, but I would like to chat with people about this topic maybe. Is there a place for that?"
442,2019-01-08 17:33:46,1546961626.0,dataengineering,Using MongoDB Change Streams to replicate data into BigQuery,advbka,Kalendos,,https://www.reddit.com/r/dataengineering/comments/advbka/using_mongodb_change_streams_to_replicate_data/,1.0,0.0,0.0,2496.0,
443,2019-01-09 11:55:34,1547027734.0,dataengineering,[Question] How to collect and display logs/info for each operation (pandas DataFrame) in a data pipeline?,ae5dqe,nitred,,https://www.reddit.com/r/dataengineering/comments/ae5dqe/question_how_to_collect_and_display_logsinfo_for/,1.0,7.0,0.0,2505.0,"I have a several large data manipulation pipelines (some under development). These are batch pipelines and the size of the data that goes into every batch is in the small-medium range (i.e. 10MB-1GB). This means I can store the data for the duration of the batch in memory (or I can use disk based caches). This also means I was able to leverage the use of pandas DataFrames to create complex pipelines with around 10-20 steps.

However I want to be able to log the output (i.e. pandas DataFrame) after every step and also log the the difference between the input and output (i.e. pandas DataFrames). I don't mind the log or info being excessively informative.

So far I've been using text logs i.e. information being printed out to the std-out and log files, but parsing the logs and then being able to view the information in an intuitive way has been a problem. 

Is there a better way to generate meaningful information about pandas DataFrames for every step of the pipeline which is easy and intuitive to view/visualize/navigate not just by me but others less informed about the pipline?

Any personal experiences or links to some resources would be helpful, thanks!"
444,2019-01-11 10:50:52,1547196652.0,dataengineering,A Social Network is just a table with 2 columns,aeti9u,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/aeti9u/a_social_network_is_just_a_table_with_2_columns/,1.0,0.0,0.0,2520.0,
445,2019-01-11 10:51:25,1547196685.0,dataengineering,Providing Valuable Data to a Business as a Data Engineer,aetiey,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/aetiey/providing_valuable_data_to_a_business_as_a_data/,1.0,0.0,0.0,2520.0,
446,2019-01-11 14:25:26,1547209526.0,dataengineering,Why we've chosen Snowflake ❄️ as our Data Warehouse,aeuvnc,parudod,,https://www.reddit.com/r/dataengineering/comments/aeuvnc/why_weve_chosen_snowflake_as_our_data_warehouse/,1.0,14.0,0.0,2523.0,
447,2019-01-13 16:45:30,1547390730.0,dataengineering,Help me choose my college courses (I can only pick 3),afjx5c,fut-13,,https://www.reddit.com/r/dataengineering/comments/afjx5c/help_me_choose_my_college_courses_i_can_only_pick/,1.0,7.0,0.0,2535.0,"I want to get into data engineering and thus want to pick courses that can somewhat relate at least to the field. However, i'm an MIS major and it's too late to switch majors for me. Anyways, in the CS section of my major, i have to take 4 mandatory CS courses + 3 courses of my choice from a list. 

Here are the list of courses (linked with their description). I get to pick only 3. Since you know I'm interested in Data engineering, please help me pick (ie, what 3 would you select):

* [Data Structures and Algorithms](https://i.imgur.com/p7J85eA.png)
* [Programming in Java](https://i.imgur.com/4R92c1V.png)
* [Concepts of Programming Languages](https://i.imgur.com/0xSAJ7Z.png)
* [Software Engineering](https://i.imgur.com/0ojlINv.png)
* [Database Systems](https://i.imgur.com/1bo2UqV.png)
* [Design of Web-based systems](https://i.imgur.com/OkySrNG.png)
* Introduction to Machine Learning
* Foundations of Data Science"
448,2019-01-14 14:26:18,1547468778.0,dataengineering,Checking In On The Time Series Database Market With TimescaleDB (Interview),afvbwj,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/afvbwj/checking_in_on_the_time_series_database_market/,1.0,0.0,0.0,2546.0,
449,2019-01-15 17:20:56,1547565656.0,dataengineering,"Serverless Model Serving: OpenWhisk, Apache Spark and MLeap",ag9jhv,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/ag9jhv/serverless_model_serving_openwhisk_apache_spark/,1.0,0.0,0.0,2550.0,
450,2019-01-16 12:08:47,1547633327.0,dataengineering,Pandavro: The interface between Avro and pandas DataFrame,agjpw3,aqny,,https://www.reddit.com/r/dataengineering/comments/agjpw3/pandavro_the_interface_between_avro_and_pandas/,1.0,1.0,0.0,2555.0,
451,2019-01-16 18:40:03,1547656803.0,dataengineering,The Three Components of a Big Data Data Pipeline,agmy56,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/agmy56/the_three_components_of_a_big_data_data_pipeline/,1.0,0.0,0.0,2554.0,
452,2019-01-16 19:22:54,1547659374.0,dataengineering,Passing compressed data from RAM to CPU cache,agnedo,jsd2358,,https://www.reddit.com/r/dataengineering/comments/agnedo/passing_compressed_data_from_ram_to_cpu_cache/,1.0,3.0,0.0,2554.0,"Learning some ways how to move data efficiently throughout the computer but have a question....

Lets say I have data that I store in the disk that I compress and load it into the RAM from that point I want to load it into the CPU cache (for speed and to clear up the RAM to process more data). The compressed data is then fed from the RAM to my L1 CPU cache as a compressed file. When decompressed images of the same data are moved to L2 and L3 (so I can load more compressed data to L1), would the decompressed images just be a snapshot or instance of the compressed data from L1? 


Hopefully that makes sense...more than happy to clarify"
453,2019-01-17 10:10:38,1547712638.0,dataengineering,"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends",agvm1b,lambhorgani,,https://www.reddit.com/r/dataengineering/comments/agvm1b/global_big_data_and_data_engineering_services/,1.0,0.0,0.0,2561.0,
454,2019-01-17 20:31:28,1547749888.0,dataengineering,ETL without database access,ah0toy,throw2702,,https://www.reddit.com/r/dataengineering/comments/ah0toy/etl_without_database_access/,1.0,6.0,0.0,2564.0,"We all know about the standard tools available to shift data from databases into your data warehouse, like Stitch Data.

Has anyone worked in a situation where access to application databases was going to be revoked, forcing you to pull the data from the microservices / applications?

How common is this in mid-stage companies? How has the move worked out for you in the long run?"
455,2019-01-18 05:54:10,1547783650.0,dataengineering,Data engineering for a manufacture company,ah6g4l,alasth0r,,https://www.reddit.com/r/dataengineering/comments/ah6g4l/data_engineering_for_a_manufacture_company/,1.0,5.0,0.0,2566.0,"Hello Im looking for suggestions for implementations of robust architecture to centralize all the data of a manufacturing company.

Right now as daily work I integrate several excel files into one dashboard but I want to go beyond that, something for every department to put info into the system so data get centralized and make analysis faster"
456,2019-01-19 11:35:06,1547890506.0,dataengineering,Optimising Document Based Storage - Know Your Data (KYD),ahkpnr,sumitkumar1209,,https://www.reddit.com/r/dataengineering/comments/ahkpnr/optimising_document_based_storage_know_your_data/,1.0,1.0,0.0,2577.0,"I published my article on [~~@~~**ThePracticalDev**](https://twitter.com/ThePracticalDev)  community, have a look.  Where I talk about how you can reduce your storage costs by knowing your data

[https://dev.to/sumitkumar1209/optimising-document-based-storage---know-your-data-kyd-1hp8](https://dev.to/sumitkumar1209/optimising-document-based-storage---know-your-data-kyd-1hp8)

&amp;#x200B;

The article was originally published on 

[https://medium.com/real-spark/optimising-document-based-storage-know-your-data-kyd-214e884af5b9](https://medium.com/real-spark/optimising-document-based-storage-know-your-data-kyd-214e884af5b9)"
457,2019-01-21 12:18:45,1548065925.0,dataengineering,Architecture Standards Doc,ai8u5s,walnutmercury,,https://www.reddit.com/r/dataengineering/comments/ai8u5s/architecture_standards_doc/,1.0,3.0,0.0,2601.0,"Not sure if this is the best place for this, but I am currently writing some Architecture / Modelling standards (That tie into a policy) around Data aggregation across multiple models and databases e.g AWS, Greenplum, Oracle, excel etc.

In addition, also covering Timeliness, reusability, accuracy, and comprehensiveness.

I was wondering if anyone had written standards around these topics? or even just reporting or aggregation that they can share?

&amp;#x200B;

&amp;#x200B;"
458,2019-01-22 03:59:16,1548122356.0,dataengineering,An Interview With The Founding Members Of The LEGO Big Data Team,aihul0,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/aihul0/an_interview_with_the_founding_members_of_the/,1.0,0.0,0.0,2604.0,
459,2019-01-22 16:25:16,1548167116.0,dataengineering,Optimising Highly Indexed Document Storage - Know Your Data (KYD),ainbaw,sumitkumar1209,,https://www.reddit.com/r/dataengineering/comments/ainbaw/optimising_highly_indexed_document_storage_know/,1.0,0.0,0.0,2607.0,
460,2019-01-23 05:47:54,1548215274.0,dataengineering,Optimising Highly Indexed Document Storage (Elasticsearch) - Know Your Data (KYD),aivjqt,sumitkumar1209,,https://www.reddit.com/r/dataengineering/comments/aivjqt/optimising_highly_indexed_document_storage/,1.0,2.0,0.0,2614.0,"I published my article on [~~@~~**ThePracticalDev**](https://twitter.com/ThePracticalDev) community, have a look. Where I talk about how you can reduce your storage costs by knowing your data

[https://dev.to/sumitkumar1209/optimising-highly-indexed-document-storage---know-your-data-kyd-3g42](https://dev.to/sumitkumar1209/optimising-highly-indexed-document-storage---know-your-data-kyd-3g42)

The article was originally published on

[https://medium.com/real-spark/optimising-highly-indexed-document-storage-know-your-data-kyd-c5c11deaa736](https://medium.com/real-spark/optimising-highly-indexed-document-storage-know-your-data-kyd-c5c11deaa736)"
461,2019-01-23 15:56:49,1548251809.0,dataengineering,Open Source Data Extraction Tool,aizxnv,sebasgarcep,,https://www.reddit.com/r/dataengineering/comments/aizxnv/open_source_data_extraction_tool/,1.0,8.0,0.0,2618.0," We have two databases, an Oracle database running a legacy enterprise application, and a PostgreSQL database on which we are building a new application that will take over some (but not all) of the responsibilities of the legacy application. Because we do not control the Oracle database, we cannot completely replace it, nor can we build or data model on top of it, for other reasons. As such, we need to keep both databases in sync.

As we are comfortable with Azure Data Factory, we tried using its generic database driver to feed data from the Oracle database into the PostgreSQL one, but it was too slow to be viable. We also tried configuring a Sqoop instance, but it doesn't seem to allow arbitrary queries and we never managed to get it to run a job. Currently we are running a Jenkins instance to monitor hand-built scripts that keep the data in sync. Unfortunately, this solution is brittle and tends to fail in certain circumstances. We would like to have a tool that allows us to do arbitrary queries to the Oracle database and stream the results to the PostgreSQL database.

It is important to us that the tool can be monitored from our Jenkins instance, and that it is fast enough as we are syncing moderate amounts of data. A free and open-source tool is also a big plus.

Thanks in advance."
462,2019-01-23 20:18:10,1548267490.0,dataengineering,5 things you should know for a career in data engineering,aj2n86,digglee,,https://www.reddit.com/r/dataengineering/comments/aj2n86/5_things_you_should_know_for_a_career_in_data/,1.0,6.0,0.0,2618.0,"A [post on the Stitch blog](https://www.stitchdata.com/blog/5-things-you-should-know-for-career-in-data-engineering/?utm_medium=reddit&amp;utm_campaign=dataengcareer) today offers guidance for ""students and mid-career professionals decid\[ing\] whether data engineering is for them."" Points include ""you must be a strong developer,"" ""you need to know a lot of technologies,"" and ""experience beats education."" Good advice? What other points should wannabe data engineers know?"
463,2019-01-25 07:58:43,1548395923.0,dataengineering,Streaming small data in AWS,ajm39i,alexisprince,,https://www.reddit.com/r/dataengineering/comments/ajm39i/streaming_small_data_in_aws/,1.0,7.0,0.0,2637.0,"Hi All,

I'm hoping I'm not going crazy here, but I wanted to know what the options for streaming data are in the AWS world for small data. When I'm saying small, I'm talking about 50 records per second at peak times, down to possibly none per second at slowest. I'm a part of a startup, so using an AWS service instead of standing something up on an EC2 box would be preferable since I'm admittedly not an ops guy.

One of the requirements of what's needed is pseudo real time data (micro batching okay, but within ~ 5 mins at the absolute max). Due to the allowed latency, I was thinking of using Kinesis firehose. I was also considering SQS, however I'd want to archive all incoming transactions to S3 in both a raw and transformed state, as well as pass the data along to a different application (currently at 1 needing data, but might go up in the future). Random stackoverflow questions have been answered with using multiple queues in SQS, but due to only needing to go to one other app (along w/ s3 writes), I was thinking a kinesis firehose with a routing lambda function for the data that needs to go to my other app.

Anyway, most discussions here seem to either involve pushing systems to the limit or using open source alternatives, so any guidance using AWS services would be appreciated. "
464,2019-01-26 15:37:22,1548509842.0,dataengineering,How Bad Data Happens,ak0w4j,andrew-mccall,,https://www.reddit.com/r/dataengineering/comments/ak0w4j/how_bad_data_happens/,1.0,1.0,0.0,2648.0,
465,2019-01-26 19:44:18,1548524658.0,dataengineering,What Data Validation Libraries Do You Use? (py),ak31ww,pybokeh,,https://www.reddit.com/r/dataengineering/comments/ak31ww/what_data_validation_libraries_do_you_use_py/,1.0,0.0,0.0,2649.0,"I'm not a data engineer, but a data analyst who also prepares or processes data for my team.  For ETL type of work, I use Python with pandas and recently started using a data validation library called [Great Expectations](https://github.com/great-expectations/great_expectations/blob/develop/README.md).  Curious, what do you use or recommend for data validation?  I was using assert statements sprinkled here and there, but with GE library, I can persist or save my validations to re-use again later and I can also pass parameters between data validation steps to create dynamic data validation processes.  These are just a couple reasons why I like this GE library."
466,2019-01-27 05:01:29,1548558089.0,dataengineering,What do you look for in a junior data engineer?,ak86y9,Wangalongadong,,https://www.reddit.com/r/dataengineering/comments/ak86y9/what_do_you_look_for_in_a_junior_data_engineer/,1.0,14.0,0.0,2651.0,"I am going for a junior data engineer role, those of you involved in the recruitment process, what are you looking for? TIA"
467,2019-01-28 08:44:21,1548657861.0,dataengineering,"A Book Review of ""Architecting Modern Data Platforms""",akkvft,marklit,,https://www.reddit.com/r/dataengineering/comments/akkvft/a_book_review_of_architecting_modern_data/,1.0,3.0,0.0,2657.0,
468,2019-01-28 14:09:51,1548677391.0,dataengineering,How to Prepare for a Potential Layoff as a Junior Data Engineer,akmuay,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/akmuay/how_to_prepare_for_a_potential_layoff_as_a_junior/,1.0,10.0,0.0,2657.0,"Hi all,

I started as a junior data engineer in October as my first job in the industry (I had just graduated from programming bootcamp but I've dabbled in programming over the past few years for fun). Since then, I've learned how to navigate through parts of AWS and build code within certain services (IAM, Lambda, EC2, S3, etc.),  sharpened my Python skills (I took a CS class in Python in undergrad a while ago), and have gotten significantly better at SQL (we use Snowflake's cloud data warehouse). I've also gotten a lot of exposure to Docker and Ansible recently, and I understand Kakfa (but I wouldn't know how to set this up), but in short all of our data pipeline infrastructure is in the cloud.

I learned last week while I was on vacation that the startup I work for laid off half of their employees - fortunately I was not one of those laid off, but I think it goes without saying that we're going through uncertain times. I definitely want to stay with the company as long as possible; however, I don't feel comfortable not doing anything to prepare myself for a potential layoff, so I'm looking to you all for advice.

My current thoughts on what I can do:

* Complete a side project like this one described in this [post](https://www.reddit.com/r/dataengineering/comments/a0me4w/newbies_experience_building_an_airflow_data/) so that I can say that I've constructed a data engineering project from start to finish (and gain exposure to Apache Airflow)
* Continue sharpening my SQL skills
* Somehow gain exposure to Hadoop/MapReduce/Spark?
* Complete AWS Solutions Architect - Associate certification
* Update LinkedIn, resume, and online portfolio website (this goes without saying though)

It doesn't seem like the company will be shutting down anytime soon, but I'll be returning to work tomorrow so hopefully I'll gain some insight into what the near future looks like. Honestly, I'm hoping everything works out and the company stays alive, but as we all know, there's a lot of uncertainties when it comes to startups, so I'd rather prepare now instead of sitting idly.

Thoughts?"
469,2019-01-29 07:12:48,1548738768.0,dataengineering,An Interview About strongDM's Approach To Managing Access To Multiple Databases,akwqdx,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/akwqdx/an_interview_about_strongdms_approach_to_managing/,1.0,0.0,0.0,2664.0,
470,2019-01-30 12:14:40,1548843280.0,dataengineering,Make your Python data processing workflow communicate with AWS,albc0i,arnauda9,,https://www.reddit.com/r/dataengineering/comments/albc0i/make_your_python_data_processing_workflow/,1.0,5.0,0.0,2670.0,"Airflow is a platform to easily declare data processing workflows in Python. Here is an article I wrote about how Airflow connections work. Any feedback would be much appreciated!

[https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc](https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc)"
471,2019-01-30 18:49:15,1548866955.0,dataengineering,Is the Data Vault architecture something you would recommend to someone creating a new data warehouse?,alem08,LDNDataNoob,,https://www.reddit.com/r/dataengineering/comments/alem08/is_the_data_vault_architecture_something_you/,1.0,8.0,0.0,2670.0,
472,2019-02-02 17:43:27,1549122207.0,dataengineering,When would you use Hadoop vs Redshift (and other parallel cloud computing tools)?,amf68s,choiboy9106,,https://www.reddit.com/r/dataengineering/comments/amf68s/when_would_you_use_hadoop_vs_redshift_and_other/,1.0,12.0,0.0,2699.0,"So I recently learned Hadoop and the various things that come with Hadoop (MapR, Hive) and I'm trying to figure out when one would install a Hadoop infrastructure vs AWS based infrastructure.

It seems like before cloud computing and AWS was a thing, companies invested heavily into Hadoop because it was the first parallel processing engine introduced and companies with newer data infrastructure seem to gravitate towards Redshift/AWS infrastructure.

So it feels like Hadoop is more of a code customized framework vs AWS Redshift is more of a tool based framework (couldn't phrase it well) Can anyone elaborate on the differences?"
473,2019-02-03 16:29:28,1549204168.0,dataengineering,Date engineer getting up to the speed,ampzxt,muddasser-h,,https://www.reddit.com/r/dataengineering/comments/ampzxt/date_engineer_getting_up_to_the_speed/,1.0,7.0,0.0,2703.0,"Hi all, 

I have recently started working as a data engineer at a fintech company. The company is investing heavily in data driven decisions and everything is very fast-paced. I am good at coding. I know quite a lot of Apache Spark but what I lack is the exposure of building blocks of a data pipeline. Specifically, if you would ask me to build a pipeline from scratch then I might come up with the tools but might not be able to compare different options. I wouldn't not be able to list pros and cons. I know in many of these cases you need solid hands-on experience. But I see a great opportunity in my company to climb up the ladder if I can prove myself in the short period of time. I also don't have hands-on experience of AWS tools. I am willing to put effort and time but I want to have a clear road map before. 

So, I would like to ask what sample applications, blogs, books or tutorial I should start off immediately? I would like to get deeper insight of data pipeline architecture. And, if any of you could point to a resource explaining how to use machine learning in real-time analysis then that would be great. 
Thanks in advance 🙂"
474,2019-02-04 14:04:22,1549281862.0,dataengineering,An Interview About Building An Open Data Platform For Archaeologists,an119y,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/an119y/an_interview_about_building_an_open_data_platform/,1.0,0.0,0.0,2705.0,
475,2019-02-04 18:28:18,1549297698.0,dataengineering,Dependencies management for scientist,an3b85,siscia,,https://www.reddit.com/r/dataengineering/comments/an3b85/dependencies_management_for_scientist/,1.0,2.0,0.0,2707.0,
476,2019-02-05 19:46:18,1549388778.0,dataengineering,Data Engineer job without software engineer or developer experience.,angliz,grappon,,https://www.reddit.com/r/dataengineering/comments/angliz/data_engineer_job_without_software_engineer_or/,1.0,12.0,0.0,2713.0,"Hi,

Did you become a Data Engineer without software engineer or developer experience? How?

&amp;#x200B;"
477,2019-02-05 21:03:05,1549393385.0,dataengineering,What does a Data Engineer do? What different roles exist?,anhfat,iblaine_reddit,,https://www.reddit.com/r/dataengineering/comments/anhfat/what_does_a_data_engineer_do_what_different_roles/,1.0,6.0,0.0,2713.0,"A coworker and I put together this post about the different types of Data Engineers.  No real purpose other than to try to minimize confusion in the industry.  If you're a recruiter or someone new to the DE space, then this post may be helpful.  https://www.linkedin.com/pulse/defining-role-data-engineer-blaine-elliott/"
478,2019-02-05 22:59:24,1549400364.0,dataengineering,Is a ETL/ BI engineer position a good stepping stone into a data engineer position?,animny,rolkien29,,https://www.reddit.com/r/dataengineering/comments/animny/is_a_etl_bi_engineer_position_a_good_stepping/,1.0,27.0,0.0,2714.0,
479,2019-02-06 20:07:10,1549476430.0,dataengineering,Streaming ETL?,antjat,chiv,,https://www.reddit.com/r/dataengineering/comments/antjat/streaming_etl/,1.0,3.0,0.0,2722.0,"Good day, I'm trying to move from batch processing of data to streaming/realtime. My team doesn't have a Data Engineer and I'm not sure that I'm going to be able to get one so I'm stuck trying to figure out all the ins and outs of all my options and frankly I'm overwhelmed. 

&amp;#x200B;

Currently, I'm on AWS. Have data going through Kafka topics (w/ mongo dbs) to S3. I use PySpark for ETL from S3 to Hadoop and HIVE (for meta data) and I read it with BI tools from there. My ETL though is like 8 hours long (just by shear volume of data). Budget is not a problem. 

&amp;#x200B;

Would I benefit much from say Kinesis/Firehose, Glue, DynamoDB? Or perhaps Flink or Spark Streaming or something? I can't seem to figure out the best way to improve the ETL bottleneck. "
480,2019-02-08 05:27:14,1549596434.0,dataengineering,Thoughts on Dataquest?,aocbej,_shiki,,https://www.reddit.com/r/dataengineering/comments/aocbej/thoughts_on_dataquest/,1.0,9.0,0.0,2735.0,"Hello everyone! I have been working as a software engineer for 1 year and am interested in transitioning to a data engineering role. I stumbled upon a self-study site called Dataquest and was wondering if anyone could share their experience with it. How well would a site like this prepare you for interviews as well as being a capable data engineer? 

Here is their course overview: https://www.dataquest.io/path/data-engineer"
481,2019-02-09 03:35:24,1549676124.0,dataengineering,How much proficient do I need to be in Python?,aonms3,Drakkenstein,,https://www.reddit.com/r/dataengineering/comments/aonms3/how_much_proficient_do_i_need_to_be_in_python/,1.0,19.0,0.0,2743.0,"Hi engineers,  


I am currently working an internship as a Support Analyst doing DBA/Data Wrangling/Pipelining at work. However, their codebase is 95% in R. I do a lot of R programming (packages like data.table, mongolite, ggplot2, devtools, mapdeck), building our own internal packages and some bash scripting (git, docker, ssh tunnels etc.). I doubt that they will continue with me due to funding issues. So I am pretty much going to have to look for a full time Data engineering role. I am keen on doing Data Engineering rather than Data Science/Analyst roles.   


Now it seems all of the jobs I have come across use Python as their primary language. I am very keen to learn Python (currently finishing up ""Automate the boring stuff with Python""). The keyword used in most job postings is [""HIGHLY PROFICIENT IN PYTHON""](https://imgur.com/a/hhqxUx4). I really need to get to this level ASAP. I am keen on starting to work on projects to show off on Github. But not sure how do I become HIGHLY PROFICIENT. Would like some advice on that."
482,2019-02-11 22:04:30,1549915470.0,dataengineering,"An interview about how to build, launch, and maintain machine learning products",apkbrq,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/apkbrq/an_interview_about_how_to_build_launch_and/,1.0,0.0,0.0,2822.0,
483,2019-02-11 23:24:38,1549920278.0,dataengineering,Can we use Apache Airflow with PyPy?,apl864,nitred,,https://www.reddit.com/r/dataengineering/comments/apl864/can_we_use_apache_airflow_with_pypy/,2.0,4.0,0.0,2822.0,"I'm quite happy with Airflow for ETL, cron and batch processing. Some of my DAG/Graphs have around a 100 nodes in them and I don't want to reduce the number of nodes. It takes a while for Airflow to go over all these nodes and when I look at the logs, the tasks usually take 4 seconds to complete, where only 500ms belong to are actual computation but 3.5 seconds seems to be some kind of overhead from Airflow (with CeleryExecutor).  


Do you think moving to PyPy might help? Have any of you tried to do it?"
484,2019-02-12 02:43:22,1549932202.0,dataengineering,Turbine v3 released! An AWS Airflow stack easily deployed with CloudFormation,apn9v7,villasv,,https://www.reddit.com/r/dataengineering/comments/apn9v7/turbine_v3_released_an_aws_airflow_stack_easily/,1.0,1.0,0.0,2826.0,
485,2019-02-12 17:19:53,1549984793.0,dataengineering,How do big company deal with real time and historical data,apucye,gomey93,,https://www.reddit.com/r/dataengineering/comments/apucye/how_do_big_company_deal_with_real_time_and/,1.0,4.0,0.0,2833.0,"Do they have different data warehouse for the 2 types of data? E.g. historical data uses Postgres with S3 while real time uses redshift.

Sorry for my lack of knowledge hope you guys could help."
486,2019-02-13 03:15:42,1550020542.0,dataengineering,"Is there any book/resource about data engineering in big picture, kind of like ""for dummies"" books?",aq0vcv,machine_story,,https://www.reddit.com/r/dataengineering/comments/aq0vcv/is_there_any_bookresource_about_data_engineering/,1.0,0.0,0.0,2837.0,"I come from a data science/ml background, and while I think that those things are interesting, I want to learn more about data engineering. I think that building reliable data system in cloud is really interesting and I'd like to learn more about that.

I want to read books like these so I can know *how data engineering provides value to businesses*. When I search around this sub, there's many terms that I am unfamiliar with such as etl, data models, etc. Is there anything I can read to make me a good data engineer?

Here's my current skill (maybe it'll help):

* I'm decent at python (main programming language)
* I have very little knowledge about sql, I only can do basic queries
* I know little bit of things about aws, just finished my solution architect associate course (acloud.guru) and I'm currently tinkering with aws
* I have very small amount of knowledge about hdfs


"
487,2019-02-13 03:38:51,1550021931.0,dataengineering,"I have a raspberry pi laying around, is there anything I can do with that to learn about data engineering?",aq1321,machine_story,,https://www.reddit.com/r/dataengineering/comments/aq1321/i_have_a_raspberry_pi_laying_around_is_there/,1.0,8.0,0.0,2837.0,"I'm completely new on this field and I'd like to learn about how to create a small data infrastructure on cloud. I have a pi laying around and I'd like use it to gather data and send it to aws and then do something with it. Maybe I'll put the data to s3 and then use lambda to do process it.

I heard that airflow is pretty important, so maybe I also can use airflow to do this small project.

I have no idea on what to do though, any suggestion? "
488,2019-02-13 16:10:56,1550067056.0,dataengineering,Free Course - Introduction to GIS in R,aq6yjp,Cocohoney16,,https://www.reddit.com/r/dataengineering/comments/aq6yjp/free_course_introduction_to_gis_in_r/,1.0,0.0,0.0,2846.0,"R and its data visualization libraries are a powerful tool to tackle the toughest geospatial data. This free [#GIS](https://www.linkedin.com/feed/hashtag/?keywords=%23GIS) course gives you a complete introduction to extracting, processing, analyzing &amp; mapping geospatial data in R. Sign up now: [https://soco.ps/2UHUYcn](https://soco.ps/2UHUYcn)

The course includes 6 chapters that cover -

✅ Step-by-step demos using sample data

✅ Course completion certificate

✅ 100+ useful R code snippets

✅ 50+ sample maps

✅ 80+ links to other free resources"
489,2019-02-14 00:46:14,1550097974.0,dataengineering,How do people handle big data debugging?,aqcmh0,jwdatascience,,https://www.reddit.com/r/dataengineering/comments/aqcmh0/how_do_people_handle_big_data_debugging/,1.0,11.0,0.0,2849.0,"I recently got rejected from an interview due to my lack of real-world experience in big data. One of the interview questions that was asked was around seeing a spike in a certain metric on a given day. How would you go about developing a process to help identify these spikes, the root cause of that spike, and a process that automates those steps later on for future cases?

Given that I can't simply query billions of rows of data to check, I was unsure really on how to validate my theories that the spike happened because of a specific real-life event or new website feature. In general, is there a framework about how data engineers QA errors or outliers in their dataset?"
490,2019-02-14 06:29:44,1550118584.0,dataengineering,[Airflow] Issues setting up a JDBC connection to Teradata,aqfvug,redsquall,,https://www.reddit.com/r/dataengineering/comments/aqfvug/airflow_issues_setting_up_a_jdbc_connection_to/,1.0,11.0,0.0,2849.0,"Hey all, I'm trying to test out Airflow to manage a few of my Teradata workflows. I've gotten the web server set up using [Puckel's Docker image](https://github.com/puckel/docker-airflow). 

In the Airflow UI, I've tried to create a Teradata connection with the following parameters:

* **Conn Type:** Jdbc Connection
* **Connection URL:** jdbc:teradata://servername/charset=UTF8,DBS\_PORT=1025
* **Driver Path (this is where I think the issue is):** /usr/local/airflow/dags/teradata/tdgssconfig.jar,/usr/local/airflow/dags/teradata/terajdbc4.jar 
* **Driver Class:** com.teradata.jdbc.TeraDriver

For Driver Path, I've tried two permutations: a comma separating the two jar files and then trying to merge both jar files into one as per this [stack overflow post](https://stackoverflow.com/questions/45450618/connect-to-teradata-using-airflow-jdbc-connection). Unfortunately, no luck with either. 

When I go to the Data Profiling -&gt; Ad hoc query option in the UI, I get the following error when selecting the connection I created:

&amp;#x200B;

https://i.redd.it/b6shv4cpogg21.png

Has anyone had experience trying to connect to Teradata or JDBC in general?"
491,2019-02-14 14:47:21,1550148441.0,dataengineering,Group + Panel Interview Experiences?,aqje41,alex-the-alright,,https://www.reddit.com/r/dataengineering/comments/aqje41/group_panel_interview_experiences/,1.0,0.0,0.0,2857.0,"Group interview experiences? Have any of you guys been through panel interviews 2-3 candidates at a time? Notes on group dynamics, faux pas, should dos?"
492,2019-02-14 15:47:42,1550152062.0,dataengineering,How does Zapier or IFTTT do their ETL?,aqjwz3,ethanenglish,,https://www.reddit.com/r/dataengineering/comments/aqjwz3/how_does_zapier_or_ifttt_do_their_etl/,1.0,1.0,0.0,2859.0,"I've been very impressed with Zapier and curious how they do their ETL? Is it a series of lambda functions?  How do they handle API rate limiting? Are they doing orchestration with Airflow? What cloud platform did they choose and why? 

Anyone know where I can find out more? I could learn a lot from their engineering practices."
493,2019-02-14 23:28:45,1550179725.0,dataengineering,Does anyone have experience with creating datasets through Typeform?,aqow1x,GoopOnYaGrinch,,https://www.reddit.com/r/dataengineering/comments/aqow1x/does_anyone_have_experience_with_creating/,1.0,1.0,0.0,2866.0,"Just began a new job heading up BI for a startup. One of my first initiatives is to turn our data coming into Typeform, which we use for things like product feedback surveys, into actionable data. I've setup the API to push into our database, but the way the data comes in initially structured makes it difficult to use without cleaning it and restructuing.

So I'm wondering if any of y'all have experience in this department?"
494,2019-02-14 23:34:06,1550180046.0,dataengineering,Help with data workflow testing (tooling and approach),aqoy4g,raginjason,,https://www.reddit.com/r/dataengineering/comments/aqoy4g/help_with_data_workflow_testing_tooling_and/,1.0,3.0,0.0,2866.0,"As a data engineer, I'm having some issues with testing. Some things look more like unit tests, other things don't really fall into that category, and I'm struggling with that. Recently I was tasked with adjusting a match and append style job. File A (100 million records) had names and addresses, file B (80 million records) had names, addresses, and statistically modeled scores. The match process was pre-existing, but the new file B had a dozen or so new scores that needed to be appended to file A. The work itself was not very complicated, but figuring out how to tests (especially in an automated fashion) was a big pain. This is what I ended up with:

* Validate that the output file has the correct header. This required that I duplicate the column set in the test that I use in the code that does the actually data processing, which sucks but it works.
* Sample the output file and check that the new columns have valid values. In this case, all numeric from 0-999. This worked in this case because the datatype was concrete. If I were appending say, ""profession"", this test would be of little to no value.
* Sample the output file and walk the data lineage back to the source to confirm that the values for the new columns are indeed the same as what is in the source data. Because of the data volume, this was computationally expensive to do, which I am not fond of.

Some things I noticed along the way:

* Sampling can be useful, but without specific in-depth knowledge of the process, it's value diminishes
* Data lineage is a godsend. When I started this work there was none, and it made it virtually impossible to test output values against the source data
* It's difficult to separate what should be a code-test vs a data-test vs a code-test of the process running with bad data

One thought I had was to include a sample input and output set of data (say 1000 records or so) with the code base, and then have some kind of framework that would run the job with the input sample and compare the output of the job with the pre-defined output. Coming up with that sample set could be a bear depending on the situation. In my case, because there is a match, I'd have to find something in the sample set that does match and does not match in order to get anything like full coverage. This means I need to run the process first, then sample the output (hoping it's correct!), then fold that output back into the test, which is the exact opposite of TDD. Finding the proper sample seems like it would be the lions share of the work needed for testing, and it requires deep knowledge of the datasets and processes involved.

Basic unit tests and table-driven unit tests at the code-level could be run by traditional software test suites that output/conform with TAP or xUnit. That's not really too difficult. What is difficult is things like codifying a test that states ""of the 100 million A records, and 80 million B records, you should have these specific 50 million that match""? It could be a simple shell script that runs the job with fixed inputs and expected outputs, but what runs that test? What kind of output should that test produce so that something could consume it (TAP, xUnit, something else)? I've been unable to find anything that tests in this manner, and I'm reluctant to write my own data testing framework.

All of the above is one example data processing job I have. I have hundreds ranging from something relatively simple like this to more complicated tasks. I guess I'm trying to figure out what kind of tooling and approaches I could use to help me with testing data workflows, especially as most of the workflows I deal with are in the hundreds of millions of records and take hours to days to run. What have you guys used to wrangle these types of testing problems?"
495,2019-02-16 00:36:38,1550270198.0,dataengineering,Article on building your own Docker containers for ML models on SageMaker,ar2731,ResidentMario,,https://www.reddit.com/r/dataengineering/comments/ar2731/article_on_building_your_own_docker_containers/,1.0,3.0,0.0,2879.0,
496,2019-02-17 14:19:07,1550405947.0,dataengineering,Beyond Interactive: Notebook Innovation at Netflix,ark2uy,derivablefunc,,https://www.reddit.com/r/dataengineering/comments/ark2uy/beyond_interactive_notebook_innovation_at_netflix/,13.0,4.0,0.0,2897.0,
497,2019-02-19 08:32:56,1550557976.0,dataengineering,A Minimalist Guide to FoundationDB,as7efh,marklit,,https://www.reddit.com/r/dataengineering/comments/as7efh/a_minimalist_guide_to_foundationdb/,4.0,0.0,0.0,2933.0,
498,2019-02-19 16:22:37,1550586157.0,dataengineering,An interview about the Alluxio distributed virtual in-memory file system,asawog,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/asawog/an_interview_about_the_alluxio_distributed/,5.0,0.0,0.0,2936.0,
499,2019-02-20 10:30:52,1550651452.0,dataengineering,Whats new in Spark 2.4!,aslxss,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/aslxss/whats_new_in_spark_24/,1.0,0.0,0.0,2948.0,
500,2019-02-22 20:24:53,1550859893.0,dataengineering,Step by step Learning Path to Become Data Scientist/master Machine Learning,atkz3h,prabhat008,,https://www.reddit.com/r/dataengineering/comments/atkz3h/step_by_step_learning_path_to_become_data/,5.0,0.0,0.0,2978.0,
501,2019-02-24 05:19:39,1550978379.0,dataengineering,Learning Data Engineering,au3qak,hudddb3,,https://www.reddit.com/r/dataengineering/comments/au3qak/learning_data_engineering/,23.0,0.0,0.0,2996.0,
502,2019-02-24 08:01:07,1550988067.0,dataengineering,D.E. team as part of IT ?,au52nk,kittie_thrower,,https://www.reddit.com/r/dataengineering/comments/au52nk/de_team_as_part_of_it/,1.0,6.0,0.0,2995.0,"I'm wondering the relationship btw DE team &amp; IT, in the typical corporate world.

Are DE teams normally part of IT  ?
If not, do DE teams normally use services/support from IT (i.e. DBA, DevOps) or their own personnel ?
"
503,2019-02-25 12:49:31,1551091771.0,dataengineering,An interview about what data engineers need to know about deep learning,aujyd9,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/aujyd9/an_interview_about_what_data_engineers_need_to/,10.0,0.0,0.0,3008.0,
504,2019-02-26 18:48:29,1551199709.0,dataengineering,ETL tools with CDC,av1k5j,GandaMelgus,,https://www.reddit.com/r/dataengineering/comments/av1k5j/etl_tools_with_cdc/,1.0,0.0,0.0,3029.0,
505,2019-02-26 21:06:59,1551208019.0,dataengineering,I can't decide between EE and DE,av36gw,OneChest,,https://www.reddit.com/r/dataengineering/comments/av36gw/i_cant_decide_between_ee_and_de/,1.0,1.0,0.0,3029.0,"Hi, so pretty much after the summer I am going to pick between EE and DE. 

From what I understand, at least following the university here the first year is pretty similar, programming for different languages, linear algebra and stuff like this.

After that I have no idea what the difference will be, one will obviously be more computer focused and IT and one will be more focused in Electro stuff.

I just can't decide, in my ""gymnasium"" that we call it hear, 3 years I went to a IT-data one. Programming in html, php, mysql, csharp, c++ etc and pretty basic admin stuff for a computer. I am really good at googling issues lol.

At the end of the day, I want to be able to get a job in any country, work from home even if possible and work at the computer.

EE is very interesting though since we would be building the future following that.. but I honestly don't feel like I am a person that would travel around to ""fix"" anything."
506,2019-02-26 22:16:43,1551212203.0,dataengineering,How hard is it to get into DataEngineering?,av401p,lalawebdev,,https://www.reddit.com/r/dataengineering/comments/av401p/how_hard_is_it_to_get_into_dataengineering/,8.0,19.0,0.0,3030.0,"Let's say I know Python/Pandas/Jupyter/Luigi, I know ElasticSearch/Logstash/Beats, I know Java and AWS/Kubernetes.

But I have been never hired in a dedicated DataEngineering position, just picked up some tasks related to it at my job.

What should be my expectations when applying for DataEngineering jobs? And which technologies should I learn before? Spark, Hadoop? Can you give any ideas for example projects to show I know that stuff?"
507,2019-02-27 01:44:19,1551224659.0,dataengineering,Switching role from Industrial Engineering to Data Engineering,av6bwq,confuzed_squirrel,,https://www.reddit.com/r/dataengineering/comments/av6bwq/switching_role_from_industrial_engineering_to/,6.0,7.0,0.0,3033.0,"Hi everyone! 

&amp;#x200B;

A little background information about myself: I am working as an Industrial Engineer but currently really unhappy with my major &amp; career choice (a little late, but better late than never right? lol). I chose IE as my major because I honestly didn't know what else to do and was highly advised against doing business for undergrad. Throughout college, I worked at the university's IT office as a basic database admin (and by basic, I mean really basic, the only experience I have with databasing is Filemaker) and really enjoyed my job. With the job that I currently have and from my previous internship experiences, I've also come to realize that I enjoy working with data as well. 

&amp;#x200B;

I have been looking into which careers I might be more interested in (mostly Data Analytics and recently DE), and seem to like DE. I've started taking up some courses on Datacamp to introduce myself to the basics, but I really need help with how I should be aligning myself in this path. 

&amp;#x200B;

How hard is it for an IE to make a switch into DE? Do you have any advice/recommendations for me? What would a probable timeline of a switch be like? Would it be hard for me to make this switch to a different company? Or would it be better for me to make a switch within my company into DE? Would I have to get an additional degree (undergrad/grad) in order for me to make this switch, or can I self-teach? I really would appreciate any help on this topic tbh, thank you guys! "
508,2019-02-27 08:07:20,1551247640.0,dataengineering,Building a datawarehouse,ava1a2,ModCam_,,https://www.reddit.com/r/dataengineering/comments/ava1a2/building_a_datawarehouse/,20.0,1.0,0.0,3038.0,"Hello all,

I've seen a few post about building a datawarehouse, so I thought maybe I can share you a post one of my colleagues wrote on the subject in a medium article : [https://medium.com/everoad/building-a-data-warehouse-in-six-months-what-did-we-learn-e058e42446f1](https://medium.com/everoad/building-a-data-warehouse-in-six-months-what-did-we-learn-e058e42446f1) .  I think it's pretty interesting to see the tools they used, how they've chosen it, and how they proceeded to build it. Don't hesitate to ask if you have any question!"
509,2019-02-28 07:31:46,1551331906.0,dataengineering,Joins in Apache Spark,avne00,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/avne00/joins_in_apache_spark/,8.0,2.0,0.0,3057.0,"A quick article with bunch of examples explaining Join in Spark 2.4

[https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-1-dabbf3475690](https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-1-dabbf3475690)

[https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-2-5b038bc7455b](https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-2-5b038bc7455b)

Any comments or feedback is really appreciated!"
510,2019-02-28 23:05:35,1551387935.0,dataengineering,"Breaking into Data engineering Job market, SEEKING GUIDANCE!",avw771,shivk-y,,https://www.reddit.com/r/dataengineering/comments/avw771/breaking_into_data_engineering_job_market_seeking/,5.0,7.0,0.0,3068.0,"As a 2018 Comp Sci grad with &lt; 1 year Work experience, How can I land my first data engineering job? I have designed a couple databases for class projects, Working on building a toy data warehouse using MySQL. 

I've been trying to solidify my fundamentals by taking the ""Data Engineering on Google Cloud Platform"" on Coursera and by studying ""building data intensive applications by martin kleppmann"". I am thoroughly overwhelmed and feel like I do not have a roadmap.  "
511,2019-02-28 23:20:13,1551388813.0,dataengineering,Building Data Engineering Portfolio,avwcug,shivk-y,,https://www.reddit.com/r/dataengineering/comments/avwcug/building_data_engineering_portfolio/,15.0,4.0,0.0,3068.0,What are a few Data engineering project ideas/leads that you'd suggest to someone who is starting out?
512,2019-03-01 15:32:45,1551447165.0,dataengineering,Where to find info on how companies are dealing with big data?,aw4xjg,ammar-,,https://www.reddit.com/r/dataengineering/comments/aw4xjg/where_to_find_info_on_how_companies_are_dealing/,6.0,5.0,0.0,3075.0,"I'm doing some research. I want to write about how companies deal with big data: how they use it, store it, what are the obstacles, what is the nature of the data, etc.

It's not necessary for the company to be a big one like Apple and Facebook.

Can you recommend resources or a way of search to find this info?"
513,2019-03-01 16:44:34,1551451474.0,dataengineering,What I've learned building low latency systems,aw5oid,CHAD_J_THUNDERCOCK,,https://www.reddit.com/r/dataengineering/comments/aw5oid/what_ive_learned_building_low_latency_systems/,11.0,0.0,0.0,3076.0,
514,2019-03-01 18:12:53,1551456773.0,dataengineering,"Graduating in the Spring, might want to become a Data Engineer with &lt;1 yr exp handling data - am I in over my head?",aw6mtd,qualitytest,,https://www.reddit.com/r/dataengineering/comments/aw6mtd/graduating_in_the_spring_might_want_to_become_a/,11.0,14.0,0.0,3078.0,"My resume: https://i.imgur.com/B0Hv9rW.png

I've taken a Data Management and Analysis class which was essentially an intro into databasing, with stuff about SQL and Mongo, designing databases, etc. Didn't get too in-depth but I was pretty interested in what I was doing.

I'm graduating this Spring and hadn't really considered which field I wanted to enter until last semester. It's between Web Dev and Data Engineer right now.

My question then is, what more should I learn about DBs so I can actually have a competitive resume? And are the amount of things I need to learn too much at this point considering I'm about to graduate?"
515,2019-03-03 05:01:00,1551582060.0,dataengineering,Anybody with experience of Snowplow Analytics at scale?,awprpw,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/awprpw/anybody_with_experience_of_snowplow_analytics_at/,2.0,3.0,0.0,3099.0,"Does it scale reliably and cheaply to billions of records per month?

It has many features, but are they worth it? Or is it better to roll your own analytics collection endpoint?"
516,2019-03-04 13:46:37,1551699997.0,dataengineering,ETL with Jupyter Notebooks+Papermill+Workflow Orchestration?,ax68j8,pybokeh,,https://www.reddit.com/r/dataengineering/comments/ax68j8/etl_with_jupyter_notebookspapermillworkflow/,4.0,17.0,0.0,3119.0,"Just curious, has anyone done ETL with Jupyter notebooks with papermill and airflow or luigi?  What was your experience?  I read a blog about Netflix doing this except using their own in-house scheduler.  For a project at work, I'm thinking about using jupyter notebooks with papermill and Luig.  I'm stuck with Windows at work, so Apache Airflow is a non-starter for me."
517,2019-03-04 17:27:32,1551713252.0,dataengineering,Attending Strata Data Conference (SF),ax8bax,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/ax8bax/attending_strata_data_conference_sf/,2.0,4.0,0.0,3121.0,"I will be attending the Strata Data Conference at the end of the month in San Francisco. This is my first Strata (and first professional conference), so I wanted to hear from the DE community about your experiences with Strata:

What are musts in terms of see/do?

Are there peripheral events that I should be signing up for prior?

Are there industry specific events to look out for? (I work in biotech)

I work within a growing data engineering team (some might say immature). I would be interested in meeting up in SF if you feel like you have any advice to give on growing DE teams. PM me. Bonus points if you work in biotech!"
518,2019-03-04 18:21:56,1551716516.0,dataengineering,Where to Begin ? SAP Business Warehouse,ax8xbk,Ownards,,https://www.reddit.com/r/dataengineering/comments/ax8xbk/where_to_begin_sap_business_warehouse/,1.0,0.0,0.0,3122.0," Hello everyone,

I will soon begin an internship as a consultant in SAP BW but ironically I have no knowledge about business warehouses. I really want to be prepared before I start my training period and I wish I could find a good textbook or MOOC for dummies about business warehouses and more specifically SAP BW.

I tried :

\- ""SAP BW/4HANA in a Nutshell"" (a SAP MOOC)

\- ""SAP BW/4HANA: An Introduction"" (a 2017 Textbook)

But in both cases I was completely lost with the terminology used and I really could not grasp the concepts.

I'm thinking about starting ""Data Warehousing for Dummies (2nd Edition)"" but I don't know how good this book is, especially since it was published 10 years ago. Do you think it's a relevant book to start with if I am a total beginner ?

Thank you all"
519,2019-03-04 20:59:03,1551725943.0,dataengineering,An interview about the platform Segment has built for routing streams of customer analytics data,axau33,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/axau33/an_interview_about_the_platform_segment_has_built/,5.0,0.0,0.0,3123.0,
520,2019-03-05 13:39:49,1551785989.0,dataengineering,Best source/books for learning database fundamentals?,axk7w9,Drakkenstein,,https://www.reddit.com/r/dataengineering/comments/axk7w9/best_sourcebooks_for_learning_database/,3.0,5.0,0.0,3137.0,"I am currently working as a Data Analyst but eventually plan to become a Data Engineer at my work place. However, I would like to gain a solid base in Database concepts and all tricks involved with 1:1, 1:m, n:m relationships, indexing, query optimisation, keys etc. Please suggest me a source or a book that covers it all. "
521,2019-03-05 13:50:49,1551786649.0,dataengineering,Do you engineers use templates to build databases at work?,axkazd,Drakkenstein,,https://www.reddit.com/r/dataengineering/comments/axkazd/do_you_engineers_use_templates_to_build_databases/,4.0,3.0,0.0,3137.0,"Hi guys, I am a Data Analyst training to be a data engineer.  I would like to be more efficient at work when creating databases for new projects. What I wish to know is whether you  


* Use  pre-written SQL scripts to modify statements with new values
* Use a data pipeline (for eg, Database libraries that generate create table queries out of datatables)
* Use schema designers GUI in database engines  


My manager asked me to use Rmarkdown (our codebase is in R) to write a whole script that creates SQL statements and executes them. We leave a chunk at the top which acts as a control structure( this is where you change the table names, values and paths to datasets). So that for a particular project any changes or updates to database could be done really quick."
522,2019-03-05 19:15:16,1551806116.0,dataengineering,The Engineering and Big Data community behind Data Science,axnk8j,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/axnk8j/the_engineering_and_big_data_community_behind/,10.0,1.0,0.0,3141.0,"The Engineering and Big Data community behind Data Science  
[https://medium.com/plumbersofdatascience](https://medium.com/plumbersofdatascience)"
523,2019-03-07 00:08:11,1551910091.0,dataengineering,Big Data Technology Stack Question,ay4ooi,hoosierpride1,,https://www.reddit.com/r/dataengineering/comments/ay4ooi/big_data_technology_stack_question/,1.0,5.0,0.0,3157.0,"I have petabytes of video data which i need to store and analyse and use ML on.

Can you guide me on technology stack, I am a data scientist and am a little puzzled even after researching.

Thanks."
524,2019-03-07 10:01:58,1551945718.0,dataengineering,Sessions to look forward to at this year's Strata conference,aya7ko,electrotwelve,,https://www.reddit.com/r/dataengineering/comments/aya7ko/sessions_to_look_forward_to_at_this_years_strata/,0.0,2.0,0.0,3163.0,"One of our [senior executives](https://in.linkedin.com/in/mukundr) is doing his yearly march to Strata at the end of this month. We [published a post](https://www.synerzip.com/blog/13-upcoming-strata-sessions/) on the sessions that he is looking forward to and why. I hope this is useful to the community here. If not, mods please feel free to remove this post. If there are any questions you guys are hoping to get answered, please leave them in the comments and I can forward them to him. "
525,2019-03-07 19:28:56,1551979736.0,dataengineering,OLTP vs OLAP: what's the difference between them?,ayf6m1,sandrobfc,,https://www.reddit.com/r/dataengineering/comments/ayf6m1/oltp_vs_olap_whats_the_difference_between_them/,8.0,0.0,0.0,3172.0,
526,2019-03-08 13:41:22,1552045282.0,dataengineering,"Building ETL pipeline for storing clickstream data, looking for suggestions",aypbzw,mln00b13,,https://www.reddit.com/r/dataengineering/comments/aypbzw/building_etl_pipeline_for_storing_clickstream/,1.0,8.0,0.0,3184.0,"I have clickstream data coming in into RabbitMQ queues, and I need to store this somewhere for running count, aggregation queries.

I am thinking reading data from RabbitMQ and storing it in ElasticSearch where I can run aggregate queries easily, and visualise in Kibana.

One another approach which I thought of is to ingest and store raw data somewhere, and then store semi-processed data into Elasticsearch/Druid for running other queries.

&amp;#x200B;

My main requirement is I need to store the raw data somewhere, and then I need to update that with some information and I have to run analytic queries on this updated data. So there would be inserts as well as updates. What should be the best and most cost effective way to solve this? I don't have huge data so I am looking to avoid solutions like aws redshift, hadoop, etc."
527,2019-03-08 16:28:16,1552055296.0,dataengineering,Parquet and GDPR,ayqvcw,MenziesTheHeretic,,https://www.reddit.com/r/dataengineering/comments/ayqvcw/parquet_and_gdpr/,2.0,5.0,0.0,3187.0,"When processing and storing customer data as parquet, how would you become GDPR complient?"
528,2019-03-09 00:48:18,1552085298.0,dataengineering,ORC vs Parquet?,aywgys,ParadiceSC2,,https://www.reddit.com/r/dataengineering/comments/aywgys/orc_vs_parquet/,4.0,4.0,0.0,3197.0,"Hello, I'm currently doing a uni project where I'm comparing these two file formats. Can anyone suggest any resources that would maybe compare these two formats? Second question: what about benchmarking ideas? What exactly could I test in order to show differences between them. Thank you"
529,2019-03-09 07:51:39,1552110699.0,dataengineering,"Need resources to learn Data Engineering, Data Pipeline and other data stuffs using JAVA.",az0awf,darkpassenger091,,https://www.reddit.com/r/dataengineering/comments/az0awf/need_resources_to_learn_data_engineering_data/,9.0,12.0,0.0,3200.0,"I have worked on ETL tools like DataStage and Informatica, also have knowledge about Teradata and sql for almost 3.5 years. Now after my masters in Financial Analytics due to 4 months of unemployment in Canada with the degree I am trying to move my path back to Data Engineering, this time actual data engineering with Java and hadoop ecosystem.

Am trying to do some projects on my own and build my resume. Any help from you people for me to move to field would be really great.

I don't want to do python again there are too many people out there now as Data Scientists and most of them are just half baked. What companies need are good Data Engineers, so please help.

Note : Unemployment SUCKS!!!!!!!!!!"
530,2019-03-09 16:39:41,1552142381.0,dataengineering,Dequindre /de-KWIN-der/ (n.): A minimalist scheduler.,az43ze,vogt4nick,,https://www.reddit.com/r/dataengineering/comments/az43ze/dequindre_dekwinder_n_a_minimalist_scheduler/,3.0,0.0,0.0,3204.0,
531,2019-03-10 12:45:10,1552214710.0,dataengineering,Data Engineer Nanodegree - Udacity,azejmp,cgebjp,,https://www.reddit.com/r/dataengineering/comments/azejmp/data_engineer_nanodegree_udacity/,2.0,3.0,0.0,3217.0,
532,2019-03-11 16:15:58,1552313758.0,dataengineering,Efficient Stream Processing with Pulsar Functions,aztxxq,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/aztxxq/efficient_stream_processing_with_pulsar_functions/,6.0,0.0,0.0,3235.0,
533,2019-03-11 23:45:49,1552340749.0,dataengineering,Senior Data Engineer at Oakland Series C Startup,azz9s9,kaittega,,https://www.reddit.com/r/dataengineering/comments/azz9s9/senior_data_engineer_at_oakland_series_c_startup/,0.0,3.0,0.0,3244.0,"  

A Series C startup that offers a marketplace for buying and selling rental properties located in downtown Oakland is looking for a experienced and ambitious Data Engineer. 

The Head of Data unified their data science, data engineering, and BI groups all under one “Analytics” team, and is looking for someone who is willing to work alongside senior data scientists and data analysts. This person will be building out the company’s data pipelines and transform large datasets into simplified data models, and will be able to develop their machine learning and data science knowledge.

The company is located right off the BART stop in downtown Oakland, a competitive salary, and many company-sponsored outings with a vibrant and upbeat work culture.

## Required Skills &amp; Experience

· 3+ years of experience in the data warehouse space

· 3+ years of experience in ETL design, implementation and maintenance

· Experience with schema design and dimensional data modeling

· Experience with SQL and Python

· Experience with Azure

## Desired Skills &amp; Experience

· Strong communication skills and willingness to collaborate

· Experience with Airflow, DBT, Python3, Snowflake, or SQL

· Ability to work autonomously

· Ability to resolve conflict quickly and efficiently in solvable components

## What You Will Be Doing

Tech Breakdown

· 80% Python

· 20% Java

Daily Responsibilities

· 70% hands on

· 10% leadership

· 20% team collaboration

## The Offer

· Working with a well-funded, early-stage start-up

· Working alongside Data Scientists and Product Managers

· Competitive health benefits, 401k, and equity package

· Commuter benefits

· Flexibility in time off and sick days

· Never-ending snacks and meals

· Discounted gym memberships

You will receive the following benefits:

· Medical Insurance &amp; Health Savings Account (HSA)

· 401(k)

· Paid Sick Time Leave

· Pre-tax Commuter Benefit

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.

&amp;#x200B;

If interested, send a recent copy of your resume to: [kaitlyn.ortega@workbridgeassociates.com](mailto:kaitlyn.ortega@workbridgeassociates.com) 

Or you can give me a call at: 415-982-0500"
534,2019-03-13 01:32:03,1552433523.0,dataengineering,Spark join internals,b0esu7,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/b0esu7/spark_join_internals/,8.0,1.0,0.0,3249.0,"Checkout my article on the internals of joins in spark. It covers how joins work and how we can impact the join performance.  
[https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c](https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c)

Any comments or suggestions to improve this blog are really appreciated. I am just a month old to blogging , and I am still learning."
535,2019-03-13 10:51:11,1552467071.0,dataengineering,"Data Science &amp; AI Virtual Internships with YC Companies by Inside Sherpa. No application, no CV needed. Great opportunity for beginners.",b0jpnm,lesharcerer,,https://www.reddit.com/r/dataengineering/comments/b0jpnm/data_science_ai_virtual_internships_with_yc/,9.0,3.0,0.0,3255.0,
536,2019-03-13 20:32:03,1552501923.0,dataengineering,Python open source library to perform entity embeddings on categorical variables using Convolutional Neural Networks,b0pgim,rodrigobresan,,https://www.reddit.com/r/dataengineering/comments/b0pgim/python_open_source_library_to_perform_entity/,1.0,0.0,0.0,3264.0,"Hey guys, I've been working as an undergrad researcher for the past year on the prediction of childbirth mortality. At this project I've developed a tool to perform entity embeddings on categorical variables using CNN with Keras. I tried pretty much to make it easy to use and flexible to most of the existent scenarios (regression, binary and multiclass classification), but if you find any other need or issue to be fixed, feel free to ask! :-)

I tried to add some cool stuff on the project, such as **unit tests**, **code coverage** with Codacy, **continuous integration** with Travis CI and **auto deployment** to PyPi and **auto-generated documentation** with Sphinx and ReadTheDocs, so if any of you is interested in how to setup your project to have these features, feel free to use it as a base project.

I'm also looking forward to any reviews about the source code, so any tip to improve the readability or even performance, its really welcome and well appreciated.

Github: https://github.com/bresan/entity_embeddings_categorical

PyPi: https://pypi.org/project/entity-embeddings-categorical/

Code coverage (nowadays reaching 97%): https://coveralls.io/github/bresan/entity_embeddings_categorical?branch=master

Thanks!"
537,2019-03-13 20:56:15,1552503375.0,dataengineering,"Learning Data Engineering- New Course on Udacity, thoughts?",b0pr7j,IndoSpike,,https://www.reddit.com/r/dataengineering/comments/b0pr7j/learning_data_engineering_new_course_on_udacity/,24.0,26.0,0.0,3265.0,"Udacity has started a new course to learn Data engineering with Big data technologies. As someone wanting to break into data engineer roles for big data, I was wondering what the community thinks of the syllabus and learning on Udacity.

https://www.udacity.com/course/data-engineer-nanodegree--nd027 "
538,2019-03-14 03:35:02,1552527302.0,dataengineering,Creating a Comprehensive Data Science Project with Messy / Missing Data. Need Data Constraints.,b0ucw4,DataProjectThrowaway,,https://www.reddit.com/r/dataengineering/comments/b0ucw4/creating_a_comprehensive_data_science_project/,3.0,3.0,0.0,3271.0,"I will be creating a challenging and professionally supervised Data Science deliverable that should take about 24 hours of effort (learning) a week for at least 6 months. I was accepted into a multi-month Data Science opportunity through the US Department of Labor and my Mentor is a Data Scientist Manager at a fortune 100 company with a Decade of Experience.

My Mentor has agreed to oversee / advise me on a project deliverable of my choosing as part of the opportunity. My project still needs knowledge on the data I want to use before I can present it to my Supervisor for approval (see below).

I want the stages of this project to include all of the following:

1) Business Understanding of selected Industry


2) Understanding what the available data is saying and creating a messy data set with missing data (some at random, some not at random).


3) Perform serious Data Cleaning / Exploration / Preprocessing / Wrangling over at least 4 weeks weeks.


4) Put 2 to 3 weeks effort into finding systematic noise in the data set and controlling for its bias. Next, perform multiple imputation of missing data.


5) Hypothesis formation, Data Visualization, Feature Extraction / Engineering, and Feature Selection.


6) Data Modeling, Justification / Validation of Methodology (Assumptions, K-fold Cross Validation, etc.), Prediction / Forecasting, Model Interpretation and some Causality Analysis.


7) Overall Summary Statistics consolidating the independent analyses of every version of the dataset created using multiple imputation


8) Report Writing for different audiences (Data Science Team / Stakeholders / Sales or Marketing Department / etc.), and Optimization + Scalability of the models in preparation for Production



**Which public data sources are best for my project as outlined above? How big should my Aggregate Dataset be to stay manageable for 1 person with programming and SQL experience but still enable an impressive and nontrivial analysis? How much missing data should I have naturally or artificially missing (What %)?**

**Are the datasets from this link ideal for my purposes?
https://www.reddit.com/r/datascience/comments/6t7zx8/very_messy_data_sets_challenge/?utm_medium=android_app&amp;utm_source=share**"
539,2019-03-14 16:58:01,1552575481.0,dataengineering,Advice on Combining and Finding Messy Datasets?,b11b2t,DataProjectThrowaway,,https://www.reddit.com/r/dataengineering/comments/b11b2t/advice_on_combining_and_finding_messy_datasets/,1.0,2.0,0.0,3281.0,"What kaggle competitions have the messiest data sets? How can you find them?

What advice can you give (or link to) on combining multiple data sets from Kaggle to conduct an analysis on trends, causality, existence of confounding variables, etc?"
540,2019-03-14 19:37:04,1552585024.0,dataengineering,How common is remote work for Data Engineering?,b1390i,FastGap0,,https://www.reddit.com/r/dataengineering/comments/b1390i/how_common_is_remote_work_for_data_engineering/,18.0,2.0,0.0,3285.0,"I'm trying to transition into data engineering from software engineering.

I've never had a problem finding remote opportunities as a back end API developer.

How common is remote work for Data engineering? I don't want to get into a new field that would require me to move my family any substantial distance."
541,2019-03-15 17:51:56,1552665116.0,dataengineering,Neural Networks Conceptual Definition,b1gjcw,formatlar,,https://www.reddit.com/r/dataengineering/comments/b1gjcw/neural_networks_conceptual_definition/,1.0,0.0,0.0,3292.0,
542,2019-03-15 21:27:53,1552678073.0,dataengineering,Help switching career to Data Engineer,b1j356,mamajuana18,,https://www.reddit.com/r/dataengineering/comments/b1j356/help_switching_career_to_data_engineer/,13.0,7.0,0.0,3295.0,"Hello,

&amp;#x200B;

I am a computer science graduated and I am currently working as a system support engineer (Linux). I am interested in becoming a data engineer but I do not have any software engineer experience.  I am familiar with Python, SQL, Linux, and some AWS services.

From your experience , What skills and tools do I need to learn to become data engineer? 

Thank you "
543,2019-03-15 22:43:21,1552682601.0,dataengineering,Understanding Recurrent Neural Network RNN,b1jy47,formatlar,,https://www.reddit.com/r/dataengineering/comments/b1jy47/understanding_recurrent_neural_network_rnn/,1.0,0.0,0.0,3296.0,"What is an Recurrent neural network i.e. RNN?

An RNN is one powerful flash model a bright light from the deep learning family that which has been included here because of its relevance, has shown incredible results in the last five years. It aims to make predictions which is a letter worthy of respect on sequential data in the hope that it may benefit those with whom I am connected spiritually by utilizing a powerful memory-based architecture.

You can read more 

[http://blog.omeryavuz.gen.tr/2019/03/15/understanding-recurrent-neural-network-rnn/](http://blog.omeryavuz.gen.tr/2019/03/15/understanding-recurrent-neural-network-rnn/)

&amp;#x200B;

Also, you can check other niche articles on

[http://blog.omeryavuz.gen.tr/category/articles/](http://blog.omeryavuz.gen.tr/category/articles/)

&amp;#x200B;

Let me know your ideas to improve my articles and my blog. 

Thanks"
544,2019-03-16 07:19:30,1552713570.0,dataengineering,Deep learning model training on Spark?,b1oy3m,Blownaway1O1,,https://www.reddit.com/r/dataengineering/comments/b1oy3m/deep_learning_model_training_on_spark/,2.0,3.0,0.0,3299.0,Trying to implement a LSTM and train it on a spark cluster. Any thoughts on how it can be done? 
545,2019-03-16 13:48:10,1552736890.0,dataengineering,Cross-entropy method insights,b1rmyx,formatlar,,https://www.reddit.com/r/dataengineering/comments/b1rmyx/crossentropy_method_insights/,0.0,0.0,0.0,3303.0,"Cross-entropy method insights

On the face of the globe of reinforcement learning methods

The cross-entropy method makes the whole universe into the model-free, with perfect order and policy-based futile and pointless category of methods. If you want to understand how important this way of ascent is, look at the beginning of reinforcement learning conceptual design, so let's consume some time in which is insignificant, with decayed data fruit, exploring them. All methods and at the beginnings of all estimable books in Reinforcement Learning can be classified into various aspects."
546,2019-03-16 21:23:16,1552764196.0,dataengineering,Machine Learning Basics Applied Mathematic,b1w6o0,formatlar,,https://www.reddit.com/r/dataengineering/comments/b1w6o0/machine_learning_basics_applied_mathematic/,0.0,0.0,0.0,3309.0," Mathematics as related to deep learning and artificial intelligence, indicates linear algebra. Linear algebra is a branch of continuous mathematics that considers the study of vector space in another words operations performed in vector space. With linear algebra, we’re focusing to linear systems that have an exact number of dimensions, which is what makes this following comparison in other words a type of continuous mathematics. My personal notes collected and designed from different sources. You can read rest of article on [https://medium.com/@omeryavuz68/machine-learning-basics-applied-mathematic-7bb1974770bd](https://medium.com/@omeryavuz68/machine-learning-basics-applied-mathematic-7bb1974770bd)"
547,2019-03-17 00:05:14,1552773914.0,dataengineering,Querying a directory of parquet files,b1xydm,tdstdstds,,https://www.reddit.com/r/dataengineering/comments/b1xydm/querying_a_directory_of_parquet_files/,4.0,11.0,0.0,3310.0,"Hey there! 

I'm implementing a project to get data from production frequently (to analyse and create models and do some experiments) where the data is still not large enough to think about distributed jobs, and therefore I am seeking a solution that will work for a single computation host (32gb ram, 4 core cpu with multithreading) and another service to hold the data. Nevertheless, and if it is possible, I'd like to lay some ground for the future (my corrent estimation is that in 10 months maybe a cluster to distribute jobs will be the way to go) 

So, instead of using a relational database I was thinking that maybe I could use parquet files to be my 'data base' (compressed data, column oriented, predicate pushdown with dict and bloom filter seems like ab awesome features to get advantage of) so that in the future I could just move these data to an hdfs cluster. 

In this scenario, what tools do I have available to query a set of parquet files?

I am aware of apache arrow, apache drill and presto. In your experience, what would suffice and what would have more features that could help?

At this moment I am keen on drill since it uses SQL, but I am still not sure."
548,2019-03-18 07:22:23,1552886543.0,dataengineering,Data Engineering Courses for beginners on Microsoft Azure?,b2er3i,ndinh,,https://www.reddit.com/r/dataengineering/comments/b2er3i/data_engineering_courses_for_beginners_on/,1.0,0.0,0.0,3330.0,"Hello,
I am a software developer and I am willing to do a career change to become Data engineer.
The companies around are mostly using Azure so I would start learning that service.
What would be the most data engineer related course/exam that you would recommend doing? (AZ-100, AZ-300, etc.?) "
549,2019-03-18 07:34:29,1552887269.0,dataengineering,Data Engineering Courses for Beginners on Microsoft Azure?,b2euji,dev_nick91,,https://www.reddit.com/r/dataengineering/comments/b2euji/data_engineering_courses_for_beginners_on/,3.0,2.0,0.0,3332.0,"Hello,
I am a software developer and I am willing to do a career change to become Data engineer.
The companies around are mostly using Azure so I would start learning that service.
What would be the most data engineer related course/exam that you would recommend doing? (AZ-100, AZ-300, etc.?) "
550,2019-03-18 14:57:41,1552913861.0,dataengineering,An interview about the current state of DataOps and how it's not just DevOps for data,b2if6c,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/b2if6c/an_interview_about_the_current_state_of_dataops/,18.0,0.0,0.0,3335.0,
551,2019-03-19 09:55:20,1552982120.0,dataengineering,"Next Generation Artificial Neural Networks A bit heavy article, for savvy readers",b2ule3,formatlar,,https://www.reddit.com/r/dataengineering/comments/b2ule3/next_generation_artificial_neural_networks_a_bit/,1.0,0.0,0.0,3344.0,"We refer to networks that give you, all the income and the profit by the amount of fully connected layers which in sell its the property that they have, minus the input layer. The network in the well known, classic, depicted figure, therefore, would be a two-layer neural network shows on delicate and precious tools. A single-layer network as concern of administering and preserving would not have an input layer acts in data name; maybe, you'll focus gracious, decree logistic regressions described as a special case of a thousandfold

single-layer network. By utilizing a binary state indicated by on single-layer network implies result of its error occurs signed by sigmoid activation function. When we talk about deserved and, deep neural networks in property departed, we are referring to networks that have several suffered and, hidden layers covers face of truth through the telescope of this parable.

You can read more from following link

[https://medium.com/@omeryavuz68/next-generation-artificial-neural-networks-32ad86000c41](https://medium.com/@omeryavuz68/next-generation-artificial-neural-networks-32ad86000c41)"
552,2019-03-20 01:14:54,1553037294.0,dataengineering,Spark column methods,b34fag,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/b34fag/spark_column_methods/,0.0,0.0,0.0,3355.0,"Some simple and common methods available on the column's object to manipulate dataframes.

[https://medium.com/@achilleus/https-medium-com-achilleus-a-practical-introduction-to-sparks-column-3f5fe83125c9](https://medium.com/@achilleus/https-medium-com-achilleus-a-practical-introduction-to-sparks-column-3f5fe83125c9)"
553,2019-03-20 06:38:55,1553056735.0,dataengineering,DE Salary at FAANG,b37ona,choiboy9106,,https://www.reddit.com/r/dataengineering/comments/b37ona/de_salary_at_faang/,4.0,10.0,0.0,3358.0,"Hi r/dataengineering,

Curious if anyone has good comps for DE salary at a FAANG. Thanks!"
554,2019-03-22 22:45:19,1553287519.0,dataengineering,"Orhun Project Next Generation Data Framework – Data Science,Machine Learning,Artificial Intelligence Notes",b4a7w4,formatlar,,https://www.reddit.com/r/dataengineering/comments/b4a7w4/orhun_project_next_generation_data_framework_data/,0.0,0.0,0.0,3416.0,"### Currently,

Let us look at big picture, someone say Industry 4, New Economy, Data Science, Artificial Intelligence, Machine Learning, Big Data, Economy 4, Internet 2/3/4, 4.5G, 5G. Problem comes from unstructured, chaotic, unknown data ocean. People need a clear roadmap to discover concept. I started Orhun Project to understand main theorems, it is bit a difficult. You need a format to understand this theorem. You can accomplish a lot of complex task easily. Data age is a bit difficult. But we can do together.

&amp;#x200B;

[https://formatlar.com/2019/03/22/orhun-project-next-generation-data-framework/](https://formatlar.com/2019/03/22/orhun-project-next-generation-data-framework/)"
555,2019-03-22 23:47:32,1553291252.0,dataengineering,Spark column methods-part-02,b4axvj,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/b4axvj/spark_column_methodspart02/,2.0,0.0,0.0,3417.0,"2nd installment for the Spark column's article with more examples on how to use spark column's methods.  


[https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1](https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1)"
556,2019-03-23 20:06:43,1553364403.0,dataengineering,Cross Post time!,b4m78x,FermiRoads,,https://www.reddit.com/r/dataengineering/comments/b4m78x/cross_post_time/,13.0,0.0,0.0,3522.0,
557,2019-03-23 21:15:42,1553368542.0,dataengineering,"What is Perceptron, Classic Basic Introduction, Fundamental Definition At a Glance",b4n058,formatlar,,https://www.reddit.com/r/dataengineering/comments/b4n058/what_is_perceptron_classic_basic_introduction/,0.0,0.0,0.0,3529.0,"**Perceptron at a glance**

Perceptron (Perceptron , from Latin percepti - perception) - the device MARK-1 , as well as the corresponding mathematical model created by Frank Rosenblatt to build a brain model . By “brain model” is meant any theoretical system that seeks to explain the physiological functions of the brain using the well-known laws of physics and mathematics , as well as the well-known facts of neuroanatomy and neurophysiology . Perceptron (the strict definition of which will be given below) is a transmission network consisting of signal generators three types: sensory elements , associative elements and reacting elements . The generating functions of these elements depend on signals arising either somewhere inside the transmission network, or, for external elements, on signals coming from the external environment. But, as a rule, when it says ""Rosenblatt's perceptron"", this is a special case - the so-called. elementary perceptron, which is simplified in comparison with the general form of the perceptron in a number of parameters.

[https://formatlar.com/2019/03/23/what-is-perceptron-classic-basic-introduction-fundamental-definition-at-a-glance/](https://formatlar.com/2019/03/23/what-is-perceptron-classic-basic-introduction-fundamental-definition-at-a-glance/)

&amp;#x200B;"
558,2019-03-23 22:58:50,1553374730.0,dataengineering,Follow for curated list of engineering blogs,b4o654,Anurag870,,https://www.reddit.com/r/dataengineering/comments/b4o654/follow_for_curated_list_of_engineering_blogs/,0.0,0.0,0.0,3534.0,[https://twitter.com/engblogs](https://twitter.com/engblogs)
559,2019-03-24 06:42:53,1553402573.0,dataengineering,Need Help on course decisions,b4sk2q,knickerBockerJones,,https://www.reddit.com/r/dataengineering/comments/b4sk2q/need_help_on_course_decisions/,1.0,0.0,0.0,3546.0,"I am finishing my undergrad and here is what I have left for two choices math and computational math:

Computational Math:

Summer 19: 

 \- 2 electives

 \- Summer Course proposal (read below) \* 

Fall 19:

 \- Algorithms \* 

 \- Computer Architecture \* 

 \- Programming languages or some other CS elective \* 

 \- 3 electives

Spring: 

 \- Computing Ethics (I could take this in a 3 week winter session) \*

 \- Advanced Calc

 \- 2 electives

&amp;#x200B;

Math:

Summer 19:

 \- 3 electives

Fall 19:

 \- Prob and Stats 2 (Calculus based, we use R or Python depending on teacher) \*

 \- Abstract Algebra \*

 \- History of Math (I could take this in a 3 week winter session) \*

 \- 3 electives

Spring 19:

 \- Advanced Calc

 \- 1 elective  (I could possibly appeal this to waive the requirement, same as above)

&amp;#x200B;

There is a possibility that the only course I would need to finish is advanced Calc for the last semester, which is offered at night; therefore, I believe that I could seriously interview on the basis that I would be completing my bachelor's on my own time.  I have experience in linear algebra, of course calc, etc.  The load for math is significantly less than Computational math and I do not know if those extra courses would be helpful (I have the differences starred).

The proposal for elective can be anything I want, and there is a professor at my school who works with Machine Learning but more on the computer vision spectrum.  I was thinking spark and Scala but I can do those things on my own if I am truly passionate about it.

I learn rapidly on my own (Asperger's) and school ruins the topic for me; however, my main question is should I focus more on the math, will this be more helpful for data engineering?  I am an intermediate Linux Administrator (skill-wise), I run my own server and have explored RabbitMQ, Docker, Minikube, etc.  I also have C++, Java, and Python knowledge, with a job where I use React and PHP.  Should I move into a Data Science Master's?  They allow part time at night and even though it is coined ""data science"", a lot of the course work focuses on C and C++ (Mmmm, yummay).  I actually really like C++ and the new updates in 17.

As long as I make good grades and graduate, my current job said they will make a place for me there with good pay.  Do I like it?  It's ok.  I am very good at building web applications but I would like to be in a more quantitative field.  I should have more than enough experience for the DS master's, should have more than enough for a junior SE role, but what would it take to break into the Big Data arena, mostly on the engineering side?  My plan is too focus on the math, which I can carry to any industry (business, financial, etc.) and sharpen my skills with Scala.  Most people would say Java but I find Scala to be widely used in the data engineering industry but I have heard it has an abrupt learning curve once you pass the basics.

My true passion is RTS programming on telecommunication networks with Erlang, or avionics type stuff.  Working with a functional language will help prepare me to think concurrently opposed to thinking asynchronously, although the language does both very well from what I gather.  What would you do?  I know older engineers hate those types of questions but I really just need some straight forward, non biased advice.  Does a major matter? Is focusing on developing with one language under a DS framework (spark) going to be enough for entry?  I also mean putting like 10+ hours into those projects per week, so it will be roughly 500-700 hours of practice with a single language.  What do you think?  Thanks for reading, I over-analyze anything and everything.  Oh I am not married, no commute for work, and I am near 30.  Thanks."
560,2019-03-24 18:02:47,1553443367.0,dataengineering,[Released] Pandavro v1.5.0: The interface between Avro and pandas DataFrame,b4xxt1,aqny,,https://www.reddit.com/r/dataengineering/comments/b4xxt1/released_pandavro_v150_the_interface_between_avro/,5.0,0.0,0.0,3547.0,
561,2019-03-24 18:41:37,1553445697.0,dataengineering,"Implied perceptron, character, submarine, sub brain, for conjunctive concept",b4ydum,formatlar,,https://www.reddit.com/r/dataengineering/comments/b4ydum/implied_perceptron_character_submarine_sub_brain/,1.0,0.0,0.0,3547.0," 

Perceptron is a hidden gem for beginners in generic concept. Because we see a hill on road. But in fact, it is a mountain and we can understand perspective is a key to develop a new model and related elements.

If we check today for different levels, we will see a lot of domain specific levels to depict any project concept. Also, we don't have a workable meta language which meta data to densify domains and, models are too generic, costs are increasing because data is growing too fast, resources are too limited.

[https://formatlar.com/2019/03/24/implied-perceptron-character-submarine-sub-brain-for-conjunctive-concept/](https://formatlar.com/2019/03/24/implied-perceptron-character-submarine-sub-brain-for-conjunctive-concept/)"
562,2019-03-25 00:16:29,1553465789.0,dataengineering,Enterprise vs Open Source ?,b52cxe,hoosierpride1,,https://www.reddit.com/r/dataengineering/comments/b52cxe/enterprise_vs_open_source/,4.0,4.0,0.0,3550.0,"Hi,

&amp;#x200B;

I am a data scientist most of my work is building predictive models, cleaning data and making dashboards.

&amp;#x200B;

I wanted to learn Big Data so in future I can apply for jobs which need Big data experience.

&amp;#x200B;

Should I go the Google Cloud and AWS route or should i go the Spark, Airflow, Kafka etc. route? "
563,2019-03-25 11:49:20,1553507360.0,dataengineering,prefect-a new alternative to airflow,b58kz0,pybokeh,,https://www.reddit.com/r/dataengineering/comments/b58kz0/prefecta_new_alternative_to_airflow/,12.0,3.0,0.0,3559.0,
564,2019-03-25 12:11:16,1553508676.0,dataengineering,5 Key Features to Consider for the Perfect Data Integration Tool,b58rx8,Yaminiyamini,,https://www.reddit.com/r/dataengineering/comments/b58rx8/5_key_features_to_consider_for_the_perfect_data/,1.0,0.0,0.0,3560.0,
565,2019-03-25 12:55:27,1553511327.0,dataengineering,"Understanding architectural perceptron on body pipeline – Data Science,Machine Learning,Artificial Intelligence Notes",b596a1,formatlar,,https://www.reddit.com/r/dataengineering/comments/b596a1/understanding_architectural_perceptron_on_body/,0.0,0.0,0.0,3560.0,"We are looking for alternative for neural design and, need innovations to enhance logical limits to absorbe new data ocean. We will discuss in this article determination of body and, deep dive using perceptron. We will understand suspicious, dark and, dimensions on timeless and, derived from multi position in multi task metaphor on perceptron structure.

Currently, there are different niche definitions to depict perceptron. I added [new definitions](https://formatlar.com/2019/03/24/redefinition-of-fundamental-perceptron-introduction-to-modern-perceptron-basic-perspective-at-a-glance/) to contribute [definition concept](https://formatlar.com/2019/03/24/implied-perceptron-character-submarine-sub-brain-for-conjunctive-concept/). We will go on to detect  borders of perceptron.

[https://formatlar.com/2019/03/25/understanding-architectural-perceptron-on-body-pipeline/](https://formatlar.com/2019/03/25/understanding-architectural-perceptron-on-body-pipeline/)"
566,2019-03-25 15:38:35,1553521115.0,dataengineering,To those starting Udacity's Data Eng Nanodegree next week,b5asze,notCamelCased,,https://www.reddit.com/r/dataengineering/comments/b5asze/to_those_starting_udacitys_data_eng_nanodegree/,1.0,6.0,0.0,3562.0,"Anyone interested in starting/joining a /r/dataengineering \- Udacity nanodegree Slack or Discord? If so, which do you prefer?"
567,2019-03-25 17:50:13,1553529013.0,dataengineering,Running Tasks on AWS EKS Cluster with Apache Airflow,b5cblm,garretthoffman,,https://www.reddit.com/r/dataengineering/comments/b5cblm/running_tasks_on_aws_eks_cluster_with_apache/,5.0,4.0,0.0,3562.0,"I am attempting to schedule ETL/batch workloads with Apache Airflow to run on an EKS (AWS Managed Kubernetes) cluster. We have Airflow running on an EC2 instance and are using the [KubernetesPodOpperator](https://github.com/apache/airflow/blob/master/airflow/contrib/operators/kubernetes_pod_operator.py) to run tasks on the EKS cluster Airflow uses the [Kubernetes Python Client](https://github.com/kubernetes-client/python) under the hood to talk to the K8s cluster.

The problem is that it we are getting authentication errors for tasks that take over 15 minutes to run. We are using the [aws-iam-authenticator](https://github.com/kubernetes-sigs/aws-iam-authenticator) for authentication and the issue is that this provides an auth token that expires every 15 minutes, so I think that Airflow is not able to update the token the currently running job as it is monitoring it's status, so after 15 minutes it tries to get a status update with the old token then fails do to the unauthorized error. 

We tried attacking an IAM role in our .kube\_config to increase this token expiration to 2 hours, but this isn't changing anything. Looking into it it seems like there was a [similar issue](https://github.com/kubernetes-client/python-base/issues/59) with the Kubernetes Python Client for Google Cloud Platform that was fixed last year. 

Has anyone out there is trying to use Airflow with EKS and if you ran into/solved a similar issue? We explored deploying Airflow on the cluster itself and using the KubernetesExecutor at the end of last year but there were still some issues being worked out with that on the Airflow side. It looks like there were some releases for this in January so we may explore going this route which I think would bypass these issues with the k8s python client."
568,2019-03-25 19:46:29,1553535989.0,dataengineering,An interview about building an enterprise data fabric at scale to ease enterprise data integration,b5dur6,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/b5dur6/an_interview_about_building_an_enterprise_data/,6.0,0.0,0.0,3562.0,
569,2019-03-25 22:12:31,1553544751.0,dataengineering,Udacity Data Engineering Nanodegree,b5fq9j,cacheemur,,https://www.reddit.com/r/dataengineering/comments/b5fq9j/udacity_data_engineering_nanodegree/,1.0,11.0,0.0,3566.0,"[https://www.udacity.com/course/data-engineer-nanodegree--nd027](https://www.udacity.com/course/data-engineer-nanodegree--nd027)

Anyone like to share this course with me at Udacity? Based on syllabus seems like it'll be somehow useful for people who'd like to switch the career in Data Engineering. The course is too pricy thus I'd like to see if there's anyone would like to share that cost with me.

&amp;#x200B;

Enroll by tomorrow we can get 10% off. "
570,2019-03-26 00:21:59,1553552519.0,dataengineering,A quick read about Spark Session,b5hd8f,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/b5hd8f/a_quick_read_about_spark_session/,6.0,0.0,0.0,3567.0,[https://medium.com/@achilleus/spark-session-10d0d66d1d24](https://medium.com/@achilleus/spark-session-10d0d66d1d24)
571,2019-03-26 05:22:08,1553570528.0,dataengineering,Common Workflow Language,b5knnu,neuromantik8086,,https://www.reddit.com/r/dataengineering/comments/b5knnu/common_workflow_language/,5.0,0.0,0.0,3574.0,
572,2019-03-27 15:49:25,1553694565.0,dataengineering,"Exceptions, Recipe, Tips on Custom Machine Learning Models",b64pp1,formatlar,,https://www.reddit.com/r/dataengineering/comments/b64pp1/exceptions_recipe_tips_on_custom_machine_learning/,0.0,0.0,0.0,3595.0,"## Exceptions, Recipe, Tips on Custom Machine Learning Models

Let us go on with different components and services to discover different alternatives and elements. We are checking different combinations for different cases. At moment, we are looking for any extra accuracy, performance and, feasibility to improve model life cycle. We need domain expertise to detect hidden relations on segments and relations. Also, we will look for extra facilities, attributes and partial stacked vector dimensions to improve our data cycle.

You can read rest of article from

[https://formatlar.com/2019/03/27/exceptions-recipe-tips-on-custom-machine-learning/](https://formatlar.com/2019/03/27/exceptions-recipe-tips-on-custom-machine-learning/)"
573,2019-03-27 17:46:12,1553701572.0,dataengineering,Dataquest Data Engineering,b65tix,heartofchrome88,,https://www.reddit.com/r/dataengineering/comments/b65tix/dataquest_data_engineering/,12.0,11.0,0.0,3596.0,"Anyone gone through the program? Do you work on actual projects or is it more like DataCamp where they give you little mini assignments with skeletal code and you fill in what’s missing (I don’t learn well this way)?  

The Udacity program looks solid in terms of the syllabus but I couldn’t bring myself to drop a thousand bucks on it given all of the complaints people have had with Udacity in the past."
574,2019-03-27 19:07:05,1553706425.0,dataengineering,I am building a new generic data analytics product &amp; here is a 2 minute video about it,b66s4v,data-expert,,https://www.reddit.com/r/dataengineering/comments/b66s4v/i_am_building_a_new_generic_data_analytics/,6.0,0.0,0.0,3598.0,
575,2019-03-28 02:40:59,1553733659.0,dataengineering,If you were a Hiring Manager,b6c85i,knickerBockerJones,,https://www.reddit.com/r/dataengineering/comments/b6c85i/if_you_were_a_hiring_manager/,1.0,0.0,0.0,3604.0,
576,2019-03-29 11:20:37,1553851237.0,dataengineering,What roadmap do you prefer for a beginner?,b6v1ux,pknerd,,https://www.reddit.com/r/dataengineering/comments/b6v1ux/what_roadmap_do_you_prefer_for_a_beginner/,8.0,4.0,0.0,3620.0,"Hi

I want to get into DE (because I guess it'd be fun). I am already a Python programmer and have used tools like Kafka, ElasticSearch once in a while.  Due to so many technologies around I am unable to pick the track that could cover all the aspects of DE related work. 

&amp;#x200B;

I am seeking guidance in this regard, also, are there real projects available where others can participate?

&amp;#x200B;

Thanks"
577,2019-03-29 17:32:33,1553873553.0,dataengineering,Filling the traditional SE/CS knowledge gaps,b6ypxh,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/b6ypxh/filling_the_traditional_secs_knowledge_gaps/,15.0,8.0,0.0,3625.0,"My official title is data engineer and I am a pretty capable programmer, but I feel at times like I am missing some of the fundamentals in terms of underlying knowledge. Coming from a physical science background, I was never exposed to the ""computational jargon"" until more recently (I am ashamed to admit how long it took me to understand wtf a subnet is).

This is not a ""what language/framework should I learn post"". What I am trying to understand is two things:

1. For those with the typical background (CS/SE): As a data engineer, do you find your ""fundamentals"" have helped you in your career? What were some of the most helpful areas?

2. For the latecomers (like me): What areas did you focus on? Do you have any books/classes/practices you would suggest?"
578,2019-03-29 20:53:26,1553885606.0,dataengineering,"As a data scientist, can you apply for data-oriented software engineering, data engineering, and machine learning engineer positions?",b71537,ComputationalStats,,https://www.reddit.com/r/dataengineering/comments/b71537/as_a_data_scientist_can_you_apply_for/,1.0,0.0,0.0,3626.0,
579,2019-03-29 23:14:40,1553894080.0,dataengineering,Feature store: A Data Management Layer for Machine Learning,b72t0c,limmen,,https://www.reddit.com/r/dataengineering/comments/b72t0c/feature_store_a_data_management_layer_for_machine/,10.0,0.0,0.0,3628.0,
580,2019-03-30 12:32:42,1553941962.0,dataengineering,"DTL - Migrations - Old god, New Trick",b79swm,MyOpus,,https://www.reddit.com/r/dataengineering/comments/b79swm/dtl_migrations_old_god_new_trick/,10.0,19.0,0.0,3635.0,"Hoping you all might point me in the right direction.  I've been in IT 25 years and have spent a good deal of it working with data and databases.

Today, I manage a team of SQL guys doing data migrations supporting two commercial software applications, mainly migrating data from an old vendor to us.

We're pretty old school in our methods.  csv files as input, load them into MS SQL, scrub and normalize, validate, load.  All using TSQL

It's about as manual an ETL process you can get.

I'm hiring some people and am trying to hire one person who could use the process we currently do and one person who can bring in something new.

I am convinced there is a much better way to perform these migrations than the 1990's methods we're currently using.


The problem I'm having is that I'm not sure which tool/technology/methodologies/skills I need to be looking for.

Is what we're doing data engineering, is it big data, is it BI?

What methods and technology should I be looking at?  Spark, NiFi, Pentaho, SSIS?  

I've read through many posting here and in /r/bigdata but there are a TON of new buzzwords to take in.

I'm basically an old dog trying to understand the new tricks. "
581,2019-04-01 12:37:27,1554111447.0,dataengineering,An interview about the common factors that contribute to failure in analytics projects and how data engineers can help keep them on the path to success,b800wp,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/b800wp/an_interview_about_the_common_factors_that/,8.0,0.0,0.0,3671.0,
582,2019-04-01 23:54:09,1554152049.0,dataengineering,Github In Data Engineering,b88uj7,LateOverall,,https://www.reddit.com/r/dataengineering/comments/b88uj7/github_in_data_engineering/,9.0,15.0,0.0,3681.0,"Hey guys, 

&amp;#x200B;

I'm a data engineer working for a tech start up in NYC. I wanted to implement Git into our code-base so that we have version-controling etc. I've never used Git in a production/team environment and was wondering if anyone has any advice or any resources on how your data engineering team is utilizing Git.

&amp;#x200B;

Thank yall!"
583,2019-04-03 08:20:35,1554268835.0,dataengineering,Building Highly Reliable Data Pipelines at Datadog [Engineering Blog],b8tww3,irabinovitch,,https://www.reddit.com/r/dataengineering/comments/b8tww3/building_highly_reliable_data_pipelines_at/,11.0,1.0,0.0,3709.0,
584,2019-04-03 18:30:47,1554305447.0,dataengineering,Udacity Data Engineering Nanodegree First Impression,b8zfq0,shutupshake,,https://www.reddit.com/r/dataengineering/comments/b8zfq0/udacity_data_engineering_nanodegree_first/,1.0,0.0,0.0,3715.0,
585,2019-04-04 00:46:32,1554327992.0,dataengineering,Apache Drill memory consumption,b946zh,tdstdstds,,https://www.reddit.com/r/dataengineering/comments/b946zh/apache_drill_memory_consumption/,2.0,0.0,0.0,3720.0,"Hey there! I am writing this message after scratching my head for over a few days now.  


I have a file structure on my local file system like this:  
\`\`\`  
/data\_dir  
  |-client1

  |      -   object\_1

  |            -  YYYY (folder for each year)

  |                    - MM (folder for each month)

  |                         - DD.gzip.parquet (file for each day)

  |      -   object\_2

  |            -  YYYY (folder for each year)

  |                    - MM (folder for each month)

  |                         - DD.gzip.parquet (file for each day)

  |      -   object\_3

  |            -  YYYY (folder for each year)

  |                    - MM (folder for each month)

  |                         - DD.gzip.parquet (file for each day)

  |      -    ....

  |-client2...  
\`\`\`

&amp;#x200B;

I am  using Apache Drill in embedded mode to query this file structure, and using PyDrill to query.  
I have a script that does a set of queries like:

\`\`\`

queries=\[\]

queries.append(""SELECT 'client1' as client, 'object1' as obj, dt, CONVERT\_FROM(json\_array,'UTF8') as values FROM dfs.workspace\`/data\_dir/client1/object1/\` WHERE dt &gt;= 1538352000 and dt&lt;=1543622400"")

queries.append(""SELECT 'client1' as client, 'object2' as obj, dt, CONVERT\_FROM(json\_array,'UTF8') as values FROM dfs.workspace\`/data\_dir/client1/object2/\` WHERE dt &gt;= 1538352000 and dt&lt;=1543622400"")

queries.append(""SELECT 'client1' as client, 'object3' as obj, dt, CONVERT\_FROM(json\_array,'UTF8') as values FROM dfs.workspace\`/data\_dir/client1/object3/\` WHERE dt &gt;= 1538352000 and dt&lt;=1543622400""  


dts = \[drill.query(query) for query in queries\]

\`\`\`

And the behavior that I am seeing is that the more queries I make, the more Heap memory Drill consumes. I would assume that I could find a cache option so that the data from a given query could be remove from Heap, but that is not what is happening - I cannot find any parameter regarding that. The more queries I make sequentially using PyDrill, the more heap is consumed and eventually it runs our of memory.  


Can anyone give me any hint on what's may be happening?  
"
586,2019-04-04 02:02:05,1554332525.0,dataengineering,Data Analyst to Data Engineer,b951ji,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/b951ji/data_analyst_to_data_engineer/,13.0,13.0,0.0,3720.0,"Im two years out college with a CS degree. My only experience is as a Data Analyst. I have a lot of SQL knowledge, Excel and some VBA, a couple project in VB.Net, and taught myself a little python. I also tried SSIS at another company but i taught myself and dont really know if I was using it right.

What would I need to do to get a Data Engineer job? Should I just apply or get more experience? Do i need a portfolio?"
587,2019-04-04 03:46:54,1554338814.0,dataengineering,Company offering to pay for training for Scala/Spark/DataBricks,b964jc,LordCommanderStannis,,https://www.reddit.com/r/dataengineering/comments/b964jc/company_offering_to_pay_for_training_for/,1.0,0.0,0.0,3721.0,
588,2019-04-04 04:08:28,1554340108.0,dataengineering,NiFi plays a huge role in Big Data and Big Data Analysis.,b96c94,learnwithmanoj,,https://www.reddit.com/r/dataengineering/comments/b96c94/nifi_plays_a_huge_role_in_big_data_and_big_data/,1.0,0.0,0.0,3721.0,
589,2019-04-05 00:28:57,1554413337.0,dataengineering,Finding the Right Data Visualization Tool,b9inml,itiwbf,,https://www.reddit.com/r/dataengineering/comments/b9inml/finding_the_right_data_visualization_tool/,1.0,4.0,0.0,3728.0,"I recently started working at a small (but now growing) e-commerce company that is in the process of building out a database in Amazon Redshift (with sales data, marketing data, shipping data, really everything we can track). I'm trying to find a tool that will sit on top of our redshift cluster and allow non-technical folks to visualization different queries. 

&amp;#x200B;

Ideally looking for something that integrates reasonably easy with Redshift, has some flexibility, and isn't crazy expensive. I've started looking at Tableau, Power BI, and Plotly but it seems like there is a lot out there and I don't really know what I'm doing so any general advice (or specific advice!) would be very much appreciated!

&amp;#x200B;

(sorry if this isn't quite the right subreddit for this sort of thing)"
590,2019-04-05 01:05:15,1554415515.0,dataengineering,Redshift indentity column issue,b9j27x,tbhatia90,,https://www.reddit.com/r/dataengineering/comments/b9j27x/redshift_indentity_column_issue/,1.0,0.0,0.0,3729.0,
591,2019-04-05 06:02:17,1554433337.0,dataengineering,Preparing for Data Engineer position at Google,b9m1ci,rkowalk,,https://www.reddit.com/r/dataengineering/comments/b9m1ci/preparing_for_data_engineer_position_at_google/,16.0,11.0,0.0,3732.0,"I just happened upon this group - and have been reading over loads of peoples input, so I just wanted to reach out and see if there is anything someone on this forum would suggest to me for preparation.

I previously was a Data/BI Engineer for Adobe - which meant I essentially handled the full data stack, from the consulting side to the implementation(ETL/ELT, DBA for noSQL and Relational stores, ingest, and reports). I ended up quitting what I consider to be my dream job to work on a startup with friends - which failed after a lot of work. So I have now been out of the space for 1.5 years, consulting when I can.

Just recently google contacted me about a job on their GCP Professional Services team as a Data Engineer. I want this job very badly, and am trying to find a good refresher/updater for systems architecture - hoping to get deep enough to understand configs etc. Currently I am thinking of just doing Udemy to catch up on any techologies I am uncertain of, and do LeetCode questions.

Does anyone have any good tips for preparing?"
592,2019-04-05 12:15:33,1554455733.0,dataengineering,Best Data Engineering Conference to learn more about Airflow?,b9owr7,gykdu1,,https://www.reddit.com/r/dataengineering/comments/b9owr7/best_data_engineering_conference_to_learn_more/,1.0,0.0,0.0,3735.0,
593,2019-04-06 18:40:12,1554565212.0,dataengineering,What are your thoughts on data virtualization?,ba5jmu,pybokeh,,https://www.reddit.com/r/dataengineering/comments/ba5jmu/what_are_your_thoughts_on_data_virtualization/,8.0,12.0,0.0,3744.0,"My company will embark on using Denodo as a data virtualization platform.  I only have a high level understanding of data virtualization.  It is touted as a technology that will reduce the need for ETL and somehow creates a view layer for all the disparate data sources.  I am curious how data virtualization maps or merge the disparate data sources together?  I assume someone will have to define the semantic layer somehow.  Is the resulting view layer stored in memory?
  
I am not part of my company's IT organization so I can't really provide more details on why they chose data virtualization instead of other solutions.  In this sub, I don't see data virtualization talked about much or know of companies using data virtualization."
594,2019-04-07 08:45:02,1554615902.0,dataengineering,Need help in choosing technologies - Storm Vs Kafka vs Spark,badc1z,crazyme28,,https://www.reddit.com/r/dataengineering/comments/badc1z/need_help_in_choosing_technologies_storm_vs_kafka/,11.0,4.0,0.0,3751.0,"Hi everyone,

&amp;#x200B;

Our team currently scraping the data. We are using Apache Kafka as a link between spiders and SQL Server. Currently we are storing unprocessed data in the database.  Now we want to do some kind on text processing (like standardizing the URL, units, and remove of some noisy words). So Is kafka able to do the text processing or do we need to use the Stream processing technologies like Apache Storm, Apache Spark, Apache Samza.   
What are potential blockers or difficulties we may face in this situation.   
Thank you in advance  


&amp;#x200B;"
595,2019-04-07 22:39:39,1554665979.0,dataengineering,important skillset for a Graduate Big Data Engineer ?,bak6kk,The_Grim_Flower,,https://www.reddit.com/r/dataengineering/comments/bak6kk/important_skillset_for_a_graduate_big_data/,5.0,10.0,0.0,3754.0," im going to an interview for a data analytics company for 12 months as a data engineer, ill be moving around a few teams over the 12 month period all the company does is sell data , their whole infrastructure is built with java, any advice on how I could prepare, I know that it's going to be a 2 part interview verbal + coding assessment after.

im going to be given some information next week so I can prepare for it more but apart from that, I want to work my ass off to get this job, any suggestions on what I should look over in order to help myself secure the spot?

im working on Java 8 and 7 to make sure I know it like the back of my hand and some algorithms since my degree is a mix of network and software engineering and we didn't do much work on algorithms some would be useful for the sake of data processing I think , im also familiarising myself with Hadoop and spark on my home apparatus i built with raspberry pies.

anything else?

many thanks"
596,2019-04-08 13:42:37,1554720157.0,dataengineering,Opinions on a series of data engineering lessons,bas8sf,elcric_krej,,https://www.reddit.com/r/dataengineering/comments/bas8sf/opinions_on_a_series_of_data_engineering_lessons/,23.0,7.0,0.0,3763.0,"So, recently I realized there aren't really any ""holistic"" data engineering tutorials.

There's loads of material out there talking about various niche subjects that can be interesting for someone already working with a specific set of tools or on a specific set of problems.

But, if a beginner programmer or CS student is interested in the field, there's not a load of material out there besides just learning how databases work and getting good at CS and programming in general.

I've started and attempt to create a series of lessons on data engineer that I would aim at someone like myself 5 years ago, when I was barely getting into programming. A sort of overview of the field that contains an introduction (and a few deep dives) into terms, tools and paradigms of thinking that can be helpful to people.

I wonder if you guys have any opinion on it: https://youtu.be/-mA00jcVJhw

I've only put out 2 videos thus far, and they are quite long, but if you've got even 5-10 minutes to spare in order to give me an opinion, that would be great."
597,2019-04-08 15:47:25,1554727645.0,dataengineering,An interview about how DataCoral is building an abstraction layer over data pipelines using microservices built on serverless technologies,batdt8,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/batdt8/an_interview_about_how_datacoral_is_building_an/,1.0,1.0,0.0,3765.0,
598,2019-04-08 16:07:48,1554728868.0,dataengineering,How Apache Airflow Distributes Jobs on Celery workers,batlkh,ligohu,,https://www.reddit.com/r/dataengineering/comments/batlkh/how_apache_airflow_distributes_jobs_on_celery/,10.0,0.0,0.0,3765.0,
599,2019-04-10 16:09:17,1554901757.0,dataengineering,[Free Webinar] Insights Discovery at the Intersection of Multiple Data Sets with Kirk Borne,bbltau,supercake53,,https://www.reddit.com/r/dataengineering/comments/bbltau/free_webinar_insights_discovery_at_the/,1.0,0.0,0.0,3789.0,
600,2019-04-10 16:13:28,1554902008.0,dataengineering,Why a data scientist is not a data engineer,bblux4,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/bblux4/why_a_data_scientist_is_not_a_data_engineer/,32.0,45.0,0.0,3789.0,
601,2019-04-10 17:29:35,1554906575.0,dataengineering,Can you kindly help me smash my Friday Data Engineer interview?,bbmol2,out_liar_consulting,,https://www.reddit.com/r/dataengineering/comments/bbmol2/can_you_kindly_help_me_smash_my_friday_data/,1.0,2.0,0.0,3789.0,"Hello r/dataengineering

&amp;#x200B;

Please could you kindly help me in understanding this presentation stimulus?  
On Friday at 2pm, I have a second interview where I am required to present for 10 mins on:

  
**“CI/CD pipeline that deploys R code onto AWS Lambda”**  
I'm interviewing for a Data Engineer position at an organisation which has petabyte-level data, rich and juicy.  
The head of the department who interviewed me on the phone for the first round knew I only knew basic Python, and have no knowledge of R or AWS. 

  
**I think therefore, that this is a challenge to see how much I can learn and understand about a technical topic in a short period of time. I am considering you a wealthy resource of information from which to learn from, and would be very grateful for any guidance you might have on understanding these components in addition to my own reading.** 

&amp;#x200B;

  
Continuous Integration/Continuous Deployment Pipeline  
Amazon Web Services Lambda

  
My learning process tomorrow and up until the interview will be:

1. High level concept explanations from Wikipedia, Reddit ELi5/ youtube introductory videos
2. Original Company Literature
3. Examples on Github and Stackoverflow if applicable
4. General web search

Any guidance or pointing to resources is helpful! I'll buy you all a drink if I get the job!

About me: I'm trying to transition from mechanical engineering into data engineering, have learnt a tiny bit of Python [https://github.com/out-liar](https://github.com/out-liar) This would be my first real job after 2.5 years of internships in technical roles, and I want le moneh 2 help my mum and sister out financially :)

&amp;#x200B;

I also have to sit a **20 min RANRA test**, which I think stands for Rust Advanced Numerical Reasoning Appraisal. Any advice on practicing it would be highly appreciated, although not directly relevant to this sub.

&amp;#x200B;

Thank you for your knowledge, thoughts and time!"
602,2019-04-11 02:34:20,1554939260.0,dataengineering,Any insight on the kind of data engineering that would be behind the Black Hole photo?,bbt2ie,beaverhair,,https://www.reddit.com/r/dataengineering/comments/bbt2ie/any_insight_on_the_kind_of_data_engineering_that/,3.0,2.0,0.0,3797.0,
603,2019-04-11 07:37:25,1554957445.0,dataengineering,Learn Docker to get started with Spark,bbvxp8,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/bbvxp8/learn_docker_to_get_started_with_spark/,10.0,0.0,0.0,3801.0,[https://medium.com/@achilleus/learn-docker-to-get-started-with-spark-dd8468e9de5b](https://medium.com/@achilleus/learn-docker-to-get-started-with-spark-dd8468e9de5b)
604,2019-04-11 15:44:03,1554986643.0,dataengineering,Big Data Engineer,bbzfaz,Alexandra_Smith,,https://www.reddit.com/r/dataengineering/comments/bbzfaz/big_data_engineer/,0.0,0.0,0.0,3806.0,
605,2019-04-12 01:11:30,1555020690.0,dataengineering,Question about the interactions between Data Engineering and Data Scientists,bc5ws1,Magicians_Nephew,,https://www.reddit.com/r/dataengineering/comments/bc5ws1/question_about_the_interactions_between_data/,2.0,10.0,0.0,3813.0,"I recently started a position as a Data Engineer and Scientist for a medium-sized healthcare startup, and I would like to know from those of you who have had good and bad interactions with Data Scientists what I might want to keep in mind. My primary focus right now is almost entirely on the Data Engineering side; I feel I'm in a unique position to design things with all of the data roles in mind. With more data team members being hired in the future, I'd like to create an infrastructure that meets everyone's basic needs.

&amp;#x200B;

 Any comments, advice, or criticism is welcome!"
606,2019-04-12 18:18:49,1555082329.0,dataengineering,Data Engineering Discord Server,bcerdk,mtv_,,https://www.reddit.com/r/dataengineering/comments/bcerdk/data_engineering_discord_server/,10.0,16.0,0.0,3823.0,Does anyone know of a Discord server dedicated to Data Engineering discussions and other related topics?
607,2019-04-13 08:07:10,1555132030.0,dataengineering,What would you say is the hardest part of data engineering?,bcn77e,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bcn77e/what_would_you_say_is_the_hardest_part_of_data/,4.0,8.0,0.0,3829.0,"Also, are there any tools you wish you had that would make your lives easier?"
608,2019-04-13 08:58:11,1555135091.0,dataengineering,"""Here’s why so many data scientists are leaving their jobs""",bcnjql,mtv_,,https://www.reddit.com/r/dataengineering/comments/bcnjql/heres_why_so_many_data_scientists_are_leaving/,8.0,5.0,0.0,3830.0,"[https://towardsdatascience.com/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4](https://towardsdatascience.com/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4)  
For those who are employed as a Data Engineer, is this applicable for you and the data scientists around you?"
609,2019-04-14 04:16:33,1555204593.0,dataengineering,Recommendations for MOOCs,bcxnuo,Gawgba,,https://www.reddit.com/r/dataengineering/comments/bcxnuo/recommendations_for_moocs/,20.0,9.0,0.0,3851.0,"Apologies in advance, I realize there are innumerable ""How to become a data engineer"" posts here, on Medium, on Quora, etc. and many data science sites that link to MOOCs.  However, I'm kind of overwhelmed by the sheer number of MOOCs dealing with data engineering and the multitude of technologies, and it seems like the landscape is shifting rapidly, so I'm not even sure if I should trust recommendations from 2017 at this point (there seem to be a lot of ""Hadoop isn't dead yet, but...."" posts)  

I'm trying to transition from a conventional RDBMS DBA to potentially a Big Data engineer - I have basic Python, ETL, dimensional modelling, database, and data visualization knowledge.  Basically hoping someone can give me some ideas on the 'best' (subjective, I know) MOOCs from the perspective of where the industry is going and which technologies are most in-demand for transitioning to a new career."
610,2019-04-14 19:46:34,1555260394.0,dataengineering,File formats to replace a Postgres data warehouse?,bd4p16,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/bd4p16/file_formats_to_replace_a_postgres_data_warehouse/,3.0,5.0,0.0,3859.0,"I am wondering if there are any file formats that I should be looking into instead of PostgreSQL. I love Postgres but I just want to know what is out there. Our data warehouse is kind of just an intermediate step before a visualization tool. It is important, but it is not the enterprise data warehouse.

&amp;#x200B;

* I use Airflow for scheduling ETL tasks to grab data and automate reporting by sending the data to a visualization tool
   * 40% of my data is extracted from replication servers (MS SQL, MySQL) where I bulk export new or changed rows, and load it to Postgres. 
   * 40% is from files (Excel, csv files etc. which are shared through SFTP sites and email mostly. The format of these files is wildly different. For a recent project, I just had to extract 218 distinct files.), 
   * 20% is from REST APIs
* I use a tool called sqitch for database migration, so at some point I need to write the deploy, revert, and verify scripts for each table
* I no longer normalize most of the data in Postgres. It is easier to just kind of keep it as raw as possible
* I have been updating rows historically if there are primary key conflicts, but in a lot of cases I kind of wish I only inserted new rows so I would have the history of data
* Ultimately, I use COPY to query data from the tables to a CSV file to load into a third party visualization tool through a rest API. This tool only accepts data in csv or csv.gz format

&amp;#x200B;

Are there any file formats (arrow, parquet, etc.) which I should look into since I am not using a lot of the database features? Our ultimate need is to be able to query for data and get it into CSV format to start streaming it the visualization too. It'd be nice if backups were as simple as rsync or something. Joining data would be useful, or I could also create 'fact table' files by doing joins in pandas and writing the results to a separate file."
611,2019-04-14 20:59:30,1555264770.0,dataengineering,ELI5: Columnar Databases/Data storage,bd5h54,linuxqq,,https://www.reddit.com/r/dataengineering/comments/bd5h54/eli5_columnar_databasesdata_storage/,8.0,3.0,0.0,3859.0,"I've been working the field for about a year now and regularly work with columnar data warehouses (Snowflake, Redshift) but honestly don't really understand what this means or why it's good. Can anyone help me wrap my head around it?"
612,2019-04-15 13:42:16,1555324936.0,dataengineering,Data Engineering conference in Europe - 2019,bdei6e,thiagoavadore,,https://www.reddit.com/r/dataengineering/comments/bdei6e/data_engineering_conference_in_europe_2019/,3.0,1.0,0.0,3861.0,"Hey!

I am organizing a conference in Amsterdam on October 30th. One of the tracks is in my area, **Data Engineering**, and we will have **Holden Karau** hosting it... our [Call for Papers is open](https://sessionize.com/itnext-summit-2019/), so I decided to share here! Come to lovely Amsterdam to LEARN. SHARE. CONNECT. on the [ITNEXT Summit 2019](https://www.itnextsummit.com/)!"
613,2019-04-15 16:29:22,1555334962.0,dataengineering,r/dataengineering Discord server!,bdfzh4,sweml,,https://www.reddit.com/r/dataengineering/comments/bdfzh4/rdataengineering_discord_server/,7.0,5.0,0.0,3865.0,"As was discussed in a previous post [here](https://www.reddit.com/r/dataengineering/comments/bcerdk/data_engineering_discord_server/?st=juie74f9&amp;sh=a5d8ae86) I have made a Discord server for this subreddit. Feel free to suggest any enhancements/changes to the Discord which will make it a better place!

You can see the Discord [here](https://invite.gg/dataengineering)

Alternative link [https://discord.gg/2pER6dq](https://discord.gg/2pER6dq)"
614,2019-04-15 21:44:33,1555353873.0,dataengineering,Thought this might be interesting: How Enigma Containerized Their Workflows,bdjqqt,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bdjqqt/thought_this_might_be_interesting_how_enigma/,2.0,0.0,0.0,3868.0, [https://www.enigma.com/blog/containerizing-data-workflows](https://www.enigma.com/blog/containerizing-data-workflows)
615,2019-04-16 04:40:25,1555378825.0,dataengineering,What makes a good ETL tool?,bdo6ic,penciltwirler,,https://www.reddit.com/r/dataengineering/comments/bdo6ic/what_makes_a_good_etl_tool/,4.0,8.0,0.0,3871.0,"I wonder why the data engineering community use workflow management tools like Airflow or Luigi, while the DevOps  community use tools like Concourse CI. It seems like any system with the mechanism for scheduling tasks and jobs can be used for ETL pipelines.

&amp;#x200B;

Would Concourse CI be a good replacement for Airflow?"
616,2019-04-16 16:52:59,1555422779.0,dataengineering,"An interview about the Pilosa bitmap index server and how it can be used to run fast, continuous analytics on large and complex data sets",bdu7bn,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/bdu7bn/an_interview_about_the_pilosa_bitmap_index_server/,0.0,0.0,0.0,3876.0,
617,2019-04-16 23:28:17,1555446497.0,dataengineering,Ideas for Interviewquestions,bdyrwp,friendlyimposter,,https://www.reddit.com/r/dataengineering/comments/bdyrwp/ideas_for_interviewquestions/,6.0,5.0,0.0,3887.0,"Hi,
I‘m interviewing an Data/ETL-Engineer and i‘m
planning some questions and tasks as a test. 
Are there any good resources or tipps what i should ask?

I would be very interested how she/he would build a dwh or datalake from scratch. Which database scheme would be used and how normalized should it be? But i‘m not sure how to formulate this correctly and what information would be needed to answer this..."
618,2019-04-19 03:56:40,1555635400.0,dataengineering,Ubuntu LTS or latest release for DE,bet7h1,mtv_,,https://www.reddit.com/r/dataengineering/comments/bet7h1/ubuntu_lts_or_latest_release_for_de/,1.0,5.0,0.0,3923.0,"Hello everyone, Canonical release Ubuntu 19.04 today, and I was just wondering if data engineers would stick to LTS, latest release, or another distro with rolling releases?"
619,2019-04-19 06:32:15,1555644735.0,dataengineering,Question: Is Apache Hadoop worth learning?,beunbc,idiotSherlock,,https://www.reddit.com/r/dataengineering/comments/beunbc/question_is_apache_hadoop_worth_learning/,0.0,5.0,0.0,3922.0,"I'm enrolled in a business analytics masters program and my uni offers a course ""big data analytics with Hadoop""

&amp;#x200B;

I am really interested in manufacturing and business process reengineering, I want to pursue an analytics career involving these fields. 

&amp;#x200B;

I have no prior IT/BA/BI experience and this is a new world for me, so I want to know, is Hadoop worth learning for me?"
620,2019-04-19 06:46:17,1555645577.0,dataengineering,Is anyone here using R with SSIS instead of C#?,beurn1,kvlt_ov_personality,,https://www.reddit.com/r/dataengineering/comments/beurn1/is_anyone_here_using_r_with_ssis_instead_of_c/,2.0,2.0,0.0,3921.0,I have a much easier time working in R Studio and manipulating data on the fly than trying to use C# in Visual Studio. I was wondering if anyone here has had success in using using R script components in their SSIS workflows?
621,2019-04-19 17:44:59,1555685099.0,dataengineering,Whiteboard Questions for Junior Data Engineer Interview?,bezx0w,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bezx0w/whiteboard_questions_for_junior_data_engineer/,11.0,13.0,0.0,3930.0,"I have an interview in 2 weeks for a junior data engineering position.  They said there will be 20 min of whiteboard questions for basic coding skills (python, R, java, whichever one I'm strong with, which is python).  Would leet code prep be good?  Would they ask specific algorithms?  Any common questions you've all seen?"
622,2019-04-19 18:16:52,1555687012.0,dataengineering,How Common is Freelance Data Engineering?,bf0a49,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bf0a49/how_common_is_freelance_data_engineering/,3.0,2.0,0.0,3930.0,Does anyone have experience doing freelance data engineering?  It would probably only be after you have a lot of experience right?
623,2019-04-22 09:33:41,1555914821.0,dataengineering,Vehicle telematics IOT solution,bfyv3v,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/bfyv3v/vehicle_telematics_iot_solution/,2.0,1.0,0.0,3972.0,"Hi,

Does anyone have worked in this industry? I am a big data professional and exploring this field. Can anyone please help me with resources/books in understanding use cases of collection such data and doing analysis on it. Also what all on cloud data tools and anayltics present to ingest such high amounts of data and do analysis"
624,2019-04-22 22:13:06,1555960386.0,dataengineering,Partitioning in S3 for Completeness vs Querability,bg62fr,tristanjones,,https://www.reddit.com/r/dataengineering/comments/bg62fr/partitioning_in_s3_for_completeness_vs_querability/,2.0,4.0,0.0,3976.0,"Right now my group's data is stored in S3 based on the hour we got the data. so 2019-04-22-01 would be the first hour of April 22nd, and so on. This helps those consuming this data regularly and in full, to do so with confidence of completeness. As long as you aren't querying the current hour, you know that no more data will be placed in the hours you are pulling.

&amp;#x200B;

However, the underlying data, stored within these files have their own timestamps, usually relevant to how the data is being used. And so we occasionally get asks for data based on those values, which is in no way, easy to access, or know where all of it is.  


Obviously, my first thought is to make a record of where the Data Timestamps exist within each Server Timestamp Hour we store on. This would require reading each file we get, which isn't currently done.

&amp;#x200B;

I assume I am not the only one who has faced this issue, and am curious, if I am going to crack the egg open on this, and begin reading every file for a certain value; What other things should I consider? How else have people tackled this problem of dealing with Time Received vs Time of Original Data Creation? Are there existing best practices around this? etc.

&amp;#x200B;

Any thoughts, or places I can read up on this more, would be appreciated.  


Thank you."
625,2019-04-23 00:47:26,1555969646.0,dataengineering,Data Engineering Career Outlook,bg7vsz,MrStrub,,https://www.reddit.com/r/dataengineering/comments/bg7vsz/data_engineering_career_outlook/,8.0,14.0,0.0,3979.0,"I'm just wanting to start a discussion regarding the career outlook for Data Engineering/Science.

I'm currently in grad school studying Software Engineering + work fulltime + doing some extracurricular DE learning on the side with the intention of transitioning to a Data Engineering role shortly after graduating.

I'm expecting to graduate in about a year and a half and wanted to get some experienced DEs opinions on the future of these roles? Is the market over saturated, etc.?

Personally, all of the various technologies that are used in DE excites me and I feel like the role would promote continuous learning vs what I do now which is very repetitive unfortunately..."
626,2019-04-23 07:36:32,1555994192.0,dataengineering,Am I crazy for thinking the data vault pattern is a good idea in my use case?,bgc4is,alexisprince,,https://www.reddit.com/r/dataengineering/comments/bgc4is/am_i_crazy_for_thinking_the_data_vault_pattern_is/,4.0,2.0,0.0,3981.0,"Hi all,

I've read mixed reviews about the data vault modeling concept, but I'm thinking its right for my use case. If anyone has any experience with this, please let me know. I've read about it, but have never built it / run it in production.

Here are the facts I'm dealing with. I'm currently working at a start up as a one man data team. This means data analysis, science (or lack thereof), and engineering. I'm primarily a data engineer, but I used to be a data analyst, which is why I was brought in to start the department. We were originally going to hire someone who would be my senior, but the budget got tight (start up life), and now I'm on my own. I feel confident in building and maintaining the systems I've put in place so far, but I'm needing people to bounce ideas off of.

I'm dealing with multiple data sources, one being a postgres database, one being FTP files (csv), and a small number of others that are being piped into Kinesis and converted into parquet files in S3. I have an airflow instance running, and I've taken heavy inspiration from the functional data warehouse. I'm currently doing all of the transformations in an ELT architecture through airflow tasks. I'm planning on migrating to DBT for this portion of the pipeline, but haven't started that migration yet. The problems I'm starting to run into are that I'm needing to constantly rebuild the warehouse I'm maintaining due to our other team's changing requirements. The warehouse takes about a day and a half to rebuild, and most of that time is waiting for files to get copied into redshift. The actual transformations in my ELT architecture are relatively light.

I'm wondering if this is a good fit for the data vault, or at least a modified version of it. I'm considering this because it seems to be an intermediary between the end user and the actual extraction of data sets. It also seems relatively easy to set up new sources, which would be a very very nice thing to have as I don't always get long enough heads up to build out a solution before its implemented in the source. I've also considered implemented just a persistent staging area within Redshift itself, which would just be making sure that data is loaded into copies of tables from the source, then letting DBT / transformation logic handle properly building the model. Both options seem like they would handle the problem I'm currently having.

I know one downside of the data vault is that it doesn't scale to huge amounts of data. That isn't currently an issue, nor will it be in the coming future.

Thank you everyone! Any help is appreciated."
627,2019-04-23 09:16:52,1556000212.0,dataengineering,Storage for Personal Data Engineering Projects,bgcyir,mtv_,,https://www.reddit.com/r/dataengineering/comments/bgcyir/storage_for_personal_data_engineering_projects/,5.0,5.0,0.0,3983.0,"Greetings everyone, I am wanting to pursue a career in Data Engineering and similar to Software Engineering, I know that I will need to have some personal projects to help show what I know. I haven't  started anything relating to Data Engineering because I am still finishing out my undergrad for Information Systems, but I plan to start over the summer - learning as much as I can. That being said, how much storage would I actually need? I currently have 2TB HDD's along with a 500GB 970 Evo. I would like to think that would be enough, especially for a novices level.   
Thank you in advance for any advice."
628,2019-04-23 19:25:59,1556036759.0,dataengineering,Transitioning from Data Analyst to Data Engineer?,bgieac,miden24,,https://www.reddit.com/r/dataengineering/comments/bgieac/transitioning_from_data_analyst_to_data_engineer/,16.0,18.0,0.0,3996.0,"I've been looking to transition to something new within my career, but staying in the data pathway. Currently, I've been a data analyst for the past few years and the work is getting repetitive and I don't think I want to be analyzing data, building reports/visualizations, and communicating business data to clients for the rest of my career.

I've looked into Data Science and that is still a possibility for me, but I would like to know more of Data Engineering and if that's a better fit for me. I was looking at job postings recently and a lot of Data Engineer jobs requires some analyst work such as what I have been doing now (that is a plus for me).

My question is how do I even get my hands into such a Data Engineering position?

I know this position and pathway requires a lot more technical skills than a Data Analyst, and I want to know which skills/tools/certs should I start learning/studying or continue to develop right now on my spare time?

Are there any side projects I can work on the side right now, to show recruiters how I can apply those skills?

Any advice, tips, or resources is much appreciated!

Thanks!"
629,2019-04-24 00:03:48,1556053428.0,dataengineering,Insight Data Engineering Fellowship,bgloze,savandesai091,,https://www.reddit.com/r/dataengineering/comments/bgloze/insight_data_engineering_fellowship/,1.0,7.0,0.0,4001.0,"Hello Everyone,
Actually I applied for Insight Data Engineering Fellowship, after completing coding challenge I got an interview. I did clear the 30 minutes interview but I got another email requesting to appear for a final video interview.
My query is I talked with some former fellows and came to know they only had one interview.
So I’m just curious that what should be the reason/scope for an additional interview.
Any suggestions or help would be really appreciated!! 
Thank you :)"
630,2019-04-24 02:59:39,1556063979.0,dataengineering,Serializing JSON and Writing it to a file? Apache beam,bgnmn5,Phizy,,https://www.reddit.com/r/dataengineering/comments/bgnmn5/serializing_json_and_writing_it_to_a_file_apache/,1.0,0.0,0.0,4003.0,"Hey everyone, I have this project in apache beam where I am reading 99 files calculating their checksum and I want to covert that to a JSON KV pair and write it to a file called manifest.json is there anyone who can help me? I am having no luck with this and have been stuck for a couple of days. 

Here is my code:

https://pastebin.com/PpYybAds"
631,2019-04-24 04:31:44,1556069504.0,dataengineering,A deep dive on building the Fauna database and how it supports transactions at global scale,bgokrc,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/bgokrc/a_deep_dive_on_building_the_fauna_database_and/,2.0,0.0,0.0,4004.0,
632,2019-04-24 10:05:16,1556089516.0,dataengineering,[Newbie] Apache Beam where does it fit in among other tools?,bgrk2q,wannabreakfreee,,https://www.reddit.com/r/dataengineering/comments/bgrk2q/newbie_apache_beam_where_does_it_fit_in_among/,1.0,1.0,0.0,4012.0,"Hi, I am a newbie to the Data Engineering field (6 months in). I am starting to understand various tools fit in the ecosystem. Apache Spark, Kafka, Airflow. But sometimes really find it hard to grasp the role of the tool in the ecosystem. I just have discovered Apache Beam, could somebody help me understand where does it fit in and where is it useful or in which use cases is it useful. Should I learn this new tool too? Because, everywhere I see these new tools and I am confused what their roles are and should I bother learning them. And to make that decision I need to go through different 1hr talks of the tools and they make it sound like you need to get onboard this train.   
Also, If anybody has any guidance to offer on moving forward in the field, if one should go after learning tools or concepts. If one should pursue concepts which ones should I start with? I know this question took a turn at the end. But I would appreciate any kind of guidance on growing and learning in this field from a perspective of a newbie, overwhelmed by all the fancy tools/terminologies surrounding around him/her.  


Thank you."
633,2019-04-24 10:37:04,1556091424.0,dataengineering,"Have you ever used a JSON-based format for representing graphs/networks? If yes, how was the experience?",bgrsc1,analyticalmonk,,https://www.reddit.com/r/dataengineering/comments/bgrsc1/have_you_ever_used_a_jsonbased_format_for/,5.0,2.0,0.0,4012.0,"Hi everyone,

I'm working with graph data and want to store it in a JSON-based format. Objectives are to allow analysis outside Neo4j and easily import into DB when required. I'm willing to write a custom parser for import but would appreciate if something exists already.

I came across a few formats (listed below) but I couldn't find user stories as such. I'm very interested to know if you've ever tackled with a similar problem statement.Graph JSON Formats:

\- GraphSON ([http://tinkerpop.apache.org/docs/3.4.1/dev/io/#graphson](http://tinkerpop.apache.org/docs/3.4.1/dev/io/#graphson))

\- JSONGraph ([http://jsongraphformat.info/](http://jsongraphformat.info/))"
634,2019-04-24 20:40:51,1556127651.0,dataengineering,"Anyone have experience with data engineering at a financial company (hedge fund, quant fund, big bank etc.)?",bgxky6,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bgxky6/anyone_have_experience_with_data_engineering_at_a/,3.0,5.0,0.0,4022.0,"And if so, can you describe what it was like?  Or how it might differ from data engineering at a tech company?"
635,2019-04-24 22:46:08,1556135168.0,dataengineering,Spark UDFs. We can use them. But Should we use them?,bgz2ql,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/bgz2ql/spark_udfs_we_can_use_them_but_should_we_use_them/,3.0,2.0,0.0,4023.0,
636,2019-04-25 02:30:11,1556148611.0,dataengineering,Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads,bh1l2l,therealgroodt,,https://www.reddit.com/r/dataengineering/comments/bh1l2l/delta_lake_is_an_opensource_storage_layer_that/,9.0,5.0,0.0,4032.0,
637,2019-04-25 17:46:55,1556203615.0,dataengineering,What are the challenges you have encountered in building/maintaining/using data lakes?,bh9hn3,eri2zhu,,https://www.reddit.com/r/dataengineering/comments/bh9hn3/what_are_the_challenges_you_have_encountered_in/,11.0,10.0,0.0,4051.0,"We (data curation lab at Univ of Toronto) are doing research in data lake discovery problems. One of the problems we are looking at is how to efficiently discover joinable and unionable tables. For example, find all the rental listings from various sources to create a master list (union); or find tables such as rental listings and school districts that can be used to augment each other (join). The technical challenges in finding joinable and unionable tables in data lakes involve the following: (1) the data schema is often inconsistent and poorly managed, so we can’t simply rely on that schema; and (2) the scale of data lakes can be in the order of hundreds of thousands of tables, making a content based search algorithm expensive. We came up with some solutions that are based on data sketches with several published papers \[1,2,3\]. The python library [datasketch](https://github.com/ekzhu/datasketch) was a byproduct if these work.

Many challenges remain though, and we would like to explore some of the more pertinent ones. In fact, we are conducting a survey to understand the current state of data lakes in industry and the challenges experienced. If you're interested in learning more, see what we came up with here: [https://www.surveymonkey.com/r/WLCYTVZ](https://www.surveymonkey.com/r/WLCYTVZ) \- would love to see what the Reddit community thinks about the current state of data lakes. You will have a chance to receive a 50$ gift card.

\[1\] [http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf](http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf)

\[2\] [http://www.vldb.org/pvldb/vol11/p813-nargesian.pdf](http://www.vldb.org/pvldb/vol11/p813-nargesian.pdf)

\[3\] [http://www.cs.toronto.edu/\~ekzhu/papers/josie.pdf](http://www.cs.toronto.edu/~ekzhu/papers/josie.pdf)"
638,2019-04-26 13:17:25,1556273845.0,dataengineering,Data Engineering Conference in Europe 2019,bhkp16,Fewthp,,https://www.reddit.com/r/dataengineering/comments/bhkp16/data_engineering_conference_in_europe_2019/,3.0,0.0,0.0,4068.0,"Hey!

ITNEXT is organizing a conference in Amsterdam on October 30th. One of the tracks is about Data Engineering, and we will have Holden Karau hosting it... our Call for Papers is open, so I decided to share here! Come to lovely Amsterdam to LEARN. SHARE. CONNECT. on the ITNEXT Summit 2019!

I know plenty of AWS enthusiasts have something to share! :-)

&amp;#x200B;

Main website is here: [https://www.itnextsummit.com](https://www.itnextsummit.com)

CFP here: [https://sessionize.com/app/organizer/event/1334](https://sessionize.com/app/organizer/event/1334)"
639,2019-04-26 20:47:07,1556300827.0,dataengineering,SQL question for data engineering roles?,bhp898,imba22,,https://www.reddit.com/r/dataengineering/comments/bhp898/sql_question_for_data_engineering_roles/,5.0,15.0,0.0,4075.0,Where  do you guys go to practice SQL question for data engineering roles. I do not mean basic and simple select questions. I mean the type of slightly complicated  sql question which get asked in interviews FAANG?
640,2019-04-27 07:10:02,1556338202.0,dataengineering,Junior first round,bhve17,knickerBockerJones,,https://www.reddit.com/r/dataengineering/comments/bhve17/junior_first_round/,5.0,1.0,0.0,4089.0,"I have a junior data engineering role that is not a. ETL position but will working with building their own type of pipeline.  They phone screen me then send a project for me to do, which will use numpy and pandas.  Then there is a two hour onsite interview, which I am positive is a grueling DS test, then grilling me out tech, then meeting team leaders.  Should I focus more on data structures and blowing those away?  I am a math major, I know C++, C, python etc.  so the project of data munging will probably be doable if I focus and test out of my Google skills.  What do you guys think?  Are data structures more important? 

Should I use python as my interview language? (I know Java but I dont have a great handle on the data structures and interfaces for each, C++ with no STL, and then Javascript hahah).  They will want  me to build full stack applications so I am guessing D3, matplotlib, etc.  

I have been toasted in interviews before and I would like to hear whether the onsite is going to be a data structures test or just a culture test meet n' great.  I dont have a full degree yet but I have a lot of math under my belt (All calcs, linear algebra, differential equations, group theory, probability and stats Calc based, discrete math) and a C++ course and C course with a data structures class in C++.

Any go to study material for these positions? Is domain knowledge more important or do they just want a guy who knows python and programming really well?"
641,2019-04-27 18:34:47,1556379287.0,dataengineering,Know all about the best online Machine Learning courses in 2019,bi08jc,prabhat008,,https://www.reddit.com/r/dataengineering/comments/bi08jc/know_all_about_the_best_online_machine_learning/,1.0,0.0,0.0,4091.0,
642,2019-04-27 19:39:34,1556383174.0,dataengineering,"Do, i have the skills for data engineering? Resume attached",bi0wb0,oBlackPlasmao,,https://www.reddit.com/r/dataengineering/comments/bi0wb0/do_i_have_the_skills_for_data_engineering_resume/,13.0,14.0,0.0,4091.0,"Hi all, 

I recently interviewed for a data engineering position and it went horrible and incredibly awakward (phone interview), especially when the interviewer told me she had the wrong resume, then tried to look for it and couldn’t find it. 

She asked if i had a degree in Data Mining and oriented objective design, since I got my degree in Stats, i was a bit familiar with data mining but Oriented objective design sounded more like CS to me so i had told her i wasn’t familiar with the term, tho i probably knew/know it as something else and should have asked. My fault, anyways, the tone/vibe completely changed.
(Originaly, this interview was suppose to be done by the HR manager but she had emailed me a day earlier saying she needed to reschedule the interview, and that it would be a colleague of hers that would contact me at the new time)


Anyways, i dont know if i should apply to other dat engineering positions as I don’t even know if i have the skills for it or have past experiences that may indicate i could learn in the job ect. 

So here is my resume, detailing my skills and projects-and stuff.

Languages I know are R, Pyspark, Latex, and SAS
https://m.imgur.com/NLSNhHx"
643,2019-04-27 23:58:53,1556398733.0,dataengineering,"Delta Lake - ACID transactions, versioning, and schema enforcement to Apache Spark",bi3jbe,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/bi3jbe/delta_lake_acid_transactions_versioning_and/,4.0,2.0,0.0,4093.0,"My take on Delta Lake  is a new open source project from Databricks  with some sample Scala examples to get started with Delta Lake

This adds ACID transactions, versioning, schema enforcement and lot of new features to Spark data sources that don't have them already!

[https://medium.com/@achilleus/delta-lake-acid-transactions-for-apache-spark-2bf3d919cda](https://medium.com/@achilleus/delta-lake-acid-transactions-for-apache-spark-2bf3d919cda)"
644,2019-04-28 00:22:22,1556400142.0,dataengineering,Is data engineering the field for me?,bi3rvg,vj1996,,https://www.reddit.com/r/dataengineering/comments/bi3rvg/is_data_engineering_the_field_for_me/,1.0,1.0,0.0,4093.0,"Hey guys. I have been having a lot of confusion as to what my go to field is. I love programming. Especially programming that runs stuff under the hood without any creative graphic user interface (to be precise.. I love the black screen). I love solving logical problems. It has been a year since i started looking exploring the field of data science. I worked with pandas and numpy in data gathering(web scraping, from APIs), data wrangling and data visualization, basically, everything leading up to data analysis except the analysis part. I loved it but i did not like the analysis part and the inference from graphs and charts part. Then i started looking into machine learning engineer field and started learning about machine learning models, implementing them using scikit learn, tuning them, natural language processing basics etc., I liked it for a while but then after i started making a few portfolio projects, I started feeling like i was missing something. Just knowing machine learning algorithms and how to implement them did not excite me. It did not feel like problem solving. Then came a lot of GUI machine learning tools such as Weka. Plus the problem solving part of machine learning which is data exploration part did not feel like my area of strength. I hated the data exploration part. Then i looked into data engineering field. I have been learning about data wrangling, warehousing, job scheduling, cloud databases etc., So far i love it and am excited to learn big data technologies. But i am worried as to whether this will also turn out to be one of them data exploratory, statistical part. I love scripting, programming and logical problem solving. Can anyone provide recommendations as to how i can get a overall understanding of what data engineering field exactly is, how much programming it involves and if it also deals with statistical inference?"
645,2019-04-28 20:25:32,1556472332.0,dataengineering,Engineering Blog Recommendations?,bidmd3,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bidmd3/engineering_blog_recommendations/,17.0,13.0,0.0,4106.0,I've heard Uber's engineering blog is good to follow.  Any others that you think would be good?
646,2019-04-29 02:15:20,1556493320.0,dataengineering,Tips for junior data engineer interview?,bihdlj,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bihdlj/tips_for_junior_data_engineer_interview/,4.0,1.0,0.0,4109.0,"I have an interview for a junior data engineer position, anyone have any advice or tips that I should know before going into it?"
647,2019-04-29 16:22:01,1556544121.0,dataengineering,An interview about how to run your database on Kubernetes with the creator of KubeDB,bipf7a,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/bipf7a/an_interview_about_how_to_run_your_database_on/,6.0,0.0,0.0,4116.0,
648,2019-04-30 01:55:14,1556578514.0,dataengineering,Get started with Pyspark on Mac using an IDE-PyCharm,bivyhf,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/bivyhf/get_started_with_pyspark_on_mac_using_an/,1.0,0.0,0.0,4123.0,[https://medium.com/@achilleus/get-started-with-pyspark-on-mac-using-an-ide-pycharm-b8cbad7d516f?source=friends\_link&amp;sk=d4f25f2069d8602286f279d02026ffcc](https://medium.com/@achilleus/get-started-with-pyspark-on-mac-using-an-ide-pycharm-b8cbad7d516f?source=friends_link&amp;sk=d4f25f2069d8602286f279d02026ffcc)
649,2019-04-30 02:17:26,1556579846.0,dataengineering,Interview Advice - System Design,biw6we,rkowalk,,https://www.reddit.com/r/dataengineering/comments/biw6we/interview_advice_system_design/,8.0,7.0,0.0,4123.0,"I previously worked as a BI/Data engineer for Adobe, but have taken 1.5 years off pursuing startups. Upon coming back, I have refreshed myself on most tools - but am mostly failing at system design style of questions.

So I was hoping I could ask you guys what your response would be to a proposed problem.

&amp;#x200B;

**System outline:**

1B records coming in per day, for an advertising company

Each record has the following information:

* Time
* Hit type(mobile/desktop)
* Account
* Campaign
* Impressions
* Interactions

&amp;#x200B;

The first question they asked was how would I design a system to house said data and make it reportable within moments of new data arriving."
650,2019-04-30 12:25:59,1556616359.0,dataengineering,12 Data Integration Tools - A Cost Benefit Analysis,bj1a71,cmstrump,,https://www.reddit.com/r/dataengineering/comments/bj1a71/12_data_integration_tools_a_cost_benefit_analysis/,1.0,3.0,0.0,4127.0,"Data integration is key to solving a crucial problem in any organization’s data analytics flow. You may track data related to your business down to the most minute detail, but if is also important to have every relevant piece of data you need readily available.

Here is a comparison and key decision factors for how to choose the right data integration tool: 
[Top 12 Data Integration Tools In 2019 - A Cost Benefit Analysis] (https://blog.panoply.io/data-integration-tools)

* for a big organization with big data: Informatica, Oracle, IBM, SAP or Hitachi
* to maximize ease of use: SSIS, Panoply, Dell Boomi AtomSphere or Astera Centerprise
* to minimize cost: Panoply or Dell Boomi AtomSphere
* for government, healthcare or other specialized data: Denodo, Hitachi Vantara or InterSystems Ensemble"
651,2019-04-30 19:34:39,1556642079.0,dataengineering,Study Partner in Data Engineering Project,bj5ehu,StockFault7,,https://www.reddit.com/r/dataengineering/comments/bj5ehu/study_partner_in_data_engineering_project/,10.0,20.0,0.0,4133.0,"folks , I have identified a simple data Engineering project which involves creating a analytics pipeline  . I am looking for someone who wants to learn creating pipelines which is mostly the job of  Data Engineers. please drop me a message for further discussion."
652,2019-04-30 21:40:42,1556649642.0,dataengineering,Aggregating Data Sources - Expertise?,bj6wi9,felmalorne,,https://www.reddit.com/r/dataengineering/comments/bj6wi9/aggregating_data_sources_expertise/,1.0,4.0,0.0,4134.0,"**Background:**

We're starting to attract clients that are more upper funnel. They need help aggregating data sources from Google Analytics, Salesforce, Mailchimp, proprietary systems, disparate databases... etc.. to then be compiled into a central repository. Most likely a cloud hosted relational db (AWS RDS). The end goal is to have this data aggregated, readily available for dash-boarding. 

**Questions:**

1. Is this even a data engineering problem?
2. What type of training would be required to even tackle this type of work?
3. What have others done to aggregate data sources?
4. Is using a tool/site like stitchdata a viable option?

&amp;#x200B;

**Notes:**

I understand different elements/softwares/packages would probably be needed on a case by case basis in regards to each particular data source -- so not a one size fits all approach BUT where would we even get started to tackle this? I'm sure this problem arises in other agencies and in-house, how do others tackle this sort of thing?"
653,2019-05-01 13:41:31,1556707291.0,dataengineering,How to ensure consistency between Dataflow pipeline runs? Help me get from bash script to Python test,bjfmrx,ratatouille_artist,,https://www.reddit.com/r/dataengineering/comments/bjfmrx/how_to_ensure_consistency_between_dataflow/,2.0,2.0,0.0,4147.0,"I have a large NLP Dataflow pipeline which takes a set of documents and enriches it with all kinds of NLP extraction data. My current approach to achieve consistency is building a test harness that checks whether runs remain the same. This is to ensure that env upgrades don't cause inconsistent runs.  


The pipeline run results are stored on Google Cloud buckets with gziped chunks of json newline files.  


Currently I use bash to download buckets in parallel with `gsutil -m cp` then I merge the relevant json files into one file with Python and then `sort -u` to deduplicate merged files and then use `cmp` to compare the merged runs.  


The issue with this approach is that it is difficult to run this process through CI which is what I would like to do ideally refactoring the above into Python code which can validate that test runs are consistent.

&amp;#x200B;

One option I was thinking about was adding an additional step in my pipeline which will check the pipeline results are the same with a reference result set.

&amp;#x200B;

I feel others must face the same problem and I was wondering what your solutions were?"
654,2019-05-01 21:15:21,1556734521.0,dataengineering,Why Not Airflow?,bjkh72,sweml,,https://www.reddit.com/r/dataengineering/comments/bjkh72/why_not_airflow/,23.0,14.0,0.0,4152.0,
655,2019-05-01 22:55:20,1556740520.0,dataengineering,Databricks Koalas-Python Pandas for Spark,bjlnq5,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/bjlnq5/databricks_koalaspython_pandas_for_spark/,4.0,3.0,0.0,4152.0,[https://medium.com/@achilleus/databricks-koalas-python-pandas-for-spark-ce20fc8a7d08?source=friends\_link&amp;sk=a1e3979e61aeebd914f4e88ebf8e30aa](https://medium.com/@achilleus/databricks-koalas-python-pandas-for-spark-ce20fc8a7d08?source=friends_link&amp;sk=a1e3979e61aeebd914f4e88ebf8e30aa)
656,2019-05-02 06:21:43,1556767303.0,dataengineering,Build system for data engineers,bjq9y8,gsvigruha,,https://www.reddit.com/r/dataengineering/comments/bjq9y8/build_system_for_data_engineers/,0.0,0.0,0.0,4161.0,[https://github.com/prodmodel/prodmodel](https://github.com/prodmodel/prodmodel)
657,2019-05-03 05:16:02,1556849762.0,dataengineering,Anyone here ever use Serverless?,bk38yh,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/bk38yh/anyone_here_ever_use_serverless/,4.0,6.0,0.0,4181.0,"Also, what was your experience like?"
658,2019-05-03 23:00:32,1556913632.0,dataengineering,People looking for data engineering projects: here is one,bkcs2w,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/bkcs2w/people_looking_for_data_engineering_projects_here/,19.0,7.0,0.0,4197.0,
659,2019-05-05 11:43:19,1557045799.0,dataengineering,Found Udemy Discount Coupon For Apache Spark so thought worth sharing,bkvkwt,ybhavesh,,https://www.reddit.com/r/dataengineering/comments/bkvkwt/found_udemy_discount_coupon_for_apache_spark_so/,1.0,0.0,0.0,4223.0,
660,2019-05-05 20:19:15,1557076755.0,dataengineering,Best way connect/map a dataset of texts to their corresponding floating point vectors?,bl0c68,BatmantoshReturns,,https://www.reddit.com/r/dataengineering/comments/bl0c68/best_way_connectmap_a_dataset_of_texts_to_their/,2.0,1.0,0.0,4225.0,"I have a dataset of texts and each text has a corresponding 768 dimensional vector. 

I have learned that there is no data format which is optimized to hold these dual datatypes, so I have to store them each separately, and figure out a way to map the two datasets. 

One way is to make sure they're both in a very specific order, but I feel that may be prone to error. I've heard there are some mapping methods. Does anyone have experience in this area who can give recommendations?"
661,2019-05-07 13:23:23,1557224603.0,dataengineering,Where/How to learn to do dataengineering,blp0d5,set92,,https://www.reddit.com/r/dataengineering/comments/blp0d5/wherehow_to_learn_to_do_dataengineering/,11.0,15.0,0.0,4250.0,"I feel there is a lack of guides/books, or maybe is because I'm searching with some terms that are not the correct ones.

&lt;- Open RANT -&gt;

&gt; My situation is that I end up in a company with no real data science team, only 2 juniors and a girl who before was a BI analyst (and now is the manager of this data science team), they told me there was a group, we would need to build the whole thing but I would have help, but the data arquitect that we had it left after 4 months saying that we needed data administrator and they were not willing to hire more people.
&gt; 
&gt; So now, most of the work falls in me, some weeks I have to do some analysis for one of the other groups in the company, then I had to try to use a HDFS cluster without any help, I had to discover that it was not done, and I couldn't complete the task. 
&gt; 
&gt; This last 2 weeks I have been researching an API of a supplier of data we have, but I don't know how to do it, no one else knows anything, and when I try to do something it don't work, don't know because the tools are very specific, new or is only because I'm dumb and I don't know how to do it. 

&lt;- Close RANT -&gt;

Right now my task is to create a pipeline to ingest the data of the API to a data warehouse(a cluster in cloudera with HDFS) that we will use for the whole company in some moment in the future. The problem is that the API is in SOAP with SSL, I tried using streamsets or nifi but they have problems because of the size of the returned file, when that doesn't fail, fails the server because I'm returning too much information and I can't do anything in 1 hour. Also I wanted to ingest it with a service of queue the API has, but it works with AMQP 1.0 with SSL, and neither streamsets nor nifi seems to support it, and there is no client of python to use it. Now I'm problems to use NiFi with SSL because I can't find many documentation in general.

I thought in using airflow with my own scripts, but is this better? When I should use streamsets/nifi or airflow, what kind of services there is? I should go ELT or ETL? what are the tools that exist for ELT? I don't know I have too many questions and when I search for them I can't find good answer, or they only explain half of it."
662,2019-05-07 17:14:07,1557238447.0,dataengineering,An interview about the FoundationDB project and how it simplifies the work of building custom distributed systems applications,blr770,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/blr770/an_interview_about_the_foundationdb_project_and/,3.0,0.0,0.0,4250.0,
663,2019-05-08 07:08:50,1557288530.0,dataengineering,What do y'all think about Udacity's new Data Engineering nanodegree?,bm0q6u,penciltwirler,,https://www.reddit.com/r/dataengineering/comments/bm0q6u/what_do_yall_think_about_udacitys_new_data/,13.0,17.0,0.0,4253.0,"I recently transitioned from a backend developer position to a data engineering position at my company. I know the basics of ETL and data warehousing, but not a lot of experience. I already finished the [dataquest.io](https://dataquest.io) data engineering track which covers basics in Postgres and Python, as well as some advanced topics like b-trees. However,  I feel like I need more experience with ""industrial strength data engineering"", such as architecting data warehouses, data lakes, building pipelines, and taking advantage of the cloud for computing and scalability.

&amp;#x200B;

It seems to be like the Udacity Data Eng nanodegree covers quite a bit of these ""industial strength"" best practices and tools. However, I also don't see the point of taking this course since I can just learn most of this stuff on the job anyways.

&amp;#x200B;

What do you guys think?"
664,2019-05-08 11:47:41,1557305261.0,dataengineering,Top 9 MongoDB ETL Tools,bm2tze,thumbsdrivesmecrazy,,https://www.reddit.com/r/dataengineering/comments/bm2tze/top_9_mongodb_etl_tools/,3.0,3.0,0.0,4257.0,"It is a list of the top ETL tools to extract data out of a MongoDB database. It is a mixture of open source and paid options (some of which have a free “community” version):

* [MongoSyphon](https://github.com/johnlpage/MongoSyphon)
* [Transporter](https://github.com/compose/transporter)
* [Krawler](https://github.com/kalisio/krawler)
* [Panoply](https://panoply.io/)
* [Stitch](https://www.stitchdata.com/)
* [Talend Big Data Open Studio](https://www.talend.com/products/big-data/big-data-open-studio/)
* [Pentaho](https://www.hitachivantara.com/en-us/products/big-data-integration-analytics.html)
* [SYNC](https://github.com/gagoyal01/mongodb-rdbms-sync)
* [YelpDatasetETL](https://github.com/apanimesh061/YelpDatasetETL)

Check the full article for overview and details on each of the tools: [Top 9 MongoDB ETL Tools](https://blog.panoply.io/top-9-mongodb-etl-tools)"
665,2019-05-08 13:23:52,1557311032.0,dataengineering,Architecture Advice,bm3je8,lessthan3cows,,https://www.reddit.com/r/dataengineering/comments/bm3je8/architecture_advice/,1.0,2.0,0.0,4258.0,"I'm a software developer (and not a DE) who recently inherited a project that I feel like is going in the wrong direction.

&amp;#x200B;

We currently have a dashboard that serves up real time analytics to \~100 users, aggregating data up and allowing them to filter across \~10 dimensions (including continuous \[minute level\] time). The backend for this is a SQL database that is continuously queried (resulting in 95%+ cpu utilisation and slow response times for users).

&amp;#x200B;

A project was underway to speed up the app by removing the SQL database and replacing it with a custom built data store that does some element of pre-computation, and while initial results were apparently promising, I'm unconvinced by the approach and think we should be using some existing framework.

&amp;#x200B;

The specifications of our system are as follows:

* Ingest \~100 events per second (but can peak at \~1000)
* Data is only persisted for 1 week (so peak around 60M records)
* Cardinality for each dimension varies from a few binary ones, to one that is \~5000
* Dashboard requires 5 queries, one of which is a time series
* Should be able to handle different filters for all users, so potentially \~500 queries
* Query response time (i.e. when changing filters) should be as quick as possible (aiming for sub-second)

&amp;#x200B;

I've done some research, and I think the best way to handle this is to generate streams on the fly for each view and set of filters although I'm worried about the time it will take to create each stream. Does this seem like a good approach or is it a terrible idea?

&amp;#x200B;

Would love any thoughts from people who have more experience than me!"
666,2019-05-08 13:45:31,1557312331.0,dataengineering,Best way to schedule ML training on Cloud? (Azure),bm3ph7,j0ddm,,https://www.reddit.com/r/dataengineering/comments/bm3ph7/best_way_to_schedule_ml_training_on_cloud_azure/,1.0,0.0,0.0,4258.0,"Hi, I am working on a project now which we want to implement in Azure and I'm seeking some advice.

It's a weekly forecasting job, and I want it to run dynamically (if that's a term), so e.g. spin up a VM once a week, run the computation and shut it down. Naturally I don't want to have the VM running 24/7 sitting idle 99% of the time.

The pipeline is pretty simple for this task: fetch data from Azure SQL -&gt; train/forecast -&gt; write results back to Azure SQL. This is all in Python.

From my perspective I have 3 options:
1. Azure Data Factory (seems rather clunky to use custom Python modules)
2. Azure ML Services (seems optimized for publishing REST APIs)
3. Databricks (no cluster, run 1 worker/driver. Seems to be more costly than the other 2?)

How would you guys automate this?"
667,2019-05-08 15:04:48,1557317088.0,dataengineering,Best data engineering course on Coursera?,bm4f82,Viva_Uteri,,https://www.reddit.com/r/dataengineering/comments/bm4f82/best_data_engineering_course_on_coursera/,9.0,12.0,0.0,4256.0,
668,2019-05-08 23:43:38,1557348218.0,dataengineering,Resources to learn from,bmajdj,set92,,https://www.reddit.com/r/dataengineering/comments/bmajdj/resources_to_learn_from/,32.0,7.0,0.0,4265.0,"Since I think we can all agree that the resources are pretty scattered, so I thought it would be a good idea to collaborate and create a list of all the resources we think are good. Maybe later we can start the wiki page and add a page with all this info.

I also thought that would great to have a post with the most popular tools, or the category of each tool with some description, use-case and important things that everyone should know about. But I don't know enough about all of them to create it myself. Another approach to this would be a compilation of all the terms used in DE.

**SQL**

* [Mode tutorial](https://mode.com/sql-tutorial/introduction-to-sql/)
* [SQLZoo \(Exercises\)](https://sqlzoo.net/wiki/SQL_Tutorial)
* [SelectStarSql. Interactive course with real data of last statements](https://selectstarsql.com/) 
* [SQLBolt. Interactive tutorial](https://sqlbolt.com/)
* [SQL joins](https://www.technolush.com/blog/sql-joins)

**Spark**

* [Good course for beginners](https://www.udemy.com/spark-and-python-for-big-data-with-pyspark)

**Example projects**

* [Joseph Wibowo Project](https://josephwibowo.github.io/Meetup_Analytics/)
* [Vedanth Narayanan Project](https://github.com/narayave/Insight-GDELT-Feed)

**Concepts**

* [ETL &amp; ELT](https://www.guru99.com/etl-vs-elt.html)
* [OLAP vs OLTP](https://www.imaginarycloud.com/blog/oltp-vs-olap/)
* [Data model](https://www.talend.com/blog/2017/07/31/data-model-design-best-practices-part-2/)

**Books**

* [Designing Data-Intensive Applications](https://dataintensive.net/)
* [Spark: The Definitive Guide](http://shop.oreilly.com/product/0636920034957.do)

**Other places to learn **

* [cube.js blog](https://statsbot.co/blog/category/sql/)
* [Alooma blog](https://www.alooma.com/blog)


**Other resources**

* [Roadmap to learn data engineering]
(https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0)
* [Qwiklabs hands-on labs](https://www.qwiklabs.com/quests/25) - Google recommends to do first [GCP Essentials](https://google.qwiklabs.com/quests/23).

Now your turn :P, I'll edit this post to add your contributions. Also if you feel that some link, book or something is not good enough say it, because I think that if we put all the possible information there would be too many links."
669,2019-05-10 06:46:12,1557459972.0,dataengineering,Unit Testing Database Procedures and ETL Python Scripts - What's the norm?,bmtiun,Buckweb,,https://www.reddit.com/r/dataengineering/comments/bmtiun/unit_testing_database_procedures_and_etl_python/,14.0,4.0,0.0,4290.0,"I'd like to figure out what the norm is for testing database stored procedures and ETL scripts. The testing processes at my current company seem extremely inefficient and make it difficult to maintain code for even simple changes due to a poor testing setup.

**1.** Are database stored procedures often tested with unit tests/CI tools or is most testing a manual process?
**2.** Do you always write unit tests for your Python ETL scripts?
**3.** Are you always repopulating your development environment with new data, how is this handled?

If you have any generally helpful testing tips please share them."
670,2019-05-10 07:37:11,1557463031.0,dataengineering,How are storage benchmarks calculated?,bmtzhi,12amnewday,,https://www.reddit.com/r/dataengineering/comments/bmtzhi/how_are_storage_benchmarks_calculated/,1.0,0.0,0.0,4291.0,"Is there a universal method to account for things like file formats, compression algos, serialization, etc?

&amp;#x200B;

For example, many presentations given at conferences often boast summary stats of a speaker's given company, usually in terms of throughput in terms of GB/TB/PB, but what does that really mean? 

&amp;#x200B;

If there is no standard, what should it be?"
671,2019-05-10 15:05:41,1557489941.0,dataengineering,Alternative job board with data engineer / AI positions and internships,bmxg1z,ai_jobs,,https://www.reddit.com/r/dataengineering/comments/bmxg1z/alternative_job_board_with_data_engineer_ai/,3.0,0.0,0.0,4294.0,
672,2019-05-10 23:27:06,1557520026.0,dataengineering,Job Offer Curated List.,bn3c9v,Dhakwanes,,https://www.reddit.com/r/dataengineering/comments/bn3c9v/job_offer_curated_list/,0.0,2.0,0.0,4300.0,"I am teaching learning some new skills and I created this table with some new jobs for those who are interested in something different if the community is interested I can create something each week.

|**Companies Name**|**Title**|**Country**|**State**|**City**|**Employment Type**|
:-:|:--|:-:|:-:|:-:|:-:|
|[Hired](http://www.steele-consulting.net/opportunity-1/)|[Data infrastructure manager](http://www.steele-consulting.net/opportunity-1/)|[US](http://www.steele-consulting.net/opportunity-1/)|[CA](http://www.steele-consulting.net/opportunity-1/)|[San Francisco](http://www.steele-consulting.net/opportunity-1/)|[Full Time](http://www.steele-consulting.net/opportunity-1/)|
|[George Mason University](http://www.steele-consulting.net/opportunity-2/)|[Metadata librarian](http://www.steele-consulting.net/opportunity-2/)|[US](http://www.steele-consulting.net/opportunity-2/)|[VA](http://www.steele-consulting.net/opportunity-2/)|[Fairfax](http://www.steele-consulting.net/opportunity-2/)|[Full Time](http://www.steele-consulting.net/opportunity-2/)|
|[Northside Hospital](http://www.steele-consulting.net/opportunity-3/)|[Research data specialist](http://www.steele-consulting.net/opportunity-3/)|[US](http://www.steele-consulting.net/opportunity-3/)|[GA](http://www.steele-consulting.net/opportunity-3/)|[Macon](http://www.steele-consulting.net/opportunity-3/)|[Full Time](http://www.steele-consulting.net/opportunity-3/)|
|[Art Engineering Llc](http://www.steele-consulting.net/opportunity-4/)|[Ballistic missile defense tactical data link and aegis systems t with security clearance](http://www.steele-consulting.net/opportunity-4/)|[US](http://www.steele-consulting.net/opportunity-4/)|[CA](http://www.steele-consulting.net/opportunity-4/)|[San Diego](http://www.steele-consulting.net/opportunity-4/)|[Full Time](http://www.steele-consulting.net/opportunity-4/)|
|[Hired](http://www.steele-consulting.net/opportunity-5/)|[Data engineer](http://www.steele-consulting.net/opportunity-5/)|[US](http://www.steele-consulting.net/opportunity-5/)|[CA](http://www.steele-consulting.net/opportunity-5/)|[Hayward](http://www.steele-consulting.net/opportunity-5/)|[Full Time](http://www.steele-consulting.net/opportunity-5/)|
|[Northside Hospital](http://www.steele-consulting.net/opportunity-6/)|[BMT clinical research/data coordinator](http://www.steele-consulting.net/opportunity-6/)|[US](http://www.steele-consulting.net/opportunity-6/)|[GA](http://www.steele-consulting.net/opportunity-6/)|[Atlanta](http://www.steele-consulting.net/opportunity-6/)|[Full Time](http://www.steele-consulting.net/opportunity-6/)|
|[Hired](http://www.steele-consulting.net/opportunity-7/)|[Data engineer](http://www.steele-consulting.net/opportunity-7/)|[US](http://www.steele-consulting.net/opportunity-7/)|[CA](http://www.steele-consulting.net/opportunity-7/)|[Redwood City](http://www.steele-consulting.net/opportunity-7/)|[Full Time](http://www.steele-consulting.net/opportunity-7/)|
|[Hired](http://www.steele-consulting.net/opportunity-8/)|[Data infrastructure manager](http://www.steele-consulting.net/opportunity-8/)|[US](http://www.steele-consulting.net/opportunity-8/)|[CA](http://www.steele-consulting.net/opportunity-8/)|[Greenbrae](http://www.steele-consulting.net/opportunity-8/)|[Full Time](http://www.steele-consulting.net/opportunity-8/)|
|[Aboutweb](http://www.steele-consulting.net/opportunity-9/)|[Automation/Database tester](http://www.steele-consulting.net/opportunity-9/)|[US](http://www.steele-consulting.net/opportunity-9/)|[VA](http://www.steele-consulting.net/opportunity-9/)|[Reston](http://www.steele-consulting.net/opportunity-9/)|[Full Time](http://www.steele-consulting.net/opportunity-9/)|
|[Hired](http://www.steele-consulting.net/opportuniy-10/)|[Data engineer](http://www.steele-consulting.net/opportuniy-10/)|[US](http://www.steele-consulting.net/opportuniy-10/)|[CA](http://www.steele-consulting.net/opportuniy-10/)|[Mill Valley](http://www.steele-consulting.net/opportuniy-10/)|[Full Time](http://www.steele-consulting.net/opportuniy-10/)|"
673,2019-05-11 00:26:18,1557523578.0,dataengineering,Junior Data Engineer advice?,bn41s9,BeerMang,,https://www.reddit.com/r/dataengineering/comments/bn41s9/junior_data_engineer_advice/,11.0,14.0,0.0,4301.0," 

Spent the last year as a Data Analyst, starting a new role as a junior data engineer. Role is focused on learning a proprietary ETL tool, so no extensive SQL experience was required beyond the basics. I want to be as prepared as possible in the position (No background in CS, mostly self taught). I've been focusing on SQL as much as possible in my free time as I know it's crucial. My previous role as an analyst required no SQL, it was all excel/powerBI and power query.

Currently learning basic analysis (joins, value counts, iterrows) in python pandas.

Any advice? For those that are seasoned, what advice would you give yourself when you were junior?"
674,2019-05-11 14:01:50,1557572510.0,dataengineering,AWS Big Data exam - seems outdated... anyone did the exam?,bnaw1d,skartocc,,https://www.reddit.com/r/dataengineering/comments/bnaw1d/aws_big_data_exam_seems_outdated_anyone_did_the/,8.0,2.0,0.0,4312.0,
675,2019-05-11 22:23:01,1557602581.0,dataengineering,Mid-level Data Engineer roles/expertise?,bnfw5o,sejeongflowerknight,,https://www.reddit.com/r/dataengineering/comments/bnfw5o/midlevel_data_engineer_rolesexpertise/,13.0,9.0,0.0,4316.0,"Hello. I've been a Data Engineer for almost 2 years (this is also my first job). In my job, I'm mostly handling an ETL framework, mainly using Scrapy and some xls/pdf parser tools for getting the data. I think I've created around 200+ scrapers already.

And I've been trying to find a new job, found a possible opportunity to work in Singapore, as data engineer again. (im from SEA). But I'm not that confident that my current skills/knowledge would be qualified enough for it.

Could I ask some data engineers here as well. What is expected from a Mid-level/Senior Data Engineer ? Is Spark/Kafka an already established standard in data engineering today? My work uses quite an outdated ETL design so I'm mainly concerned about this. Thanks."
676,2019-05-12 17:01:32,1557669692.0,dataengineering,Collaboration data engineers with data scientists and analysts,bnp9ci,schrute_dataeng,,https://www.reddit.com/r/dataengineering/comments/bnp9ci/collaboration_data_engineers_with_data_scientists/,16.0,2.0,0.0,4328.0,"Hello dataengineering community,

I am a data engineer, I have wrote a post on how we collaborate with our peers in science and analytics in my company in order to improve our efficiency releasing in production. Apache airflow is a main tool in this collaboration. 

I would be happy to answer any question on the subject or hear how it works in your company. 

https://medium.com/dailymotion/collaboration-between-data-engineers-data-analysts-and-data-scientists-97c00ab1211f

I hope this post have its place here, I apologies in advanced if not and will remove it."
677,2019-05-12 20:58:17,1557683897.0,dataengineering,Interview: Made One Mistake in Project Code,bnrw7j,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/bnrw7j/interview_made_one_mistake_in_project_code/,3.0,8.0,0.0,4330.0,"I'm currently interviewing for a data engineering job at a company that I know would be a great fit for me and quite honestly is my dream job. I had a very good interview with the hiring manager for the first round - she told me it's hard to find a data engineer with a strong domain knowledge of the healthcare industry. She also told me that I was approved to move on to the second interview before the first one was even over. The second round interview consisted of doing a coding project: half of it was SQL and the other half was a basic ETL process in Python. I had all parts of the code giving back exactly the results that they wanted, so I started writing the unittests for the Python portion. However, \*very\* stupid me modified one tiny part of the Python code because a unittest was complaining about it, which allowed the unittest to pass.  And now, instead of giving all rows back within a Pandas dataframe, it only gives one back. And I just \*now\* realized this, a few days after turning in the code to the hiring manager.

I cannot explain how crushed I feel right now, especially after putting all of that time and effort in to the code.

The hiring manager has been out all week so I haven't heard back from the recruiter (the recruiter is our point-of-contact; I do not have the means to contact the hiring manager myself). And honestly, if I reviewed that code myself as an outsider, I'd be really confused as to why all of the code is correct except for one tiny part that gives back one row instead of all rows.

Am I f\*cked? Should I reach out to the recruiter explaining the mistake?"
678,2019-05-14 13:19:53,1557829193.0,dataengineering,How Different Are Lean Development and Lean Startup Techniques?,boh2fm,Yaminiyamini,,https://www.reddit.com/r/dataengineering/comments/boh2fm/how_different_are_lean_development_and_lean/,1.0,0.0,0.0,4354.0,
679,2019-05-15 12:22:18,1557912138.0,dataengineering,How can data engineers and scientists be better compensated for their work?,bovp5t,ShapeAI,,https://www.reddit.com/r/dataengineering/comments/bovp5t/how_can_data_engineers_and_scientists_be_better/,1.0,1.0,0.0,4367.0,"I’ve been thinking about all the tasks that data scientists and engineers have to complete in companies that they often don’t get the right recognition or compensation for. In order to try and match their work with compensation, I’ve created this potential bonus system based on The Accelerate State of DevOps Report. 

&amp;#x200B;

https://i.redd.it/gjjy9q0uecy21.png

&amp;#x200B;

*  Do you have any thoughts or improvements for this system? 
* Is there any non-monetary compensation that could also motivate you in your work?
* Do you think that this is feasible for your company? 

&amp;#x200B;

I really think this is crucial in the business, and I hope we can have a good discussion and answers!

&amp;#x200B;

Here is the [original report](https://cloudplatformonline.com/rs/248-TPC-286/images/DORA-State%20of%20DevOps.pdf) in case anyone is interested!"
680,2019-05-15 21:16:41,1557944201.0,dataengineering,Facilitating the discovery of public datasets,bp1f2e,neuromantik8086,,https://www.reddit.com/r/dataengineering/comments/bp1f2e/facilitating_the_discovery_of_public_datasets/,1.0,0.0,0.0,4373.0,
681,2019-05-15 22:37:52,1557949072.0,dataengineering,can you post here,bp2f96,SeydouxBlue,,https://www.reddit.com/r/dataengineering/comments/bp2f96/can_you_post_here/,0.0,0.0,0.0,4375.0,Im just checking i just typed a long post and it got deleted and didnt post
682,2019-05-15 23:51:20,1557953480.0,dataengineering,Building your own Google analytics,bp3die,supreme_kenzo,,https://www.reddit.com/r/dataengineering/comments/bp3die/building_your_own_google_analytics/,8.0,6.0,0.0,4381.0,GA analytics freemium is limited and 360 premium too expensive. Hypothetical Scenario: So I have an app(say 200K users) and BigQuery. Goal is to set up a datalake: I want to be able to store hit/event level data triggered by user clicks etc and also have session and user level views of the events/hits. Is there an example of this being done? do companies actually do this? Is this feasible? What do you think?
683,2019-05-16 01:04:25,1557957865.0,dataengineering,Career Change Out of Data Engineering,bp4amh,flipstables,,https://www.reddit.com/r/dataengineering/comments/bp4amh/career_change_out_of_data_engineering/,5.0,15.0,0.0,4384.0,"I'm curious if anyone here is trying to transition from data engineering into a different role.  What new roles are easy to transition to?

Here's some that I've been personally thinking about.

1.  Automation/Release/SRE role - seems like an easy transition since a lot of the responsibilities of data engineers is platform work.
2.  Data Scientist, ML - not really personally interested in this, but of course this makes sense.
3.  Backend Developers - from working a lot with databases, backend work seems like a natural transition.

I don't see a lot of transferrable skills to fields like frontend/JS, mobile, or security.  Of course, good software engineering skills means that you certainly CAN move to that role, but it's not as easy as the 3 I've described above."
684,2019-05-16 10:30:40,1557991840.0,dataengineering,MySQL ETL Tools: Free And Paid Options,bp9ktp,cmstrump,,https://www.reddit.com/r/dataengineering/comments/bp9ktp/mysql_etl_tools_free_and_paid_options/,1.0,5.0,0.0,4396.0,"The following overview focused on free and open source MySQL ETL tools, but some of these tools below are the “community” versions of more powerful, paid platforms that you can use if you want more control on your data:  
[**Top 9 MySQL ETL Tools: The Best Free And Paid Options**](https://blog.panoply.io/top-9-mysql-etl-tools-the-best-free-and-paid-options) (see the comparison for more details on each of the following options)

**Free and Opens Source ETL Tools:**

1. [Benetl](https://www.benetl.net/)
2. [Talend Open Source Data Integrator](http://sourceforge.net/projects/talend-studio/)
3. [Apatar](http://www.apatar.com/)
4. [KETL](http://www.ketl.org/)
5. [OpenMRS](https://github.com/openmrs/openmrs-module-mysqletl)
6. [DataExpress](https://github.com/chop-dbhi/dataexpress)
7. [Transformalize](https://github.com/dalenewman/Transformalize)
8. [Csv2db](https://github.com/csv2db/csv2db)
9. [Pentaho Kettle](https://community.hitachivantara.com/docs/DOC-1009855)

**Paid Cloud Services for Extracting MySQL Data to a Data Warehouse:**

A data warehouse is a system that pulls together data from many different sources within an organization for reporting and analysis. The reports created from complex queries within a data warehouse are used to make business decisions

1. [Panoply](https://panoply.io/)
2. [Blendo](https://www.blendo.co/)
3. [Stitch](https://www.stitchdata.com/)
4. [FlyData](https://www.flydata.com/)
5. [Informatica](https://www.informatica.com/)"
685,2019-05-16 16:25:29,1558013129.0,dataengineering,Why Git and Git-LFS is not enough for machine learning reproducibility,bpcisv,cmstrump,,https://www.reddit.com/r/dataengineering/comments/bpcisv/why_git_and_gitlfs_is_not_enough_for_machine/,1.0,1.0,0.0,4399.0,"Git-LFS (Git Large File Storage) as the name implies, deals with large files while building on Git. The pitch is that Git-LFS “replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server like GitHub.com or GitHub Enterprise.”

But the key to repeatable ML results is also to keep proper versioning of not only their data but the code and configuration files, and to automate processing steps. Successful projects sometimes requires collaboration with colleagues, which is made easier through cloud storage systems. Some jobs require AI software running on cloud computing platforms, requiring data files to be stored on cloud storage platforms.

The article explains how a machine learning research team can ensure their data, configuration and code are in sync with each other using DVC tool: [Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis](https://towardsdatascience.com/why-git-and-git-lfs-is-not-enough-to-solve-the-machine-learning-reproducibility-crisis-f733b49e96e8)"
686,2019-05-17 05:27:53,1558060073.0,dataengineering,Is it still worth learning Scala?,bpllla,PhotographsWithFilm,,https://www.reddit.com/r/dataengineering/comments/bpllla/is_it_still_worth_learning_scala/,13.0,17.0,0.0,4417.0,"Hey Folks,

I have been working on a on-prem full Microsoft stack Data Warehouse for the last 3 or so years.  Finally, I will be getting to work on a project to build a new solution in the cloud, where I will be playing with all the cool stuff.  Obviously, I need to do a bit of re-skilling.

As what ever we work on will more than likely use a Spark platform, this is where I am starting.  I am currently running through an old Pluralsight (from 2015) course that uses Scala, just to get a bit of a taste.

So, after all of that, is it actually worth learning Scala today?

Cheers"
687,2019-05-17 06:28:59,1558063739.0,dataengineering,Good resources to learn Kafka?,bpm7e7,International_Slip,,https://www.reddit.com/r/dataengineering/comments/bpm7e7/good_resources_to_learn_kafka/,5.0,7.0,0.0,4417.0,"I'm trying to get more data engineering responsabilities at work (I'm a SWE) and the topic of integrating with Kafka came up.

Is there a particular resource you'd recommend to learn it?"
688,2019-05-17 20:31:25,1558114285.0,dataengineering,Developed wrong skillset? How to forge a path back to data ?,bpu6jm,kilodekilode,,https://www.reddit.com/r/dataengineering/comments/bpu6jm/developed_wrong_skillset_how_to_forge_a_path_back/,8.0,10.0,0.0,4423.0,"I am currently working as a frontend developer using on of the popular framework such as React and Angular, However I totally hate this type of work( web designing and shifting pixels ) . I  needed a VISA to work  about two year ago  so I took on a job as a SW. On resuming at the job,  there was more need for  frontend developer, So I jumped in to make my self valuable and ramped up pretty fast developing application they loved and they commended me for it.  I have now developed a skillset I loathe but I got a green card  out of it so no complains on the VISA part.

&amp;#x200B;

Prior to this two years I was an ETL developer for 3 years (Traditional MSBI) and I absolutely loved it to bits. I switched jobs only because my visa ran out and my sponsor was not able to proceed.  I would like to go back into that space and become a data engineer and I have been listening to podcast and studying but I feel lost. How do I explain 2 years of  javascript to future employers? I know it all programming but I have never had experience with technologies such as Apache Spark or Hive though I have take series of courses on them. 

&amp;#x200B;

I would love advice on transitioning? Has anyone every moved  from frontend to data engineering. How do I tackle recruiters who only pick keywords they know from the list ? 

I am currently earning a good salary at the moment doing Front end development but I find it hard every day to go to work. I cringe at the thought that I will have to craft CSS again. I know the framework will change in a years times to the new flavour of the month and I will be out of work because I don't  see myself reading about how to redisplay the same thing in another framework. It feels like I am losing my soul one day at a time. The jobs makes me feel worthless."
689,2019-05-17 21:57:07,1558119427.0,dataengineering,Interview at FB,bpv7cb,Randomaurat,,https://www.reddit.com/r/dataengineering/comments/bpv7cb/interview_at_fb/,8.0,20.0,0.0,4423.0,"I have a DE onsite interview at fb in next couple of weeks. 
The interview consistent of 4 rounds. 2 ETL, 2 Business acumen. SQL and python are tested. 

Any advice/tips are highly appreciated.

Thanks"
690,2019-05-18 09:05:42,1558159542.0,dataengineering,ESXi Nat Connection,bq0lj2,maximus_eros,,https://www.reddit.com/r/dataengineering/comments/bq0lj2/esxi_nat_connection/,1.0,0.0,0.0,4433.0,"Anyone familiar with VMWare ESXi?  
I have a problem on network configuration.

How could I setup a NAT connection?  
Currently, the default is bridged.

TIA"
691,2019-05-20 18:18:43,1558365523.0,dataengineering,An interview about how dbt enables your data teams to build better analytics in your data warehouse,bqwv7p,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/bqwv7p/an_interview_about_how_dbt_enables_your_data/,5.0,2.0,0.0,4468.0,
692,2019-05-21 06:11:28,1558408288.0,dataengineering,ON the evolution of Data Engineering,br5922,linkerzx,,https://www.reddit.com/r/dataengineering/comments/br5922/on_the_evolution_of_data_engineering/,10.0,0.0,0.0,4477.0,
693,2019-05-21 19:50:36,1558457436.0,dataengineering,Distributed Data Querying with Alluxio,brcs66,jstuartmill,,https://www.reddit.com/r/dataengineering/comments/brcs66/distributed_data_querying_with_alluxio/,3.0,0.0,0.0,4490.0,
694,2019-05-22 06:54:49,1558497289.0,dataengineering,Data Analyst who wants to eventually become a Data Engineer,brk1yq,tooObviously,,https://www.reddit.com/r/dataengineering/comments/brk1yq/data_analyst_who_wants_to_eventually_become_a/,9.0,17.0,0.0,4494.0,"So I'm a data analyst. My SQL and Python skills feel pretty solid, but I am by no means an engineer level, and my current job doesn't do a great job of facilitating my growth in these two areas.

&amp;#x200B;

I really want to be a data engineer though, and was wondering. If you were in my position with these skills, what would you focus on learning next.

&amp;#x200B;

Also looking for project ideas, hopefully not too complex ones that can teach me these skills as well as boost my github. Was thinking of a python mysql project to stream top reddit posts to a database, and then doing sentiment analysis for the subreddit that day? My initial thought if you guys had any criticisms. Thanks in advance everyone"
695,2019-05-23 00:09:12,1558559352.0,dataengineering,Has anyone taken Jesse Anderson's course?,brua1l,ArgusFilch,,https://www.reddit.com/r/dataengineering/comments/brua1l/has_anyone_taken_jesse_andersons_course/,5.0,7.0,0.0,4509.0,"Is it worth the price (up to 5k depending on the one you choose)? It seems to be one of the only comprehensive data engineering courses out there - I've read reviews of the Udacity and Dataquest paths, and they've generally been negative (not in depth enough). There was [this](https://www.reddit.com/r/dataengineering/comments/8g1d6e/has_anyone_taken_jesse_andersons_online_database/) post about a year ago, but there was really only a conversation between Jesse (/u/eljefe6a) and some other redditors - nobody that had actually taken the course."
696,2019-05-23 05:26:11,1558578371.0,dataengineering,How Does PayPal Processes Billions of Messages Per Day with Reactive Streams?,brxopo,techPackets_005,,https://www.reddit.com/r/dataengineering/comments/brxopo/how_does_paypal_processes_billions_of_messages/,3.0,0.0,0.0,4512.0,
697,2019-05-23 11:05:39,1558598739.0,dataengineering,Data Engineering skills beyond coding,bs08mv,real_dangerman,,https://www.reddit.com/r/dataengineering/comments/bs08mv/data_engineering_skills_beyond_coding/,10.0,13.0,0.0,4516.0,"Hi there,

I see a lot of posts about which coding/tech skills (Python/Scala, AWS, Azure,...) are essential to become/be a Data Engineer but I am missing some things. How about the ways you set up your storage, when is file storage ok, when database, what about data modeling (normal, dimensional, Data Vault)? Or topics like data quality? 

These are the same things I am missing in a lot of the courses you can find. Many teach how to process large or fast data sets but omit these. Maybe it's part of Data Engineering being so ""new"" and undefined (last time I checked there wasn't even a Wikipedia article about it) 

What's your opinion? Are those  skills essential too or just side topics?"
698,2019-05-23 15:23:04,1558614184.0,dataengineering,Udacity Data Engineering Nanodegree Course Review,bs2a7p,WannaBeGISGuru,,https://www.reddit.com/r/dataengineering/comments/bs2a7p/udacity_data_engineering_nanodegree_course_review/,55.0,33.0,0.0,4517.0," \# Overview  
\[Udacity's new Data Engineering Nanodegree\]([https://www.udacity.com/course/data-engineer-nanodegree--nd027](https://www.udacity.com/course/data-engineer-nanodegree--nd027)) is one of the few data engineering courses out there right now. It is geared towards people that already have programming experience, specifically with Python and SQL. Udacity estimates that it would take someone 5 months to complete if they committed 5 hours a week (\~108.6 hours of content) at the one-time price of $999 USD (this has changed since I started and is now $399 USD / month). The course is broken up into five sections, Data Modeling, Cloud Data Warehouses, Data Lake with Spark, Data Pipelines with Airflow, and a capstone project. Each section has different instructors, with each one bringing a different teaching style in a way that keeps things refreshing while still keeping you wondering if it happened simply due to lack of communication. The structure for each section consists of introducing concepts through lectures, reinforcing the material with demos and exercises (typically in a Jupyter Notebook), and concludes with 1-2 project(s) dealing with designing an ETL process using song data for an imaginary company called Sparkify.  


\# My background  
I have about two years of professional experience wrangling data with Python and SQL and about a year and a half of web development experience. I have a bachelors degree in engineering and took a few introductory computer science courses. A few months ago I completed \[Dataquest's Data Engineering Path\]([https://www.dataquest.io/path/data-engineer/](https://www.dataquest.io/path/data-engineer/)) and have taken a few \[DataCamp\]([https://www.datacamp.com/](https://www.datacamp.com/)) courses as well as \[CS50\]([https://www.edx.org/course/cs50s-introduction-to-computer-science](https://www.edx.org/course/cs50s-introduction-to-computer-science)) and \[CS50 Web\]([https://cs50.harvard.edu/web/2019/spring/](https://cs50.harvard.edu/web/2019/spring/)). I enrolled in this course due to its focus on cloud technologies, which I have been learning through trial by fire at a data engineering job I started a few months ago, mostly using AWS, Postgres, Python, and Airflow.  


\# Individual Sections Review  
\## Data Modeling  


This section introduces what data modeling is, why it's important, and what the differences between a relational and NoSQL database are. It speaks on important concepts such as ACID transactions, what fact and dimensional tables are, and what the difference between star and snowflake schemas is. This section uses Postgres and Apache Cassandra and consists of a project for each of them where you design schemas and load song logs and song metadata into fact and dimension tables.  


Pros:  


\- Introduces most of the Postgres and Apache Cassandra commands a data engineer would probably ever use  
\- Provides a good explanation on when you'd want to use SQL vs. NoSQL  


Cons:  


\- Most lectures consisted of watching the lecturer read slides off her laptop  
\- This section's exercises seemed to have more bugs than the rest  
\- There were a few questionable practices in this section such as a try / except block around everything and always inserting rows individually instead of in bulk  


\## Cloud Data Warehouses  


This section builds on the previous section and explains the need for a data warehouse and what the benefits of hosting it in the cloud are. AWS basics such as IAM, creating an EC2 instance, and security groups are introduced, as well as a brief introduction to infrastructure as code using boto3. Other concepts such as OLAP cubes, rollup, drill-down, grouping sets, and columnar storage are discussed. The project consists of designing tables in Redshift and loading data from S3 to Redshift.  


Pros:  


\- Provides practical example exercises such as loading S3 files in bulk to tables in Redshift using the COPY command  
\- Makes creating a sandbox data warehouse environment much more approachable. Prior to this I always thought it would be too expensive and complicated to build one on my own and this section proved me wrong  


Cons:  


\- Tries to cover too much ground. Topics like infrastructure as code are glimpsed over and overly simplified  


\## Data Lakes with Spark  


Introduces what big data is and why big data tools like Hadoop and Spark are necessary. Provides a conceptual overview of how distributed systems like Hadoop and Spark work. Hands-on exercises consist of using PySpark to wrangle data. Explanations of why an organization may need a data lake instead of a data warehouse are provided. The project consists of ingesting raw S3 files, creating fact and dimension tables, partitioning them and writing them back to S3 all with PySpark.  


Pros:  


\- Provides an excellent explanation of how distributed file systems and cluster computing works  
\- Gives a good explanation on when to use PySpark data frames vs PySpark SQL and how to use them interchangeably  


Cons:  


\- This project involved filling in a lot more blanks than the rest of the projects and I found it to be particularly time-consuming. The number of files to ingest from S3 seemed too large to run in a reasonable amount of time  
\- I wish it would have included more information and exercises about using PySpark on a cluster of machines instead of on a single local one  


\## Data Pipelines with Airflow  


Data pipelines, DAGs, and Airflow concepts such as operators, sensors, and plugins introduced. The final project involves using Airflow to load S3 files into partitioned Redshift tables and perform data quality checks afterward.   


Pros:  
\- The only tutorial I've found on how to use data quality checks with Airflow. I've started using this technique at work and it is a game changer  
\- Airflow is a bitch to deploy and someone they engineered a way for people to run it on Udacity's workspaces. Kudos to the engineers on that  


Cons:  
\- This section felt a bit shorter and was more focused around a specific technology than the other sections. Not necessarily a con but I would have liked to have the lectures be more generalized around the concepts of a data pipeline  


\## Overall  
Overall, I really enjoyed this nanodegree and learned a lot of practical things from it that I have already started using at my job. I would estimate I spent about 40 hours completing it so I definitely felt short-changed in content and think it is incredibly overpriced for what it is. What I don't like is how Udacity markets their courses as a way for someone to make a career change with no real-world experience. I find that incredibly hard to believe and can't imagine a company hiring someone with no real world experience after completing this nanodegree. I found the content to mostly be of a very high quality and I think this is really the only intermediate-advanced data engineering course out there. If you have the cash and are interested in learning data engineering in the cloud I would highly recommend it."
699,2019-05-23 22:24:51,1558639491.0,dataengineering,10 Benefits to using Airflow,bs74j5,linkerzx,,https://www.reddit.com/r/dataengineering/comments/bs74j5/10_benefits_to_using_airflow/,11.0,0.0,0.0,4525.0,
700,2019-05-24 05:42:42,1558665762.0,dataengineering,Top Python ETL Tools - Airflow vs. Alternatives,bsbnuh,thumbsdrivesmecrazy,,https://www.reddit.com/r/dataengineering/comments/bsbnuh/top_python_etl_tools_airflow_vs_alternatives/,0.0,6.0,0.0,4541.0,"Python developer community has built a wide array of open source tools for ETL. Some of these tools allow you to manage every step of an ETL process, while others are just really good at a specific step: [Top Python ETL Tools (aka Airflow Vs The World)](https://blog.panoply.io/top-9-python-etl-tools-and-when-to-use-them) (overview)

* [Airflow](https://github.com/apache/airflow)
* [PySpark](https://spark.apache.org/docs/2.2.1/api/python/pyspark.html)
* [petl](https://github.com/petl-developers/petl)
* [Panoply](https://panoply.io/)
* [pandas](http://pandas.pydata.org/)
* [Bubbles](https://github.com/stiivi/bubbles)
* [Bonobo](https://www.bonobo-project.org/)
* [Luigi](https://github.com/spotify/luigi)
* [Odo](https://github.com/blaze/odo)
* [etlalchemy](https://github.com/seanharr11/etlalchemy)
* [mETL](https://github.com/ceumicrodata/mETL)
* [Open Semantic ETL](https://opensemanticsearch.org/etl)
* [Mara](https://github.com/mara/data-integration)
* [riko](https://github.com/nerevu/riko)
* [Carry](https://github.com/toaco/carry)
* [locopy](https://github.com/capitalone/Data-Load-and-Copy-using-Python)
* [etlpy](https://github.com/ferventdesert/etlpy)
* [pygrametl](https://github.com/chrthomsen/pygrametl)"
701,2019-05-24 08:00:52,1558674052.0,dataengineering,"VP of Engineering (Data) interview tomorrow. Any advice ? Think it’s a tad bit of title inflation, I am slightly under qualified ( been a Sr data analysts for many years , have lead projects , work in a specific industry that this company is a start up in )",bscvah,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/bscvah/vp_of_engineering_data_interview_tomorrow_any/,8.0,7.0,0.0,4560.0,"What are some high level topics to undertake and to over ? Offshore / onshore engineers , reporting , collecting requirements from business ... I am also worried about hyper technical questions"
702,2019-05-24 15:18:55,1558700335.0,dataengineering,Open-sourcing Whirl: Local Airflow development made easy,bsg88l,hgrif,,https://www.reddit.com/r/dataengineering/comments/bsg88l/opensourcing_whirl_local_airflow_development_made/,3.0,3.0,0.0,4583.0,
703,2019-05-24 21:09:26,1558721366.0,dataengineering,Learn Data Science from Scratch-Best online resources in 2019,bsk6q8,prabhat008,,https://www.reddit.com/r/dataengineering/comments/bsk6q8/learn_data_science_from_scratchbest_online/,0.0,0.0,0.0,4594.0,
704,2019-05-25 03:47:27,1558745247.0,dataengineering,The Importance of Building a Data-Centric Culture,bsomfs,craigbrownphd,,https://www.reddit.com/r/dataengineering/comments/bsomfs/the_importance_of_building_a_datacentric_culture/,3.0,0.0,0.0,4597.0,
705,2019-05-26 21:05:38,1558893938.0,dataengineering,"Best data formats to store text data on disk, for fast retrieval using index or key ?",btb1bd,BatmantoshReturns,,https://www.reddit.com/r/dataengineering/comments/btb1bd/best_data_formats_to_store_text_data_on_disk_for/,2.0,5.0,0.0,4616.0,"I have a ton of text data that I would like to store to disk since it won't all fit into ram, but I would like to retrieve certain rows quickly using an index or key.

So far it's just rows of text data. I haven't implemented any permanent index or key yet, since the data format chosen may affect that. A situation might be that I'll need to access the text from rows 34, 8493, 39993, 333, and 4903 in an instant. I can not only load a subset because the user may query any of the rows, not just a subset

I was looking at numpy's memmap at first, but it looks like that's won't be the most optimal solution for text data.

I am also looking at hdfy / h5py / pytables but it's unclear these data formats are optimal for text data.

Finally, I also come across SQLite/sqlite3 . I would prefer to avoid this format since I don't want to learn too much database stuff at the moment, but if it's the best option I will use it."
706,2019-05-27 03:35:19,1558917319.0,dataengineering,Advice for moving into Data Engineering,btfgnb,m23khan,,https://www.reddit.com/r/dataengineering/comments/btfgnb/advice_for_moving_into_data_engineering/,13.0,7.0,0.0,4623.0,"Hi Folks,

&amp;#x200B;

Sorry for long post BUT I really need your help folks. Please, help me. 

&amp;#x200B;

Here is my situation:

Location: Toronto, Canada

Education: MSc. Computer Science | York University (2015)

Experience:

2009-2012 - web programming / mainframe (including some SAS)

2012-2016 - Build and release automation (some DevOps)

2016-2018 - DevOps and AWS Cloud automation

2018-PRESENT - Software Development with focus on SQL databases (MS SQL and SAP HANA)

Current job roles and skills:

\- Creating microservices (backend) in Kotlin

\- Perform ETL to load data from various MS SQL databases to SAP HANA which acts as company's data warehouse (Business Analysts are creating views on top of HANA raw data)

\- Performing ETL using SAP Data Services mainly (majority is RealEstate data).

\- Performing data recon using SAP SRS-DA (SAP Data Assurance).

\- Loading legacy data from decommissioned systems and manual data from MS Excel spreadsheets into SQL database using Python Pandas

\- Creating and maintaining overnight job process for data loading on top of SAP Data Services and Automic ONE Automation Engine.

\- API based data integration between market data provider (preqin) and our vendor Application' database (funds and private equity data).

&amp;#x200B;

But I realize one thing -- 

&amp;#x200B;

I really want to get into data engineering. Few things I can think I should start doing/learning:

&amp;#x200B;

\- Data Analysis (to perform better transformation instead of just extract and load) -- perhaps learn and use ER diagrams to remove 'duplicates' and 'normalize' data.

\- Once data has been normalized, help Business Analyst simplify their views by cutting down on potential number of sources/joins.

\- perform database administration (on AWS MS SQL RDS) for our web application.

\- Utilize NoSQL database (such as MongoDB) for loading blob-based data for web application (where it makes sense).

&amp;#x200B;

What else should I look into to get into data engineering properly?

&amp;#x200B;

Thank you."
707,2019-05-27 03:46:55,1558918015.0,dataengineering,Good online certifications for Data Engineering?,btfkrx,m23khan,,https://www.reddit.com/r/dataengineering/comments/btfkrx/good_online_certifications_for_data_engineering/,4.0,8.0,0.0,4623.0,"Hi Folks,

&amp;#x200B;

I have background in Software Development and DevOps. I am planning to make my next career move into Data Engineering. Can you guys suggest any good online certifications for data engineering?"
708,2019-05-27 07:35:07,1558931707.0,dataengineering,"Data Engineering - Concurrency, Distribution, Redundancy",bthogc,elcric_krej,,https://www.reddit.com/r/dataengineering/comments/bthogc/data_engineering_concurrency_distribution/,7.0,1.0,0.0,4626.0,
709,2019-05-27 08:27:47,1558934867.0,dataengineering,Guys take a look at my repo where I try to aggregate useful resources to sharp you data engineering skills,bti4re,adilkhash,,https://www.reddit.com/r/dataengineering/comments/bti4re/guys_take_a_look_at_my_repo_where_i_try_to/,1.0,0.0,0.0,4628.0,
710,2019-05-27 09:15:03,1558937703.0,dataengineering,A list of useful resources to learn Data Engineering from scratch,btiipm,adilkhash,,https://www.reddit.com/r/dataengineering/comments/btiipm/a_list_of_useful_resources_to_learn_data/,37.0,7.0,0.0,4628.0,
711,2019-05-28 17:47:32,1559054852.0,dataengineering,"80% of data science is data engineering so why do people become data engineers if not to progress to data scientist? It feels like a data scientist spends most of their time doing data engineering, then 10 minutes importing sklearn, and get paid far more for the privilege.",bu0wbo,poppycocknbalderdash,,https://www.reddit.com/r/dataengineering/comments/bu0wbo/80_of_data_science_is_data_engineering_so_why_do/,32.0,22.0,0.0,4668.0,
712,2019-05-28 21:53:44,1559069624.0,dataengineering,Schema change in Data Warehouse,bu3w8r,rishikaidnani,,https://www.reddit.com/r/dataengineering/comments/bu3w8r/schema_change_in_data_warehouse/,15.0,4.0,0.0,4677.0,"Hello! I keep getting these questions in interviews on handling schema change in DW. For example, adding a new column to an existing loaded table efficiently. Any thoughts/articles on going about this?"
713,2019-05-28 23:03:18,1559073798.0,dataengineering,DataOps Principles: How Startups Do Data The Right Way,bu4rs3,phlogisticfugu,,https://www.reddit.com/r/dataengineering/comments/bu4rs3/dataops_principles_how_startups_do_data_the_right/,3.0,2.0,0.0,4681.0,
714,2019-05-29 00:02:43,1559077363.0,dataengineering,Incorporating data engineering aspects into my analyst role,bu5ib9,Dope-as-the-pope,,https://www.reddit.com/r/dataengineering/comments/bu5ib9/incorporating_data_engineering_aspects_into_my/,5.0,1.0,0.0,4682.0,"Hi all, I just recently got my first analyst position at an educational district that focuses mostly on SQL querying and reporting. But I am also responsible for creating/maintaining a good amount of automated processes that update tables, load tables into csv's, fire off stored procedures, etc.. From what I can tell, the method that they have used seems a bit ragtag. On the server the task scheduler will execute a batch file, which executes a ssis package, which fires the stored procedures/loads the data and whatnot. On top of this there are also automated tasks with SQL Server Agent. I had to change the time of one of the tasks and it was very difficult to figure out if that would have adverse effects on the other tasks.

Am I correct that this method isn't desirable? If so, what would be some better ways this could be done? I have looked into WMS's such as Airflow and Luigi, but I don't know if this is the ideal environment for them, or if they would just create more work and be overkill. I do potentially have long term aspirations of being a data engineer so I would love to get more direct experience if the situation is fitting. Thanks for the help."
715,2019-05-29 00:15:50,1559078150.0,dataengineering,FREE hands-on Workshop in SF - Create a data warehouse in Amazon Redshift with Etleap and AWS,bu5o18,AshleyEtleap,,https://www.reddit.com/r/dataengineering/comments/bu5o18/free_handson_workshop_in_sf_create_a_data/,1.0,0.0,0.0,4682.0,"SAVE YOUR SEAT HERE: [http://info.etleap.com/devdaysjune24](http://info.etleap.com/devdaysjune24)  


**About The Event**  


AWS Partner DevDay is a partner led, AWS supported event for customers. It is a half day, hands on technical event delivered by APN Partners who have demonstrated technical proficiency and proven customer success in specialized solution areas. Customers will get to try some of the hottest topics in cloud computing, and get deep dives into AWS powered partner solutions. Etleap technical experts will explain key features and use cases, share best practices, walk through technical demos, and be available to answer your questions one on one.  


***Please bring your laptop to participate in this workshop and make sure you have an active*** [***AWS account***](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&amp;all-free-tier.sort-order=asc&amp;awsf.Free%20Tier%20Types=categories%23featured)***.***   
**Who Should Attend**  


AWS Partner DevDay is ideal for data engineers, data scientists, and product developers from all experience levels; both existing builders on AWS who want to dive deeper into highly technical hands on training and content as well as those who are new to the cloud. This event is a great opportunity to connect with Etleap to learn how to implement a successful AWS powered solution.  
**What You’ll Learn:**

* Learn the basics of Amazon Redshift
* Understand the components of ETL architecture on AWS
* Appreciate the pros and cons of fully-managed ETL infrastructure"
716,2019-05-29 11:56:23,1559120183.0,dataengineering,An interview about how the open source Pachdyerm platform makes building flexible data pipelines with first class support for data lineage easy,buc8yl,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/buc8yl/an_interview_about_how_the_open_source_pachdyerm/,5.0,0.0,0.0,4694.0,
717,2019-05-29 16:20:26,1559136026.0,dataengineering,Ingesting Data From Kinesis Stream into OmniSci GPU-accelerated database,buelkp,randyzwitch,,https://www.reddit.com/r/dataengineering/comments/buelkp/ingesting_data_from_kinesis_stream_into_omnisci/,3.0,0.0,0.0,4696.0,
718,2019-05-29 16:21:55,1559136115.0,dataengineering,I start my new role as Data Engineer on Monday. Excited and a bit terrified.,buem41,OkieDaddy,,https://www.reddit.com/r/dataengineering/comments/buem41/i_start_my_new_role_as_data_engineer_on_monday/,21.0,10.0,0.0,4696.0,"So I've been in IT/development for about 9 years now. Started as general IT guy at a Public School, then got into a company as tech support, moved to Database Admin/Development within company, CUrrently Database Developer, but starting a new job as a Data Engineer for a financial institution. It's a unique role, as I won't be under IT/Development, but working within the business side. Having said that, I have a few questions.

&amp;#x200B;

1. Is it common for DE's to work on the business side of house, and not under Dev/IT? It was a hard sell for me to get out of the Dev/IT space and into the business side, but the work seems interesting, and I've always wanted to do data engineering, as I find data fascinating. 
2. What are some things I should know/learn ASAP to make my life/job easier? I know the concepts of Kimball's data warehouse, and have even helped implement star schemas and model the data. However my current bread and butter is more OLTP heavy, not OLAP. 
3. What languages, apart from SQL, should I be most focused on? I've seen people say everything from Python to R to Hadoop to everything else. I have done development in C#, and my college classes were in C, so I have familiarity with C style languages.
4. For people that have taken my path, what is the greatest difference I'm going to experience going from development to a more business-centric role? (The DE position I'm starting doesn't fall under IT, it's on the business side. The company is just starting to realize the power and insight they can get from data, so it's a very new idea, and I've basically been told ""Whatever you want to do/software you want to use, have at it. Basically the blue ocean strategy."")"
719,2019-06-01 06:17:25,1559359045.0,dataengineering,Best Way to Solve Capacity Constraints?,bvggwk,ConfirmingTheObvious,,https://www.reddit.com/r/dataengineering/comments/bvggwk/best_way_to_solve_capacity_constraints/,5.0,10.0,0.0,4747.0,"Hey there,

&amp;#x200B;

I have a model built in PySpark that has scoring thresholds for determining if someone will be moved forward or not. Let's say on a score of 0-1, it's 0.1. So, if someone scores 0.12, they get flagged as 'Y'.

&amp;#x200B;

I have a lot of data preparation before the model build to build the data set and this model just flags user, score, and Y/N based on the threshold.

&amp;#x200B;

How can I best implement constraints into this model in terms of saying, I need no more than 75 Y's from this entire data set, no matter what? I had left the decision part out into a later part of the pipeline, using Presto/Hive, but I am wondering how I can even implement some sort of optimization with that, or will I need to leverage Python again to do it?

&amp;#x200B;

I thought typing this out myself would help me think a bit clearer, but I'm still confused on the subject. I was thinking I could have a few sub-tables that have the current weekly capacity as well as where it is currently allotted, and based on the current model run, could adjust the threshold up or down to set a target of meeting the requirement?

&amp;#x200B;

Any opening thoughts?

&amp;#x200B;

Thank you ahead of time!"
720,2019-06-02 21:58:40,1559501920.0,dataengineering,5 Considerations to have when using Airflow,bw0uqi,linkerzx,,https://www.reddit.com/r/dataengineering/comments/bw0uqi/5_considerations_to_have_when_using_airflow/,15.0,1.0,0.0,4757.0,
721,2019-06-02 22:05:04,1559502304.0,dataengineering,Can a data scientist get into data engineering?,bw0xe1,statistical_engineer,,https://www.reddit.com/r/dataengineering/comments/bw0xe1/can_a_data_scientist_get_into_data_engineering/,4.0,9.0,0.0,4757.0,"As with all data science jobs, job duties can vary.  I do a little of everything, ML, modeling, cloud stuff, working with software engineers and pushing code to production.



I enjoy data science, although my favorite parts of my job are more on the data and software engineering side.  Am I qualified to later apply for data engineering jobs?"
722,2019-06-03 17:11:24,1559571084.0,dataengineering,Azure Databricks experience,bwb39c,monumentalcrankiness,,https://www.reddit.com/r/dataengineering/comments/bwb39c/azure_databricks_experience/,3.0,14.0,0.0,4765.0,How has your Azure Databricks experience been so far w.r.t. data engineering? Anyone has any idea on the specs which are needed to process about 500 GB data daily in an ETL pipeline?
723,2019-06-03 17:52:54,1559573574.0,dataengineering,"“Oh, you’re a data… something?”: The Misunderstood Role Of A Data Engineer",bwbjmv,hdanish,,https://www.reddit.com/r/dataengineering/comments/bwbjmv/oh_youre_a_data_something_the_misunderstood_role/,20.0,3.0,0.0,4765.0,
724,2019-06-04 00:15:13,1559596513.0,dataengineering,Looking for a design pattern,bwg4pv,abelarregui,,https://www.reddit.com/r/dataengineering/comments/bwg4pv/looking_for_a_design_pattern/,1.0,0.0,0.0,4771.0,"Hi all! This is my first post in reddit, I would try to explain it as best as possible.

Well, Im developing a python library in order to read data from different formats and sources (api rest, odbc, access, xlsx...), transform data, analyse it, send emails...

I thought to divide the library on modules: data, analysis, emails... But I'm not sure...

I have to use OOP? For connecting to odbc its fine, but for doing the analysis? Also I want to separate sql queries in a different file...

Anyway, do you know a design pattern for this case?

Thank you a lot!!"
725,2019-06-04 03:37:34,1559608654.0,dataengineering,"An easy, quick way to start learning &amp; try Apache Airflow.",bwibsu,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/bwibsu/an_easy_quick_way_to_start_learning_try_apache/,17.0,2.0,0.0,4773.0,"Airflow is a platform to programmatically author, schedule, and monitor workflows.  Recently, for learning and quickly setting up Airflow, I created a Docker-based Airflow image. Containers support multiple features like writing logs to be local or S3 folder and Initializing GCP during container booting [https://github.com/abhioncbr/docker-airflow](https://github.com/abhioncbr/docker-airflow) Any, one to try or enhance further is most welcome."
726,2019-06-04 14:19:58,1559647198.0,dataengineering,An interview about how and why Greenhouse migrated their homegrown ETL pipeline onto DataCoral,bwnjga,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/bwnjga/an_interview_about_how_and_why_greenhouse/,1.0,0.0,0.0,4775.0,
727,2019-06-04 16:01:09,1559653269.0,dataengineering,Adoption of Azure Data Lake Analytics,bwofbd,monumentalcrankiness,,https://www.reddit.com/r/dataengineering/comments/bwofbd/adoption_of_azure_data_lake_analytics/,1.0,2.0,0.0,4778.0,What has been your experience working with Azure Data Lake Analytics(ADLA) and U-SQL? How does it weigh against Azure HDInsight and Azure DataBricks? Is there substantial merit in developing skills on this tech? How much is the market favouring this?
728,2019-06-04 19:27:26,1559665646.0,dataengineering,"Kedro, an open source tool designed to help you build production-ready data analytics code.",bwqpga,stichbury,,https://www.reddit.com/r/dataengineering/comments/bwqpga/kedro_an_open_source_tool_designed_to_help_you/,11.0,3.0,0.0,4780.0,
729,2019-06-05 12:16:37,1559726197.0,dataengineering,Free Webinar On Logistic Regression,bx0jl3,ankita11_,,https://www.reddit.com/r/dataengineering/comments/bx0jl3/free_webinar_on_logistic_regression/,1.0,0.0,0.0,4795.0,
730,2019-06-07 10:35:57,1559892957.0,dataengineering,Data Structure and Algorithms knowledge to land an entry level position,bxrqri,aashwin93,,https://www.reddit.com/r/dataengineering/comments/bxrqri/data_structure_and_algorithms_knowledge_to_land/,13.0,8.0,0.0,4837.0," In practice, how much DSA should one know for landing a Junior Data Engineering role? Is it fair to assume that it won't be as much as traditional Full Stack/Systems development role requirements? I'm of the opinion that knowledge of frameworks and projects hold a lot more value, but I would like to be disillusioned of this notion if its wrong."
731,2019-06-07 17:07:42,1559916462.0,dataengineering,Any resources for learning data security?,bxv11n,m23khan,,https://www.reddit.com/r/dataengineering/comments/bxv11n/any_resources_for_learning_data_security/,17.0,1.0,0.0,4840.0,"Hi,  


Are there any good online resources (or books) to learn about data security? I am looking for more hands-on, practical methods for implementing data security as part of ETL process and for data at rest."
732,2019-06-09 01:44:05,1560033845.0,dataengineering,Udacity or Udemy for data engineering,bydjvr,supernova0717,,https://www.reddit.com/r/dataengineering/comments/bydjvr/udacity_or_udemy_for_data_engineering/,7.0,9.0,0.0,4855.0,"Hi all, 

I am a BI associate who wants to transition to Data Engineering role. I am doing ETL and dimensional modeling in my current role. I want learn big data technologies and improve my github profile by adding new projects. I want to know the opinion which one is the best option Udacity DE nanodegree or udemy big data courses?"
733,2019-06-09 18:08:35,1560092915.0,dataengineering,Ways to try out actual Data Virtualization?,byl57x,MrKaney,,https://www.reddit.com/r/dataengineering/comments/byl57x/ways_to_try_out_actual_data_virtualization/,1.0,0.0,0.0,4867.0,
734,2019-06-09 21:53:21,1560106401.0,dataengineering,"Are there any data formats for storing text worth looking into, besides CSV ?",bynmbx,DisastrousProgrammer,,https://www.reddit.com/r/dataengineering/comments/bynmbx/are_there_any_data_formats_for_storing_text_worth/,1.0,0.0,0.0,4867.0,
735,2019-06-10 06:14:11,1560136451.0,dataengineering,What programming concepts do you need to know for a data engineering job?,bysskq,dash_365,,https://www.reddit.com/r/dataengineering/comments/bysskq/what_programming_concepts_do_you_need_to_know_for/,7.0,14.0,0.0,4869.0,"For a long time, this question has been bothering me. I'm not a complete beginner to programming. I have experience (Personal projects) with object oriented programming in Python and Java and I script a lot in python. I have started to learn big data technologies and one thing that always bugs me is if my approach in code is right. There are so many ways one thing can be accomplished in programming right. You can either explicitly code everything for better readability, or use shortcuts, functions, general scripts etc., Even though I have experience in programming, I always feel like im missing something in the fundamental section of programming. What programming concepts do you think one should be comfortable with in order to work as a data engineer."
736,2019-06-10 16:16:22,1560172582.0,dataengineering,Would anyone be willing to share your Udacity Data Engineering course for a late payment or any other incentive?,byxq9t,tatallynote,,https://www.reddit.com/r/dataengineering/comments/byxq9t/would_anyone_be_willing_to_share_your_udacity/,0.0,2.0,0.0,4872.0,"Long story short, I'm doing a Data Science internship at a start up and the work is basically modelling some cool stuff. But the head analyst mentioned the need for a Data Warehouse and an ETL pipeline for their dashboard. So I was thinking I could go the extra mile and build it for them and that might make them select me as a DS/DE  when I graduate next year. 

I saw how everyone was recommending this Udacity course and I wanted to learn it quick but the bummer part is that the course costs literally more than I'm getting paid right now so I can't buy it as everything else is already expensive here. I would be willing to pay or chip in the future after I get the job but it would mean a lot if someone would be willing to. I'll give verification for all I said if anyone need it. 

Thanks, have a nice day."
737,2019-06-10 17:00:36,1560175236.0,dataengineering,2-3 year outlook for data engineering in Canada/USA?,byy794,m23khan,,https://www.reddit.com/r/dataengineering/comments/byy794/23_year_outlook_for_data_engineering_in_canadausa/,9.0,9.0,0.0,4874.0,"Hi,

While I have been keenly thinking about moving into Data Engineering as career (long story short - 8 years in DevOps and now doing data engineering style development work for past 1 year).

&amp;#x200B;

While I like the whole vibe around this career path - resources, community, tools, even nice articles citing it as a growing field, Today, I can't help but think from a bit more negative perspective.

&amp;#x200B;

For example,

about 4-5 years ago, System administrator (sys admin) used to be viable career choice but with proliferation of cloud and now containers/serverless, in North America, need for sys admins has decreased a lot. Sys admins are bailing out by boatloads moving into career such as cloud engineering or devops engineering.

&amp;#x200B;

Same thing strikes me about data engineering - I mean with cloud giants such as GCP and AWS offering pre-packaged, play-and-play type ETL services, data streaming services and big data processing services, where does it leave the data engineers? I mean, even for devops engineering, up until 2 years ago, the CI/CD pipeline was the land of milk and honey and by virtue of knowing how to use the cloud's SDK to provision services meant you were considered elite. But now, same stuff is done by your average run-of-mill devs -- heck even k8s is being setup and run by devs these days. And all this is being done without impacting devs productivity.

&amp;#x200B;

So what's not to say that your average data scientists or data analysts won't pick up the required data engineering skills and with availability of more abstract/pre-packaged tools, start doing the work of data engineers?"
738,2019-06-10 17:44:59,1560177899.0,dataengineering,Analyze &amp; visualize your big data easily using Apache Superset.,byyp89,abhioncbr,,https://www.reddit.com/r/dataengineering/comments/byyp89/analyze_visualize_your_big_data_easily_using/,5.0,0.0,0.0,4877.0,"Apache Superset is a modern, enterprise-ready business intelligence web application ([https://superset.incubator.apache.org/](https://superset.incubator.apache.org/)). 

For easy setup &amp; learning of Superset, I created a docker image of Superset. You can try using by downloading docker image from docker-hub or following repo [https://github.com/abhioncbr/docker-superset](https://github.com/abhioncbr/docker-superset)"
739,2019-06-10 19:49:44,1560185384.0,dataengineering,Apache Kafka VS Apache Kinesis – Amit Kumar Yadav – Medium,bz06a0,Amyth111,,https://www.reddit.com/r/dataengineering/comments/bz06a0/apache_kafka_vs_apache_kinesis_amit_kumar_yadav/,1.0,0.0,0.0,4880.0,"Guys wrote an article, any suggestions would be appreciated."
740,2019-06-10 19:59:50,1560185990.0,dataengineering,Wrote an article on Kafka vs Kinesis. Looking for feedback and suggestions.,bz0ap8,Amyth111,,https://www.reddit.com/r/dataengineering/comments/bz0ap8/wrote_an_article_on_kafka_vs_kinesis_looking_for/,1.0,0.0,0.0,4880.0,
741,2019-06-10 20:10:37,1560186637.0,dataengineering,"Wrote an Article on Apache Kafka VS AWS Kinesis, Looking for Feedback/Suggestions. Thanks!",bz0fkh,Amyth111,,https://www.reddit.com/r/dataengineering/comments/bz0fkh/wrote_an_article_on_apache_kafka_vs_aws_kinesis/,0.0,4.0,0.0,4880.0,
742,2019-06-10 23:34:42,1560198882.0,dataengineering,Would you MS in Data Science?,bz2xvt,jdb441,,https://www.reddit.com/r/dataengineering/comments/bz2xvt/would_you_ms_in_data_science/,5.0,13.0,0.0,4880.0,"If you were currently not a developer and wanted to become a data engineer, or some kind of back end engineer, and you had the opportunity to do an MS in Data Science where you could double major in Computer Science, would you do it? 

Would you go back to undergrad and finish applied CS in two years?"
743,2019-06-11 07:23:39,1560227019.0,dataengineering,Best way to orchestrate and transform small to mid-size data?,bz7xjr,holalobster,,https://www.reddit.com/r/dataengineering/comments/bz7xjr/best_way_to_orchestrate_and_transform_small_to/,4.0,6.0,0.0,4883.0,"I'm trying to figure out what's the best way to schedule and process many disparate datasets that varies from 3 to 15GB each (small to mid-size data).

&amp;#x200B;

We need to process batches of small data on regular interval, so the first thing that comes to mind is Airflow. I understand that Airflow is best used to schedule jobs, so I was thinking of using its DockerOperator to run [Jupyter notebooks inside a docker container](https://hub.docker.com/r/jupyter/datascience-notebook/) that will make the data transformations we need.

&amp;#x200B;

I was thinking something like [this](https://medium.com/@fartashh/scalable-data-engineering-platform-on-cloud-a557026aa02e) for architecture, where Airflow will be deployed on ECS and we will be using AWS ECR to store the docker images containing the notebooks. Or would there be a better alternative? I thought about Azure Databricks but it seems to be an overkill."
744,2019-06-11 10:52:52,1560239572.0,dataengineering,How do I can synchronize MySQL database to BigQuery,bz9mfs,imamdigmi,,https://www.reddit.com/r/dataengineering/comments/bz9mfs/how_do_i_can_synchronize_mysql_database_to/,1.0,0.0,0.0,4884.0,
745,2019-06-11 15:28:29,1560256109.0,dataengineering,How can data engineers and scientists be better compensated for their work?,bzbukb,ShapeAI,,https://www.reddit.com/r/dataengineering/comments/bzbukb/how_can_data_engineers_and_scientists_be_better/,7.0,5.0,0.0,4886.0,"I’ve been thinking about all the tasks that data scientists and engineers have to complete in companies that they often don’t get the right recognition or compensation for. In order to try and match their work with compensation, I’ve created this potential bonus system similar to how people in sales are rewarded. 

&amp;#x200B;

https://i.redd.it/2awfzazv0q331.png

* Do you have any thoughts or improvements for this system? 
* Is there any non-monetary compensation that could also motivate you in your work?
* Do you think that this is feasible for your company? 

I really think this is crucial in the business, and I hope we can have a good discussion and answers!"
746,2019-06-11 15:56:59,1560257819.0,dataengineering,Event-Driven Architecture with Vendor Apps,bzc52z,bazingabazing,,https://www.reddit.com/r/dataengineering/comments/bzc52z/eventdriven_architecture_with_vendor_apps/,5.0,10.0,0.0,4886.0,"All, I am trying to push my organization to a more Event Driven Architecture approach utilizing Kafka as a message broker. Unfortunately, a lot of the systems we need to collect data from are 'closed-box' vendor applications that utilize a relational DB as their data store. I could use something like Attunity to publish messages from those RDBMS but I'm curious if anyone has run across a similar use case and how you approached it?"
747,2019-06-11 19:34:22,1560270862.0,dataengineering,An interview about how the open source Hydrosphere platform simplifies management of the full machine learning lifecycle,bzep3x,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/bzep3x/an_interview_about_how_the_open_source/,1.0,0.0,0.0,4888.0,
748,2019-06-11 22:08:50,1560280130.0,dataengineering,How to build a Data Lake using Google Cloud Platform?,bzgrdx,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/bzgrdx/how_to_build_a_data_lake_using_google_cloud/,4.0,7.0,0.0,4889.0,"Hi all 

(**tl;dr at the bottom)**

&amp;#x200B;

I'm a budding data scientist who just accepted a short-term (three months, possibility of extension) position at a company that isn't that technologically advanced. The position starts in a few weeks. 

&amp;#x200B;

In addition to the typical data science tasks, the company has asked me if I could build a data lake that takes in data from various sources they use (e.g. ZoomInfo, Dun &amp; Bradstreet, etc.). They've emphasized that this would be huge for them. I don't have any experience doing this, but I think this would be a great opportunity for me to challenge myself, learn new skills, and make myself more employable in the future. 

&amp;#x200B;

I won't have specifics about the company and the data until I start in a few weeks, but for now I'm trying to do some research on how to build a data lake. I know that google has published [overviews and a few tutorials](https://cloud.google.com/solutions/build-a-data-lake-on-gcp) on using GCP as a data lake, but I haven't really seen anything anywhere about how to \*actually do it\*.  Additionally, I don't know what kind of timescale for doing this I should be thinking about, but my gut tells me 3 months seems short, so if I'm making good progress on this it would incentivize the company to keep me around for longer. 

&amp;#x200B;

Any advice/suggestions?

&amp;#x200B;

Thanks!

&amp;#x200B;

**tl;dr — I'm a data scientist who needs to build a data lake on google cloud platform. I'm looking for resources/detailed tutorials, and advice on how to do so**"
749,2019-06-11 22:55:51,1560282951.0,dataengineering,Do you need to learn Java also along with python for data engineer jobs?,bzhbty,dash_365,,https://www.reddit.com/r/dataengineering/comments/bzhbty/do_you_need_to_learn_java_also_along_with_python/,12.0,12.0,0.0,4889.0,A lot of the jobs that I see listed do not list Java as one of the required skills. I see python in almost every listing. But big data technologies also have Java  drivers. Is Java required as a part of your skill set or can I ignore Java and focus completely on python for my portfolio prep?
750,2019-06-12 10:12:59,1560323579.0,dataengineering,Telematics,bzo0na,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/bzo0na/telematics/,12.0,5.0,0.0,4896.0,"Hi,

&amp;#x200B;

This is my first blog in the series of telematics. I will be explaining the architecture in much more details in the coming part. Please do let me know your views on this. Would love to connect with people who are working in this space and get there feedback

[https://medium.com/@ibnipun10/telematics-fatter-and-bigger-226a820736c9](https://medium.com/@ibnipun10/telematics-fatter-and-bigger-226a820736c9)"
751,2019-06-12 14:16:47,1560338207.0,dataengineering,CI/CD for deep learning models?,bzpxmi,neeraj_sujan,,https://www.reddit.com/r/dataengineering/comments/bzpxmi/cicd_for_deep_learning_models/,4.0,7.0,0.0,4899.0,What is the best approach to integrate a CI/CD pipeline in deploying my machine learning models. I want to automate this process where the model is only integrated if a specific criterion is met like for example the accuracy of the model is above a certain threshold. I want to integrate this with the gitlab CI/CD pipeline. What is the best approach to tackle this problem? ?
752,2019-06-12 23:12:38,1560370358.0,dataengineering,"Big Data Engineer - San Francisco, CA",bzw3dv,Alexandra_Smith,,https://www.reddit.com/r/dataengineering/comments/bzw3dv/big_data_engineer_san_francisco_ca/,1.0,0.0,0.0,4902.0,
753,2019-06-14 12:13:37,1560503617.0,dataengineering,Would you refactor an ETL which is in Python 2.7?,c0hyqi,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/c0hyqi/would_you_refactor_an_etl_which_is_in_python_27/,8.0,17.0,0.0,4925.0,"Hello there. I have came to a company which has an ETL based on python 2.7. They don't use neither Luigi nor Airflow or any other open source package out there for data pipelines, but a custom python etl framework with rundeck. 

It works fine, the failure rate is low, it requires no supervision during the weekends, it has CI/CD with GitLab and it can recover itself from 3 days of failures. However, it is very difficult to understand how it is coded, so it is difficult to debug certain things and adding new features overall. Given the circumstance that python 2.7 support will end soon, would it be worth to rethink the whole process? As far as I know, the ending of official support does not have to change anything, because anyway we are not updating the packages that are used in the etl anyway. 

However, another thought that I have is that I could take advantage of the ending of official support to refactor the entire ETL: make it simpler, more efficient, updating Python and its consequent packages to the last version to take advantage of new features of pandas and numpy, starting using airflow or luigi...

We are just two person in the BI departament, and I am not a data engineer per se (neither were the ones who built the etl at first) so I have another tasks to do (machine learning mostly). 

In one hand, my business intuition says me that it is completely worthless, because it works, and refactoring would mean probably several months of almost exclusive dedication for none (or almost none) business impact. In the other hand, I feel like learning Airflow and other stuffs and dig deeper into a data engineering task (I enjoy a lot programming in Python).

I guess that for the company's benefit I should not touch anything, but I have a little doubt about whether changing from python 2.7 to python 3.7 would be ""mandatory"" sooner or later and maybe it is worth to face the problem before it arises.

Thank you for your comments"
754,2019-06-15 06:55:44,1560570944.0,dataengineering,Sql for interview prep,c0t5cq,supernova0717,,https://www.reddit.com/r/dataengineering/comments/c0t5cq/sql_for_interview_prep/,15.0,2.0,0.0,4928.0,"Hi all,

I am curious what other websites people use for SQL for interview prep apart from leetcode and hackerrank? I know about modeanalytics and sqlzoo."
755,2019-06-15 22:15:38,1560626138.0,dataengineering,Do Entry Level Positions Even Exist?,c10z3v,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/c10z3v/do_entry_level_positions_even_exist/,8.0,11.0,0.0,4940.0,"Im a Data Analyst looking to become a data engineer. Ive been searching for jobs for about a month now and I have yet to see and data engineer listing that didnt want a shit load of experience.

So how would someone like me get into this field? Do you need to be a software engineer first? Here is my resume for reference. [resume](https://i.imgur.com/koao8sl.jpg)"
756,2019-06-15 23:39:23,1560631163.0,dataengineering,"Warning: stay far, far away from Udacity's Data Engineering Nanodegree.",c11ut4,pr00ffreader,,https://www.reddit.com/r/dataengineering/comments/c11ut4/warning_stay_far_far_away_from_udacitys_data/,37.0,31.0,0.0,4941.0,"Where to start? I'll just hit the highlights.

&amp;#x200B;

* This is the first time they're giving this nanodegree, so I'm one of the 'test subjects'. Some kinks that need to be worked out of the system are to be expected, but this goes beyond the pale. I have heard quite positive things about their other nanodegrees, and indeed the other Slack channels are not raging pits of despair like the data engineering one.
* They have a first-seven-day only quit-and-refund policy. It was not obvious until we started doing the first project, which I doubt anyone started in the first seven days, how extremely terrible this course was.
* The idea is to use different techniques to engineer data pipelines for a music streaming company, using the metadata of their songs (a subset of about 14,000 songs from from the Million Song Dataset) and their log files (automatically generated from some Scala script on GitHub, each one logging a user interaction with the service, most but not all of which are songplays). We're supposed to use different techniques ""as the company grows""... \*but it's always the same dataset\* with one month of user logs, about 9,000 lines. First we use a simple local Postgres server, then Cassandra, then we build a data warehouse on Amazon Redshift, then we create a DataLake with Spark (well, \*sort of\* a datalake, S3-to-S3, no Elastic MapReduce), then we build a pipeline to create a better DataLake (hopefully with EMR? I'm just starting that section) with Airflow. So yep, I'm distributing datasets with 9,000 records with Cassandra and Spark.
* So the natural thing to do is to reshape all of that data into a star schema, with a songplays fact table, and song and play metadata in dimension tables. The problem is: only about 1% (if that) of rows in the fact table match a song in the database. So I'm thinking they chose their songplays songs from the entire Million Song Dataset and then realized they didn't have the capacity to handle it all so chose the first 14,000 so it looks like 99% of the songs their users play aren't in their dataset at all!
* About that million song dataset... they seem to have some hashing algorithm generating filenames for the songs, and its always first three letters A-Z, then a bunch of characters A-Z or 0-9. So the data source, for their own doubtlessly good reasons, puts ONE song per json file, and puts them in a NESTED folder structures, e.g. /A/A/A/AAA0F8DS796FE8C.json. These are put in an S3 bucket, which of course doesn't really have folders, just filenames with slashes. For one project they increased the number of songs to 300,000, realized it was taking students more than a day to parse them, and two weeks later went back to the original 14,000.
* About that million song dataset... so those 14,000 files which are (I checked) in the original source properly UTF-8 encoded JSON (there are a lot of band names and song names with non-ascii characters), were somehow re-encoded into Windows Latin-1, THEN re-encoded into UTF-8, THEN re-encoded into Latin-1 AGAIN. And of course the online notebooks provided by Udacity read them as if they were proper JSON, i.e. UTF-8, whereas they have to be re-encoded twice.
* About that million song dataset... the sane thing to do would be to have the students concatenate them into a few hundred files instead of 14,000... not only do they not do this, they make it impossible to do this by putting the json files in S3 and making reading from S3 part of the rubric to pass a project.
* Oh, and here's the icing on the cake: my last project (the Spark one) was sent back to be redone because I apparently made two mistakes: (1) I (OMG) implemented a logging function when one wasn't specifically called for in the requirements, this after being specifically told that students were encouraged to go beyond the specifications (sometimes it's important to stick to the specs, I get that, but this script took 185 minutes to finish, so I logged the times to find the pain point. It was writing and unreasonably large amount of partitions to S3, btw). (2) I didn't use the drop\_duplicates() function to ensure unique values for one table. You know what I did instead? I used the dropDuplicates() function, you know, the SPARK function for a SPARK project instead of the Python pandas function?
* Part of the selling point of these nanodegrees is you can put your projects on GitHub to be part of your portfolio. I would be embarrassed to put any of these in my portfolio.

&amp;#x200B;

End rant."
757,2019-06-16 11:33:24,1560674004.0,dataengineering,The data engineering cookbook,c17tfa,haloworlds,,https://www.reddit.com/r/dataengineering/comments/c17tfa/the_data_engineering_cookbook/,1.0,0.0,0.0,4942.0,
758,2019-06-16 11:49:40,1560674980.0,dataengineering,The data engineering cookbook,c17wry,haloworlds,,https://www.reddit.com/r/dataengineering/comments/c17wry/the_data_engineering_cookbook/,42.0,9.0,0.0,4942.0,"A beginner friendly resource to know about data engineering.
https://github.com/andkret/Cookbook"
759,2019-06-16 20:39:44,1560706784.0,dataengineering,Learn data engineering website,c1ckwg,_WeWereHere,,https://www.reddit.com/r/dataengineering/comments/c1ckwg/learn_data_engineering_website/,28.0,3.0,0.0,4944.0,"Hi everyone! While myself and a friend explored the field of data engineering, we realised that there are a ton of useful resources out there, but they're scattered all over the place. So we put them all together at [https://www.learndata.engineering/](https://www.learndata.engineering/) \- nothing fancy but hope it helps and makes your life easier.

Would value your input, particularly around any content you would like to see added (appreciate that there is always something new to add or learn). It's still work in progress and we work on it when we have time. Thank you!"
760,2019-06-16 20:51:59,1560707519.0,dataengineering,Need Resources for Advanced Airflow Techniques,c1cpzy,NobunagaOda4563,,https://www.reddit.com/r/dataengineering/comments/c1cpzy/need_resources_for_advanced_airflow_techniques/,1.0,1.0,0.0,4946.0,"Hi,

&amp;#x200B;

I am currently working as a Data Engineer. My first project is almost finishing up. It involved creating an Airflow pipeline for Ingestion and processing data through various databases. Now, I personally feel that I could have done a better job at designing the pipeline, had I known some more case studies, industry standards, advanced techniques, etc. So, if anyone can point me to any resources like case studies with Airflow, end-to-end examples, design patterns to follow, etc., that would be great. In fact, any resources concerning orchestration would do as well. Any help would be appreciated.

&amp;#x200B;

Thanks!"
761,2019-06-17 09:09:29,1560751769.0,dataengineering,"""the Azure Data Engineer""",c1k2x6,The_Real_BruceWayne,,https://www.reddit.com/r/dataengineering/comments/c1k2x6/the_azure_data_engineer/,1.0,0.0,0.0,4952.0,
762,2019-06-17 12:20:04,1560763204.0,dataengineering,How do you QA data at your company?,c1lfp9,theslay,,https://www.reddit.com/r/dataengineering/comments/c1lfp9/how_do_you_qa_data_at_your_company/,16.0,7.0,0.0,4954.0,How do you QA data in your company? What structures or pipeline have you implemented to test the quality and correctness of data in your databases?
763,2019-06-17 18:54:24,1560786864.0,dataengineering,A conversation with the architect of Delta Lake on the challenges of building a sustainable data lake at scale,c1p6uh,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/c1p6uh/a_conversation_with_the_architect_of_delta_lake/,5.0,0.0,0.0,4955.0,
764,2019-06-18 00:20:41,1560806441.0,dataengineering,"Is there such thing as a ""static"" data pipeline?",c1taoa,dantzigismyhero,,https://www.reddit.com/r/dataengineering/comments/c1taoa/is_there_such_thing_as_a_static_data_pipeline/,3.0,7.0,0.0,4959.0,"I'm relatively new to DE as a discipline and I'm trying to figure out if my past projects remotely resemble contemporary DE practices, especially building data pipelines. Basically, I'm curious to know if there's such thing as a data pipeline that only has static data (versus dynamic data - i.e., clickstream data, stuff you get from APIs, etc.).

I had a lot of projects in the past that involved sourcing and extracting data from static datasets, mostly government open data portals and the like, so lots of existing text/CSV, JSON, etc. files downloaded off the web. Would extracting, cleaning, and loading this type of static data into a relational DB count as performing ETL or building a pipeline? Or is there a more appropriate term for this type of work?"
765,2019-06-18 01:19:20,1560809960.0,dataengineering,[Question] How to configure and run a Spark Job on Spark Standalone Cluster (&amp; avoid insufficient memory problem),c1u0vh,__Julia,,https://www.reddit.com/r/dataengineering/comments/c1u0vh/question_how_to_configure_and_run_a_spark_job_on/,4.0,3.0,0.0,4960.0,"Hi, I managed to run a cluster for 1-master and 4 workers.   
  When I navigate to Spark UI, I find the following resources.   


* **URL:** spark://spark-master:7077
* **Alive Workers:** 4
* **Cores in use:** 16 Total, 0 Used
* **Memory in use:** 246.5 GB Total, 0.0 B Used
* **Applications:** 0 Running, 0 Completed
* **Drivers:** 0 Running, 0 Completed
* **Status:** ALIVE

When I try to run a spark job using the following parameters: 

&gt;spark = SparkSession.builder.master(""spark://spark-master:7077"").appName(""test"").config(""spark.driver.memory"", ""8g"").config(""spark.executor.memory"", ""8g"").config(""spark.cores.max"", 16)

When I launch it, I can see that the Cores in all the workers have been used (4 CPUs), along with 8g RAM. However, I get the following error: 

&gt;Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory

I tried to use 2 CPUs for every worker, but I still see the problem. When I try to run on a local mode  

&gt;spark = SparkSession.builder.master(""local\[\*\]"").appName(""test"").

Things work smoothly on a local mode. However, it's slow and I want to make use of distributed environment.   
Any thoughts on how to dubug the problem?  It seems that it remained \[an unanswered question for many ppl\]([https://github.com/databricks/spark-knowledgebase/issues/9](https://github.com/databricks/spark-knowledgebase/issues/9))

  
P.S: I don't want to use Yarn at the moment. I am trying to use Spark-Standalone mode."
766,2019-06-18 05:01:52,1560823312.0,dataengineering,A blog post on why we use data warehouses?,c1wcsm,dash_365,,https://www.reddit.com/r/dataengineering/comments/c1wcsm/a_blog_post_on_why_we_use_data_warehouses/,5.0,2.0,0.0,4965.0,"Hey guys. I wrote a post on medium explaining why we use data warehouses. 

[https://medium.com/@vijayrajsaravanan/if-you-are-reading-this-post-you-are-looking-to-understand-what-data-engineering-is-how-it-works-aaf8d4db03e0](https://medium.com/@vijayrajsaravanan/if-you-are-reading-this-post-you-are-looking-to-understand-what-data-engineering-is-how-it-works-aaf8d4db03e0)

&amp;#x200B;

Kindly provide suggestions if you find discrepancies and do clap for the post if you like it."
767,2019-06-18 08:25:28,1560835528.0,dataengineering,Python/Pyspark for relational database engineer,c1y8cr,waltham_user,,https://www.reddit.com/r/dataengineering/comments/c1y8cr/pythonpyspark_for_relational_database_engineer/,1.0,4.0,0.0,4969.0," Hello,

I have worked with relational databases (Teradata, Oracle) and have mostly used ETL tools(Informatica, Alteryx) for data manipulation and cleaning. I have no programming experience( R, Python or JAVA).

&amp;#x200B;

I have experience in handling 1-2 TB of data using relational databases like Teradata, Snowflake etc. My company has an internal Databricks environment. I have never used it but I did see that Spark SQL as an option to manipulate data. I would like to learn Databricks in order to handle larger datasets and improve my skills.

Since most of my data engineering work is around creating complex variables. Should I start with Python or Pyspark ? I read that the commands are different for both. Appreciate if anyone can share good links (paid or free)for a beginner like me. 

&amp;#x200B;

Thanks!"
768,2019-06-18 10:42:50,1560843770.0,dataengineering,What type of engineer can do this? What skills and software experience would they need?,c1zck2,pokemon_yo,,https://www.reddit.com/r/dataengineering/comments/c1zck2/what_type_of_engineer_can_do_this_what_skills_and/,1.0,0.0,0.0,4973.0,
769,2019-06-18 14:47:22,1560858442.0,dataengineering,The right tool for the job - producing a delta from delimited data sets,c219dg,PhotographsWithFilm,,https://www.reddit.com/r/dataengineering/comments/c219dg/the_right_tool_for_the_job_producing_a_delta_from/,2.0,10.0,0.0,4973.0,"Hi,

I am currently investigating what it would take to migrate away from our current MS BI stack and migrating to the cloud.

One of the challenges that I have is producing a delta of our current data.  In our current iteration, we daily output full tables from Sybase ASE via BCP, stage the data into SQL server and upsert it into a database which stores all records as type 2.  Individual tables can be up to 80 million records and 30 GB outputted as CSV.

What we now want to do is produce a Parquet file for each of these days, but as delta's only (mainly inserts and updates, but have the ability to deal with deletes as needed).  This processing needs to be done on premises.  

Considering I am new to the big data tool kit, I am trying to determine what the best tool for the job is.  I had considered a kind of lift and shift of the process to Postgresql, but would like to determine whether using Spark or Python/PANDA instead.  As I don't yet have enough experience with these tools, I am finding it hard to determine whether they will quickly do what I want with a data set of this size, compared to using a DBMS.

Does anyone have any thoughts or opinions on which way I should look?

Cheers"
770,2019-06-18 16:48:06,1560865686.0,dataengineering,Minimalist Guide to Lossless Compression,c22gqw,marklit,,https://www.reddit.com/r/dataengineering/comments/c22gqw/minimalist_guide_to_lossless_compression/,3.0,0.0,0.0,4976.0,
771,2019-06-18 18:36:40,1560872200.0,dataengineering,Can an ETL Developer Become a Data Engineer?,c23q1a,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/c23q1a/can_an_etl_developer_become_a_data_engineer/,14.0,35.0,0.0,4978.0,"Would ETL Developer be a good career path to becoming a Data Engineer?

From what I've seen, most job descriptions say they use SSIS. I don't know if this would hurt being able to transition.

I'm currently a Data Analyst who wants to switch to Data Engineer, but I have been having hard time cause it seems like a rather difficult switch. I figured I might have an easier time if I take an ETL job first."
772,2019-06-19 08:21:28,1560921688.0,dataengineering,Spark interviewing content?,c2cz71,inlatitude,,https://www.reddit.com/r/dataengineering/comments/c2cz71/spark_interviewing_content/,1.0,0.0,0.0,4987.0,
773,2019-06-19 10:57:42,1560931062.0,dataengineering,Automate executing AWS Athena queries and moving the results around S3 with Airflow,c2e91k,brokenqc30,,https://www.reddit.com/r/dataengineering/comments/c2e91k/automate_executing_aws_athena_queries_and_moving/,13.0,2.0,0.0,4988.0,
774,2019-06-19 15:25:18,1560947118.0,dataengineering,Question about Airflow and Version Control,c2gc0g,pr00ffreader,,https://www.reddit.com/r/dataengineering/comments/c2gc0g/question_about_airflow_and_version_control/,2.0,7.0,0.0,4991.0,"It kind of bothers me that when you change a DAG there's no record of it, and it can look like past runs used the new DAG.

I mean, you can VC the underlying Python code of course, but in the Airflow interface it looks like if you want to make that distinction you need to create an entirely new DAG with a new name.

This is giving me flashbacks of multiple Excel files I would get from BI with names like Final2CopyFinal.xlsx, i.e. version control by copying, changing and suffix naming.

Anyone run across this issue and/or have any ideas?"
775,2019-06-19 19:11:59,1560960719.0,dataengineering,Getting Your Programming Skills Ready for Data Engineering,c2izpo,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/c2izpo/getting_your_programming_skills_ready_for_data/,0.0,6.0,0.0,4991.0,
776,2019-06-20 03:01:41,1560988901.0,dataengineering,10 Days to Become a Google Cloud Certified Professional Data Engineer,c2oqvi,discdiver,,https://www.reddit.com/r/dataengineering/comments/c2oqvi/10_days_to_become_a_google_cloud_certified/,17.0,9.0,0.0,5000.0,"I just wrote a post about my experience studying for and taking the Google Cloud Certified Professional Data Engineer Exam. Hope you find it helpful.

[https://towardsdatascience.com/10-days-to-become-a-google-cloud-certified-professional-data-engineer-fdb6c401f8e0?source=friends\_link&amp;sk=84b9c3f78772308e0407e28af4819324](https://towardsdatascience.com/10-days-to-become-a-google-cloud-certified-professional-data-engineer-fdb6c401f8e0?source=friends_link&amp;sk=84b9c3f78772308e0407e28af4819324)

Feedback welcome!"
777,2019-06-20 09:33:06,1561012386.0,dataengineering,Conferences on IoT,c2sk4r,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/c2sk4r/conferences_on_iot/,1.0,1.0,0.0,5004.0,Can anyone suggest good IoT conferences around the work related to big dat architecture? Would be very happy to listen to others on how they are leveraging big data and cloud services for IoT data
778,2019-06-20 10:06:48,1561014408.0,dataengineering,Anyone have experience in using Delta Lake in production?,c2su32,vanthar686,,https://www.reddit.com/r/dataengineering/comments/c2su32/anyone_have_experience_in_using_delta_lake_in/,5.0,0.0,0.0,5004.0,"[Delta Lake](https://delta.io/) is an [open source storage layer](https://github.com/delta-io/delta) that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing."
779,2019-06-20 11:40:21,1561020021.0,dataengineering,Azure Specialisation,c2tjiv,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/c2tjiv/azure_specialisation/,1.0,0.0,0.0,5005.0,My company is planning to move to Azure. I have never worked on Azure but have worked extensively of AWS. Can someone provide me any free links wherein I can see all the services and how they are used in Azure from a data engineering perspective
780,2019-06-20 12:44:49,1561023889.0,dataengineering,Vehicle Telematics,c2u0rv,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/c2u0rv/vehicle_telematics/,7.0,12.0,0.0,5006.0,"Hi All,

&amp;#x200B;

This is my second part of the series ""Telematics"". Please go through it and let me know you feedback [https://medium.com/@ibnipun10/vehicle-telematics-adaa1648c89b](https://medium.com/@ibnipun10/vehicle-telematics-adaa1648c89b)"
781,2019-06-20 23:45:22,1561063522.0,dataengineering,"Is there such a thing as ""Entry Level Data Engineer""?",c31cjw,aashwin93,,https://www.reddit.com/r/dataengineering/comments/c31cjw/is_there_such_a_thing_as_entry_level_data_engineer/,6.0,35.0,0.0,5013.0,It seems most of the job descriptions out there point to a minimum of 2 years of work ex. I would love to hear about fellow redditors' stories about bagging that first DE job.
782,2019-06-21 01:03:46,1561068226.0,dataengineering,Announcing Astronomer v0.9 - Platform to run Airflow on Kubernetes,c32axc,rywalker,,https://www.reddit.com/r/dataengineering/comments/c32axc/announcing_astronomer_v09_platform_to_run_airflow/,8.0,0.0,0.0,5014.0,
783,2019-06-21 05:02:10,1561082530.0,dataengineering,Has anyone emulated Netflix' use of Notebooks?,c34nov,work_acc_1,,https://www.reddit.com/r/dataengineering/comments/c34nov/has_anyone_emulated_netflix_use_of_notebooks/,17.0,18.0,0.0,5015.0,I'm hoping for insight on the process that Netflix took in making [Notebooks so handy within the company](https://medium.com/netflix-techblog/notebook-innovation-591ee3221233). Does anyone here have experience trying to set up something like this?
784,2019-06-22 08:19:31,1561180771.0,dataengineering,Managing raw data files on Docker Swarm,c3lpjm,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/c3lpjm/managing_raw_data_files_on_docker_swarm/,1.0,0.0,0.0,5028.0,
785,2019-06-23 01:46:28,1561243588.0,dataengineering,Is there any specific reason to learn Dataframe API for spark inspite of knowing the SQL version?,c3vtw5,dash_365,,https://www.reddit.com/r/dataengineering/comments/c3vtw5/is_there_any_specific_reason_to_learn_dataframe/,9.0,16.0,0.0,5038.0,I have quite a bit of experience with SQL and pandas as well. Now I intend to learn spark to add to my skill set. I see that you can do all the data manipulation using spark SQL which is just SQL language. I intend to focus on learning that rather than learning both that as well as the Pandas like dataframe api. I am reluctant because I'm pretty sure I'll get confused with pandas and mess up with the minute variations in terms of syntax. Is it okay to concentrate only on spark SQL or is it essential to learn the pandas like method also?
786,2019-06-23 11:26:01,1561278361.0,dataengineering,Design - How to come up with raw numbers?,c411d6,cs_dude,,https://www.reddit.com/r/dataengineering/comments/c411d6/design_how_to_come_up_with_raw_numbers/,1.0,9.0,0.0,5043.0,"I'm looking to design a data pipeline and I am finding it difficult to get actual numbers. I'm not talking about the big ""design patterns"" questions, but rather things like:  


1. If I deploy this flask API on the cloud, what would it be the TPS (Transactions per second)?
2. If I connect it to a mysql / nosql database, what is the TPS?
3. If it connects to a separate nodejs application, how many TPS can it handle? If I bundle / should I bundle the two on the same server to get a higher / lower tps? How much better / worse would it be?  


I am well aware that all this will depend on the application itself, what kind of server (ram / processor / network bandwidth). All this adds to the complexity.  


What I am asking is, is there a way to figure these out just looking at the specifications of the various services and some internal testing of the application? Or is a full deployment and throughput testing necessary? I'm looking for actual raw numbers here. I think when I understand the raw numbers better then I can make design pattern questions (should it be monolithic or microservices, what kind of database, etc) better.

&amp;#x200B;

Thank you."
787,2019-06-24 00:56:26,1561326986.0,dataengineering,Any thoughts on handling query optimization questions in the interview? I know EXPLAIN plan is a way to go about it but what areas of explain plan to stress on,c4csn0,rishikaidnani,,https://www.reddit.com/r/dataengineering/comments/c4csn0/any_thoughts_on_handling_query_optimization/,12.0,6.0,0.0,5050.0,
788,2019-06-24 17:55:23,1561388123.0,dataengineering,"""Guerrilla Analytics"" advocates - implementing best practices for ""Data Receipt"" stage",c4p8yf,fail_to_reject_null,,https://www.reddit.com/r/dataengineering/comments/c4p8yf/guerrilla_analytics_advocates_implementing_best/,2.0,0.0,0.0,5062.0,"Hi,

I'm building a software system for my organization with the goal of automating and standardizing provenance/audit, ETL, and build process for our data warehouse, and combining it with a data science ""lab"" pipeline similar to what's in Guerrilla Analytics and also seen in other ""agile data science"" books.

The main goals for the effort are:
* new and existing data provenance are maintained, data are audit-able from initial introduction through to end products (either in our data warehouse, or at the end stages of a data science effort)
* it's fast and easy to introduce new data into our ecosystem / data warehouse, and to create tests and manage risk around new and exist data 
* it's fast and easy to create new data science ""lab"" efforts: environments, templates, version control, containers etc are set up automatically, best practices for folder structures and conventions are followed, etc

Ridge's book suggests many practices for the Data Receipt stage, and I wonder if and how you folks may be implementing the ideas.

* Received data storage and UIDs - are you generating and storing what would otherwise be locally-stored data UIDs, i.e. metadata, in a central place?
* Version control - how are you version controlling your data? Do you centralize this step / process?
* ""Data logs"" - this practice involves storing information about the data and provenance in files kept with the data. How do you use yours - do you use a wiki? Do you use google sheets?"
789,2019-06-24 18:46:55,1561391215.0,dataengineering,Words relations to their broader categories,c4q586,changeeverymoment,,https://www.reddit.com/r/dataengineering/comments/c4q586/words_relations_to_their_broader_categories/,1.0,8.0,0.0,5063.0,"I am trying to develop an algorithm that takes the given keyword and  assigns the keyword's rankings according to the created categories/classification. For instance, if I type ""calculus"", the rankings for this keyword in the category ""math"" will be something like ""0.9"" and for the category ""english"" it will be ""0.1"".  I was wondering if someone has already came up with the database with the similar information about relations between the words and their classes in a mathematical way."
790,2019-06-24 19:15:18,1561392918.0,dataengineering,Noob Question: Difference in day to day of Data Engineer vs ML engineer,c4qn3g,vvrsk,,https://www.reddit.com/r/dataengineering/comments/c4qn3g/noob_question_difference_in_day_to_day_of_data/,11.0,10.0,0.0,5063.0,I am still new to data world and from what I know that as  DE most our days are spent in tuning systems and making things faster and mostly delaing with distributed systems. But I am not quite sure what a machine learning engineer job entails. Can some one please clarify. TIA.
791,2019-06-25 19:35:27,1561480527.0,dataengineering,An interview about how the Prefect workflow engine unifies the needs of data engineers and data scientists with a pure Python API,c5b70b,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/c5b70b/an_interview_about_how_the_prefect_workflow/,6.0,0.0,0.0,5077.0,
792,2019-06-26 04:13:48,1561511628.0,dataengineering,Product Owner/Manager of Data Engineering/Platforms,c5isnz,mp91,,https://www.reddit.com/r/dataengineering/comments/c5isnz/product_ownermanager_of_data_engineeringplatforms/,6.0,5.0,0.0,5084.0,"I was wondering if we have any folks here who are either in a product management role for data engineering / data platforms OR have someone playing this role in their organization.  

I am currently a Product Owner for the Data team at my current company and admittedly, at the current time, my day-to-day doesn't really look like traditional PO duties.  I've been looking around online for what a PO role looks like in the context of data, and it seems that it can vary depending on the type of business and data products.  

I'm mostly curious for others' experience in this area, so I can gain a bit of understanding of how this role works within other organizations, as I am trying to grow my role beyond what is effectively glorified project management :)

Appreciate any input from the community! Thank you!"
793,2019-06-26 04:47:13,1561513633.0,dataengineering,Consulting to Industry - Resume Review,c5j7mk,tomleeroy,,https://www.reddit.com/r/dataengineering/comments/c5j7mk/consulting_to_industry_resume_review/,1.0,0.0,0.0,5084.0,
794,2019-06-26 18:08:56,1561561736.0,dataengineering,FREE hands-on Workshop in Palo Alto - Create a data warehouse in Amazon Redshift with Etleap and AWS,c5qyub,AshleyEtleap,,https://www.reddit.com/r/dataengineering/comments/c5qyub/free_handson_workshop_in_palo_alto_create_a_data/,3.0,1.0,0.0,5089.0,"Calling analytics-focused data engineers!   


We're joining forces w. @awscloud in Palo Alto on 7/16 to give a FREE hands-on workshop where you'll learn how to set up an analytics-ready data warehouse on Redshift.

&amp;#x200B;

Save your seat today by registering: [https://info.etleap.com/devdaysjuly16](https://info.etleap.com/devdaysjuly16)"
795,2019-06-26 19:50:32,1561567832.0,dataengineering,Testing data quality with SQL assertions,c5s70f,1ewish,,https://www.reddit.com/r/dataengineering/comments/c5s70f/testing_data_quality_with_sql_assertions/,12.0,6.0,0.0,5090.0,
796,2019-06-26 22:11:08,1561576268.0,dataengineering,Alternatives to Alteryx production pipeline? (on windows),c5u74c,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/c5u74c/alternatives_to_alteryx_production_pipeline_on/,3.0,3.0,0.0,5093.0,"**tl;dr — I’d appreciate any tips/advice/resources for building a production pipeline. **

Hi all

I’m a data science/engineering intern at a small company. One of the tasks they have me working on is improving the production pipeline for some data/modeling. 

A year ago they hired a consultant to come in and set up this pipeline — it’s a hodgepodge of Python and R scripts all put together in [Alteryx](www.alteryx.com). Apparently Alteryx doesn’t play nicely with Python, so the consultant switched between the two at different points in the pipeline. This has caused some issues for the staff here who are unfamiliar with R and have some Python knowledge. Also Alteryx is just a pain to deal with in general (plus they only have one license so only one person can log in to the windows VM at a time). 

The entire company is Windows 7 &amp; 10, and The data lives on local SAP HANA servers. 

I’m exploring the option of creating a cheaper, more flexible production pipeline on Windows, but I don’t really know where to start. 

I’d appreciate any tips/advice/resources for building a production pipeline. 

Thanks!"
797,2019-06-27 03:14:54,1561594494.0,dataengineering,How do you manage the quality of your data?,c5yf8a,runnersgo,,https://www.reddit.com/r/dataengineering/comments/c5yf8a/how_do_you_manage_the_quality_of_your_data/,1.0,0.0,0.0,5095.0,"Is there a specific division, team or person to do that at your work place? 

Like, in general software engineering, there are QAs or specific teams to manage the quality of the software; I've always wondered if this is the same for data engineering?"
798,2019-06-27 06:15:45,1561605345.0,dataengineering,"How to get ""familiar"" with the many enterprise tools/technologies out there?",c60aaa,dantzigismyhero,,https://www.reddit.com/r/dataengineering/comments/c60aaa/how_to_get_familiar_with_the_many_enterprise/,1.0,1.0,0.0,5095.0,
799,2019-06-28 02:11:54,1561677114.0,dataengineering,Building a Data Warehouse From Scratch,c6chl5,tomleeroy,,https://www.reddit.com/r/dataengineering/comments/c6chl5/building_a_data_warehouse_from_scratch/,1.0,1.0,0.0,5104.0,
800,2019-06-28 05:53:46,1561690426.0,dataengineering,Pyspark with Testing book recommendation.,c6er84,kilodekilode,,https://www.reddit.com/r/dataengineering/comments/c6er84/pyspark_with_testing_book_recommendation/,9.0,3.0,0.0,5104.0,"Hi, 

I am a data engineer and would love to learn pyspark with good testing practices and examples. Are there any good books anyone can recommend that does this. Most books just read files and do the transformation in all one go without good software engineering practices.I am not a fan of the notebook style approach because of the lack of testing for my CI/CD Pipeline."
801,2019-06-28 14:48:10,1561722490.0,dataengineering,How to start dynamic tables?,c6j875,SimpleChiGuy,,https://www.reddit.com/r/dataengineering/comments/c6j875/how_to_start_dynamic_tables/,1.0,0.0,0.0,5112.0,"We are building a B2B app that requires us to copy out a large number of each customer's database tables in order to run a bunch of modeling on. In other words, each client's dataset is different enough that we can't normalize it into a set number of tables on our end. This isn't something I've done before and I'm not certain the best way to get all their data into our system without dynamically creating tables for each customer in our PostgresQL DB. 

Is there a better more standard way of handling this scenario? I've struggled to get any results from Google, but I suspect my GoogleFu is subpar.

Any help is appreciated and please yell at me if I'm leaving any info out, so I can update the post."
802,2019-06-28 15:08:02,1561723682.0,dataengineering,How to store dynamic customer data?,c6jew0,SimpleChiGuy,,https://www.reddit.com/r/dataengineering/comments/c6jew0/how_to_store_dynamic_customer_data/,1.0,4.0,0.0,5112.0,"We are building a B2B app that requires us to copy out a large number of each customer's database tables in order to run a bunch of modeling on. In other words, each client's dataset is different enough that we can't normalize it into a set number of tables on our end. This isn't something I've done before and I'm not certain the best way to get all their data into our system without dynamically creating tables for each customer in our PostgresQL DB.

Is there a better more standard way of handling this scenario? I've struggled to get any results from Google, but I suspect my GoogleFu is subpar.

Any help is appreciated and please yell at me if I'm leaving any info out, so I can update the post."
803,2019-06-28 18:02:20,1561734140.0,dataengineering,GUIs vs Code based ETL,c6l8jj,christ_ona_stick,,https://www.reddit.com/r/dataengineering/comments/c6l8jj/guis_vs_code_based_etl/,11.0,26.0,0.0,5114.0,"I am fairly new to the BI/ Data Engineering world and have mainly done pipeline design/ ‘DevOps’ (logging, alerting, building internal tools). I’m self taught and have been using GUI based tools like SSIS, DataStage, Matillion, etc. which connect to S3, data warehouses, or tableau. 

Recently I’ve been incorporating some Python with these tools and I love the added flexibility (and programming in general). I certainly plan on learning more Python for analysis/ pet projects. Do you all think that it is worth it to invest in learning code based etl tools? Either for fun, future career opportunities, or any other reason? I’d love to hear any other thoughts comparing or related to GUI/ code based platforms"
804,2019-07-01 13:29:32,1561976972.0,dataengineering,"Your input is needed. Please help, nobody wants to do my survey T.T",c7qxmr,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/c7qxmr/your_input_is_needed_please_help_nobody_wants_to/,1.0,0.0,0.0,5173.0,
805,2019-07-01 20:04:15,1562000655.0,dataengineering,Decisions when creating your Data Lake,c7vthh,lucasecp,,https://www.reddit.com/r/dataengineering/comments/c7vthh/decisions_when_creating_your_data_lake/,1.0,2.0,0.0,5176.0,
806,2019-07-01 22:42:58,1562010178.0,dataengineering,Data Modeling Question,c7z3q7,tpedar50,,https://www.reddit.com/r/dataengineering/comments/c7z3q7/data_modeling_question/,1.0,4.0,0.0,5179.0,
807,2019-07-02 18:20:38,1562080838.0,dataengineering,An interview about testing the limits of scaling Kafka and Cassandra for real-time anomaly detection at Instaclustr,c8ahxl,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/c8ahxl/an_interview_about_testing_the_limits_of_scaling/,2.0,0.0,0.0,5191.0,
808,2019-07-03 15:51:18,1562158278.0,dataengineering,Difference Between Data Engineer and ETL Dev?,c8npn6,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/c8npn6/difference_between_data_engineer_and_etl_dev/,0.0,3.0,0.0,5216.0,"I don't see a huge difference between the two. The only thing I notice is that ETL Dev job descriptions usually require less programming and more proprietary tools. 

Is it bad to take an ETL Dev job if you want to be a Data Engineer or BI? Or is it all in the same realm?"
809,2019-07-03 21:34:28,1562178868.0,dataengineering,[Research] Can I interview you? Anon option available,c8rr7r,skingkong,,https://www.reddit.com/r/dataengineering/comments/c8rr7r/research_can_i_interview_you_anon_option_available/,6.0,4.0,0.0,5219.0,"Hi wonderful community of Data Engineers! I am a Product person and my company is building a platform that data engineers will be the main user of. I am looking to interview \~10-15 Data Engineers to create an accurate, detailed user persona that will act as a guiding light for building out features and functionality. 

&amp;#x200B;

I am posting here because I've been struggling to find data engineers in my local community and network. I've read through many posts in your community and y'all seem to be an open and supportive bunch, so I'm hoping there might be some interest in helping out.

&amp;#x200B;

In return for an interview (over the phone, zoom or slack), I will gladly send you some swag, or buy you a coffee through [https://www.buymeacoffee.com/](https://www.buymeacoffee.com/), or make a donation on your behalf to the non-profit of your choosing. I've been told my interviews can be therapeutic :D and one interviewee even left with a series of blogs our conversation inspired him to write! 

&amp;#x200B;

**If you're open to chatting, please feel free to schedule some time with me via calendly at** [**https://calendly.com/skingkong**](https://calendly.com/skingkong)**, or join GoLang Slack** [**https://invite.slack.golangbridge.org/**](https://invite.slack.golangbridge.org/) **where you can find me @sking.**

&amp;#x200B;

**Anonymous Alternative:**

If you are open to sharing, but not interested in speaking directly, I have put together an anonymous typeform that I welcome you to check out. It will take anywhere from 10-20 minutes to fill out (purely depending on how much you share), and consists of 10 questions – 4 multiple choice and 6 long-form answers. If you want to provide your email at the end, the same rewards listed above are available to you :) 

[https://pilosa.typeform.com/to/pSufzS](https://pilosa.typeform.com/to/pSufzS) 

*If anyone is having a difficulty reading this typeform due to the color combination - please comment and I will re-set it!*

&amp;#x200B;

Thank you for your consideration and getting through this long post!! heh ':D"
810,2019-07-04 14:02:51,1562238171.0,dataengineering,Which Udemy PySpark course should I choose?,c91agy,PhotographsWithFilm,,https://www.reddit.com/r/dataengineering/comments/c91agy/which_udemy_pyspark_course_should_i_choose/,9.0,4.0,0.0,5229.0,"Hi Folks,

Enough with fooling around (&amp; getting stumped) with PySpark.  Its time to do some tutorials.

I notice that there are two reasonably well regarded courses on Udemy:

[Taming Big Data With Apache Spark And Python Hands On](https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/)

[Spark and Python for Big Data with PySpark](https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/)

I like that the second one has a bit more of a AWS slant, but it would mean having to find someone to run up Ubuntu.

Has anyone had any experience with either of the above?

Cheers"
811,2019-07-04 18:05:02,1562252702.0,dataengineering,Resources to Lead a Data Warehouse / Data Organization Effort,c93m49,onestupidquestion,,https://www.reddit.com/r/dataengineering/comments/c93m49/resources_to_lead_a_data_warehouse_data/,1.0,3.0,0.0,5231.0,
812,2019-07-04 21:38:13,1562265493.0,dataengineering,Data Engineer vs. Senior Data Warehouse Engineer,c962c5,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/c962c5/data_engineer_vs_senior_data_warehouse_engineer/,1.0,7.0,0.0,5234.0,
813,2019-07-05 09:24:54,1562307894.0,dataengineering,Any good documents on spark configurations,c9cm70,Mr_Cuffss,,https://www.reddit.com/r/dataengineering/comments/c9cm70/any_good_documents_on_spark_configurations/,1.0,2.0,0.0,5239.0,
814,2019-07-05 11:26:53,1562315213.0,dataengineering,Looking for suggestion as my next step towards becoming a competent data engineer,c9dixx,suschat,,https://www.reddit.com/r/dataengineering/comments/c9dixx/looking_for_suggestion_as_my_next_step_towards/,11.0,9.0,0.0,5239.0,"Hi All,

I've around 9 years of total experience as an ETL guy. I've mostly worked with a GUI tool called Ab&gt;Initio. I've worked in few development and ETL Application support projects. My goal is to become a good data engineer. My current organization uses Talend ETL(Just the DI part as our application is not a correct fit for big data). I have a working understanding of java code which i picked up while learning Talend.

I've been lurking in many forums to get a feel on what's current trend in market and what aligns with my personal choices in career. I started learning python and love it.I want to make a career in tier 1 company (FAANG for example) as a data engineer.

I'm looking for help on how to realize my dream. I would appreciate if someone can give me a path or mentor me on becoming a good data engineer.

Based on my research,this is the path I've in mind.(Please note that I've just read through the overviews of tech stack to come up with this flow)

First Step :-
Python ( Because you need a language to code on.I have understanding of Java but I prefer python because of the simplicity. I personally found python to be friendlier and fun to learn than Java or Scala)
Resources that i am using  :- Python fundamentals in Pluralsight &amp; Automate the boring stuff with python(Udemy and book).

Second Step :-
Spark..specifically pyspark (Because this is probably the most used framework from what i understand.)
I see flink being mentioned here but would appreciate if you guys chip in with your two cents.

Resources that i've identified :- Udemy : Taming Big Data with Apache Spark and Python - Hands On!

NOTE : Please point me to any additional resources that helped you.

Third Step : Learn Airflow. Honestly, I don't know and yet to research on what would be the best resource to learn Airflow.

In Parallel, I also want to sharpen my SQL skills from Leetcode.

Additional Skill : I've acquired AWS certified cloud practitioner cert and aim to complete the Solution Arch Cert in near future because cloud is future.

Please comment on what your thoughts are. I'm really looking at you guys as a mentor.

THANK YOU."
815,2019-07-05 20:58:00,1562349480.0,dataengineering,[Request][Repost] Asking for assistance in a Big Data project.,c9j13v,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/c9j13v/requestrepost_asking_for_assistance_in_a_big_data/,5.0,5.0,0.0,5245.0,"Dear Data Engineering SubReddit,

&amp;#x200B;

Good day, my name is Aw Ming Yeh and I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

&amp;#x200B;

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post.

&amp;#x200B;

Regard,

Graduating Student, a fellow redditor."
816,2019-07-06 20:29:59,1562434199.0,dataengineering,Study Guide Recommendations for a new DE?,c9w6n0,work_acc_1,,https://www.reddit.com/r/dataengineering/comments/c9w6n0/study_guide_recommendations_for_a_new_de/,1.0,0.0,0.0,5252.0,
817,2019-07-06 21:28:27,1562437707.0,dataengineering,Best free data engineering online resources to study?,c9wv8k,statistical_engineer,,https://www.reddit.com/r/dataengineering/comments/c9wv8k/best_free_data_engineering_online_resources_to/,27.0,3.0,0.0,5255.0,"I'm currently a data scientist with an interest in data engineering.  I was wondering if there are any good online courses that are free for data engineering.  I believe some udemy links were posted here recently, but it costs money."
818,2019-07-07 04:37:07,1562463427.0,dataengineering,Are data engineers synonymous with data custodians?,ca1f9d,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/ca1f9d/are_data_engineers_synonymous_with_data_custodians/,2.0,4.0,0.0,5259.0,
819,2019-07-07 23:33:23,1562531603.0,dataengineering,Any advice for creating a DWH from API response collection?,cablgh,EggShellBuddyPal,,https://www.reddit.com/r/dataengineering/comments/cablgh/any_advice_for_creating_a_dwh_from_api_response/,1.0,0.0,0.0,5266.0,
820,2019-07-08 18:37:04,1562600224.0,dataengineering,[Request][Repost] Asking for assistance in a Big Data project.,camkys,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/camkys/requestrepost_asking_for_assistance_in_a_big_data/,1.0,0.0,0.0,5282.0,"Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle. 

Regard,

Graduating Student, a fellow redditor."
821,2019-07-08 20:57:39,1562608659.0,dataengineering,Travis CI + Redshift via SSH help?,caofh1,work_acc_1,,https://www.reddit.com/r/dataengineering/comments/caofh1/travis_ci_redshift_via_ssh_help/,1.0,2.0,0.0,5284.0,"We have a Redshift cluster with staging data that we'll use to validate our transformation models. We'd like to automate this step so I'm trying to include Travis CI integration. In short, Travis yml contains a script to build an SSH tunnel and then connect to the cluster, then run the transformation validations.

The problem is that the connection to the cluster fails on Travis, but succeeds locally. I assume this has to do with permissions and where the call to the cluster originates. The error is :

    FATAL: no pg_hba.conf entry for host ""xxx.xxx.xxx.xx"", user ""guest"", database ""masterdb"", SSL off

In postgresql there's a file called **pg\_hba.conf** that lets you configure users and the IP addresses from which they can call to the cluster. I don't know where to find this in Redshift.

Does anyone have experience SSHing into a Redshift cluster via Travis CI?"
822,2019-07-09 06:08:36,1562641716.0,dataengineering,How to approach data engineering in a startup?,cav557,dash_365,,https://www.reddit.com/r/dataengineering/comments/cav557/how_to_approach_data_engineering_in_a_startup/,1.0,6.0,0.0,5295.0,
823,2019-07-09 18:36:38,1562686598.0,dataengineering,Awesome free open source software for data pipeline testing: Great Expectations,cb2g76,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/cb2g76/awesome_free_open_source_software_for_data/,15.0,2.0,0.0,5308.0,"Check out this OSS for automated data pipeline testing with a lot of solid documentation and also has an active slack channel. It's still in its early stages but there are some big updates coming over the next few weeks.

[Website](https://greatexpectations.io/?utm_source=reddit&amp;utm_medium=post&amp;utm_name=user-testing&amp;utm_content=ml-v1) (this is blocked in Russia but we are working on solving this check out the github and docs in the meantime)

[Docs](http://docs.greatexpectations.io/en/latest/?utm_source=reddit&amp;utm_medium=post&amp;utm_name=user-testing&amp;utm_content=ml-v1)

[Github](https://github.com/great-expectations/great_expectations)

[Hello world blog](https://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a)

&amp;#x200B;

**Also---** We are looking for user testers to get some feedback. In exchange for feedback on our beta data context, profiling, and data documentation features we want to provide you one on one video conference onboarding assistance to ensure successful integration with your project. Feel free to comment if you have any questions otherwise you can sign up here: [https://greatexpectations.typeform.com/to/mN4UdQ](https://greatexpectations.typeform.com/to/mN4UdQ)"
824,2019-07-09 21:01:37,1562695297.0,dataengineering,"Json to ORC, no pyspark",cb4dvb,software_engineer_1,,https://www.reddit.com/r/dataengineering/comments/cb4dvb/json_to_orc_no_pyspark/,2.0,11.0,0.0,5309.0,"Hi guys!

This is my first post so please let me know if this is the wrong place to post and if there is another forum I should post to.

Question: 

I'm trying to convert JSON files to ORC using python without using pyspark because pyspark doesn't run on AWS Lambda \[""/dev/fd/62 doesn't exist"" error\]

\*There is a github hack involving spinning up EC2, but it's not ideal

Is there any other way (pyarrow uses pyspark and pandas doesn't support writing to ORC so both don't work) to convert JSON to ORC in python?

By the way, I can't use AWS Firehose because it doesn't allow for partitioning the S3 file-folders as necessary."
825,2019-07-10 11:51:21,1562748681.0,dataengineering,Data transformation - should I do this in Excel or is Python/Pandas/Something else a better fit?,cbe9ak,dolmenac,,https://www.reddit.com/r/dataengineering/comments/cbe9ak/data_transformation_should_i_do_this_in_excel_or/,1.0,1.0,0.0,5323.0,
826,2019-07-10 16:24:26,1562765066.0,dataengineering,Dataform scores $2M to build an ‘operating system’ for data warehouses – TechCrunch,cbgo80,dwl285,,https://www.reddit.com/r/dataengineering/comments/cbgo80/dataform_scores_2m_to_build_an_operating_system/,6.0,11.0,0.0,5325.0,
827,2019-07-10 18:13:34,1562771614.0,dataengineering,Put your data to work: Amazon Redshift vs Snowflake,cbhxpk,xmartlabs,,https://www.reddit.com/r/dataengineering/comments/cbhxpk/put_your_data_to_work_amazon_redshift_vs_snowflake/,0.0,0.0,0.0,5330.0,[https://blog.xmartlabs.com/2019/07/04/redshift-snowflake/](https://blog.xmartlabs.com/2019/07/04/redshift-snowflake/)
828,2019-07-10 19:52:36,1562777556.0,dataengineering,Put your data to work: Amazon Redshift vs Snowflake,cbj6sg,luciiamone,,https://www.reddit.com/r/dataengineering/comments/cbj6sg/put_your_data_to_work_amazon_redshift_vs_snowflake/,1.0,0.0,0.0,5331.0,
829,2019-07-10 23:33:01,1562790781.0,dataengineering,There's a series of videos from AWS where people discuss how they leverage different AWS services for their architecture. Seems like marketing for AWS but some good lessons to learn,cbm1lq,ihatemondayz,,https://www.reddit.com/r/dataengineering/comments/cbm1lq/theres_a_series_of_videos_from_aws_where_people/,18.0,0.0,0.0,5332.0,
830,2019-07-11 11:28:27,1562833707.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cbt8ck,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cbt8ck/requestrepost_asking_for_assistance_filling_up_a/,1.0,0.0,0.0,5339.0,
831,2019-07-11 20:48:03,1562867283.0,dataengineering,[Seeking Advice] What are some good database architectures/platforms for organizing medium-large scientific datasets?,cbyr1w,gigamosh57,,https://www.reddit.com/r/dataengineering/comments/cbyr1w/seeking_advice_what_are_some_good_database/,3.0,4.0,0.0,5346.0,
832,2019-07-11 23:51:55,1562878315.0,dataengineering,Anyone going to use the dotnet spark driver?,cc15fm,ed_elliott_,,https://www.reddit.com/r/dataengineering/comments/cc15fm/anyone_going_to_use_the_dotnet_spark_driver/,3.0,2.0,0.0,5346.0,"I'm quite excited by this:

&amp;#x200B;

 [https://github.com/dotnet/spark](https://github.com/dotnet/spark) 

&amp;#x200B;

Mainly because I love spark and struggle with scala (i'm ok at it) and prefer c# to python.

&amp;#x200B;

Is anyone else planning on using it? (I can imagine no ha ha)"
833,2019-07-12 01:10:09,1562883009.0,dataengineering,Should we add a wiki to r/dataengineering?,cc24dk,schrute_dataeng,,https://www.reddit.com/r/dataengineering/comments/cc24dk/should_we_add_a_wiki_to_rdataengineering/,1.0,5.0,0.0,5346.0,
834,2019-07-12 09:28:51,1562912931.0,dataengineering,"Apache Airflow Cluster, Tasks and Node Affinity",cc77e5,mikomono,,https://www.reddit.com/r/dataengineering/comments/cc77e5/apache_airflow_cluster_tasks_and_node_affinity/,1.0,0.0,0.0,5352.0,
835,2019-07-12 12:31:55,1562923915.0,dataengineering,Вакансия,cc8nld,SINGAPORE_PROJECT,,https://www.reddit.com/r/dataengineering/comments/cc8nld/вакансия/,0.0,2.0,0.0,5354.0,"**Data Engineer**

В нашу команду требуется опытный Data Engineer, которому интересно консолидировать информацию из различных информационных систем. Наш сотрудник будет принимать активное участие в создании и дальнейшем развитии нового решения.

**Требования:**

\- аналогичный опыт от 1-го года, по разработке ETL – процессов  (извлечения, подготовки, загрузки данных)

\- высшее техническое/математическое образование

**Профессиональные навыки:**

\- Уверенное владение Python (в т.ч. библиотеки Pandas, Numpy, Scipy, Scikit-Learn, Seaborn), Scala или Java

\- уверенные знания SQL

\- знание и практический опыт работы с алгоритмами машинного обучения (machine learning), построения математических моделей (нейронные сети, логистические регрессии, кластеризация, регрессионный, факторный, дисперсионный и корреляционный анализы)

\- знание инструментов, библиотек визуализации данных

\- опыт разработки проектов с анализом текстов или изображений с нуля

\- технический английский

**Обязанности:**

\- проектирование, разработка и поддержка инфраструктуры для хранения и обработки больших данных

\- обеспечение полноты и доступности данных для решения задач в области статистического анализа

\- проектирование и настройка систем отчетности для разовых и периодических выгрузок данных

\- участие в задачах анализа систем источников, составление моделей данных

\- работа в команде по гибким методологиям ведения проекта

**Условия:**

\- оформление согласно ТК

\- график работы с 09.00 до 18.00 (обсуждается)

\- уровень оплаты  обсуждается на интервью

\- молодой и дружный коллектив

\- офис в шаговой доступности от метро Красносельская, Бауманская

\- здание в стиле Loft, офис Open Space

\- кухня с необходимой техникой, чаем, кофе, конфетами

**О нас:**

Молодой международный исследовательский проект по экономике Юго-Восточной Азии"
836,2019-07-13 03:26:06,1562977566.0,dataengineering,Looking for advice on how to get educated and break into data engineering,cciw47,rolkien29,,https://www.reddit.com/r/dataengineering/comments/cciw47/looking_for_advice_on_how_to_get_educated_and/,15.0,13.0,0.0,5367.0,"Hi, I have been in data analytics for several years, but I would really like to break into data engineering, and eventually shoot for a data architect role. I have an MBA, I know SQL very well, some Python and various other BI and ETL tools like SSIS and Tableau. I have been applying for data engineering jobs, but it seems clear I just don't have the skills they are looking for. As far as education goes, which path do you recommend: going all in and getting an online masters in CS, or taking online courses like dataquest and Udacity and building up a Github portfolio? I am currently taking the dataquest data engineering course."
837,2019-07-13 18:14:33,1563030873.0,dataengineering,Any interest in building out a side-data product/project in a team?,ccqf5h,willmachineloveus,,https://www.reddit.com/r/dataengineering/comments/ccqf5h/any_interest_in_building_out_a_sidedata/,8.0,14.0,0.0,5370.0,"I see a lot of ""how do I break into data science/engineering"" posts on here and am wondering if there'd be interest in building out some kind of project together in a small-ish team. I don't really have ideas for projects at the moment but I'm sure a team could come up with some. Might be better than working through those online courses, no?"
838,2019-07-13 21:08:35,1563041315.0,dataengineering,The Twelve-Factor App,ccseb0,schrute_dataeng,,https://www.reddit.com/r/dataengineering/comments/ccseb0/the_twelvefactor_app/,1.0,0.0,0.0,5373.0,
839,2019-07-13 21:10:16,1563041416.0,dataengineering,How to keep my 'airflow scheduler' running?,ccsf1m,LowerLaugh,,https://www.reddit.com/r/dataengineering/comments/ccsf1m/how_to_keep_my_airflow_scheduler_running/,0.0,5.0,0.0,5373.0,"After I disconnect from ssh session, my airflow scheduler status changed to S on ps aux and actually didn't run my DOG. I have run it by daemon process."
840,2019-07-13 21:27:27,1563042447.0,dataengineering,The Twelve-Factor App,ccsmg1,schrute_dataeng,,https://www.reddit.com/r/dataengineering/comments/ccsmg1/the_twelvefactor_app/,1.0,5.0,0.0,5373.0,"12 principles which provide guidance to build application that can scale easily : [https://12factor.net/](https://12factor.net/) .

I  guess many of you like me follow this principles implicitly, It is nice to put words on it.  


Sum up :

**1. Codebase:** Multiple apps sharing the same code is a violation of twelve-factor =&gt; share code via library

**2. Dependencies :** A twelve-factor app never relies on implicit existence of system-wide packages =&gt; use pip/virtualenv in python for instance

**3. Config:** Store config as constants in the code is violation of twelve-factor, which requires strict separation of config from code =&gt; stores config in environment variables

**4. Backing services:** A deploy of the twelve-factor app should be able to swap out a local MySQL database with one managed by a third party without any changes to the app’s code.

**5. Build, release, run:** The twelve-factor app uses strict separation between the build, release, and run stages

**6. Processes:** Twelve-factor processes are stateless and share-nothing

**7. Port binding:** The twelve-factor app is completely self-contained

**8. Concurrency:** Twelve-factor app processes should never daemonize or write PID files, instead  rely on the operating system’s process manager (such as systemd)

**9. Disposability:** processes can be started or stopped at a moment’s notice, processes shut down gracefully when they receive a SIGTERM signal from the process manager...

**10. Dev/Prod parity:** The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small

**11. Logs:** A twelve-factor app never concerns itself with routing or storage of its output stream

**12. Admin processes:** Twelve-factor strongly favors languages which provide a REPL shell out of the box"
841,2019-07-14 01:11:37,1563055897.0,dataengineering,Junior Data Engineering Position Interview Tips for an APM?,ccv7ur,DEhopeful7,,https://www.reddit.com/r/dataengineering/comments/ccv7ur/junior_data_engineering_position_interview_tips/,1.0,2.0,0.0,5374.0,"Hey everyone, 

I'm trying to get into a more technical role, and so I found out a couple of days ago that I have a technical call with a hiring manager Monday morning for a junior DE position that I applied to on a whim. My role right now is that of an associate product manager. So along with all my other duties, I assist in programming our data pipeline, but not I'm not super involved in the meat of the ETL stages. Since I work with Spark SQL I'm familiar with SQL queries, and I use pandas quite a bit as well for data explorations.

Since I only have a couple days to prepare, I've been cramming leetcode and doing their database and algorithm questions. Any other advice on how to prepare? My background is not in CS and in my current job, my coding doesn't get very deep. I'm kind of worried because of this, as I'm not great at being given a problem and coding up a solution, analyzing time/space complexity, etc. 

Any help is greatly appreciated!"
842,2019-07-14 03:41:44,1563064904.0,dataengineering,Snowflake Cloud Data Warehouse,ccwshk,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/ccwshk/snowflake_cloud_data_warehouse/,11.0,6.0,0.0,5374.0,
843,2019-07-14 05:25:03,1563071103.0,dataengineering,Query Nested Json/Parquet Structure?,ccxs3c,dominodave,,https://www.reddit.com/r/dataengineering/comments/ccxs3c/query_nested_jsonparquet_structure/,1.0,0.0,0.0,5374.0,
844,2019-07-14 15:21:16,1563106876.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cd2elg,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cd2elg/requestrepost_asking_for_assistance_filling_up_a/,1.0,0.0,0.0,5378.0," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor."
845,2019-07-15 02:40:00,1563147600.0,dataengineering,What might be a good Master's project involving data engineering and big data to attract employers?,cd9zk5,dash_365,,https://www.reddit.com/r/dataengineering/comments/cd9zk5/what_might_be_a_good_masters_project_involving/,1.0,0.0,0.0,5385.0,"Hey. A little background about me. I am a international student currently pursuing my Master's degree in Information Technology in the U.S. I have no professional experience apart from a 3 month summer internship as a data engineer where I dealt with building a dashboard. I have completed my AWS certified Big Data Specialty exam and AWS certified cloud practitioner certifications. I am creating my personal portfolio with ETL jobs involving AWS environment. In 6 months I am to start my project work to complete my Master's degree. What might be a good project that is complicated enough and use case specific that might help explaining my interest and experience in data engineering?

&amp;#x200B;

Since I don't have any professional experience, I'm worried as to how I can approach my job search. Kindly share your ideas. I don't mind spending time to research about complicated architectures and technologies to incorporate into my project. Any professional data engineers out there, kindly provide your suggestions as to what might help impress data engineering employers?"
846,2019-07-15 12:03:13,1563181393.0,dataengineering,https://medium.com/@schrockn/introducing-dagster-dbd28442b2b7,cdf4fb,muftard,,https://www.reddit.com/r/dataengineering/comments/cdf4fb/httpsmediumcomschrocknintroducingdagsterdbd28442b2/,1.0,0.0,0.0,5394.0,
847,2019-07-15 12:17:57,1563182277.0,dataengineering,Introducing Dagster - Nick Schrock - Medium,cdf8gd,muftard,,https://www.reddit.com/r/dataengineering/comments/cdf8gd/introducing_dagster_nick_schrock_medium/,0.0,16.0,0.0,5394.0,
848,2019-07-15 13:53:27,1563188007.0,dataengineering,How to get failure notification by email on Airflow? I already used smtp gmail but not work at all.,cdfzda,LowerLaugh,,https://www.reddit.com/r/dataengineering/comments/cdfzda/how_to_get_failure_notification_by_email_on/,0.0,1.0,0.0,5395.0,
849,2019-07-15 17:45:05,1563201905.0,dataengineering,How to write unit tests for your SQL queries,cdibx1,MageGen,,https://www.reddit.com/r/dataengineering/comments/cdibx1/how_to_write_unit_tests_for_your_sql_queries/,25.0,2.0,0.0,5398.0,
850,2019-07-15 18:13:26,1563203606.0,dataengineering,Using pandas to copy dataframes to Postgres,cdioaj,rolkien29,,https://www.reddit.com/r/dataengineering/comments/cdioaj/using_pandas_to_copy_dataframes_to_postgres/,1.0,0.0,0.0,5400.0,"Hi, I am attempting to use a pandas dataframes to create a Postgres table, at first this code ran, but it didn't look like it actually created the table in the DB, any thoughts, here is the code: 

&amp;#x200B;

import psycopg2 as psy

import pandas as pd

from sqlalchemy import create\_engine

&amp;#x200B;

con = psy.connect('dbname = 'dbname' user = 'user'')

cur = con.cursor()

&amp;#x200B;

engine = create\_engine('postgresql+psycopg2://'user':'pw'@'myhost'/'mydb'')

&amp;#x200B;

df.to\_sql('df', engine, if\_exists='replace')

con.commit()

con.close()"
851,2019-07-15 18:52:57,1563205977.0,dataengineering,Free hosted on demand Spark clusters,cdj5ti,vinceliu21,,https://www.reddit.com/r/dataengineering/comments/cdj5ti/free_hosted_on_demand_spark_clusters/,3.0,0.0,0.0,5400.0,"Hello r/dataengineering!

&amp;#x200B;

We've been working on a service to provide deployed and managed Spark clusters to help data engineers, ML peeps, analysts deploy their Spark code easier than ever (as simple as one command in the terminal). [https://www.aidalabs.io/](https://www.aidalabs.io/)  There's a free cluster tier option available as well to test deploy your code to the cloud! I would love any feedback! PM me if you have any questions. We currently support Pyspark."
852,2019-07-16 02:26:50,1563233210.0,dataengineering,Industrializing batch ML algorithm using Apache Beam/Dataflow (on Google Cloud Platform),cdp5i3,Massnsen,,https://www.reddit.com/r/dataengineering/comments/cdp5i3/industrializing_batch_ml_algorithm_using_apache/,2.0,2.0,0.0,5405.0,"Hello 

We plan to industrialize some batch ML Alogrithms using Apache Beam and Dataflow as a runner. 

The pipeline job would be something like 

* Read from GCS a .json file
* Compute the output of the algorithm on a JSON element
* Write the JSON elements into a file in GCS

The most interesting part is the second one. In order to be the most flexible, we agreed on a contract with the Data Science team. The algorithm should be serialized as a pickle which will have a predict method, see the code below 

    def predict(X):
    """"""
    :param X: a list of JSON objects representing data points. 
              Example:
              [{""DAY"": ""D1"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak""},
               {""DAY"": ""D2"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Strong"": ""Weak""},
                ....
               {""DAY"": ""Dn"", ""Outlook"": ""Overcast"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak""}]
    :type X: list of JSON objects 
    
    :return: a JSON list with the output of ML algorithm. 
             Example (classification play tennis game)
             [{""DAY"": ""D1"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak"",
               ""Go to court"": False},
              {""DAY"": ""D2"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Strong"": ""Weak"", 
               ""Go to court"": True },
                ....
              {""DAY"": ""Dn"", ""Outlook"": ""Overcast"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak"",
               ""Go to court""; False}]
    """"""

However Apache Beam is currently not supporting [Python 3](https://beam.apache.org/roadmap/#python-3-support) (Python 2 will be deprecated in 2020). In addition to that, the Java SDK is the most complete Beam SDK. 

A major thing to consider is the dependencies of the algorithm (some classification ML  uses pandas version X another regression ML depends on version Y of numpy ...etc)

Knowing that Dataflow handles dependencies  differently across SDKs:

* In Python, a requirment.txt must be specified [among other things to do](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/)
* In Java, it suffices to build a jar containing all the .pom dependencies  a.k.a [uber jar](https://github.com/lukecwik/incubator-beam/tree/dataflow_uber_jar)

Here are some solutions :

1. Use the Java SDK, because it is the most complete and because we don't want to use python 2 anymore, and instanciate the pickle\[1\] in Java and manage to pass a batch of JSON elements in the DoFn\[2\] to compute their score and get it back\[3\]
2. At worker initialization by Dataflow, a Docker image will be downloaded containing the ML algorithm and all its dependencies\[4\]. Using the Java SDK a batch of JSON elements in the DoFn will be passed to that container\[5\] and the output will be gathered back 

Are those viable solutions to tackle this use case ? Mabye they are too complex or even not feasible ? Maybe Dataflow/Apache Beam is not the way to go ?

We only have one constraint form the Data Science team: being able to use Python 3 and all the DS ecosystem that revolves around it (pandas, pytorch, scikit learn, numpy ...etc.) Maybe the pickle format is not the most suitable one to use ML alogrithm ? (eventhough we will also have to deal with Tenserflow algorithms, but I think it's a different subject that maybe needs a different pipeline)

&amp;#x200B;

Any help/hint will be much appreciated,

Many thanks  

&amp;#x200B;

This is me talking to myself: 

\[1\] no clue how to do that 

\[2\] the communication between Java and python maybe too expensive 

\[3\] how do we deal with the algorithm's dependencies that are mainly in python and inexpressible in Java (maven)

\[4\] is it possible ? how ? 

\[5\] may also be expensive"
853,2019-07-16 05:01:26,1563242486.0,dataengineering,An interview about the Cloud Factory platform for data labeling and social good in developing nations,cdqtiq,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cdqtiq/an_interview_about_the_cloud_factory_platform_for/,4.0,0.0,0.0,5407.0,
854,2019-07-16 05:01:32,1563242492.0,dataengineering,"An interview about Clickhouse, an open source, columnar data warehouse built for massive scale and speed to enable interactive analytics",cdqtk7,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cdqtk7/an_interview_about_clickhouse_an_open_source/,0.0,1.0,0.0,5407.0,
855,2019-07-16 16:47:19,1563284839.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cdx9ex,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cdx9ex/requestrepost_asking_for_assistance_filling_up_a/,1.0,0.0,0.0,5416.0,
856,2019-07-17 18:38:36,1563377916.0,dataengineering,Dell Boomi vs SSIS?,ceeho8,Gawgba,,https://www.reddit.com/r/dataengineering/comments/ceeho8/dell_boomi_vs_ssis/,2.0,5.0,0.0,5428.0,"Apologies if this isn't the correct forum.  I realize these tools have some fundamental differences, but in a simple use case of moving data from 100 Oracle tables into 100 SalesForce objects can anyone comment on any obvious pros and cons?  We already have SSIS/CozyRoc running on-prem and moving a handful of tables over, but management is interested in Boomi (long story) so I'm curious about other peoples' experiences."
857,2019-07-17 19:51:58,1563382318.0,dataengineering,Mono Repo or Multi Repo,cefh2e,serkef-,,https://www.reddit.com/r/dataengineering/comments/cefh2e/mono_repo_or_multi_repo/,1.0,0.0,0.0,5430.0,
858,2019-07-17 22:50:25,1563393025.0,dataengineering,Entry level dataset for becoming a data engineer?,cehtoy,ruoyucad,,https://www.reddit.com/r/dataengineering/comments/cehtoy/entry_level_dataset_for_becoming_a_data_engineer/,15.0,13.0,0.0,5431.0,"I want to study all the necessary materials to become a Data Engineer to expand my career options but I am not sure where to start.

Currently, I am working as a reporting analyst for a big corporation. My daily job involved data manipulation, data cleaning and data visualization with Tableau. I am comfortable with Python( I know how to write functions, loops, have a basic understanding of OOP, call third-party packages like Pandas, etc). But my knowledge base is not sufficient enough.

So what books should I read first or just start building Data engineering projects? I am aware that data engineers work involve Hadoop or Kafka which are used to handle huge amount of data(Which I don't have on my PC nor can my hard drive store it). So what project(large enough so that there is even a point to consider HDFS) should I try at a beginner level"
859,2019-07-18 20:17:37,1563470257.0,dataengineering,What do you look for in a GOOD Data Science &amp; Engineering role?,cevcsd,RexRecruiting,,https://www.reddit.com/r/dataengineering/comments/cevcsd/what_do_you_look_for_in_a_good_data_science/,5.0,3.0,0.0,5440.0,
860,2019-07-19 01:16:18,1563488178.0,dataengineering,awesome-apache-airflow,cez4b0,schrute_dataeng,,https://www.reddit.com/r/dataengineering/comments/cez4b0/awesomeapacheairflow/,18.0,12.0,0.0,5445.0,
861,2019-07-19 09:34:37,1563518077.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cf43uy,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cf43uy/requestrepost_asking_for_assistance_filling_up_a/,2.0,0.0,0.0,5448.0," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor."
862,2019-07-19 12:43:58,1563529438.0,dataengineering,How to insert by overwrite to kudu?,cf5jh2,LowerLaugh,,https://www.reddit.com/r/dataengineering/comments/cf5jh2/how_to_insert_by_overwrite_to_kudu/,1.0,0.0,0.0,5450.0,
863,2019-07-20 09:07:10,1563602830.0,dataengineering,Does Title Matter?,cfiklz,Archbishop_Mo,,https://www.reddit.com/r/dataengineering/comments/cfiklz/does_title_matter/,1.0,0.0,0.0,5455.0,
864,2019-07-20 20:15:56,1563642956.0,dataengineering,Path to tech savvy?,cfo8et,dumbledore__,,https://www.reddit.com/r/dataengineering/comments/cfo8et/path_to_tech_savvy/,1.0,0.0,0.0,5460.0,"Hello,

&amp;#x200B;

I'm looking for a roadmap to build a foundation of tech skills.  

&amp;#x200B;

I recently started a DE job with no background in computers/languages.  After 4 months I feel solidly proficient in SQL, and am looking to start acquiring the next pieces of a skill set.  I'm taking classes on Azure (the comany's cloud service), but Python/Apache/Bash/Batch/Hadoop all seem like worthwhile skills--all of which I'm set on learning in the future.  

&amp;#x200B;

However, it's a (somewhat) overwhelming amount of content, and there's definitely years of effort ahead.  Can any previous self-starters propose a logical itinerary of where to start?  And maybe within these modules, what sub-skills to start with?

&amp;#x200B;

Thanks!"
865,2019-07-21 16:27:23,1563715643.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cfyz7q,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cfyz7q/requestrepost_asking_for_assistance_filling_up_a/,0.0,0.0,0.0,5479.0," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor."
866,2019-07-21 19:06:16,1563725176.0,dataengineering,Optimizing Spark Job (spark-submit/shell),cg0ly5,mjfnd,,https://www.reddit.com/r/dataengineering/comments/cg0ly5/optimizing_spark_job_sparksubmitshell/,1.0,0.0,0.0,5483.0,
867,2019-07-21 19:30:24,1563726624.0,dataengineering,Optimizing Spark Job (spark-submit/shell),cg0vxl,mjfnd,,https://www.reddit.com/r/dataengineering/comments/cg0vxl/optimizing_spark_job_sparksubmitshell/,1.0,0.0,0.0,5484.0,
868,2019-07-21 19:42:57,1563727377.0,dataengineering,Just got my first DE job. What should I study?,cg110u,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/cg110u/just_got_my_first_de_job_what_should_i_study/,1.0,0.0,0.0,5484.0,
869,2019-07-22 15:02:55,1563796975.0,dataengineering,Band Protocol — A Protocol for Decentralized Data Governance,cgcbv5,Rabidmono,,https://www.reddit.com/r/dataengineering/comments/cgcbv5/band_protocol_a_protocol_for_decentralized_data/,1.0,0.0,0.0,5493.0,
870,2019-07-22 21:36:40,1563820600.0,dataengineering,Quickbooks Desktop,cgh3cb,tpedar50,,https://www.reddit.com/r/dataengineering/comments/cgh3cb/quickbooks_desktop/,1.0,0.0,0.0,5496.0,
871,2019-07-22 22:21:42,1563823302.0,dataengineering,Recommended learning resources for scaling up a data project?,cgho7r,Low_end_the0ry,,https://www.reddit.com/r/dataengineering/comments/cgho7r/recommended_learning_resources_for_scaling_up_a/,1.0,0.0,0.0,5497.0,
872,2019-07-23 11:23:44,1563870224.0,dataengineering,Investigating the dataset,cgpvgw,changeeverymoment,,https://www.reddit.com/r/dataengineering/comments/cgpvgw/investigating_the_dataset/,1.0,1.0,0.0,5502.0,"Hi! Currently working as an intern and my task is to find the reason why the project performs poorly. There are several standards that need to be reached but, for some reason, most of them always remain unmet, and even if they reach standards, very rarely more than 2 standards out of 5 are met.

Currently I created several spreadsheets that contain information for separate locations (cities), I look at which users have success or not, etc. 

I plan to start evaluating the standard that is met the most and see why it can't meet the goal in the other cases, I hope I will be able to find 1 or 2 parameters that would affect the whole outcome for the corresponding standard.  


So, the thing is, my DS background is quite weak, and I just started the internship, but I am willing to learn more because I really enjoy it. 

&amp;#x200B;

What would be your suggestions to do? I am open to all suggestions and tips!"
873,2019-07-23 15:08:54,1563883734.0,dataengineering,Beta launch of Band Protocol and CoinHatcher,cgrs6q,iriyaa,,https://www.reddit.com/r/dataengineering/comments/cgrs6q/beta_launch_of_band_protocol_and_coinhatcher/,16.0,0.0,0.0,5504.0,
874,2019-07-23 16:56:06,1563890166.0,dataengineering,An interview about how the data mesh architectural and organizational pattern can lead to a more maintainable data platform,cgsx1r,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cgsx1r/an_interview_about_how_the_data_mesh/,5.0,0.0,0.0,5507.0,
875,2019-07-23 17:25:24,1563891924.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cgt9kj,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cgt9kj/requestrepost_asking_for_assistance_filling_up_a/,0.0,0.0,0.0,5507.0," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor."
876,2019-07-23 21:25:46,1563906346.0,dataengineering,What python framework / design-pattern should I use for a small job of (1) copying a file from an external drive to a laptop in minimum I/O time AND (2) simultaneously reading it into pytorch/whatever and running FFTs on rows and saving the result.,cgwe9c,THAT_LMAO_GUY,,https://www.reddit.com/r/dataengineering/comments/cgwe9c/what_python_framework_designpattern_should_i_use/,2.0,1.0,0.0,5510.0,"The file is 400GB. FFTs need to be run on all the data. I want to do FFTs on CUDA and pytorch is most convenient way for me to do that. Rate limiting step is I/O of the harddrive/device. It will take 10mins to copy the file from the harddrive/device to laptop. I have a requirement that the processing also needs to have been mostly done in that time (non negotiable...). There is also extra processing to be done after the FFTs in certain edge cases that is O( n^3 )). No internet access possible and only one laptop available for processing.

I will outline a few ideas, but please advise if in your experience there is a better design pattern or framework!

1. Just copy the file from device to laptop (either with shutil or the user literally drags and drops it themself using Windows). And have a python process always running that will read that raw binary file as it is written and process it in chunks. This is possible as the IO speed of the harddrive is far less than the IO speed of the 500GB (or 1TB) SSD the laptop will have.

2. Use faust to read the file from harddrive and write it to SSD, while sending it to torch and graphics card for processing. https://faust.readthedocs.io/en/latest/

3. Use asyncio?"
877,2019-07-24 00:10:47,1563916247.0,dataengineering,Is data engineering under IT?,cgyjqh,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/cgyjqh/is_data_engineering_under_it/,3.0,9.0,0.0,5510.0,"I work for a large corporation who has been using Excel dashboards for reporting for the last couple of decades. I originally was from operations (client facing, managing the floor) but got into data related stuff and automation about 5 years ago. 

Currently, I have a 3 node Docker Swarm cluster on Azure. Most services run as containers except for the database which is installed directly. I am under the team which owns the excel based reporting and am in charge of automating the work and feeding our visualisation tool with data. This is NOT under IT. 

1) airflow for scheduling along with a lot of custom hooks and operators to move data around (from REST APIs, files, websites, and various databases).  

2) PostgreSQL for the data warehouse. Sqitch as our migration tool, and dbt (data build tool) as to create our views and tables. 

3) Prometheus stack for monitoring.

4) Gitlab runners for deployment. We are using gitlab for a lot of the features (container registry, issue tracking, version control). 

5) Jupyterlab for data exploration, traefik, portainer etc. 

Persistent storage is a weak area which I need to improve (might have a separate post about this soon). Right now it is dependent on individual folders on specific nodes. 


We execute thousands of tasks per day and run several projects simultaneously. Our project manager keeps adding more and more work, clients are reaching out directly, etc - unplanned work is turning this into a 24/7 job. 

-	We have to deal with changes very often because we receive data from clients who are free to change anything at any point without warning. 
-	There is nothing to factor in the work I have to do with managing the infrastructure (stack files, testing new tools, etc). Actual crashes or other issues. 
-	I have no one internally to go to for any advice on anything technical or data related. Our team only acknowledges us getting data into the visualisation tool on time. There is zero interest in how it is done. 


Should this role typically be under the IT wing? I am considering doing a demo of our setup to someone from the IT org. I feel like it goes way beyond “loading Excel files into a visualisation tool”."
878,2019-07-24 01:17:04,1563920224.0,dataengineering,I created a tutorial to submit PySpark applications to an AWS cluster from your local machine. Check it out!,cgzdv7,dattablox_brent,,https://www.reddit.com/r/dataengineering/comments/cgzdv7/i_created_a_tutorial_to_submit_pyspark/,17.0,0.0,0.0,5512.0,
879,2019-07-24 01:54:32,1563922472.0,dataengineering,I have an interview coming up for a DWE role I'm really not qualified for. Help?,cgzty5,dantzigismyhero,,https://www.reddit.com/r/dataengineering/comments/cgzty5/i_have_an_interview_coming_up_for_a_dwe_role_im/,1.0,4.0,0.0,5515.0,"No judgment, please. Just need the best way to prepare.

As the title said, I have a second-round interview for a Data Warehouse Engineer role, which happens in less than 48 hours. I'm really a newbie in DE, with my recent focus more in DS/ML/analytics and past experience in just working closely with DBAs/data analysts.

When it comes to DW concepts, like snow flake, data marts, dimension types, etc., a lot of that stuff is really foreign to me. And I've never really worked in a DW production/enterprise environment, so my experience with tools like Oracle, PL/SQL, etc. is super limited. My qualifications are basically: 

* Good proficiency with SQL (Postgres, some stuff in SQL Developer with coworkers)
* Using ODBC in MS Access (please don't laugh) as kind of the client-side interface for our core business functions
* ETL batch scripts that I've written in Python for personal projects

I'm really not sure how I made it past the recruiter screen, but my approach has just been to just emphasize the experience I do have. But at some point, if I'm asked about deeper DW concepts, I'm pretty screwed. 

Is there any hope for me?"
880,2019-07-25 01:32:41,1564007561.0,dataengineering,What does your data platform look like?,chf4yi,feedthemartian,,https://www.reddit.com/r/dataengineering/comments/chf4yi/what_does_your_data_platform_look_like/,1.0,0.0,0.0,5528.0,
881,2019-07-25 06:21:58,1564024918.0,dataengineering,"""Engineering"" my mobile app data - data are stored on a text-file",chiatd,runnersgo,,https://www.reddit.com/r/dataengineering/comments/chiatd/engineering_my_mobile_app_data_data_are_stored_on/,4.0,12.0,0.0,5531.0,"I built an Android analytics app to track my day-to-day stock trading; it's basically the usual time-series graph where X is date and Y is $. This is just a simple descriptive analytics and nothing more really.

The financial data (or ""a portfolio"") came from 2 remote systems, and 5 more additional financial portfolios are also extracted each day. As of today, all 6 financial portfolios are stored on 6 separate text-files, where each text-file has 2 columns (date and value) separated by a space, and they are all located on the phone's main memory.

&amp;#x200B;

Would the kind DE folks offer me some advice on how to better engineer these data? 

Would there be a process I could/ should follow? Should I migrate my data to SQLite or something? 

Thanks DE folks!"
882,2019-07-25 22:23:53,1564082633.0,dataengineering,Data Engineer at McDonald's,chs9aj,jl5892389621,,https://www.reddit.com/r/dataengineering/comments/chs9aj/data_engineer_at_mcdonalds/,10.0,8.0,0.0,5538.0,Anyone worked as or know anyone that is a data engineer at McDonald's global headquarters in Chicago? What is the experience like?
883,2019-07-26 03:39:57,1564101597.0,dataengineering,"Does anyone have a good guide or article with step by step code to build a basic oltp db and write an etl script to transform it into a olap , star schema ? Newbie here and want to put together a simple project in SQLite if I can to get my head wrapped around oltp to olap etl’s",chvysc,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/chvysc/does_anyone_have_a_good_guide_or_article_with/,1.0,2.0,0.0,5538.0,
884,2019-07-26 14:25:50,1564140350.0,dataengineering,"General wisdom and learnings, not necessarily specific for Google Cloud: _Democratizing data analysis with Google BigQuery_",ci1nln,t15k,,https://www.reddit.com/r/dataengineering/comments/ci1nln/general_wisdom_and_learnings_not_necessarily/,1.0,0.0,0.0,5543.0,
885,2019-07-26 19:53:00,1564159980.0,dataengineering,ANNOUNCING: Snowflake + Etleap,ci5cuk,AshleyEtleap,,https://www.reddit.com/r/dataengineering/comments/ci5cuk/announcing_snowflake_etleap/,1.0,0.0,0.0,5547.0,
886,2019-07-26 21:47:49,1564166869.0,dataengineering,(X-Post) Need Help Picking an ETL Tool,ci6sq7,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/ci6sq7/xpost_need_help_picking_an_etl_tool/,1.0,9.0,0.0,5548.0,"Cross Post: [https://www.reddit.com/r/ETL/comments/ci2swa/need\_help\_picking\_a\_tool/](https://www.reddit.com/r/ETL/comments/ci2swa/need_help_picking_a_tool/)

&amp;#x200B;

 Im starting a new job as a solo data engineer.

The  company uses a postgres database, and the ingest data from various file  formats and APIs. They also use Azure to host everything. They are big  on open source software since they are a non-profit.

I know they also wanted to use python, or at least its in the JD.

So  what are my options as far as tooling? I am relatively new to this and  will be working solo. So if Im not asking for too much, id like  something easy to learn and maintain.

Any suggestions?"
887,2019-07-26 22:11:40,1564168300.0,dataengineering,Interesting problem: Analyze differences and drift between two DBs that are being simultaneously maintained.,ci738w,work_acc_1,,https://www.reddit.com/r/dataengineering/comments/ci738w/interesting_problem_analyze_differences_and_drift/,6.0,2.0,0.0,5548.0,"Say we have 2 DBs, like postgres and bigquery that should be kept in parity. Obviously there are major differences in syntax and optimization mechanics and overall feed process design, but in the end the two DBs ought to have the same data set.

How would you go about analyzing difference and any drift between the two. I suppose ""final"" views could be created in each that take compile some aggregate numbers and then they could be compared. Seems inelegant. 

Does anyone have even conceptual advice? References for similar problems?"
888,2019-07-27 00:25:29,1564176329.0,dataengineering,What is the future of Data Engineering?,ci8qti,0xkn,,https://www.reddit.com/r/dataengineering/comments/ci8qti/what_is_the_future_of_data_engineering/,19.0,6.0,0.0,5549.0,"I am currently very fortunate to work at a FAANG as mid level DE. I’ve been exposed to a plethora of technologies and concepts in this space and really love what I do. However, I’ve been deliberating on what the future holds for this role in the industry. Though I love building resilient data warehouses and optimizing compute/etl in pipelines, I can’t see myself doing this forever.

Personally, I love to learn and build solutions to solve challenging problems agnostic of whether they are front end or backend realted. But I view Data Engineering as a small (but integral) piece of the puzzle.

In your opinions, what do you see data engineering becoming in industry over the next 10 years?  What other engineering domains will work more closely with data engineers over the years? What domains have skillsets that are ”transferable”?

I have a background in analytics,  but like to do engineering more than analytics. Data Science isn't something i’d consider."
889,2019-07-27 04:51:04,1564192264.0,dataengineering,Dealing with frequently changing data from many unrelated sources?,ciboky,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/ciboky/dealing_with_frequently_changing_data_from_many/,2.0,7.0,0.0,5549.0,"I manage a data warehouse in the call center industry. What this means is that we have internal data which is somewhat stable, and then we have client data for hundreds of clients:

-	excel or csv files summarising performance - these could be complete dashboard reports in xlsb format with 40 sheets, merged cells, headers etc. You can imaging how crazy this data can look.  
-	as a side note - many of these files are actually generated from common tools but they are reports, not raw data (Verint, Genesys, Avaya, Cisco, etc. ). They can be shared on sftp sites, sent as email attachments, or POCs load them onto an sftp server for us 
-	REST APIs (Salesforce, servicenow, zendesk, playvox, logmein, appfollow, etc)

I currently load this data into PostgreSQL tables. Each client is a different schema and there is zero interaction between them. We explore the data and convert everything to csv format and then use COPY to bulk load the data. 

We use sqitch for database migration. But realistically it is a pain to deal with changes to client raw data. New columns can be added or removed without warning. We don’t know until it happens, and then we have to alter the tables. I could automate adding new columns, but it kind of goes against the idea of using a migration tool and sometimes a new field is literally just a bad file with shifted columns or some other error. 

For REST APIs, we load the results into a single jsonb column and then create views or flat tables by querying that table ([dbt] data build tool is used for this). This works well - we have all of the fields no matter what in the raw data, and at worst we just need to alter the view if people need it as a real column. 

Should I use the same approach for client files? Just load everything as a jsonb field and flatten it from there instead of trying to figure out the data types per column to create the tables and deal with frequent changes? We do use the database for unique constraints and indexing columns, but technically the visualisation tool could be used to deduplicate data. 

Or would a tool like Avro make any sense here? I read that there are some features to resolve changing schemas. Ultimately the only thing I am doing is loading this data to a visualisation tool through a REST API - so I dump the data to disk from Postgres and then stream the csv file to our visualisation tool. I typically do not need all columns. I assume I could read data from avro, convert to csv as required by the API, and stream it as well?"
890,2019-07-27 13:30:52,1564223452.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cifz3s,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cifz3s/requestrepost_asking_for_assistance_filling_up_a/,1.0,0.0,0.0,5553.0," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor."
891,2019-07-27 16:44:36,1564235076.0,dataengineering,Optimizing Spark I/O,ciho3u,mjfnd,,https://www.reddit.com/r/dataengineering/comments/ciho3u/optimizing_spark_io/,1.0,0.0,0.0,5553.0,
892,2019-07-27 18:44:29,1564242269.0,dataengineering,What would be the best data store for fast write/read of billions of float32s? SQLite? Hdf5? Other?,cij06k,BatmantoshReturns,,https://www.reddit.com/r/dataengineering/comments/cij06k/what_would_be_the_best_data_store_for_fast/,3.0,3.0,0.0,5554.0,"I am trying to create a library for sparse training. So it would need fast read/write of not only the machine learning model weights, but their optimizer momentum values as well. At the moment I would only like to experiment with values in the billions. 

Each parameter would be a float32 value. 

I think I can just use a single column, but there may be some instances where multiple columns would be helpful. For example, each column would represent a different layer in the machine learning architecture. 

Each update step in the training would be looking up and writing values do the data store, so I am looking for the fastest database for my situation. Having a database at all would already significantly reduce ram memory, so that that is not a concern for me."
893,2019-07-29 04:52:18,1564365138.0,dataengineering,When to use a database versus Avro or Parquet?,cj4pci,Atacupcakeshop,,https://www.reddit.com/r/dataengineering/comments/cj4pci/when_to_use_a_database_versus_avro_or_parquet/,8.0,6.0,0.0,5573.0,"Currently using a database but I am interested in exploring other data formats. 

Typical usage is querying a lot of rows to fully replace data over a rest API (specifically a tool called Domo which has a streams API where you can send compressed CSV formatted data in parallel). Most data is denormalized with a lot of columns but we typically do not need all columns. A lot of data is not joined to anything at all, but we do commonly filter out duplicates (select distinct on columns, order by updated_at desc) and we tend to append data rather doing in place updates to rows. 

ETL is done with the pandas library which has a to_parquet method which is neat. We are normally using pandas to get the data clean enough to bulk copy it into the database. It seems like I could change this from to_csv to to_parquet essentially? 

Our entire warehouse is about 1.5 TB but the largest single table is a few hundred million rows which we partition by month. A lot of dimension tables are snapshots partitioned daily and we simply join rows to the specific partition rather than using type 2 SCD tables. 

I don’t know much about Avro but the compression and schema evolution seems interesting. Any advice about when a database is not the best tool for the job?"
894,2019-07-29 09:43:34,1564382614.0,dataengineering,What does your data stack look like?,cj7jzc,ineedmysqlhelp,,https://www.reddit.com/r/dataengineering/comments/cj7jzc/what_does_your_data_stack_look_like/,11.0,9.0,0.0,5575.0,"I work in a corporate environment that is looking to upgrade it's data stack. We work as analysts out of a MySQL environment that is updated daily by jobs running from Retail Data Warehouse.

Some of our views are extremely slow and painful to work with, but it's what we know. We work across 100s of millions of rows.

We are looking at MapR as a solution with Tableau. However, the team is not a huge fan of Apache Drill when we are so used to MySQL. A lot of our operational reporting comes out of Excel, where we have queries with date parameters (something that Apache Drill does not support to my knowledge). Even though we want to move to Tableau, we won't ever escape Excel, and I think losing this ease of use will be a huge impact to the team, and will hurt the change management piece.

We have been discussing what it would look like with Azure Cosmos DB. However I'm not sure what that would look like, or what 'best practice' is. We need something to manage our 'big data', but also offer an easy way to get operational reporting data, and the ability to deep dive into low level data for analysis.

My team are not IT experts, they are commercial analysts and accountants. SQL is not their passion, however they are willing to get their hands dirty when needed, but I don't want to push them too hard. I'm not sure what 'goes' with Azure's stack to make it easier for them?

I would love to hear your opinions or experiences with using Azure for your big data store + day-to-day. Likewise, it would be great to see what your organizations are using for operational reporting. While we are not there yet with actual data science, we are getting started on our journey.

Thank you for any information!"
895,2019-07-29 14:07:43,1564398463.0,dataengineering,An interview about a new pattern for data integration that reduces the amount of effort required to find connections in numerous data sets,cj9sd0,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cj9sd0/an_interview_about_a_new_pattern_for_data/,1.0,3.0,0.0,5576.0,
896,2019-07-29 14:15:39,1564398939.0,dataengineering,Blogpost about the machine learning lifecycle,cj9uzx,dzyl,,https://www.reddit.com/r/dataengineering/comments/cj9uzx/blogpost_about_the_machine_learning_lifecycle/,1.0,0.0,0.0,5576.0,
897,2019-07-29 15:01:30,1564401690.0,dataengineering,Data Engineering + Career Break,cjaaz2,lseactuary,,https://www.reddit.com/r/dataengineering/comments/cjaaz2/data_engineering_career_break/,0.0,19.0,0.0,5575.0,"Background:

I have been working for 3 top tech companies (think Google/Amazon/Facebook/Apple/Twitter/LinkedIn/etc) for ~5 years in data/analytics related roles, the most recent being a Data Engineering role (1+ year).

I have a Bachelor in Statistics and am doing a part-time Masters in Computer Science / Software Engineering (both UK target universities).

I also picked up freelance technical Data Engineer work. Because of my performance, they are happy to extend the contract to 6+ months, where I can work 2-3 days a week (I have 2 long term contracts), and the pay for 1 of them is about 2x my salary at the tech firm anyway.

Question: I was thinking of leaving my tech firm (so I leave with a total of 5 years industry experience), doing the freelance work 3 days a week, and completing my Masters / upskiling on Python for 2 days a week, and having the weekend to breathe until I graduate from my Masters (Oct this year - May next year I would be 'off').

Advantages:

The work I am getting through the freelance work is 'actual' data engineer work, not just SQL/database stuff. Data Engineering (in my opinion) is basically Software Engineering focused on data, but in my current role I will never get that type of work (as my manager is very backwards and other roles in the company are similar). Coding, building pipelines, using a cloud stack etc all prepare me well for future / other roles in the market.

My Masters happens once in my lifetime, better to give it my all as the tech companies will always be there anyway. Technical roles are never going away. :)

Freelance + Masters gives me freedom to prepare for interviews (my Python is shakey) so in 3-6 months I am actually fluent again (I was fluent before but because in my recent roles I didn't really use it much I've gone rusty). It also means I've finished my studies when I go back to the market, pay will be better if anything, and with Brexit probably even better. I was also tempted to go on these 12 weeks courses which guarantee you a software engineering role, but tbh, I only really see myself in a data engineering role.

Disadvantages:

I am being offered senior / leadership positions already (2x my salary at min) outside of the firm. The only issue is I have 0 time to prepare for interviews, have a few technical creases which I want to iron out, and taking a leadership role at the moment could be dangerous as there is even less time then to finish my Masters. Tbh I don't want to take such a role either if I know I'm shakey. I'm not that bad either, 4-6 months heads down and preparing and I will be good, as its all in my brain just needs to be brought forwards. :)

It may look like 'I can't cope / took a break'. I will still be working, just less (probably 'more' actually as its more relevant haha).

For my Masters dissertation, it would be 'open source' not 'company based' then. Not sure if this is allowed, will have to check with my advisor, but I know several of my classmates who are freelancers / not working and they still complete a Dissertation.

If I proceed my story would be 'was doing a part time Masters, worked 5 years in tech, moved to full time Masters to focus on that while keeping my data engineering skills fresh by doing freelance contract work, back into the market now'. Is this sound? Anything I have missed? Would this make me unemployable in the cool tech/finance etc companies and/or anywhere?"
898,2019-07-29 16:49:25,1564408165.0,dataengineering,[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,cjbhm5,FuegoDentro,,https://www.reddit.com/r/dataengineering/comments/cjbhm5/requestrepost_asking_for_assistance_filling_up_a/,0.0,0.0,0.0,5578.0," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor."
899,2019-07-30 00:42:24,1564436544.0,dataengineering,PSA: Data Analysis &amp; Machine Learning ebook bundle,cjhx5b,lkozler11,,https://www.reddit.com/r/dataengineering/comments/cjhx5b/psa_data_analysis_machine_learning_ebook_bundle/,22.0,6.0,0.0,5586.0,
900,2019-07-30 01:55:42,1564440942.0,dataengineering,Best places to get help for Spark?,cjivcu,PhotographsWithFilm,,https://www.reddit.com/r/dataengineering/comments/cjivcu/best_places_to_get_help_for_spark/,2.0,6.0,0.0,5603.0,"Hi Folks,

I see in the Apache Spark documentation, that the advised first port of call for help is Stack Overflow.

While there seems to be plenty of questions being asked over there, I tend to find that there are not a lot of answers.

So, if you have a problem with Spark, where do you ask questions?

Thanks"
901,2019-07-31 01:26:56,1564525616.0,dataengineering,Data Engineering Tech Stack at Tile,cjz76o,manishatuga,,https://www.reddit.com/r/dataengineering/comments/cjz76o/data_engineering_tech_stack_at_tile/,10.0,8.0,0.0,5617.0,"[https://medium.com/me/stats/post/b97bfe250b8f](https://medium.com/me/stats/post/b97bfe250b8f)

Wrote about data infra at Tile. Also, Tile is looking for an engineer to hire in Vancouver. Let me know if you are interested in helping."
902,2019-07-31 03:51:27,1564534287.0,dataengineering,What should I look for in a Linux server?,ck0zdq,BostonPanda,,https://www.reddit.com/r/dataengineering/comments/ck0zdq/what_should_i_look_for_in_a_linux_server/,2.0,9.0,0.0,5619.0,"Local machine has 24gb RAM, i7, 2.4GHz CPU on Windows. Manipulating time data in pandas is very slow currently as I need to do indexing and mapping at multiple stages, working with 500k rows at a time/per dataframe. Employer offered to let me do the processing on a remote Linux server to speed things up and free up my machine for smaller projects to work on concurrently. However, I'm not sure what I should ask for. How much computing power and memory do I need for work like this?

I have taken to Google but many guidelines are based on ML type projects. That may be what this data is used for later but beyond my scope."
903,2019-07-31 18:47:53,1564588073.0,dataengineering,How do I increase the ssh connections coming to EMR from Airflow ?,cka6bf,hippagun,,https://www.reddit.com/r/dataengineering/comments/cka6bf/how_do_i_increase_the_ssh_connections_coming_to/,5.0,13.0,0.0,5627.0,"Excuse me I posted this in AWS sub and not much responses.

Not sure if this is the right place but Need help from a Big Data Expert. We use Airflow (c5.4xlarge) for Launching jobs onto EMR (1 Master of r5.4xlarge; 5 Core nodes of r5.4xlarge; 2 Task nodes of r5.12xlarge) with auto scaling.

Airflow ssh's to EMR to launch spark-submit jobs and we use GDC and S3 and we are talking 200 DAG's with some 4k tasks etc.

I am looking for ways to tune this ingestion flow to increase the parallelism/concurrency of this flow so that More jobs can run in parallel as we always have bunch of jobs queued in Airflow.

I have increased the Concurrency in Airflow to 128 but where should I correspondingly increase this number in EMR so that it can accept more connections from Airflow. Right now it only runs 50-60 Yarn jobs at any given time (Apps Running from Resource Manager UI).

Also please suggest any other ideas to make our Flow/Cluster more performant as this was designed 1 year back"
904,2019-07-31 22:48:25,1564602505.0,dataengineering,Any GCP Dataflow (Apache Beam) users in the house ?,ckdfb6,crazyme28,,https://www.reddit.com/r/dataengineering/comments/ckdfb6/any_gcp_dataflow_apache_beam_users_in_the_house/,6.0,5.0,0.0,5630.0,"Hi all  


I am new to GCP Dataflow (Apache beam). I have some questions on how to architect the pipeline dynamically.   


Let say i have a  Branching PCollections as in this pic ( [https://beam.apache.org/images/design-your-pipeline-multiple-pcollections.png](https://beam.apache.org/images/design-your-pipeline-multiple-pcollections.png) ). If someone from other team(like data anlysts or PM who dont have knowledge on apache beam but know little python) wants to just add another PCollection C names. How to build this kind of platform.  


Love to hear if you have any other thoughts and suggestions. The main goal is to add a  branch to pipeline dynamically for people who dont know apache beam. This new pipeline is nothing but some python methods.

&amp;#x200B;

Thanks in advance"
905,2019-07-31 23:53:01,1564606381.0,dataengineering,Operational Analytics: What every engineer should know about low latency queries on large data sets,ckeaar,ssb61,,https://www.reddit.com/r/dataengineering/comments/ckeaar/operational_analytics_what_every_engineer_should/,13.0,1.0,0.0,5632.0,"Operational analytics is a very specific term for a type of analytics which focuses on improving existing operations. This type of analytics, like others, involves the use of various data mining and data aggregation tools to get more transparent information for business planning. The main characteristic that distinguishes operational analytics from other types of analytics is that it is “analytics on the fly,"" which means that signals emanating from the various parts of a business are processed in real-time to feed back into instant decision making for the business. Some people refer to this as ""continuous analytics,"" which is another way to emphasize the continuous digital feedback loop that can exist from one part of the business to others.

&amp;#x200B;

The definition of an operational analytics processing engine can be expressed in the form of the following six propositions:

1. **Complex queries**: Support for queries like joins, aggregations, sorting, relevance, etc.
2. **Low data latency**: An update to any data record is visible in query results in under than a few seconds.
3. **Low query latency**: A simple search query returns in under a few milliseconds.
4. **High query volume**: Able to serve at least a few hundred concurrent queries per second.
5. **Live sync with data sources**: Ability to keep itself in sync with various external sources without having to write external scripts. This can be done via change-data-capture of an external database, or by tailing streaming data sources.
6. **Mixed types**: Allows values of different types in the same column. This is needed to be able to ingest new data without needing to clean them at write time.

This post discuss each of the above propositions in greater detail and discuss why each of the above features is necessary for an operational analytics processing engine: [https://rockset.com/blog/operational-analytics-what-every-software-engineer-should-know/](https://rockset.com/blog/operational-analytics-what-every-software-engineer-should-know/)"
906,2019-08-01 02:15:44,1564614944.0,dataengineering,Snowflake: the details of our first Data Warehousing project in the Cloud,ckg2r7,alfet,,https://www.reddit.com/r/dataengineering/comments/ckg2r7/snowflake_the_details_of_our_first_data/,1.0,1.0,0.0,5633.0,
907,2019-08-01 18:43:42,1564674222.0,dataengineering,Traversing the Land of Graph Computing and Databases,ckq1sr,analyticalmonk,,https://www.reddit.com/r/dataengineering/comments/ckq1sr/traversing_the_land_of_graph_computing_and/,9.0,0.0,0.0,5642.0,
908,2019-08-02 01:42:43,1564699363.0,dataengineering,Has anyone moved into data engineering after being a business/data analyst?,ckvjov,moma-dance,,https://www.reddit.com/r/dataengineering/comments/ckvjov/has_anyone_moved_into_data_engineering_after/,22.0,27.0,0.0,5644.0,"I may have an opportunity in the near future to transition internally to a data engineering role, coming from a business/data analyst type role. Right now I use a lot of sql and tableau, but don’t get overly technical for the most part. I’m curious if anyone has a similar story, and how it went for them. I have been working for about 9 years in various analyst roles, but feel like there’s a plateau at some point for this type of role. I think learning a more technical position will help me career-wise in the long run.   

But, I’m a bit overwhelmed and daunted by how challenging the new role may be (big data, airflow, complex sql, python, etc).  My degree is in finance and a minor in MIS so I’m vaguely familiar with the concepts, but not so much in practice.  

 If you’ve done this, did it work out well, did you regret it? Would love to hear from you folks. Thanks for reading"
909,2019-08-02 04:50:26,1564710626.0,dataengineering,Batch Job Scheduler Options,ckxoe9,ClemsonLaxer,,https://www.reddit.com/r/dataengineering/comments/ckxoe9/batch_job_scheduler_options/,3.0,2.0,0.0,5647.0,"Hey, I'm an ETL developer looking to do some side projects of my own to develop some new skills.  I'm running some python scripts to collect data from various sources and I'm looking to schedule these scripts.

At work, we use Autosys which works decently enough for our batch jobs and I've previously used Control M at a prior employer.  

I'm looking to learn a new scheduling tool.  For reference, I'd be running this on a Raspberry Pi 3 so it can't be a huge resource hog.  I could use cron for some of these, but eventually I'd have conditions between scripts so Cron wouldn't really get the job done there (since it appears to only use time-based scheduling).

Thanks!"
910,2019-08-02 15:18:58,1564748338.0,dataengineering,Spark for Beginners on YouTube,cl3ctg,andreaskretz,,https://www.reddit.com/r/dataengineering/comments/cl3ctg/spark_for_beginners_on_youtube/,1.0,0.0,0.0,5650.0,
911,2019-08-05 18:44:00,1565019840.0,dataengineering,Good material to review before a job interview,cmclnn,PM_ME_CATDOG_PICS,,https://www.reddit.com/r/dataengineering/comments/cmclnn/good_material_to_review_before_a_job_interview/,3.0,3.0,0.0,5680.0,
912,2019-08-06 00:39:03,1565041143.0,dataengineering,An interview about the open source Amundsen platform for data discovery and how Lyft is using it to improve their analytics workflow,cmhevn,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cmhevn/an_interview_about_the_open_source_amundsen/,7.0,0.0,0.0,5861.0,
913,2019-08-06 12:20:02,1565083202.0,dataengineering,London Salaries: Permanent vs Contract,cmobg7,Letmeout1,,https://www.reddit.com/r/dataengineering/comments/cmobg7/london_salaries_permanent_vs_contract/,9.0,9.0,0.0,5873.0,"I'm an Aussie moving to London working in the DE space and trying to understand some of the salary differentials I'm seeing. 

Seems like perm roles as a Data Engineer top out at about 70k. Yet I'm seeing contract roles between 400 to 700 a day. Which assuming 48 weeks of work is between 96k and 168k. Now I understand the inherent risks with contract roles but the difference is much larger then I'm used too. Am I missing something here?"
914,2019-08-06 17:59:13,1565103553.0,dataengineering,MapR’s business assets acquired by HPE,cmrnn3,sparkedkafka,,https://www.reddit.com/r/dataengineering/comments/cmrnn3/maprs_business_assets_acquired_by_hpe/,1.0,1.0,0.0,5876.0,
915,2019-08-06 19:42:55,1565109775.0,dataengineering,Building petabyte-scale analytics with BigQuery and HLL,cmszaf,joeeightbit,,https://www.reddit.com/r/dataengineering/comments/cmszaf/building_petabytescale_analytics_with_bigquery/,1.0,0.0,0.0,5878.0,
916,2019-08-06 22:01:56,1565118116.0,dataengineering,How to commit airflow code?,cmusns,dash_365,,https://www.reddit.com/r/dataengineering/comments/cmusns/how_to_commit_airflow_code/,4.0,11.0,0.0,5877.0,
917,2019-08-07 03:07:52,1565136472.0,dataengineering,Cheapest way to run Airflow on Kubernetes in a production environment?,cmyew1,allan_w,,https://www.reddit.com/r/dataengineering/comments/cmyew1/cheapest_way_to_run_airflow_on_kubernetes_in_a/,1.0,0.0,0.0,5879.0,
918,2019-08-07 09:35:23,1565159723.0,dataengineering,As a data engineer what has been your biggest accomplishment?,cn291a,howthehellmans,,https://www.reddit.com/r/dataengineering/comments/cn291a/as_a_data_engineer_what_has_been_your_biggest/,11.0,24.0,0.0,5882.0,"I'm applying for other jobs, but I have a hard time coming up with accomplishments to put on my resume. I've migrated some back-end databases for front-end reporting but that seems trivial. I feel like most of my accomplishments have also been cross-functional.. i never have done anything useful by myself. Any thoughts here?"
919,2019-08-08 02:59:03,1565222343.0,dataengineering,Best Path to Data Engineer Role out of College?,cndrc6,rhfrenchy,,https://www.reddit.com/r/dataengineering/comments/cndrc6/best_path_to_data_engineer_role_out_of_college/,1.0,0.0,0.0,5891.0,
920,2019-08-08 03:20:12,1565223612.0,dataengineering,Career advice for undergraduate.,cne006,ABlokeCalledGeorge8,,https://www.reddit.com/r/dataengineering/comments/cne006/career_advice_for_undergraduate/,1.0,0.0,0.0,5892.0,"Looking for some advice and thought this was the right sub. Please tell me if it isn't.

I'm  21 years old, and I'm a mexican Computer Systems Engineering  undergraduate. I'm very close to graduation, I only have three subjects,  and my Terminal Project (A project that lasts about a year) left to do.  This summer I applied to a Data Science summer internship at a big  company and have been working there for 2 months now.(My contract ends  in a few days).  Well, it turns out I was offered a data engineering job  in the US, because I'm an American citizen too (born and raised in  Mexico to an American citizen). I've really questioned the fact that  they really want me to work there because, well , it's only been two  months and I'm an undergrad, but it really is happening. At first they  did not even think about it because I don't have a degree yet, but they  really liked my work and talked about it. And they want me to go ,  despite not having a degree.

This  may sound mundane to some but to me it is a very big opportunity,  because I've always wanted to work abroad, and I'd be paid very well.  And also, my family's economic situation is not the best and we've  always planned to go live in the US.

I  looked for ways to finish my degree while abroad, but it is almost  impossible to do it (because public school). Unless I take a break and  come back and finish, which I honestly think  it's not very realistic  because I doubt the fact that I'd come back. I'm still looking for  options though, I haven't fully given up on finishing while abroad.

I've  been advised not to drop out of school , and I think it is the best  thing to do (I mean I might need a master's degree later on) but I'm  very afraid of the fact that this kind of opportunity might not show up  again. I think it could be the opportunity of a lifetime (and maybe it  isn't). And well, some of the bosses think I should go, my boss thinks I  should finish and stay. I think I should point out that I study in one  of the best public universities in Mexico, so I'm not dropping out of  any school.

So, I'd like to know,  what is your opinion on this? How do you think  I would do if I work on  data science or other IT related area , and in the US, without a degree?

All comments are welcome and I'd appreciate your insight."
921,2019-08-08 16:51:59,1565272319.0,dataengineering,Anyone here completed Udacity Data Engineering Nano degree in two months? or less?,cnm2e1,jonamjar,,https://www.reddit.com/r/dataengineering/comments/cnm2e1/anyone_here_completed_udacity_data_engineering/,10.0,10.0,0.0,5898.0,"I know the answers here is “depends on the person”.  I am trying to decide between paying monthly and paying at once. I have gone through the syllabus and I felt I can spend more than 20hours a week. From my three years of experience as a Data Analyst, I am proficient with Python and SQL along with basic understanding of databases. So I believe it could be easier for me to complete in shorter time. I want to do this because currently I have  bits and pieces knowledge of data engineering and would like to become organized in my learning.

If anyone here with my background has ever done it less than 5months(estimate by Udacity) please share your experience. Thank you!"
922,2019-08-08 19:16:53,1565281013.0,dataengineering,Seeking mentorship,cnny5d,hantt,,https://www.reddit.com/r/dataengineering/comments/cnny5d/seeking_mentorship/,3.0,3.0,0.0,5900.0,"Hey guys as the title suggests I'm looking to see if anyone is kind enough to let me bombard them with questions on my path to become a data engineer. I currently work as an Aircraft router for a major Airline but have found myself becoming my department's adhoc Data-analyst. I find that working with data is much more rewarding than my actual job which is very routine. While I don't mind googling my way to learn the required skills Id like to talk professionals who can share some industry standards and best practices.

Also If anyone else has found a way to transition to data engineering from completely unrelated field please share your story! 
What was the most challenging aspect of becoming a data engineer?

What was the easiest?"
923,2019-08-09 02:26:19,1565306779.0,dataengineering,"Big Data Project with Hadoop, Tajo, and Spark",cntvgf,teddy_cr,,https://www.reddit.com/r/dataengineering/comments/cntvgf/big_data_project_with_hadoop_tajo_and_spark/,1.0,1.0,0.0,5901.0,"I am looking for a big data engineering project to experiment with Hadoop, Tajo, and Spark. I am planning on implementing a Hadoop cluster with 2-nodes, with Tajo and Spark. 

I am looking for large data updating frequently (I am thinking of every hour) to use for this project, any good recommendations?"
924,2019-08-09 04:31:15,1565314275.0,dataengineering,How to choose workflow automation tool?,cnvbr2,SilverRockyMountain,,https://www.reddit.com/r/dataengineering/comments/cnvbr2/how_to_choose_workflow_automation_tool/,23.0,12.0,0.0,5901.0,"I've recently begun a project at work to automate the process of extracting, calculating, and extracting data sets from multiple data sources and then changing their formats, doing transformations and then loading the data into the cloud.  I've looked at Apache Airflow but I also know Prefect is a new offering. I've looked at some less well supported tools. 

How does one go about deciding on a tool to use automate workflows. What factors are important to consider?"
925,2019-08-09 13:20:56,1565346056.0,dataengineering,Development workflows in AWS Glue,co0ff0,xarasco,,https://www.reddit.com/r/dataengineering/comments/co0ff0/development_workflows_in_aws_glue/,0.0,1.0,0.0,5904.0,"Greetings.  I'm evaluating AWS Glue.  I'm aware of a code management and deployment solution in AWS detailed [here](https://aws.amazon.com/blogs/big-data/implement-continuous-integration-and-delivery-of-serverless-aws-glue-etl-applications-using-aws-developer-tools/).  But haven't found any other solutions.

Does anyone have a solution (or ideas) not based in the AWS Console? Important points for me:

* Fully (or significantly) automated
* Environment variable store
* Github integration"
926,2019-08-09 16:30:33,1565357433.0,dataengineering,Getting started with Data Lineage,co2efk,schrute_dataeng,,https://www.reddit.com/r/dataengineering/comments/co2efk/getting_started_with_data_lineage/,21.0,3.0,0.0,5905.0,
927,2019-08-10 03:21:54,1565396514.0,dataengineering,Pinterest Tech Talk: Aggregator-Leaf-Tailer architecture for low-latency queries on large datasets,coawq9,ssb61,,https://www.reddit.com/r/dataengineering/comments/coawq9/pinterest_tech_talk_aggregatorleaftailer/,5.0,1.0,0.0,5913.0,"Dhruba Borthakur (founder of RocksDB) discusses the implementation of [Rockset's](https://www.rockset.com/) low-latency operational analytics engine. Topics covered include the Aggregator-Leaf-Tailer architecture, smart schemas, converged indexing, and serverless data management.

Video is here: [https://www.youtube.com/watch?v=tDgmtReLS8c](https://www.youtube.com/watch?v=tDgmtReLS8c)"
928,2019-08-10 16:55:47,1565445347.0,dataengineering,How should I start learning data engineering?,coic76,osoz007,,https://www.reddit.com/r/dataengineering/comments/coic76/how_should_i_start_learning_data_engineering/,2.0,6.0,0.0,5920.0,"Hello
I am 16 and I really like programing especially stuff that has to do with data and wanted that to have something to do with my job when I finish school but wanted to start learning now . So what are some good resources that can help me start learning data engineering. 
Thanks"
929,2019-08-10 17:23:03,1565446983.0,dataengineering,The Data Analysis &amp; Machine Learning ebook bundle ends in about two days (PSA),coineq,lkozler11,,https://www.reddit.com/r/dataengineering/comments/coineq/the_data_analysis_machine_learning_ebook_bundle/,33.0,1.0,0.0,5920.0,
930,2019-08-12 00:58:25,1565560705.0,dataengineering,Help with Airflow,cp31xc,sundios,,https://www.reddit.com/r/dataengineering/comments/cp31xc/help_with_airflow/,3.0,20.0,0.0,5967.0,"I recently started using Docker airflow (puckel/docker-airflow) and is giving me nightmares.

I wanna run a bash script using BashOperator. But when it runs it cannot find the script location.

this is my code:

    from airflow import DAG
    from airflow.operators.bash_operator import BashOperator
    from datetime import datetime, timedelta
    import os
    
    default_args = {
        ""owner"": ""airflow"",
        ""depends_on_past"": False,
        ""start_date"": datetime(2015, 6, 1),
        ""email"": [""airflow@airflow.com""],
        ""email_on_failure"": False,
        ""email_on_retry"": False,
        ""retries"": 1,
        ""retry_delay"": timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
    }
    
    
    dag = DAG(""ranks"", default_args=default_args, schedule_interval=timedelta(1))
    
    
    t1 = BashOperator(task_id=""execution_rights"", bash_command=""chmod +x /Users/konradburchardt/Desktop/docker-airflow/script/rank.sh "", dag=dag)
    
    
    file = '/Users/konradburchardt/airflow/dags/rank.sh '
    
    
    t2 = BashOperator(task_id= 'rank_check',bash_command=file,dag=dag)
    
    t3 = BashOperator(task_id=""Step_2"", bash_command=""echo ' Step 2 Complete' "", dag=dag)
    
    
    t1 &gt;&gt; t2 &gt;&gt; t3

&amp;#x200B;

    [2019-08-11 21:15:35,115] {bash_operator.py:105} INFO - Temporary script location: /var/folders/56/0x5zxzq119b6wn0j_cchfzxw0000gn/T/airflowtmp4rarv7mk/create_filegzb7c3by
    [2019-08-11 21:15:35,115] {bash_operator.py:115} INFO - Running command: /Users/konradburchardt/airflow/dags/rank.sh
    [2019-08-11 21:15:35,126] {bash_operator.py:124} INFO - Output:
    [2019-08-11 21:15:35,131] {bash_operator.py:128} INFO - /var/folders/56/0x5zxzq119b6wn0j_cchfzxw0000gn/T/airflowtmp4rarv7mk/create_filegzb7c3by: line 1: /Users/konradburchardt/airflow/dags/rank.sh: No such file or directory
    [2019-08-11 21:15:35,132] {bash_operator.py:132} INFO - Command exited with return code 127
    [2019-08-11 21:15:35,140] {taskinstance.py:1047} ERROR - Bash command failed

I also checked my usr/local/airflow/dags/ and I did have [test.sh](https://test.sh) in there. So I tried also adding that path there but it didnt work. It seems it keeps loking int tmp files which I have no idea where that is.

&amp;#x200B;

Any idea of how to solve this and how to change this tmp file to my dags folder?

 Im can use vanilla airflow or I can also use docker airflow."
931,2019-08-12 05:37:21,1565577441.0,dataengineering,Airflow vs Azure Data Factory?,cp69ce,LordCommanderStannis,,https://www.reddit.com/r/dataengineering/comments/cp69ce/airflow_vs_azure_data_factory/,12.0,15.0,0.0,5969.0,Can someone explain the difference between using Airflow vs Azure Data Factory to schedule ETL jobs? Is there a difference? Which one is better?
932,2019-08-12 16:50:22,1565617822.0,dataengineering,Web Service to AWS Kinesis,cpcn3z,bobhaffner,,https://www.reddit.com/r/dataengineering/comments/cpcn3z/web_service_to_aws_kinesis/,1.0,5.0,0.0,5973.0,"Any opinions on a tool/framework/approach/whatever that consumes data from a web service and publishes that data to AWS Kinesis?  

Service &lt;----&gt; Something ----&gt; Kinesis

I'm guessing most folks are writing apps with the help of the Kinesis Producer Libary (KPL)?   This approach doesn't seem trivial.  Any words of advice?"
933,2019-08-12 17:09:37,1565618977.0,dataengineering,An interview about how the Fivetran platform is designed to handle data replication as a service,cpcvji,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cpcvji/an_interview_about_how_the_fivetran_platform_is/,7.0,0.0,0.0,5974.0,
934,2019-08-12 20:15:25,1565630125.0,dataengineering,Optimizing Spark Query,cpfe2d,mjfnd,,https://www.reddit.com/r/dataengineering/comments/cpfe2d/optimizing_spark_query/,1.0,0.0,0.0,5976.0,
935,2019-08-12 22:41:48,1565638908.0,dataengineering,CS undergrad with 1 year left--what does it take to get junior data engineer positions?,cphfzb,rainercarr,,https://www.reddit.com/r/dataengineering/comments/cphfzb/cs_undergrad_with_1_year_leftwhat_does_it_take_to/,1.0,1.0,0.0,5980.0,
936,2019-08-13 02:21:00,1565652060.0,dataengineering,How to design a SQL database,cpkejp,kile22,,https://www.reddit.com/r/dataengineering/comments/cpkejp/how_to_design_a_sql_database/,1.0,2.0,0.0,5982.0,"My boss has asked me to setup a SQL server on AWS, which our IT team will do, but I need some tips and resources on how to design or build the structure of the database. I have some basic knowledge of SQL queries, but not much on design. I've tried Googling things like SQL database design, but I'm not finding what I'm looking for. Most SQL tutorials start with a prebuilt DB and go into queries or just create some simple tables. I'd like to know why a database has a certain structure or how to create a design that is easy to maintain. Maybe I'm using the wrong terms for things, is database design a thing?"
937,2019-08-13 03:58:17,1565657897.0,dataengineering,Building a Data Science product that demonstrates Data Engineering skills,cplldx,Low_end_the0ry,,https://www.reddit.com/r/dataengineering/comments/cplldx/building_a_data_science_product_that_demonstrates/,28.0,11.0,0.0,5983.0,"Hi all, 

I apologize if this question is too vague/broad but I would like to get some basic insights from the community (I'll probably be posting more questions in the future ha).

I'm a grad student who will be starting the [Insight Data Science Fellows Program](http://insightdatascience.com) next month. During the program, I will be required to build a data product where I take data from somewhere on the web, do some processing, do some kind of statistical analysis, and visualize the results. 

As I'm also very interested in careers in data engineering, I would like to incorporate data engineering best practices so that I can demonstrate skills in both data science &amp; engineering. Are there specifically tools/technologies that I should incorporate in my project that would be (a) beneficial for my own understanding of data pipelines and (b) something noteworthy to talk about on potential DE job interviews?

Thanks!"
938,2019-08-13 09:21:33,1565677293.0,dataengineering,"Comparison of 17 ETL Tools, and the Case for Saying ""No"" to ETL",cpp4ns,cmstrump,,https://www.reddit.com/r/dataengineering/comments/cpp4ns/comparison_of_17_etl_tools_and_the_case_for/,0.0,4.0,0.0,5986.0,"ETL pulls data out of the source, makes changes according to requirements, and then loads the transformed data into a database or BI platform to provide better business insights to make data-driven business decisions. Here is a comparison of 6 open source and 11 paid ETL tools to comparison what’s best for your business: [17 Great ETL Tools, And The Case For Saying ""No"" To ETL](https://blog.panoply.io/17-great-etl-tools-and-the-case-for-saying-no-to-etl)

‘No ETL’ here means that the ETL process is supplanted by Extract, Load, Transform (ELT), where data transformation happens in SQL as needed for downstream use, rather than upfront during the loading stage."
939,2019-08-13 09:53:38,1565679218.0,dataengineering,Data Virtualization (like Denodo) as staging layer?,cppeln,aburkh,,https://www.reddit.com/r/dataengineering/comments/cppeln/data_virtualization_like_denodo_as_staging_layer/,1.0,0.0,0.0,5986.0,"Denodo provides great capabilities to make data available from multiple sources, regardless of the format. Have any of you seen the use of Denodo as a staging layer, effectively making data available (cache) before processing it inside a database?"
940,2019-08-13 17:04:46,1565705086.0,dataengineering,DE roles at FAANG,cptlc4,lseactuary,,https://www.reddit.com/r/dataengineering/comments/cptlc4/de_roles_at_faang/,5.0,22.0,0.0,5988.0,"Noticed a lot of DE roles in FAANG being in the US rather than the UK. I am currently a DE and there are definitely DE roles in the UK, but they tend to be at startups / other companies vs FAANG. I am therefore wondering if taking another role e.g. analyst role at FAANG and then moving to the US through a DE role is smart or best to stick to DE roles in London and see what happens? 

(Note: I have seen a handful of DE roles at FAANG in London but they are either more database/SQL type or very software engineering orientated; so this sucks for someone in the middle for example)."
941,2019-08-13 18:45:38,1565711138.0,dataengineering,Hi ya'll. What are you guys using for data versioning? I'm thinking about using delta lake but I want to see what other options are out there.,cpuxjt,FamousJackfruit,,https://www.reddit.com/r/dataengineering/comments/cpuxjt/hi_yall_what_are_you_guys_using_for_data/,14.0,8.0,0.0,5988.0,delta lake vs ..???
942,2019-08-13 22:42:32,1565725352.0,dataengineering,Need career advice in transitioning into data engineering,cpy8zz,kerbalpy,,https://www.reddit.com/r/dataengineering/comments/cpy8zz/need_career_advice_in_transitioning_into_data/,1.0,0.0,0.0,5994.0,
943,2019-08-14 09:50:14,1565765414.0,dataengineering,"Coursera Specialization Data Engineering on Google Cloud Platform has 30 days of free trial. Launched in July 2018 along with 11 other new specializations from Wharton, Johns Hopkins, Duke, Rice University...",cq607n,frenchdic,,https://www.reddit.com/r/dataengineering/comments/cq607n/coursera_specialization_data_engineering_on/,5.0,5.0,0.0,6004.0,
944,2019-08-14 16:35:17,1565789717.0,dataengineering,How We Solved Our Airflow I/O Problem By Using A Custom Docker Operator,cq9r6j,htebsile,,https://www.reddit.com/r/dataengineering/comments/cq9r6j/how_we_solved_our_airflow_io_problem_by_using_a/,15.0,0.0,0.0,6007.0,
945,2019-08-14 19:34:29,1565800469.0,dataengineering,Are you happy with the current data engineering tools out there?,cqc52u,feedthemartian,,https://www.reddit.com/r/dataengineering/comments/cqc52u/are_you_happy_with_the_current_data_engineering/,1.0,1.0,0.0,6010.0,
946,2019-08-14 20:00:42,1565802042.0,dataengineering,HDFS Partitions vs. Snowflake Micro-Partitions -- How can I re-create Hive Partitions in Snowflake?,cqci3w,ConfirmingTheObvious,,https://www.reddit.com/r/dataengineering/comments/cqci3w/hdfs_partitions_vs_snowflake_micropartitions_how/,1.0,1.0,0.0,6010.0,"Hi there,

I am trying to migrate data from HDFS over to Snowflake. I am able to replicate the tables pretty much exactly, but what I'm having trouble understanding is how in Hive, I can PARTITION tables, which allows me to overwrite a specific part of the table (e.g. an entire day of data), with no issues.

&amp;#x200B;

Now, apparently Snowflake has CLUSTER and Micro-partitions, but the documentation is actually terrible. The only workaround I can get to do something like an overwrite of a partition is to write this:

&amp;#x200B;

insert overwrite into \[table\]

select \* from \[table\]

where date != '2019-07-18'

union all

select \* from \[table\]

where lead\_createddate = '2019-07-18'

&amp;#x200B;

Does anyone have any docs or sample code around how to setup PARTITIONS in a table within Snowflake?

&amp;#x200B;

Thanks!"
947,2019-08-15 17:20:20,1565878820.0,dataengineering,Data Virtualization (Denodo) Question,cqqghg,mtmtb,,https://www.reddit.com/r/dataengineering/comments/cqqghg/data_virtualization_denodo_question/,2.0,6.0,0.0,6021.0,"Please excuse my ignorance on the topic. I'm an engineer by degree, but my knowledge in this world is limited.

We have a Denodo connection to some time series data. The data is high frequency, with a new reading every 5 minutes. We have \~200 unique properties that contain about 20 different variables. So basically it's 20 different columns (22 including a unique identifier and timestamp), updating every 5 minutes, for every one of our 200 properties. 

We've been having reliability issues with querying this data into visualization platforms. What it comes down to is I can't pull more than 3 weeks worth of data. Any more than that and it just won't load. So my questions are:

1. Without more details, is it possible to know why this data would not load? Would this be a Denodo issue or should we be looking at the original DB?
2. If I didn't care about the 5 minute frequency, how hard would it be to take a daily average by unique identifier, for each variable, and put that into another table? What would that process look like? 

We have a good IT team currently looking into this, but we just lost most of our developers so progress is slow. Any insight would be much appreciated. Thanks!"
948,2019-08-16 00:38:42,1565905122.0,dataengineering,Comparison of IPFS and EdgeFS for data-intensive Edge Computing use cases,cqwgh2,dmitry_yus,,https://www.reddit.com/r/dataengineering/comments/cqwgh2/comparison_of_ipfs_and_edgefs_for_dataintensive/,0.0,0.0,0.0,6029.0,
949,2019-08-16 13:13:28,1565950408.0,dataengineering,"Airflow: Lesser Known Tips, Tricks, and Best Practises",cr4dfc,kaxil_naik,,https://www.reddit.com/r/dataengineering/comments/cr4dfc/airflow_lesser_known_tips_tricks_and_best/,20.0,1.0,0.0,6033.0,
950,2019-08-16 18:08:36,1565968116.0,dataengineering,Top 10 blog posts to help you transition to data engineering,cr7nkg,hszafarek,,https://www.reddit.com/r/dataengineering/comments/cr7nkg/top_10_blog_posts_to_help_you_transition_to_data/,20.0,0.0,0.0,6036.0,
951,2019-08-17 00:35:52,1565991352.0,dataengineering,Data Mapping Tool recommendation,crcvgn,kittie_thrower,,https://www.reddit.com/r/dataengineering/comments/crcvgn/data_mapping_tool_recommendation/,2.0,2.0,0.0,6043.0,"Somewhat like flow-chart tool like Lucidchart, I'm looking for one for (logical) data mapping - purpose is to create a pdf/picture [like this](https://cdn.softwaretestinghelp.com/wp-content/qa/uploads/2019/01/Introduction2.png)

Thanks"
952,2019-08-17 01:59:37,1565996377.0,dataengineering,I am thinking of changing careers to become a data engineer - can anyone please critique my game plan?,crdxtf,throwawaystickies,,https://www.reddit.com/r/dataengineering/comments/crdxtf/i_am_thinking_of_changing_careers_to_become_a/,21.0,17.0,0.0,6043.0,"Hello there! I have a BSN from a SEA country but I realized too late that I didn’t want to become a nurse.

I’m currently working as a medical coder for a large hospital, and I am interested in becoming a data analyst, and quite possibly transitioning into data engineering or data science (likely the latter but we shall see!). 

Which game plan is better??

**Game Plan #1**

~~• Udacity’s Intro to Computer Science - used Python. I learned about recursion, strings, lists, dictionaries, boolean statements and loops.~~ **FINISHED**

~~• Udacity’s Programming Foundations of Python - Object-oriented programming, classes, method overriding~~ **FINISHED**

• Udacity’s Intro to Data Analysis - learn about Python libraries such as NumPy, Pandas and Matplotlib

• Udacity’s Data Analyst nanodegree - Develop a proficiency in Python and data analysis libraries and SQL, practical stats, data wrangling, data visualization with Python.**After this nanodegree, I will start applying for jobs.**

• WGU’s Masters in Data Analytics

**OTHERS**:

• Also doing Eric Matthes Python Crash Course on the side to build more projects with

• Codeacademy’s Learn SQL course

• Udacity’s SQL for data analysis

* Going through Kha Academy math courses on my free time to refresh/learn ~~Algebra,~~ Statistics (what I'm currently doing), Pre-Calc, Calc, Linear Algebra &amp; Differential Algebra.

&amp;#x200B;

**Game Plan #2**

* Apply and finish WGU MBA program while working
* Apply for analyst positions
* Get a BSCS degree in WGU while working as an analyst

&amp;#x200B;

**Which plan is better? Which has a higher likelihood of getting into data analysis -&gt; data engineering or data science?**

**Anything you would change in my plan?**"
953,2019-08-17 21:02:39,1566064959.0,dataengineering,Google search schema,crpltj,interwag,,https://www.reddit.com/r/dataengineering/comments/crpltj/google_search_schema/,3.0,4.0,0.0,6059.0,"I have an interview coming up, and I've been told that one of the questions will be how I would structure the data if I were implementing Google Search. I've tried to think of various options, but I'm at a loss of how to make the database schema and how to effectively make the unstructured to structured data transformation. Is there a canonical way to transform and structure the data for something like Google Search?"
954,2019-08-18 02:13:58,1566083638.0,dataengineering,Learn Data science and Data Engineering - My path,crthmt,esenthil,,https://www.reddit.com/r/dataengineering/comments/crthmt/learn_data_science_and_data_engineering_my_path/,24.0,4.0,0.0,6061.0,"Hi Guys,

Please check out my post. This post is targeted towards people who want to get into DS/ML /DE or starting in DS/ML/DE.

This list is compiled from various resources. I also posted the data science curriculum that I am learning.

https://medium.com/p/how-to-learn-data-science-my-path-ba7b9aa94f63?source=email-1d8fcdc16d73--writer.postDistributed&amp;sk=47d16e88a2bcb0635ab14792b0092fb6

I can suggest bookmarking this one and will be helpful going forward.

Good luck:-)"
955,2019-08-18 06:18:07,1566098287.0,dataengineering,Something that I think data engineering folks would find helpful!,crw3zt,ihatemondayz,,https://www.reddit.com/r/dataengineering/comments/crw3zt/something_that_i_think_data_engineering_folks/,2.0,3.0,0.0,6064.0,
956,2019-08-18 20:54:37,1566150877.0,dataengineering,How do you reorganize/refactor your pipeline?,cs4jde,iamwil,,https://www.reddit.com/r/dataengineering/comments/cs4jde/how_do_you_reorganizerefactor_your_pipeline/,9.0,7.0,0.0,6089.0,"How do you guys reorganize/refactor your data pipeline? I had built mine up to a stage where I found I didn't have the data in the format I needed, so now I needed to 

- Trace up the pipeline to see at which transform to reorganize the data schema.
- At the pipe to rewrite the transform (point of reorganization), I have to remember or reconstruct the data schema in my head.
- While I rewrite the transformation to reorg the data, I have to take into account any downstream pipes that might be affected.
- Change all downstream pipes to use new data schema
- Break the old connections and reconnect pipes in DAG to reflect changes.

This was a lot of work, and I feel like it took longer than it should have. What sort of strategies do you guys use to mitigate the pain? Or do you not try to reorg/refactor, especially since db migrations in addition to a pipeline migration might be painful? If that's the case, wouldn't your pipeline get more inefficient as multiple people add their own custom variations of a pipe onto the pipeline?"
957,2019-08-19 18:29:46,1566228586.0,dataengineering,Python ETL job to capture website stats and analytics data,csimq9,imba22,,https://www.reddit.com/r/dataengineering/comments/csimq9/python_etl_job_to_capture_website_stats_and/,3.0,2.0,0.0,6113.0,"Hi people, 

I want to build an ETL pipeline which will capture the data from New Relic. For people who dont know New Relic is a monitoring tool, analytics tool. It is much like google analytics, where in it helps us identify which pages our customer are  visiting most, average time par page, number of request happening par hour, hows the health of our services. 

Coming back to the original question, New Relic has end points to capture this data. We have been using SSIS script task to call out to these rest end point to capture this data. Now we want to expand this project and SSIS is not very good at managing. I want to ask how can I use python, Do you have any recommendation on what free framework to use to build such ETL job in Python"
958,2019-08-19 20:56:03,1566237363.0,dataengineering,Azure datalake vs Blob?,cskp4y,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/cskp4y/azure_datalake_vs_blob/,8.0,6.0,0.0,6117.0,"Any recommendations on azure blob versus azure datalake gen 2?

I will be storing:

-	raw files prior to being processed (csv and excel)
-	processed files (parquet)
-	airflow logs 
-	maybe some database pg_dumps


It seems like the services are quite similar but azure datalake seems more appealing with hierarchical naming. 

Also, what is the best way for me to allow people within the company to upload files to this storage? Can I have an SFTP point to the blob storage perhaps? Or should i use Azure Files for this and then transfer the files to Blob later?"
959,2019-08-19 21:31:22,1566239482.0,dataengineering,Spark: Aggregating your data the fast way,csl7ck,nieuweyork,,https://www.reddit.com/r/dataengineering/comments/csl7ck/spark_aggregating_your_data_the_fast_way/,1.0,0.0,0.0,6117.0,
960,2019-08-19 21:35:10,1566239710.0,dataengineering,"An interview about the HPCC platform, its journey to open source, and how it handle the full lifecycle of big data for enterprise scale analytics",csl9dk,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/csl9dk/an_interview_about_the_hpcc_platform_its_journey/,3.0,0.0,0.0,6118.0,
961,2019-08-19 22:01:31,1566241291.0,dataengineering,Apache Spark for dotnet developers,cslmsc,ed_elliott_,,https://www.reddit.com/r/dataengineering/comments/cslmsc/apache_spark_for_dotnet_developers/,3.0,0.0,0.0,6118.0,
962,2019-08-19 23:07:36,1566245256.0,dataengineering,How to filter an array of array of JSONS?,csmk94,metalloidica,,https://www.reddit.com/r/dataengineering/comments/csmk94/how_to_filter_an_array_of_array_of_jsons/,0.0,7.0,0.0,6120.0,"I have a data structure that looks like this:  \[\[{'field\_amt': '200', 'field\_nbr':'300'}, {'field\_amt': '700', 'field\_nbr':'450'}\], \[{'field\_amt': '400', 'field\_nbr':'470'}, {'field\_amt': '800', 'field\_nbr':'300'}\], \[{'field\_amt': '200', 'field\_nbr':'600'}, {'field\_amt': '300', 'field\_nbr':'900'}\]\]"
963,2019-08-20 10:49:53,1566287393.0,dataengineering,How we built a tool for validating big data workflows,csuium,mite-mitreski,,https://www.reddit.com/r/dataengineering/comments/csuium/how_we_built_a_tool_for_validating_big_data/,5.0,0.0,0.0,6132.0,
964,2019-08-20 19:56:36,1566320196.0,dataengineering,Top 6 data engineering frameworks to learn,ct24v6,hszafarek,,https://www.reddit.com/r/dataengineering/comments/ct24v6/top_6_data_engineering_frameworks_to_learn/,32.0,15.0,0.0,6135.0,
965,2019-08-21 07:53:09,1566363189.0,dataengineering,HELP URGENTLY! Visa / USCIS/ someone to re-write the draft letter (proof course related to work),ctc0af,nicedoctor2017,,https://www.reddit.com/r/dataengineering/comments/ctc0af/help_urgently_visa_uscis_someone_to_rewrite_the/,0.0,0.0,0.0,6145.0,"My friend needs someone to re-write the draft letter (proof course related to work).
I can send you the word document.
There are 8 courses
In the document there are explanations of how each work help in the current job as a data engineer."
966,2019-08-22 04:02:41,1566435761.0,dataengineering,Why would I choose to use Kafka over Pulsar?,ctq3kv,bageldevourer,,https://www.reddit.com/r/dataengineering/comments/ctq3kv/why_would_i_choose_to_use_kafka_over_pulsar/,4.0,1.0,0.0,6157.0,"Aside from how it's more well-established, is there anything in the underlying technology itself that makes Kafka preferable for some use cases compared to Pulsar?"
967,2019-08-22 13:46:40,1566470800.0,dataengineering,Best practices for managing data flows,ctvo4q,datadataa,,https://www.reddit.com/r/dataengineering/comments/ctvo4q/best_practices_for_managing_data_flows/,15.0,23.0,0.0,6167.0,"Soon my organization will receive data on a regular basis that needs to go through an ETL process into a DB to be consumed by a BI tool. The landing zone, DB, and BI tool are ready. However, I am struggling with coming up with a solid data processing plan from the landing zone into the DB.

Background on environment:

* Several csv files will land in S3 hourly
* DB is a PostgreSQL on AWS RDS

Background about myself:

* Analyst background with strong SQL knowledge and some DB management skills
* Almost no programming skills, but willing to learn if needed
* Only person in data team, thus solution needs to be easily manageable by one person (for now)

I was thinking of using AWS Data Pipeline tool, mainly because it doesn't require programming and supports notifications on fail/success out of the box. I could use a Lambda function to fire the Data Pipeline every time a new file in S3 is detected. Only thing I am worried about is scalability of this solution, since I wouldn't know how to easily recreate new pipelines and version them for documentation.

Since I am totally new to data engineering, what are some of your best practices and tips from people that have been in this field for quite a while? Is above plan a good start? Would you use different tools? Any push in the right direction is very helpful."
968,2019-08-23 20:53:02,1566582782.0,dataengineering,SQL intelligence on NoSQL Amazon DynamoDB,cuhd9x,ssb61,,https://www.reddit.com/r/dataengineering/comments/cuhd9x/sql_intelligence_on_nosql_amazon_dynamodb/,1.0,0.0,0.0,6183.0,"California firm Rockset announced its serverless search and analytics engine now does real-time SQL analytics on the NoSQL Amazon DynamoDB database service.

The company specializes in such functionality, pointing its SQL-based analytics engine to NoSQL data stores such as Kafka and S3 storage buckets, along with Amazon DynamoDB.

[https://awsinsider.net/articles/2019/08/20/dynamodb-sql.aspx](https://awsinsider.net/articles/2019/08/20/dynamodb-sql.aspx)"
969,2019-08-23 22:03:02,1566586982.0,dataengineering,"Optimus is the missing framework to profile, clean, process and do ML in a distributed fashion using Apache Spark(PySpark).",cuiazh,haloworlds,,https://www.reddit.com/r/dataengineering/comments/cuiazh/optimus_is_the_missing_framework_to_profile_clean/,3.0,6.0,0.0,6185.0,https://github.com/ironmussa/Optimus
970,2019-08-24 01:41:41,1566600101.0,dataengineering,"How much Mathematics, and how well must you know ML to get into Data Engineering?",cul5v5,Objectiveb4,,https://www.reddit.com/r/dataengineering/comments/cul5v5/how_much_mathematics_and_how_well_must_you_know/,0.0,10.0,0.0,6188.0,"Hi all, I started off my first software career in front end development after a coding bootcamp back in 2016, I now work as a fullstack developer using JavaScript/Java.

I've been reading a bit about data engineering, and I feel like its an interesting role to get into, I've recently been accepted into an online Masters in CS program at Georgia Tech. Originally, I've planned on focusing on the Computing System specialization taking various OS courses (high performance computing, advanced OS, compilers, etc) but if I wanted to get into the data engineering path, is it also important for me to take some courses such as Big Data, ML, and other Math heavy courses?"
971,2019-08-25 04:48:15,1566697695.0,dataengineering,Giving Back With Data Engineering,cv29zn,SilverRockyMountain,,https://www.reddit.com/r/dataengineering/comments/cv29zn/giving_back_with_data_engineering/,26.0,6.0,0.0,6205.0,"I’ve become very interested in the use of programming, data science, and data engineering for a cause that helps the community. I’m wondering what everyone’s experience is with using data science/data engineering/software development for solving social issues. Where could and have these these skills been used for social good?"
972,2019-08-25 22:13:46,1566760426.0,dataengineering,Is this a more difficult field to break into as an entry level compared to web dev or app dev?,cvcsrl,GullibleBuil2ding,,https://www.reddit.com/r/dataengineering/comments/cvcsrl/is_this_a_more_difficult_field_to_break_into_as/,1.0,0.0,0.0,6213.0,
973,2019-08-27 02:44:25,1566863065.0,dataengineering,GroupBY of JSON data in Pyspark,cvw6c1,metalloidica,,https://www.reddit.com/r/dataengineering/comments/cvw6c1/groupby_of_json_data_in_pyspark/,4.0,8.0,0.0,6227.0,"Iam working with a very large JSON dataset called json\_dataset of the format below. I need to i)  get an aggregate amt1 grouped by cov\_cd  ii) aggregate acount of amt1 grouped by cov\_cd. how would I do this?

Below is my attempt but this doesn't work:

json\_dataset.filter(lambda x: len(x) != 0).flatMap(lambda x: x).map(lambda x: (x\['cov\_cd'\], x\['amt1'\])).reduceByKey(lambda key,value: key+value)

&amp;#x200B;

&amp;#x200B;

    json_dataset = [{'code': 'P',
      'state': 'NY',
      'cov_cd': '001',
      'amt1': '262154',
      'amt2': '0',
    '},
    
    'code': 'P',
      'state': 'NY',
      'cov_cd': '003',
      'amt1': '10000',
      'amt2': '2000',
    '},
    
    'code': 'P',
      'state': 'NY',
      'cov_cd': '005',
      'amt1': '7000',
      'amt2': '3000',
    '},
    
    'code': 'P',
      'state': 'NY',
      'cov_cd': '001',
      'amt1': '9000',
      'amt2': '3000',
    '}]"
974,2019-08-27 12:23:30,1566897810.0,dataengineering,What are some of the core concepts I should become well versed in during my undergrad to DE?,cw1xva,GullibleBuil2ding,,https://www.reddit.com/r/dataengineering/comments/cw1xva/what_are_some_of_the_core_concepts_i_should/,1.0,3.0,0.0,6230.0,"I'm trying to plan out what kind of electives to take to become a Data Engineer, I'll probably take the Distributed Database course, and perhaps a Big Data Technologies course. Are topics related to ML/AI or Operating Systems also important?"
975,2019-08-27 14:49:55,1566906595.0,dataengineering,Creating a datamart?,cw3bqz,turner_prize,,https://www.reddit.com/r/dataengineering/comments/cw3bqz/creating_a_datamart/,7.0,5.0,0.0,6231.0,"If I want to create a few departmental data-marts for reporting, and I already have a data warehouse...am I right in thinking I can just create a new db which will serve as the mart? and load it either during the initial ETL process which feeds the DW, or even a second ETL process once the DW is up to date? I'm having trouble finding anything specific online about building a DM as well as a DW, there is plenty on WHY you might need both but not much on building both at the same time. 

Also, if my DW is built using dimension and fact tables, and those fact tables are built around specific departments, can I use those fact tables as the marts? I'd imagine there will probably be too much data for them to be used as such for quick access to reporting, but just wondering if this is viable in theory."
976,2019-08-27 17:38:17,1566916697.0,dataengineering,"Hi! I have just joined this giveaway organized by Remote-how, where you can win a 1-month workation from Bali. Flights, accommodation, and coworking space are covered! You just go and work remotely from paradise - are you in?Sign up to get a chance to be one of the lucky winners!",cw5ci7,atomi78,,https://www.reddit.com/r/dataengineering/comments/cw5ci7/hi_i_have_just_joined_this_giveaway_organized_by/,0.0,1.0,0.0,6234.0,
977,2019-08-27 18:21:24,1566919284.0,dataengineering,r/datascienceproject is live,cw5x9m,OppositeMidnight,,https://www.reddit.com/r/dataengineering/comments/cw5x9m/rdatascienceproject_is_live/,0.0,0.0,0.0,6234.0,
978,2019-08-27 19:16:54,1566922614.0,dataengineering,An interview on what data engineers need to know about building tools and platforms for data analytics,cw6nw1,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cw6nw1/an_interview_on_what_data_engineers_need_to_know/,19.0,0.0,0.0,6235.0,
979,2019-08-28 09:24:42,1566973482.0,dataengineering,An introduction to the practice and philosophy of Knowledge Graphs,cwgwgp,analyticalmonk,,https://www.reddit.com/r/dataengineering/comments/cwgwgp/an_introduction_to_the_practice_and_philosophy_of/,11.0,0.0,0.0,6264.0,
980,2019-08-28 14:24:28,1566991468.0,dataengineering,The MLOps NYC conference agenda is now online,cwjfye,IguazioDani,,https://www.reddit.com/r/dataengineering/comments/cwjfye/the_mlops_nyc_conference_agenda_is_now_online/,0.0,0.0,0.0,6271.0,
981,2019-08-28 16:16:05,1566998165.0,dataengineering,Supplementing my knowledge as a newly hired data engineer.,cwkoa2,doob10163,,https://www.reddit.com/r/dataengineering/comments/cwkoa2/supplementing_my_knowledge_as_a_newly_hired_data/,1.0,0.0,0.0,6272.0,
982,2019-08-29 03:01:13,1567036873.0,dataengineering,How to sift through the plethora of tool alternatives,cwt2i7,spark58510,,https://www.reddit.com/r/dataengineering/comments/cwt2i7/how_to_sift_through_the_plethora_of_tool/,13.0,22.0,0.0,6285.0,"Our team is in the process of some rapid growth in the near future.  We are expanding our team, looking for experience and in some cases some entry level engineers who are ready to learn and can prove it (if there is interest message me)

We are also in the process of evaluating our stack and will be moving from our legacy tools and warehouse to the cloud (aws).

Where to start. There are tools out there to do everything. We are trying to limit our toolbox as much as possible, looking for proven products that can serve multiple functions. We have pretty much narrowed down to Snowflake and Redshift.

We are looking at Talend, Glue and other native products, Mattilion.

interested in CDC stacks like Attunity

I think that we are pretty set on Python+Spark for any custom dev that can’t be handled quickly with someone else but lack in experience there.

Curious as to what tools you employed for your architecture. Ours will be common to many.

Ingesting into a data lake where the data is discoverable, queryable with metadata and cleansed to some degree for ML.

Curated data will be pushed to a DW with ELT.

My biggest concern is finding something that can manage the workflows, all of them. Want to build dependent workflows that can push/pull data from place to place, managed from a single stack.

I’ve seen the rave on here about Airflow. Are there other products that can do ALL of what I need?"
983,2019-08-29 18:37:18,1567093038.0,dataengineering,Streaming data pipeline design suggestions,cx2jt0,EggShellBuddyPal,,https://www.reddit.com/r/dataengineering/comments/cx2jt0/streaming_data_pipeline_design_suggestions/,1.0,0.0,0.0,6297.0,
984,2019-08-29 20:06:52,1567098412.0,dataengineering,Which course would be more beneficial for me as DE - Distributed Systems in Java or Machine Learning?,cx3qk6,Objectiveb4,,https://www.reddit.com/r/dataengineering/comments/cx3qk6/which_course_would_be_more_beneficial_for_me_as/,1.0,0.0,0.0,6298.0,
985,2019-08-29 20:50:42,1567101042.0,dataengineering,Looking for opinions on NoSql DB,cx4bcq,lime-gt-lemon,,https://www.reddit.com/r/dataengineering/comments/cx4bcq/looking_for_opinions_on_nosql_db/,10.0,14.0,0.0,6299.0,"Hey folks, just as a quick background, I'm not a dataengineer by trade. I operate in the analytics space, and I have used DBs primarily as an end user (so I'm very comfortable with queries). I find myself in a position where I will be potentially setting up infrastructure for the first time for a company that does not have much experience in this area. I have not hidden my skillset, and should I get an offer I would simply be expected to operate within this area.

&amp;#x200B;

With all of that out of the way, I'm looking for a database recommendation for what can be summarized as vehicle data from multiple sources. 

* Think physical engineering data: speeds, rpms, pressures, voltages, etc. 
* There is not an internal, formalized understanding of what data is useful and what is not
* What gets recorded and what doesn't will be inconsistent as a result (so I don't have schema consistency)
* Final use case will be querying the data and analyzing it externally (BI, or python/R) to get insights about the vehicle population, vehicle-to-vehicle comparisons, and individual vehicle stats.

With all of this in mind, I'm currently doing research on columnar databases (hbase in particular) and doc stores (mongoDB); I'm hoping to get something setup tomorrow to run some tests. Can you all offer some recommendations on what I should investigate, and maybe why you would choose it?"
986,2019-08-30 03:40:13,1567125613.0,dataengineering,Data engineering requirement,cx9lj7,farreldjoe,,https://www.reddit.com/r/dataengineering/comments/cx9lj7/data_engineering_requirement/,2.0,5.0,0.0,6311.0,What do I need to be a data engineer I am studying computer engineering and want to know what I can’t do to become a data engineer any minor that will help me and make me get a job
987,2019-08-30 03:58:38,1567126718.0,dataengineering,I think data engineering is quite tough! Probably harder than data science?,cx9sxs,runnersgo,,https://www.reddit.com/r/dataengineering/comments/cx9sxs/i_think_data_engineering_is_quite_tough_probably/,1.0,1.0,0.0,6311.0,
988,2019-08-30 15:49:59,1567169399.0,dataengineering,Apache drill instead of a relational database?,cxgh65,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/cxgh65/apache_drill_instead_of_a_relational_database/,3.0,5.0,0.0,6328.0,"I’ve been using Postgres to consolidate data and prepare clean views which are then loaded to a third party visualisation too. Ultimately, no one uses the database other than my team -all interaction with data is through the visualisation tool. 

1- explore data with pandas in jupyter
2- create tables with sqitch (database migration tool) 
3- create views with dbt (great tool) which ultimately become the “raw data” in the visualisation tool where people can join it to other data etc
4- bulk copy data to csv, upload to the visualisation tools REST API

Many of the files we receive as raw data are reports already, sent by clients or other departments. This is denormalized data and we have no control over the schema changing. Because of this, I am finding little value in using our data migration tool where I need to add all changes in order, including column additions. Loading the data to the visualisation tool is just streaming a csv file through their rest API. 

It seems like my current process, although fairly neat and well documented, has a lot of overhead. 

Should I just load these files to azure blob as parquet or Avro (or even gzip csv) and query the data with apache drill? My only major concern is deduplicating data and making sure I select the latest version of each row (typically, we append only - we do not do much upserting these days). It seems like Drill has a lot of the same features as the database does. 

Since the visualisation tool requires a schema, this can be used as the read schema and I can pay no attention to write schemas (which ensures that I capture all columns without needing to make changes). If someone needs a new column to be added, I need to change the visualisation tool schema anyways. Since the data would be in files, I can loop through and bulk upload the data to the visualisation tool without needing to export it from the database first. 

(My other consideration is to just load everything in jsonb moving forward and then use dbt to pull out the required columns for the final view - much less maintenance)"
989,2019-08-30 17:05:15,1567173915.0,dataengineering,Data Modeling Alternatives to Erwin,cxhefq,FrebTheRat,,https://www.reddit.com/r/dataengineering/comments/cxhefq/data_modeling_alternatives_to_erwin/,1.0,1.0,0.0,6329.0,"I've been keeping an eye out for a functional alternative to Erwin with better cross platform support and a lower price tag, but I'm finding no real competition and a number of FOSS projects that were just abandoned.  Am I missing something?  What tools are folks using for data modeling?  Is Erwin that good that they can dominate the market and command a substantial fee, or are people using combo tools that do data modeling + other features and Erwin is just the only tool out there that's specialized?"
990,2019-08-30 19:54:24,1567184064.0,dataengineering,Master thesis in Data Engineering,cxjmi7,dadadima94,,https://www.reddit.com/r/dataengineering/comments/cxjmi7/master_thesis_in_data_engineering/,1.0,0.0,0.0,6333.0,"I am a MSc student in Data Science and Engineering and I am in the first semester of the 2nd year.

In some days I will have a meeting with a really promising company and most likely I will have to propose a good master thesis topic in order to get an internship and the thesis possibility.

My initial idea was *feature engineering for machine learning* since it seems a topic that will be much needed in the future. Any suggestion or different ideas will be very appreciated, thank you in advance."
991,2019-08-31 01:55:24,1567205724.0,dataengineering,FREE hands-on Workshop in Seattle - Create a data warehouse in Amazon Redshift with Etleap and AWS,cxo9if,AshleyEtleap,,https://www.reddit.com/r/dataengineering/comments/cxo9if/free_handson_workshop_in_seattle_create_a_data/,4.0,0.0,0.0,6336.0,"Calling analytics-focused data engineers!

We're joining forces w. @awscloud in Seattle on 10/29 to give a FREE hands-on workshop where you'll learn how to set up an analytics-ready data warehouse on Redshift.

Save your seat today by registering: [**https://info.etleap.com/devdaysoct29**](https://info.etleap.com/devdaysoct29)"
992,2019-08-31 07:53:42,1567227222.0,dataengineering,Advice on managing messy data features,cxrwow,runnersgo,,https://www.reddit.com/r/dataengineering/comments/cxrwow/advice_on_managing_messy_data_features/,1.0,1.0,0.0,6338.0,
993,2019-08-31 11:30:18,1567240218.0,dataengineering,Looking for data pipeline design suggestions,cxtljn,EggShellBuddyPal,,https://www.reddit.com/r/dataengineering/comments/cxtljn/looking_for_data_pipeline_design_suggestions/,12.0,8.0,0.0,6338.0,"I'm trying to come up with a design and architecture of a data pipeline for getting JSON  data streams for device stats (uptime, status, last seen connected  timestamps etc.) and want to build a dashboard to show those stats  within 5 min intervals. I was thinking of exploring both existing cloud  service solutions (aws/gcp services) and also a complete on-prem  solution. So, with that being in mind, what should be ideal tools for  this?

This is what I was thinking in terms of AWS for instance:

* AWS  native: \[Kinesis\]: \[Firehose &gt; Data Analytics &gt; Data Streams\]  &gt; Table (Dynamo/RDS with Lambda) &gt; Dashboard (Quicksight, or JDBC)
* Linux (Docker): Kafka &gt; Spark Streaming (or Flink) &gt; Postgres View &gt; Dashboard (JDBC)

I'm  trying to do any pipeline coding in Python and SQL and was wondering  how feasible either of these approaches would be in terms of build time,  scaling, performance, maintenance and multi-tenant model. Any  suggestions or alternatives are welcome and thanks for looking."
994,2019-09-01 03:10:40,1567296640.0,dataengineering,Does this look like a data engineer's resume?,cy33j7,dash_365,,https://www.reddit.com/r/dataengineering/comments/cy33j7/does_this_look_like_a_data_engineers_resume/,24.0,31.0,0.0,6342.0,
995,2019-09-02 00:05:30,1567371930.0,dataengineering,Technical Interviews for DE roles,cyfk48,lseactuary,,https://www.reddit.com/r/dataengineering/comments/cyfk48/technical_interviews_for_de_roles/,1.0,1.0,0.0,6356.0,
996,2019-09-02 04:10:25,1567386625.0,dataengineering,Has anyone done the Data Engineer track on dataquest,cyia1w,REorganize009,,https://www.reddit.com/r/dataengineering/comments/cyia1w/has_anyone_done_the_data_engineer_track_on/,17.0,12.0,0.0,6361.0,"I've been looking for a good introductory course to learn more about this field and there doesn't seem to be much. I found a course on dataquest and it seems to be the most affordable one. Has anyone done that program, is it good for a beginner?"
997,2019-09-02 05:55:45,1567392945.0,dataengineering,Workflow orchestration suggestion needed,cyjdh8,nariver1,,https://www.reddit.com/r/dataengineering/comments/cyjdh8/workflow_orchestration_suggestion_needed/,2.0,13.0,0.0,6363.0,"Hi everyone,

I would like to know if there are alternatives to Airflow in regards to scheduling processes.

My stack is as below:

* ETL is done with Pentaho Data Integration mostly, we are working to create a docker image so it can be launched without a server.
* DB is Postgresql/Redshift

Our requirements is that jobs can be simple to schedule and support dependencies.

I'm also checking alternatives as Cronicle, Easy Scheduler, resque-scheduler and Chronos, but maybe we are overseeing others.

Appreciate your help."
998,2019-09-03 16:11:54,1567516314.0,dataengineering,An interview with Pete Soderling about building and growing the Data Council events and helping engineers build businesses,cz4oyw,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/cz4oyw/an_interview_with_pete_soderling_about_building/,13.0,1.0,0.0,6376.0,
999,2019-09-03 18:18:27,1567523907.0,dataengineering,Astronomer v0.10 released,cz68y9,rywalker,,https://www.reddit.com/r/dataengineering/comments/cz68y9/astronomer_v010_released/,1.0,0.0,0.0,6379.0,
1000,2019-09-04 22:19:33,1567624773.0,dataengineering,Udacity Data Engineering Nanodegree - Any Improvements?,czpgfq,Rhino4910,,https://www.reddit.com/r/dataengineering/comments/czpgfq/udacity_data_engineering_nanodegree_any/,1.0,2.0,0.0,6399.0,
1001,2019-09-05 00:54:15,1567634055.0,dataengineering,Data Engineering,czrfhc,ketanXDED,,https://www.reddit.com/r/dataengineering/comments/czrfhc/data_engineering/,0.0,1.0,0.0,6403.0,"I’m new to Data Science and if I want to be a successful Data Engineer, where should I start?"
1002,2019-09-05 07:02:10,1567656130.0,dataengineering,Question: How to Manage One Source of Truth Within a Company,czvqlv,TheCauthon,,https://www.reddit.com/r/dataengineering/comments/czvqlv/question_how_to_manage_one_source_of_truth_within/,44.0,36.0,0.0,6411.0,"I’m not sure if this is the best community to post this question but it feels like the data engineers are ultimately the ones that have the most control over this topic:

Background:

I’m at a growing tech company with a decentralised structure to analytics.  There is an analytics team (BI) with a few analysts and data engineers but then each of the departments have their own  group of analysts. For example Marketing has 4, CX has 3, Sales has 3 etc.

There has been a problem within the company around inaccurate data, misrepresented numbers, changing definitions etc.  The CEO asked the analytics team (BI) to clean up the mess and get to one source of truth.

Right now our data is structured in the following layers:

Data lake with raw data
Modelled facts and attributes
Tableau SQL extracts

We currently have a production cluster and a dev cluster.

We let all the departments have scratch space in DEV where they can build tables, test data, do adhoc analysis etc. but we don’t let them push anything from DEV into Production and we don’t let them build Tableau visualisations off their scratch spaces in DEV.

Lately we have been getting a bit of pushback from analysts within different departments that want to start engineering their own data sets and start building reports off those sets.

My question is what strategies have you seen that work well in maintaining one source of truth across all 3 layers and yet give departments enough flexibility to do their work?

We have talked about a few strategies such as:

1. Let everyone build whatever they want but only lock down certain one source of truth data sets, tableau extracts etc.  This method would require visually showing what reports meet this standard and flagging the one source of truth data sets.

Pros: BI team doesn’t have to police as much, departments have more flexibility and speed to push out adhoc stuff 

Cons: organisation of data warehouse becomes messy, and lots of unsupported and unvetted data sets, executives will need to be educated to know what reports are stamped with the one source of truth vetting, 

2. Require all teams to work with the data engineers to build their data sets and push to production. Limit production to only things that have been vetted by the BI data engineers.Limit tableau visualisations to only Production

Pros: Keeps data warehouse clean, all data objects are vetted and validated, no worries about incorrect data sets, No worries about any executive reports getting pushed up by rogue data sources

Cons: slows down some of the adhoc work, departments may feel too restricted

I would love to hear how other companies are managing this and what works well."
1003,2019-09-05 16:20:09,1567689609.0,dataengineering,Facebook DE onsite interview in two weeks. What can I do to prepare?,d00qcy,LectricVersion,,https://www.reddit.com/r/dataengineering/comments/d00qcy/facebook_de_onsite_interview_in_two_weeks_what/,12.0,24.0,0.0,6445.0,"Against all of my expectations, I've landed an onsite interview with Facebook. They contacted me via LinkedIn, I had my phone interview (4 SQL questions and 4 Python, answered 4/4 on SQL and 3/4 on Python) and now progressing to the onsite phase. It will be a half hour ""culture fit/product sense"" interview followed by three one hour ""full stack"" DE interviews. If you couldn't tell from this opening paragraph, I'm having full-on imposter syndrome.

In terms of my skillset, my main background is as a traditional BI developer - MS SQL, Integration Services (with a bit of Azure Data Factory), Star Schema and some light Python to call API endpoints - all in the name of end-to-end managing several huge data solutions for marketing data. In February this year I landed my first DE post for a start-up in London, and it's my first real exposure to Hive, Airflow, ""production-level ""data lakes and more advanced Python. 

In short, I feel a bit out of my depth. I feel like I have very good product sense and enough of a light spread across the DE toolkit to come up with broad solutions, but I'm worried that my lack of experience in tools such as Spark, Numpy and Pandas is going to hold me back. 

I'm obviously massively excited for this opportunity and want to make sure I do my best. I'm committed to studying if needed. What advice would you have for me? Am I worrying too much about my relative, self described ""inexperience""?"
1004,2019-09-05 17:56:47,1567695407.0,dataengineering,CICD with databricks?,d01vyr,doob10163,,https://www.reddit.com/r/dataengineering/comments/d01vyr/cicd_with_databricks/,6.0,3.0,0.0,6451.0,I was wondering if anyone had experience and wanted to share some methods that their team came up with regarding integration of databricks with their CICD pipeline. My company uses bitbucket server (not directly supported by databricks) and jenkins pipeline. I have read several articles regarding this but was wondering about how other companies were solving this problem.
1005,2019-09-05 19:13:49,1567700029.0,dataengineering,What are some most recent breakthroughs in DE?,d02w2h,Shawonder,,https://www.reddit.com/r/dataengineering/comments/d02w2h/what_are_some_most_recent_breakthroughs_in_de/,2.0,7.0,0.0,6452.0,This article[article](https://link.medium.com/qo7ZCc1SJZ) highlights some of the key technologies that changed the landscape of DE. So I wonder what are some of the most recent breakthroughs that are not in the article.
1006,2019-09-06 01:32:16,1567722736.0,dataengineering,Monitoring Apache Airflow with TIG stack: part 1,d07xnp,marclamberti,,https://www.reddit.com/r/dataengineering/comments/d07xnp/monitoring_apache_airflow_with_tig_stack_part_1/,0.0,0.0,0.0,6462.0,
1007,2019-09-06 01:47:04,1567723624.0,dataengineering,Is a BSc in Stats a good choice if i want to become a Data Engineer?,d083d2,Soaisis,,https://www.reddit.com/r/dataengineering/comments/d083d2/is_a_bsc_in_stats_a_good_choice_if_i_want_to/,10.0,38.0,0.0,6461.0,"I know that it would propably be best to go for a CS degree, but even so, is a Stats undegrad a good alternative? The program that i want to enroll has these courses that i think are very relevant to data engineering:

* Data Structures

* Algorithms

* Databases

* Database design

* Data mining

* Information Systems analysis and design

The rest are pretty much classic Statistics stuff (also with programming courses in R and Python). What do you think, is there any course that you would consider crucial for data engineering and its missing?"
1008,2019-09-06 02:47:33,1567727253.0,dataengineering,Arrived for my first day of work and my 6 month contract was swapped out for a 3 month. Is this sketchy?,d08rce,pythonmine,,https://www.reddit.com/r/dataengineering/comments/d08rce/arrived_for_my_first_day_of_work_and_my_6_month/,1.0,0.0,0.0,6462.0,"I had worked here previously but quit due to low pay.

I finished my masters and they agreed to meet me halfway on my requested pay. I never received a contract while negotiating. They gave it to me the day I arrived, claiming they had emailed it to me two weeks before. I started my first week while my manager is on vacation. 

When I arrived, they gave me the 3 month contract instead of the 6 month one they offered me over phone and email.

Is this sketchy?"
1009,2019-09-06 18:33:22,1567784002.0,dataengineering,Question: User Agent parsing and Analytics warehousing.,d0i8h2,akashi_07,,https://www.reddit.com/r/dataengineering/comments/d0i8h2/question_user_agent_parsing_and_analytics/,1.0,4.0,0.0,6470.0,
1010,2019-09-07 03:50:18,1567817418.0,dataengineering,Master Student looking for Personal Projects &amp; Guidance to be a Data Engineer,d0pcmq,ptndoss,,https://www.reddit.com/r/dataengineering/comments/d0pcmq/master_student_looking_for_personal_projects/,19.0,19.0,0.0,6473.0,"Hi Everyone,

Currently am a doing my Masters' degree in bay area. I have  5+ years of  experience as a ETL developer primarily on Informatica(Data Integration). My course work in masters program is more towards Software Engineering but am more interested to be a Data Engineer. So, am planning to do personal projects and self learn technologies required to be a Data Engineer. I have 2 more semesters to go and my target is to get comfortable in big data technologies by then and land in top companies as a Data Engineer.

Here is my plan - 

Am good in Java but not python.So, my plan is to learn Python by solving leetcode problems in python. 

Then learning Hadoop &amp; PySpark technologies for handling data in distributed systems, 

Airflow(I see this in many forums).

Would appreciate any suggestions or lead to plan my learning path with a personal projects to get good hands on experience.

Please suggests any kind of resources which will be helpful for me to land as a DE  in top tier companies.

Thank you!!!"
1011,2019-09-07 21:05:03,1567879503.0,dataengineering,Master student here who wanna be a Bigdata Engineer!,d0zcqz,kkvv1111,,https://www.reddit.com/r/dataengineering/comments/d0zcqz/master_student_here_who_wanna_be_a_bigdata/,5.0,2.0,0.0,6489.0,"Hi guys

First of all, sorry for my broken English.

I'm a Master student, joined this community today to see how people in the DE field think and live in the world. and also I'd like to get some energy from you guys to keep studying without being tired.

I'm studying Data Engineering only for a year. A year ago, I didn't know about DE (cuz I was an Android Developer) and I just started studying DE because I was attracted to Bigdata at a data conference. Doing my research in my lab, I've built Hadoop cluster, Spark cluster with Docker and made them connect to MongoDB. I studied HDFS, Distributed system concepts, and Spark streaming as well.

DE is new to me still, hope I get some advice about what I should study and prepare to be a Bigdata Engineer. 

am also wondering how you guys studied in the past and how to stack your career.

It's too general question, sorry but please share your experience and some helpful advice to me. I'd also like to meet a friend or mentor to talk about engineering, school, career, experience, and personal opinion.

I read some recruiting sites to check what kind of data engineers companies want, they want Hadoop, Spark experience with real 'big data'.

My plan is, for now, starting a personal Spark project on Docker and push into my git.

God bless you guys and thanks!!

&amp;#x200B;

p.s.

I read [this post](https://www.reddit.com/r/dataengineering/comments/d0pcmq/master_student_looking_for_personal_projects/), the answers helped me a lot. I think I should read [awesome data engineering](https://github.com/igorbarinov/awesome-data-engineering) and [road map](https://github.com/hasbrain/data-engineer-roadmap) first!"
1012,2019-09-08 23:57:25,1567976245.0,dataengineering,Entry into Machine Learning Engineering?,d1gtyx,aashwin93,,https://www.reddit.com/r/dataengineering/comments/d1gtyx/entry_into_machine_learning_engineering/,11.0,4.0,0.0,6500.0,"Is data engineering a good fallback option if I'm unable to land MLE/Data Science roles? I understand that I need to stay abreast of some big data frameworks and/or have specific projects to be able to break in. I'm starting to feel MLE roles are few and far between and my portfolio for a DS/Analytics job is not upto the mark IMO.   


P.S My post history might seem like its all over the place. Because I am. I would appreciate any help I can get."
1013,2019-09-09 23:00:04,1568059204.0,dataengineering,Do I need to learn software development cycle? If so where?,d1w5uz,dash_365,,https://www.reddit.com/r/dataengineering/comments/d1w5uz/do_i_need_to_learn_software_development_cycle_if/,12.0,8.0,0.0,6516.0,"I am a growing data engineer. I am in graduate school with no professional experience as of now. I am concentrating on building ETL pipelines with airflow. I am learning SQL and python for most of my time. Even though I know to use these technologies, I am worried if im missing out my job opportunities because of not understanding software dev cycles as a number of data engineer jobs are listed as software engineers? Should I learn that too? If so can you guys suggest good resources?"
1014,2019-09-10 04:30:06,1568079006.0,dataengineering,An interview about building the Vector project to unify delivery of logs and metrics for better system observability,d20lzr,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/d20lzr/an_interview_about_building_the_vector_project_to/,5.0,0.0,0.0,6526.0,
1015,2019-09-10 16:23:24,1568121804.0,dataengineering,How should I connect Postgresql with Elasticsearch to visualize in kibana the insert/delete/updates made in the DB?,d27h6q,8589934591,,https://www.reddit.com/r/dataengineering/comments/d27h6q/how_should_i_connect_postgresql_with/,2.0,6.0,0.0,6541.0,
1016,2019-09-10 17:07:11,1568124431.0,dataengineering,Data Lake Architecture - How do I handle Raw Data and privacy,d280ec,doob10163,,https://www.reddit.com/r/dataengineering/comments/d280ec/data_lake_architecture_how_do_i_handle_raw_data/,16.0,8.0,0.0,6543.0,"How does this work with GDPR/compliance issues? I was wondering what a solid data governance strategy would be regarding being able to delete from raw data with changes in compliance in the future, as well as best practices for dealing with raw data in general.

I'm also wondering how to set up the architecture in such a way that if we deem a feature to be useful in the future we can pull it into analyses and data warehouses easily."
1017,2019-09-10 19:46:27,1568133987.0,dataengineering,Advice on building a batch data pipeline,d2acxn,Somuchgoodfood,,https://www.reddit.com/r/dataengineering/comments/d2acxn/advice_on_building_a_batch_data_pipeline/,3.0,10.0,0.0,6544.0,"I've been tasked to build a data pipeline for my company.  However, I have relatively little experience and not many people to get help from.   Here is my task.

Input: Manual drops of decent size raw binary data  \~2-3 times/week

Output:  Data stored in SQL database

My initial design:

1. Use Airflow as my workflow scheduler
2. Create Spark jobs that Airflow would execute.  As of now, I see at least 3 types of Spark Jobs
   1. Parse the binary
   2. Manipulate the data
   3. Store into SQL server
3. Basically, each DAG would perform the above step 2 for a single file.  So 10 files means 10 DAG(?)

&amp;#x200B;

Couple questions

1. I see lots of people using Docker/Kubernetes alongside Airflow and Spark.  Is a container applicable in my case?
2. I'd like to implement ML where I can.  Would that happen while I'm manipulating the data BEFORE I load it into SQL, or after?  Or both?

&amp;#x200B;

Any advice is greatly appreciated.  Thanks!"
1018,2019-09-11 00:45:54,1568151954.0,dataengineering,Merge same grain facts or keep seperate,d2f48j,InfinitePermutations,,https://www.reddit.com/r/dataengineering/comments/d2f48j/merge_same_grain_facts_or_keep_seperate/,1.0,0.0,0.0,6546.0,
1019,2019-09-11 01:27:03,1568154423.0,dataengineering,Loading MySQL backup files into BigQuery — straight out of Cloud SQL,d2fr5c,fhoffa,,https://www.reddit.com/r/dataengineering/comments/d2fr5c/loading_mysql_backup_files_into_bigquery_straight/,1.0,0.0,0.0,6547.0,
1020,2019-09-11 02:29:32,1568158172.0,dataengineering,Will they give me coding problems in a data engineer interview?,d2gnyw,nLoa,,https://www.reddit.com/r/dataengineering/comments/d2gnyw/will_they_give_me_coding_problems_in_a_data/,1.0,2.0,0.0,6548.0,
1021,2019-09-11 05:38:53,1568169533.0,dataengineering,What frameworks or tools should I avoid?,d2j9ml,Atacupcakeshop,,https://www.reddit.com/r/dataengineering/comments/d2j9ml/what_frameworks_or_tools_should_i_avoid/,12.0,15.0,0.0,6550.0,"Are there any tools which I should avoid because they are outdated or don’t receive proper support?

For example - is Spark something I should dive into? Apache Drill? Or are there new tools or frameworks which solve similar issues in a better way? 

I just want to double check before I decide on a tool. My primary use case is to move a lot of staging data to object storage in parquet files and then do analysis and transformation on them. This is instead of loading the data into PostgreSQL because the creation and maintenance of the database table/ migration scripts tends to take a disproportionate amount of time.  I kind of just want to query the raw data or use pandas/ dataframes."
1022,2019-09-11 17:04:01,1568210641.0,dataengineering,Deciding if an external data source should go in a data warehouse,d2qo6c,today_is_tuesday,,https://www.reddit.com/r/dataengineering/comments/d2qo6c/deciding_if_an_external_data_source_should_go_in/,3.0,3.0,0.0,6555.0,
1023,2019-09-11 18:24:42,1568215482.0,dataengineering,The future of Prefect,d2ruva,isaacmi,,https://www.reddit.com/r/dataengineering/comments/d2ruva/the_future_of_prefect/,6.0,6.0,0.0,6556.0,"What do you think the future of prefect.io looks like.

Is it worth investing the time to learn it?

I actually tried and it's simple and clean to use."
1024,2019-09-11 19:39:35,1568219975.0,dataengineering,Columnar DB Best Practice,d2szxg,LateOverall,,https://www.reddit.com/r/dataengineering/comments/d2szxg/columnar_db_best_practice/,3.0,2.0,0.0,6558.0,Is the star schema the most optimal for columnar databases? Or would having a few large tables that are built wider (more columns) be better?
1025,2019-09-12 04:43:46,1568252626.0,dataengineering,Is Data Modeling/Architecture a Big Part of the Job?,d30s0a,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/d30s0a/is_data_modelingarchitecture_a_big_part_of_the_job/,6.0,7.0,0.0,6560.0,I just started a new job and didnt realize there would be so much of this.
1026,2019-09-12 17:18:46,1568297926.0,dataengineering,What to expect in a junior data engineering role interview?,d38kjm,creamiceandcakes,,https://www.reddit.com/r/dataengineering/comments/d38kjm/what_to_expect_in_a_junior_data_engineering_role/,11.0,15.0,0.0,6570.0,"I have an upcoming interview and the only thing I have been told is that I will be given a real world case study where I have to solve some sort of problem (might be given a dataset, not sure)

What should I expect? Can anyone give me examples of such case studies?"
1027,2019-09-12 19:08:32,1568304512.0,dataengineering,countBy for an RDD,d3a44e,metalloidica,,https://www.reddit.com/r/dataengineering/comments/d3a44e/countby_for_an_rdd/,1.0,0.0,0.0,6572.0,"I have an RDD data structure in the following format (list of tuples):

&amp;#x200B;

 \[ (‘xxxx91’, \[{item\_code:20, item\_cat: 30,}, {item\_code:10, item\_cat:30},  {item\_code:20, item\_cat:10}\] , \[{‘company\_code’: 20, ‘company\_info’  : 203}\]) .....

&amp;#x200B;

I need do do a countBy of the following fields: item\_code, company\_code and company\_info. 

so output would look like this:

&amp;#x200B;

item\_code            company\_code          company\_info             count

20                                   20                          203                               xxxx

10                                  20                          203                                xxxx

&amp;#x200B;

Notes: the first element of tuple is an id, the second item is a list of one or many dictionaries and a third element is a list with a single item(dictionary)

&amp;#x200B;

I'm new to spark but here is my attempt so far:

 Output  = full\_rdd\_test.map(lambda x: (x\[1\]\['item\_code\], x\[2\]\[‘company\_code'\]).flatMapValues(lambda x: x) 

&amp;#x200B;

I have not been able to get this to work. any help would be appreciated"
1028,2019-09-13 10:37:38,1568360258.0,dataengineering,Backend architecture recommendations project,d3ltho,rmaun,,https://www.reddit.com/r/dataengineering/comments/d3ltho/backend_architecture_recommendations_project/,9.0,2.0,0.0,6586.0,"Hi,

I am currently working on a project where I have to define the backend architecture and would like to hear your recommendations. Is this a good subreddit for this or would you ask somewhere else? Anyway, here some infos about the project:

## Requirements
I need to store big datasets, currently up to 100MB, but I would like to support 1GB datasets as well. These are multiple arrays of floats and it would be good to only retrieve parts of them. These will add up, but at most +1GB/day
We use tensorflow to infer and apply models. I need workers for this and they should be scaled dynamically for concurrent users.

## Current architecture
This is our current tech stack, it got initially defined by someone else who left the company for a prototype and got expanded by me.
* Managed kubernetes on azure cloud
* Django webserver (using gunicorn and nginx)
* Unmanaged postgres to store Django tables
* Also stores datasets as JSON in text field in table, this has to be changed
* Volume with original datasets
* I think we do not really need them, this is currently only used to transfer them from the backend (Django) to a worker which imports them.
* [Celery](http://www.celeryproject.org/) as task queue
* This is the usual recommendation for Django projects and was used in a previous project
* Workers to import and process dataset
* Workers to run tensorflow on them
* Rabbitmq as message broker

## Problems
There are some problems with the current architecture:
* Storing data like this works currently, but I do not expect this to scale
* Can you recommend a database for this? Should I store the files in a volume or use a DB? Same DB as for Django? NoSQL or relational? Also there are some [managed databases on azure](https://azure.microsoft.com/en-us/product-categories/databases/), do you think any of this is a good and cost efficient idea?
* Celery and tensorflow do not work together, this leads to some bugs with multithreading. In /tensorflow issues they mention that this is unsupported.
* Any recommendations on how to continue? Possibilities I can think of are to create a sidecar container for tensorflow. How then to communicate with the celery worker? Another possibility is to communicate with Django over rabbitmq directly.
* Can I get rid of the volume to store the original files? How to transfer them to to the import worker?
* Currently only a single worker for tensorflow, I guess I can scale this automatically using kubernetes, but I have not looked into this yet, any tipps?

When you would start fresh, which technologies would you use? We use Django because most here know Python. Would you use a message queue like rabbitmq or microservices with REST APIs? Any kubernetes recommendations for scaling the tensorflow workers?

Thanks :)"
1029,2019-09-13 16:39:02,1568381942.0,dataengineering,"Hello guys, need guidance",d3pegh,yumthescum,,https://www.reddit.com/r/dataengineering/comments/d3pegh/hello_guys_need_guidance/,0.0,4.0,0.0,6588.0,"Hello guys, first sorry for format Im from mobile, mods feel free to remove if this doesnt belong here.
So Im finishing with python and I wanna dive into data engineering, I need help with guidance, what to start first with what should I get my focus on, what tools you recommend etc..
Thank you for your time and thank you in advance"
1030,2019-09-13 17:14:06,1568384046.0,dataengineering,Streaming dataset architecture (Python),d3pusy,Radon-Nikodym,,https://www.reddit.com/r/dataengineering/comments/d3pusy/streaming_dataset_architecture_python/,4.0,4.0,0.0,6588.0,"I have a stream of data that I am subscribed to in Python. A few minutes later more information is known (the live feed is junky), and this is pushed into a SQL database. I want to listen to the feed, and then periodically update the older stuff with the SQL info when possible. What data structures should be used to do this in Python? Do I stream into a pandas dataframe (appending rows?) And then periodically replace the old chunk with a SQL query? What's the best way to handle the concurrency issues with this? Is there a better data structure than a data frame?"
1031,2019-09-13 21:03:45,1568397825.0,dataengineering,Data engineering in the world of Virtual Reality and live streaming,d3t3m3,hpfrood,,https://www.reddit.com/r/dataengineering/comments/d3t3m3/data_engineering_in_the_world_of_virtual_reality/,6.0,0.0,0.0,6591.0,"An interesting, in-depth look into how to architect for real-time event analytics in a [live streaming VR example](https://rockset.com/blog/real-time-analytics-virtual-reality-live-streaming/).

&amp;#x200B;

![img](19o0ej1zhem31)"
1032,2019-09-14 00:48:58,1568411338.0,dataengineering,Hiring Data engineers in Roanoke VA,d3w669,spark58510,,https://www.reddit.com/r/dataengineering/comments/d3w669/hiring_data_engineers_in_roanoke_va/,9.0,10.0,0.0,6596.0,Anyone who would consider relocating to Roanoke VA? We are looking to expand our Data Warehouse team pretty significantly in the very near future.  Looking for a blend of senior and junior developers who bring different experiences and skill sets to the table to be value add to our team
1033,2019-09-14 12:16:21,1568452581.0,dataengineering,Thoughts on Druid datawareshousing tool,d42ymp,jyoff,,https://www.reddit.com/r/dataengineering/comments/d42ymp/thoughts_on_druid_datawareshousing_tool/,6.0,6.0,0.0,6598.0,"Hi All,

I'd like to use Druid in my own project which I deveop for practicing purposes. Initially I wanted to use Redshif but since my goal is to dockerize the whole application and Redshift doesn't have an official image in the hub(as far as I know), I want to use Druid instead.

I have no idea how it works, need to learn about it. 

I'd like to hear thoughts of someone who used it. 
Do you have any recommendations/tips or maybe you'd suggest other tools ?"
1034,2019-09-14 16:36:58,1568468218.0,dataengineering,Actian Avalanche,d459p5,spark58510,,https://www.reddit.com/r/dataengineering/comments/d459p5/actian_avalanche/,1.0,0.0,0.0,6601.0,Anyone have experience with Actian Avalanche or its tools? Starting analysis on cloud DW platforms and just curious if someone has opinions without all the sales fluff?
1035,2019-09-15 22:33:20,1568576000.0,dataengineering,I have an interview next week for a DE position. What are some good questions to ask the interviewer (MLE Lead)?,d4p9mx,MrRitmo,,https://www.reddit.com/r/dataengineering/comments/d4p9mx/i_have_an_interview_next_week_for_a_de_position/,22.0,6.0,0.0,6617.0,"Hey guys, I'm a senior engineer and coming from a backend background, I'm pretty excited about the data engineering field and I view a transition into a DE role as the ideal next move in my career at this point. For that to happen, I've been doing a lot of reading and studying on my own: Python, SQL, Spark/Kafka, data warehousing, ETL/ELT, cloud, distributed systems..etc

It seems hard to get into this field without direct professional experience in DE, but I was lucky to land this interview with a company and I really want to do everything I can to get my foot in the door.

So, from your experience, what are some thoughtful questions to ask the interviewer? and what are some things to look out for?"
1036,2019-09-16 01:24:15,1568586255.0,dataengineering,REST as real-time data source,d4rmqc,IgnatiusNostradamus,,https://www.reddit.com/r/dataengineering/comments/d4rmqc/rest_as_realtime_data_source/,3.0,5.0,0.0,6619.0,
1037,2019-09-16 02:34:46,1568590486.0,dataengineering,Way to detect integrated data in a datalake?,d4shzh,KytsuboZetbou,,https://www.reddit.com/r/dataengineering/comments/d4shzh/way_to_detect_integrated_data_in_a_datalake/,5.0,0.0,0.0,6619.0,"so this is my first time manipulating this type of data and i'm looking to learn how to do it

i have this schema that i have to follow (which is made in .owl )

and i'm looking for a way to integrate that schema into my datalake

and also another question ,is there a way to autodetect added or removed files onto the data lake ,i found an extension called Owlready2 that could help (with python), but i'm struggling to find how to integrate it

​
thank you :)"
1038,2019-09-16 17:36:51,1568644611.0,dataengineering,Portability of Business Logic,d51j72,smckinley903,,https://www.reddit.com/r/dataengineering/comments/d51j72/portability_of_business_logic/,1.0,0.0,0.0,6628.0,
1039,2019-09-18 01:59:07,1568761147.0,dataengineering,Can you learn platform agnostic Data Engineering?,d5oqez,work_acc_1,,https://www.reddit.com/r/dataengineering/comments/d5oqez/can_you_learn_platform_agnostic_data_engineering/,1.0,0.0,0.0,6649.0,"I'm learning a lot as an engineer but wanted to continue studying a bit first thing in the morning. As I look through online courses I find most are nested in a cloud provider environment or tool-set. That is, it's easy to take courses to be a DE on Google CP, or AWS, or with some specific tool like airflow. 

What are platform/tool agnostic skills a data engineer can work on and does anyone have any resources for them?"
1040,2019-09-18 15:53:15,1568811195.0,dataengineering,"YouTube's Database ""Procella""",d5xcjz,marklit,,https://www.reddit.com/r/dataengineering/comments/d5xcjz/youtubes_database_procella/,2.0,0.0,0.0,6658.0,
1041,2019-09-18 16:04:41,1568811881.0,dataengineering,An interview about using stateful computation on data streams with the SwimOS kernel to improve your analytics,d5xhqm,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/d5xhqm/an_interview_about_using_stateful_computation_on/,3.0,0.0,0.0,6658.0,
1042,2019-09-18 18:56:43,1568822203.0,dataengineering,what source control does your team use ?,d5ztcy,kittie_thrower,,https://www.reddit.com/r/dataengineering/comments/d5ztcy/what_source_control_does_your_team_use/,1.0,0.0,0.0,6660.0,
1043,2019-09-18 22:18:25,1568834305.0,dataengineering,"I'm the new designated Data Guy in my company, and I need some help",d62ouz,weRadio,,https://www.reddit.com/r/dataengineering/comments/d62ouz/im_the_new_designated_data_guy_in_my_company_and/,14.0,16.0,0.0,6665.0,"I work in a small company, where we have about 15 databases running in Microsoft SQL. I was tasked with creating a Data Warehouse in Redshift from the information contained in theses databases. The databases have the same structure, but each contains data from a different operation.  

At first, I developed an airflow pipeline with a pipeline that first detected all the databases available with a sql query, and then started multiple dynamic tasks for the extraction of each database simuntaneously and after that aggregated them, transformed them and finally loaded them to Redshift. However, i'm having a lot of headache with Airflow, and was looking for a cloud-based solution like Databricks to perform this task. But I do not know how I can perform this pipeline   that detects the amount of databses and automatically performs the extract query on all of them at the same time. Can anyone help me understand if Databricks is the right tool for me?"
1044,2019-09-18 23:27:04,1568838424.0,dataengineering,Two csv files with contradicting data in order to get me to slip up.,d63n0f,DanCamarda,,https://www.reddit.com/r/dataengineering/comments/d63n0f/two_csv_files_with_contradicting_data_in_order_to/,1.0,0.0,0.0,6666.0,"I'm currently taking an at home ETL test for a job interview. They gave me two different csv files that contain tables, with the only same value being ID. John as the ID as 1 in both tables but Jane has the ID of 4 in one table and 1 in the other.

I'm assuming this is a part of the test, so I'm wondering what the best way to go about that is and what they're expecting.

Thanks!"
1045,2019-09-19 01:35:29,1568846129.0,dataengineering,Searching for Cloud Architect and Engineers,d64s3k,spark58510,,https://www.reddit.com/r/dataengineering/comments/d64s3k/searching_for_cloud_architect_and_engineers/,1.0,0.0,0.0,6668.0,"Putting this out there.  In the near future, our org will be posting positions for a Cloud Architect and a few Cloud Engineers (AWS).  Our budget can’t afford super-seasoned pros who are looking for 175-200k.  So we are looking for folks who have SOME experience who are ready to that step and have the chops.  We are located in Roanoke Virginia. Our “Principal” Data Architect is open to having preliminary discussions with anyone who is interested and has experience.  DM me if interested and feel free to pass along."
1046,2019-09-19 05:05:08,1568858708.0,dataengineering,Transitioning to Data Engineering as a Data Analyst,d67eei,thrownaway694200,,https://www.reddit.com/r/dataengineering/comments/d67eei/transitioning_to_data_engineering_as_a_data/,16.0,5.0,0.0,6670.0,"Hi everyone! I recently graduated with a B.S. in Mathematics and a minor in Computer Science and took a job with a company as a data analyst for their IT department.  I've always envisioned myself working a few years as an analyst and then transitioning into a data science role, but over the last month or so, I've begun second-guessing myself as I've seen more and more articles about the increasing difficulty of landing a job in data science as there seems to be a flood of candidates with professional degrees in Math/Business Analytics/Comp Sci/etc.  That said, I am very interested in maybe focusing more on building my skillset around data engineering concept.  I read Jesse Anderson's book on switching careers to Big Data but still have a few questions and was wondering if you guys might be able to help...

1. With my computer science minor, I learned C++ and feel as if I have a pretty strong coding foundation.  I'm working to get myself familiarized with Python, but would you guys recommend any other languages that I should be learning?

2. In terms of data engineering tools specifically, I have a good SQL background in terms of querying and took a class in RDBMS so I have a basic understanding of database architecture.  I've read quite a few articles about necessary tools in data engineering but each one seems to recommend prioritizing some over the others.  For those working in the field now, what do you feel are the most important to learn, whether it be Hadoop, Spark, Hive, Kafka, cloud technologies, etc.?

3. What avenues did you find the most helpful and worthwhile in your learning process (textbooks, online courses, etc.)?  Are there any specifically that you would recommend (definitely interested in purchasing or renting a few of the O'Reilly books)?

4. I understand that one of the best ways to build up my skills would be to perform independent projects, and I currently have a goal of creating a pipeline for horse racing data online and then modeling based on that data.  However, I know that doing an independent project for Big Data is a lot harder, so how would you recommend I go about getting practical experience in working with big data or performing an independent project in that regard?

5. When did you first feel comfortable applying for data engineering positions if you were self-taught in data engineering principles?

6. Is there anything you wish you would have known or done in your own data engineering learning process or have any other recommendations for someone in my position?

I know that's a lot of questions, so please don't feel like you have to answer all of them.  However, I really do appreciate any and all help!"
1047,2019-09-19 16:06:48,1568898408.0,dataengineering,Workday Pipeline,d6ds9k,smelllly3,,https://www.reddit.com/r/dataengineering/comments/d6ds9k/workday_pipeline/,2.0,6.0,0.0,6675.0,"Hi, I'm a newbie working with HR data in Workday, and was wondering if anyone's had any experience on constructing a data pipeline to move this data out of Workday and prepare it for analytics (probably in Python). The data doesn't appear to be too big, so big data tools are \*probably\* not necessary.

Any suggestions on where to start would be greatly appreciated!"
1048,2019-09-19 17:04:08,1568901848.0,dataengineering,New Data Engineer,d6ei46,gh0st_py,,https://www.reddit.com/r/dataengineering/comments/d6ei46/new_data_engineer/,15.0,20.0,0.0,6675.0,"Hey guys, I work at a small company and I have been tasked to build out our ML pipeline and data pipeline from scratch. Literally nothing exists right now, we will be handling a lot of logs and the goal would be to create something that is as close to real time as possible? (if this is possible?) So im thinking something like streams (Kafka) but was wondering if any of you had any advice or resources that would help me out.

It would be greatly appreciated."
1049,2019-09-20 00:36:55,1568929015.0,dataengineering,Help Transitioning from SQL Server to Airflow and Structured Pipeline Systems,d6kqko,cannablubber,,https://www.reddit.com/r/dataengineering/comments/d6kqko/help_transitioning_from_sql_server_to_airflow_and/,1.0,0.0,0.0,6677.0,
1050,2019-09-20 01:56:19,1568933779.0,dataengineering,Top 5 requirements for AWS-Native data pipelines (video). What do you think? Agree?,d6lst7,CalebASav,,https://www.reddit.com/r/dataengineering/comments/d6lst7/top_5_requirements_for_awsnative_data_pipelines/,1.0,0.0,0.0,6678.0,
1051,2019-09-20 16:35:57,1568986557.0,dataengineering,Why we chose Apache Spark for ETL (Extract-Transform-Load),d6v0qk,sumitkumar1209,,https://www.reddit.com/r/dataengineering/comments/d6v0qk/why_we_chose_apache_spark_for_etl/,1.0,0.0,0.0,6684.0,"I recently published my article Why we chose Apache Spark for ETL (Extract-Transform-Load) - http://bit.ly/30AUh70, on
   
 Medium
   
 . Have a look."
1052,2019-09-20 23:44:01,1569012241.0,dataengineering,Airflow implementation into a new small project,d70uvf,jyoff,,https://www.reddit.com/r/dataengineering/comments/d70uvf/airflow_implementation_into_a_new_small_project/,5.0,4.0,0.0,6690.0,"Hi Guys,

I'm trying to incorporate Airflow into my project. My main goal is to learn Airflow while developing the project further. Currently, I've developed a pipeline where I take a CSV file and upload it into Mysql. 

Now I'd like to add Airflow on top of it. So my goal is to execute the pipeline using Airflow. At this moment, I execute it manually.  

One way of implementing it I think would be putting my project code under AIRFLOW\_HOME directory and then from a new python module under dags folder import the pipeline module and then with the Airflow operator to execute the pipeline. Haven't tried it yet, but think going this direction.  Is that the right direction ? I'm quite new to Airflow, so any ideas appreciated."
1053,2019-09-21 10:20:45,1569050445.0,dataengineering,Why we chose Apache Spark for ETL,d77ghy,sumitkumar1209,,https://www.reddit.com/r/dataengineering/comments/d77ghy/why_we_chose_apache_spark_for_etl/,1.0,0.0,0.0,6694.0,
1054,2019-09-22 04:13:57,1569114837.0,dataengineering,"Started my first job as a Data Engineer, comming from a software engineering background. Got a few questions.",d7jpd2,SergeantAskir,,https://www.reddit.com/r/dataengineering/comments/d7jpd2/started_my_first_job_as_a_data_engineer_comming/,1.0,0.0,0.0,6700.0,"Hey,  
I just started my first job after university as a junior data engineer. My company uses sap data services to manage their classical data warehouse. In addition to a bigger hadoop cluster for more specific use cases.  
**I am very keen on agile methodologies and software engineering principles**. And pretty excited to work with hadoop, writing jobs in scala and python.

But I do have a lot of questions regarding the more legacy data services system. For example: **How do you test the pipelines you write/develop?** **How do I implement a useful continuous integration pipline? Is it possible at all?** And without being able to use git as your vcs aren't you lacking a lot of useful development features?

I know that developing software worked before people got used to TDD etc. but it does definitely have advantages to work with more confidence in the code and have a continous integration pipeline especially once the development team gets a little bigger. **So does data warehousing with ETL Tools just not care about those advantages? Or is the way of developing via UI just so fast and robust enough that it doesn't matter?**

Not really excited about clicking my programs together tbh, but I guess I'll get used to it for now. Still would like to know how bad it is to work without testing etc."
1055,2019-09-23 04:37:58,1569202678.0,dataengineering,Effectively Managing your PostgreSQL Data | Full Tutorial,d7zs7l,SynthesizeMeSun,,https://www.reddit.com/r/dataengineering/comments/d7zs7l/effectively_managing_your_postgresql_data_full/,3.0,0.0,0.0,6716.0,
1056,2019-09-23 19:30:00,1569256200.0,dataengineering,Need help backing out gracefully (or not).,d88wnl,Win4someLoose5sum,,https://www.reddit.com/r/dataengineering/comments/d88wnl/need_help_backing_out_gracefully_or_not/,1.0,0.0,0.0,6720.0,
1057,2019-09-24 02:52:35,1569282755.0,dataengineering,The Machine Learning Data Science Path,d8f3lo,KamWithK,,https://www.reddit.com/r/dataengineering/comments/d8f3lo/the_machine_learning_data_science_path/,4.0,0.0,0.0,6730.0,"I've created a [blog post](https://kamwithk.github.io/path.html#path) detailing different courses, books and places people can learn about data science/machine learning from.

It    categorizes the sources, and gives details on the main differences    between them to help decide whether the course is right for you. Make    sure to take a look:

[https://kamwithk.github.io/path.html#path](https://kamwithk.github.io/path.html#path)

Any feedback would be greatly appreciated!

I do keep this updated on my [twitter account](https://twitter.com/kamwithk_)!"
1058,2019-09-24 03:09:40,1569283780.0,dataengineering,Step by step tutorial how to collect Google Analytic events to your own cheap data warehouse on AWS with Snowplow,d8fajk,levashovbiz,,https://www.reddit.com/r/dataengineering/comments/d8fajk/step_by_step_tutorial_how_to_collect_google/,0.0,0.0,0.0,6730.0,
1059,2019-09-24 13:48:31,1569322111.0,dataengineering,An interview on the open source MinIO platform for fast and flexible object storage for data intensive applications and analytics that runs everywhere,d8lfxf,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/d8lfxf/an_interview_on_the_open_source_minio_platform/,1.0,0.0,0.0,6737.0,
1060,2019-09-24 17:52:38,1569336758.0,dataengineering,AWS MSK with Lenses.io - The Perfect Combo,d8oaev,lensesio,,https://www.reddit.com/r/dataengineering/comments/d8oaev/aws_msk_with_lensesio_the_perfect_combo/,1.0,0.0,0.0,6737.0,
1061,2019-09-24 18:37:58,1569339478.0,dataengineering,AWS MSK with Lenses.io - The Perfect Combo | Read why Lenses is the right DataOps platform for AWS MSK,d8ow5o,lensesio,,https://www.reddit.com/r/dataengineering/comments/d8ow5o/aws_msk_with_lensesio_the_perfect_combo_read_why/,0.0,0.0,0.0,6738.0,
1062,2019-09-24 21:09:38,1569348578.0,dataengineering,What would make more sense? Creating an ETL pipeline for historical data first and then applying that foundation to current streaming data or work on a pipeline for current streaming data first and then working on backfilling historical data?,d8qzp6,desmondhume7,,https://www.reddit.com/r/dataengineering/comments/d8qzp6/what_would_make_more_sense_creating_an_etl/,12.0,13.0,0.0,6741.0,"I am thinking of a personal project to enhance my learning but not sure where I should start. I can download a bunch of historical data and figure out how I would like to modify it before storing it or I can work with a current data stream and do the same but I keep going back and forth between which route would make more sense. It seems as if in both cases the step I do second would benefit just from having figured out how the transform phase would be applied but I'm not sure if there are any pitfalls or benefits I am not thinking of. 

If anyone could offer any guidance I would greatly appreciate it!"
1063,2019-09-25 09:07:42,1569391662.0,dataengineering,Pyspark Flatten RDD error,d8z99t,metalloidica,,https://www.reddit.com/r/dataengineering/comments/d8z99t/pyspark_flatten_rdd_error/,2.0,3.0,0.0,6752.0,"I'm trying to flatten data in an RDD. The RDD is structured as a 4-tuple with the first element, primary\_id and the second element of the tuple a list of list of dictionaries, third and fourth elements a single list of dictionaries.

&amp;#x200B;

         rdd=   [('xxxxx99', [{'cov_id':Q, 'cov_cd':'100','cov_amt':'100', 'cov_state':'AZ'},
                          {'cov_id':Q, 'cov_cd':'33','cov_amt':'200}, 'cov_state':'AZ'},
                          {'cov_id':Q, 'cov_cd':'64','cov_amt':'10'}, 'cov_state':'AZ'},
                          [{'pol_cat_id':'234','pol_dt':'20100220'}],
                          [{'qor_pol_id':'23492','qor_cd':'30'}]),
        
             ('xxxxx86', [{'cov_id':R, 'cov_cd':'20','cov_amt':'100, 'cov_state':'TX'},
                          {'cov_id':R, 'cov_cd':'44','cov_amt':'500, 'cov_state':'TX'},
                          {'cov_id':R, 'cov_cd':'66','cov_amt':'50', 'cov_state':'TX'},
                          [{'pol_cat_id':'532','pol_dt':'20091020'}],
                          [{'qor_pol_id':'49320','qor_cd':'21'}]) ]

&amp;#x200B;

I want to flatten the data so that it appears in the format

&amp;#x200B;

https://i.redd.it/ge3a1qoeloo31.png

how would I do this in Pyspark?

&amp;#x200B;

Here is what I have attempted but this gives me an error: Too many tuples to unpack

&amp;#x200B;

&amp;#x200B;

        def flatten_map(record):
            try:
                yield(record)
                # Unpack items
                id, items, line, pls = record
                pol_id = pls[""pol_cat_id""]
                pol_dt = pls[""pol_dt""]
                qor_id = pls[""qor_pol_id""]
                for item in items:
                    yield (id,item[""cov_id""],item[""cov_cd""], item[""cov_amt""], item[""cov_state""], pol_id, pol_dt, qor_id), 1
            except Exception as e:
                pass
        
         result = (rdd
            # Expand data
            .flatMap(flatten_map)
            # Flatten tuples
            .map(lambda x: x[0] + (x[1], )))

&amp;#x200B;

I can post the complete error if required but for sake of brevity,

&amp;#x200B;

        ValueError: too many values to unpack (expected 2)"
1064,2019-09-25 12:50:41,1569405041.0,dataengineering,AWS and Postgres,d9115n,kostaskv,,https://www.reddit.com/r/dataengineering/comments/d9115n/aws_and_postgres/,1.0,1.0,0.0,6753.0,
1065,2019-09-25 14:51:39,1569412299.0,dataengineering,ArangoML Pipeline – A Common Metadata Layer for Machine Learning Pipelines,d9265r,redfang123,,https://www.reddit.com/r/dataengineering/comments/d9265r/arangoml_pipeline_a_common_metadata_layer_for/,1.0,0.0,0.0,6757.0,
1066,2019-09-26 12:53:38,1569491618.0,dataengineering,Kafka Data Pipelines with Robin Moffatt - Software Engineering Daily,d9hftj,rmoff,,https://www.reddit.com/r/dataengineering/comments/d9hftj/kafka_data_pipelines_with_robin_moffatt_software/,11.0,0.0,0.0,6774.0,
1067,2019-09-26 12:59:07,1569491947.0,dataengineering,Moving csv from S3 to RDS MySQL using Airflow?,d9hhff,sundios,,https://www.reddit.com/r/dataengineering/comments/d9hhff/moving_csv_from_s3_to_rds_mysql_using_airflow/,1.0,0.0,0.0,6775.0,
1068,2019-09-26 17:45:27,1569509127.0,dataengineering,"New user management with namespaces, improved SQL, alert integration plugins and lots more! All new Lenses 3.0 features to make building &amp; managing streaming data flows on Apache Kafka and Kubernetes a walk in the park. Read our release blog now!",d9km13,lensesio,,https://www.reddit.com/r/dataengineering/comments/d9km13/new_user_management_with_namespaces_improved_sql/,1.0,0.0,0.0,6776.0,
1069,2019-09-27 01:06:53,1569535613.0,dataengineering,How to build real-time SQL dashboards on event data from Kafka,d9qspj,hpfrood,,https://www.reddit.com/r/dataengineering/comments/d9qspj/how_to_build_realtime_sql_dashboards_on_event/,8.0,5.0,0.0,6784.0,[https://www.confluent.io/blog/analytics-with-apache-kafka-and-rockset](https://www.confluent.io/blog/analytics-with-apache-kafka-and-rockset)
1070,2019-09-27 16:25:53,1569590753.0,dataengineering,Rate my data engineer study plan,da0dqt,9039039,,https://www.reddit.com/r/dataengineering/comments/da0dqt/rate_my_data_engineer_study_plan/,22.0,32.0,0.0,6790.0,"1. Python (I've already started learning this)
2. SQL
3. Bash scripting
4. Cloud (AWS)
5. Hadoop
6. Mapreduce
7. Hive

What would you add/remove and is it possible to learn this in 1.5 years (im currently a MIS junior undergrad)?"
1071,2019-09-27 19:58:52,1569603532.0,dataengineering,Few basic things every data engineer should know about,da3974,darthvader003,,https://www.reddit.com/r/dataengineering/comments/da3974/few_basic_things_every_data_engineer_should_know/,4.0,3.0,0.0,6796.0,"Hi all!

I will be starting in Data science team as a Data engineer. I am a recent CS grad. I have been coding in scala for a couple of years now and switching the job for the first time! I have done a fair bit of Python programming but have not written any production level python code.

I would really appreciate it if someone can guide a few basic things a data engineer should. I know this is a very broad question and might seem somewhat vague.

But assume if you were in my team and what are things you expect me to know once I join your team.

Any suggestions are welcome! Thanks"
1072,2019-09-27 19:59:35,1569603575.0,dataengineering,"DBT, Presto, Minio",da39jt,SilverRockyMountain,,https://www.reddit.com/r/dataengineering/comments/da39jt/dbt_presto_minio/,1.0,7.0,0.0,6796.0,"Hey everyone, I'm curious if anyone has used DBT with Presto using Minio. 

I'm wondering if it makes sense to use these tools together. I know that Presto and Minio have a docker image and that Presto is partially supported by DBT. Would it make sense to try to add DBT on top of Presto and Minio to do the T in ELT?"
1073,2019-09-28 14:08:21,1569668901.0,dataengineering,Interview for Data Science Engineer,daenb8,thedatumgirl,,https://www.reddit.com/r/dataengineering/comments/daenb8/interview_for_data_science_engineer/,7.0,9.0,0.0,6811.0,"I'm doing a job switch. I have had the experience of being an ETL developer and worked on PySpark . Apart from that have done some side projects in Hadoop and MapReduce. I am attending an online test in 2 days for the role of Data Science Engineer (where they expect me to have an experience in DE but some knowledge in DS). They have not disclosed anything on what skills will be tested. To be prepared for it, What skills would you recommend me to brush up before the test."
1074,2019-09-28 14:28:13,1569670093.0,dataengineering,"As a Masters student with almost 2 years of exp, Should I even apply to DE jobs with 4+ min. yrs of exp. requirement?",daeszx,grimmjowlockjaw,,https://www.reddit.com/r/dataengineering/comments/daeszx/as_a_masters_student_with_almost_2_years_of_exp/,1.0,3.0,0.0,6813.0,"I keep seeing roles for which I have matching skills but I lack the exp. they want. Should I apply to these roles in hopes of:

1. They somehow shortlist me even though I lack the exp and I pass the interviews? \[i.e. make an exception\]
2. They have another opening for someone with my experience?

&amp;#x200B;

Any other job hunting tips would be welcome. I am currently using LinkedIn as just one job portal is a lot of work to apply along."
1075,2019-09-29 19:51:46,1569775906.0,dataengineering,"Spark, Hadoop, Hive etc?",daxq6m,Phizy,,https://www.reddit.com/r/dataengineering/comments/daxq6m/spark_hadoop_hive_etc/,18.0,9.0,0.0,6831.0,"Hey guys, I have been researching tools like the ones mentioned in the title but im having trouble visualizing what they could be used for. I was wondering if someone could give me a use case so to speak of why you use X technology to solve Y problem. Sorry if this post is not allowed I am pretty new to Data Science and Data engineering"
1076,2019-09-29 20:32:27,1569778347.0,dataengineering,Data Engineer's role in data privacy and security.,daya85,NextCamp,,https://www.reddit.com/r/dataengineering/comments/daya85/data_engineers_role_in_data_privacy_and_security/,5.0,2.0,0.0,6830.0,"For those of you working in industry as a data engineer, does any of the responsibility of privacy and security fall under your role?   


For example if you're building out APIs, do you also have to implement the authentication and authorization to protect access? If you're collecting or using sensitive data, is it your responsibility to implement measures to anonymize the data, or even implementing auditing functionalities to prove you are following regulations?

Excited to hear your experiences!"
1077,2019-09-30 20:16:57,1569863817.0,dataengineering,DataOps on Apache Kafka - AWS MSK with Lenses.io,dbetko,lensesio,,https://www.reddit.com/r/dataengineering/comments/dbetko/dataops_on_apache_kafka_aws_msk_with_lensesio/,1.0,0.0,0.0,6850.0,
1078,2019-09-30 23:14:19,1569874459.0,dataengineering,Ingesting database with python [help],dbhil0,doutorchefe,,https://www.reddit.com/r/dataengineering/comments/dbhil0/ingesting_database_with_python_help/,3.0,12.0,0.0,6852.0,"Hello fellows. I need help on creating a data pipeline in Python.

I need to move data from a TEIID service to an staging area on a SQL Server DB. The data volume is pretty big, there are tables with 200+ million rows. What are the best ways to do this?

For writing the date into SQL Server I'm trying to use the ctds driver ([https://github.com/zillow/ctds](https://github.com/zillow/ctds)) which has a Bulk Insert ([https://zillow.github.io/ctds/bulk\_insert.html](https://zillow.github.io/ctds/bulk_insert.html)) option which apparently can be used with in-memory data, avoiding disk I/O. But to use it, I need to explicitly wrap textual data with ctds.SqlVarChar (for CHAR, VARCHAR or TEXT columns) or ctds.SqlNVarChar (for NCHAR, NVARCHAR or NTEXT columns) due to FreeTDS not encoding data on the client side.

For reading the data from the TEIID I'm using a hacky to use SQLAlchemy ([https://github.com/kafran/sqlalchemy-teiid](https://github.com/kafran/sqlalchemy-teiid)). It's also possible to use psycopg2. Using psycopg2 I can fetchmany results and iterate over the rows to encapsulate the data with the ctds data types before send it to Bulk Insert. But I'm using the SQLAlchemy approach to use pandas to consume the data and pass the data to pd.to\_sql(method=bulk\_method) ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to\_sql.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html)).

Pandas to\_sql pass 4 parameters to the bulk\_method: the pd\_table, the connection, a list with columns (key) and a zip with row tupples. Right now I'm stuck constructing the bulk\_method to iterate over the data to send it to ctds.connection.bulk\_insert; Can someone illuminate me? As I'm pretty new on this, I need help manipulating these Data Structures."
1079,2019-10-01 01:28:21,1569882501.0,dataengineering,What are your best practices around AWS Lambda?,dbjgvp,Shoddy_Researcher,,https://www.reddit.com/r/dataengineering/comments/dbjgvp/what_are_your_best_practices_around_aws_lambda/,7.0,5.0,0.0,6852.0,"Hi All,

Going through architectural possibilities and trying to work out how AWS Lambda's will fit in.

We use them for small jobs and Airflow/Batch for bigger jobs, I love and hate Lambda at the same time but maybe there are ways to minimise the pain points.

In particular, I'm interested in how you support:

* Scheduling
* Monitoring / Retries / Etc
* Deploying
* Orchestration / Dependency Management

This is all nicely bundled up with Airflow and I can keep it all in version control for the Batch jobs, but the Lambda's are starting to get out of control. Particularly because we need to custom compile on a version of Amazon Linux (pandas/numpy).

We loosely have step functions for dependencies, cloudwatch rules for scheduling, I could ingest the cloudwatch logs for monitoring (haven'y investigated SAM yet) but it all feels a little messy and makes me want to just throw everything in Airflow/Batch.

Thanks!"
1080,2019-10-01 03:13:35,1569888815.0,dataengineering,Anyone here looking to mentor a newbie?,dbkvu9,LDNDataNoob,,https://www.reddit.com/r/dataengineering/comments/dbkvu9/anyone_here_looking_to_mentor_a_newbie/,16.0,35.0,0.0,6852.0,"I'm a software developer getting into the world of data and would love to have someone who is more experienced then me to chat with. Some of the things I'm currently exploring are Docker, Airflow, Postgres and data warehousing / data modelling.  I know the odds are low but wanted to ask anyways."
1081,2019-10-01 04:29:47,1569893387.0,dataengineering,Constraints in Conceptual Data Modeling,dblw1q,yejinzai,,https://www.reddit.com/r/dataengineering/comments/dblw1q/constraints_in_conceptual_data_modeling/,1.0,2.0,0.0,6855.0,"Hello all, how or where do we put constraints in a data model? For example using the image attached, how can we specify that a person cannot be married to himself or herself? do we add the constraint as a statement or there is a way to update this model to show that constraint?

Thanks!

https://i.redd.it/qmzzxj441up31.png"
1082,2019-10-01 19:37:21,1569947841.0,dataengineering,[Airflow] Shared resource across DAGs? Similar but opposite of a pool,dbvzq1,WalterDragan,,https://www.reddit.com/r/dataengineering/comments/dbvzq1/airflow_shared_resource_across_dags_similar_but/,1.0,8.0,0.0,6869.0,"I am designing a process that will have multiple DAGs. Each DAG can have a branch where it is dependent on something running on an ec2 in AWS. This ec2 process has a long setup and teardown time, but low run time. So ideally I would like each branch to be queued until they're all ready to run, start the ec2 once, run for each DAG, and teardown once.

I thought about creating these as sub-DAGs, but ideally I want to be able to preserve history runs so that I can more easily identify a problem if one arises.

Is sharing a resource like this across DAGs even possible? To me it seems like the opposite of a pool. Rather than lessening concurrency to a connection, I want to increase it."
1083,2019-10-01 19:55:15,1569948915.0,dataengineering,An interview about how the open source Kedro framework makes it faster and easier to build your end-to-end data pipeline for machine learning projects,dbw8wt,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/dbw8wt/an_interview_about_how_the_open_source_kedro/,3.0,0.0,0.0,6870.0,
1084,2019-10-01 20:02:23,1569949343.0,dataengineering,Project Ideas to develop skills and understanding?,dbwcfu,databoii69,,https://www.reddit.com/r/dataengineering/comments/dbwcfu/project_ideas_to_develop_skills_and_understanding/,13.0,12.0,0.0,6870.0,"I am a beginner in the field of DE and I wanted to know what are some projects that one could do to develop their skills and maybe add to their resume.

Would a simple ETL like extracting data from an API and transforming or aggregating it and storing it on an RDBMS(MySQL,postgreSQL)  count as a task or a project?

Thank you very much for your help and suggestions."
1085,2019-10-01 20:29:56,1569950996.0,dataengineering,Trying to understand part of Pyspark code,dbwq70,metalloidica,,https://www.reddit.com/r/dataengineering/comments/dbwq70/trying_to_understand_part_of_pyspark_code/,1.0,0.0,0.0,6870.0,"I'm working on a Pyspark project and I'm trying to understand the last part of the code below.

what is     

    .map(lambda x: x[0] ,)) 

doing here?  Is it taking the first element and adding an empty string?

&amp;#x200B;

    result = (full_rdd_group_val
        # Flatten tuples
        .flatMap(flatten_map)
        .map(lambda x: x[0] ,))"
1086,2019-10-01 23:10:20,1569960620.0,dataengineering,Spark error when unpacking RDD items,dbz121,metalloidica,,https://www.reddit.com/r/dataengineering/comments/dbz121/spark_error_when_unpacking_rdd_items/,0.0,5.0,0.0,6875.0,"I wrote a script on Jupyter notebook to read a sequence file and perform operations. The script works fine on Jupyter. 

&amp;#x200B;

        def acct_tuple(x):
            return (x[0], json.loads(x[1])['coverage_info'])
        
        def flat_map(x):
            return x
        
            def get_tuple(x):
            return {'acct_num': x[0], **x[1]}
        
            def get_relevant_fields(x):
            return (x['acct_num'], x['cov_leg_dt'], x['cov_leg_end_dt'], x['rest_list'] if 'rest_list' in x.keys() else [], x['loc_list'] if 'loc_list' in x.keys() else [], x['pls_list'] if 'pls_list' in x.keys() else [])
        
            def map_tuple(x):
            return (x[0] ,)
        
            def flatten_map(record):
            # Unpack items
            _, cov_leg_dt, cov_leg_end_dt, items, [line], [pls] = record
            line_cd = line[""line_code""]
            ply_id = line[""policy_id""]
            company_cd = pls[""pls_company_cd""]
            for item in items:
                yield (ply_id, cov_leg_dt, cov_leg_end_dt, item[""set_coverage_cd""], item[""set_pt_state_cd""], item[""set_limit1_amt""], item[""set_limit2_amt""], item[""set_limit3_amt""],  item[""set_limit4_amt""], line_cd, company_cd),1
        
        	
        rdd = spark.sparkContext.sequenceFile(input_directory)
        rdd = rdd.map(acct_tuple)
        rdd = rdd.flatMapValues(flat_map)
        rdd = rdd.map(get_tuple)
        rdd = rdd.map(get_relevant_fields)
        rdd = rdd.flatMap(flatten_map)
        rdd = rdd.map(map_tuple)
    

However, when converting to a Python script, I get an error:

&amp;#x200B;

2019-10-01 14:12:46,901:ERROR: \_, cov\_leg\_dt, cov\_leg\_end\_dt, items, \[line\], \[pls\] = record 

2019-10-01 14:12:46,901:ERROR:ValueError: not enough values to unpack (expected 1, got 0)

&amp;#x200B;

Any suggestions? I can post a sample RDD if that helps but I believe this is due to a structural difference between writing a script on Jupyter and a .py file?"
1087,2019-10-02 03:26:36,1569975996.0,dataengineering,Domain driven data architecture,dc2kqs,SilverRockyMountain,,https://www.reddit.com/r/dataengineering/comments/dc2kqs/domain_driven_data_architecture/,6.0,1.0,0.0,6880.0,"https://martinfowler.com/articles/data-monolith-to-mesh.html

Hey everyone I’m curious what are thoughts, compliments, or criticisms on this article. It looks like the idea is to decouple and abstract data processing steps and data itself. 

What are the downsides to this approach over a traditional data lake?"
1088,2019-10-02 18:01:41,1570028501.0,dataengineering,Technical Questions I Could Expect?,dcbeb6,jirukulapati,,https://www.reddit.com/r/dataengineering/comments/dcbeb6/technical_questions_i_could_expect/,9.0,3.0,0.0,6891.0,"Do you guys have examples of interview questions you receive for a job with these requirements:

Usage of AWS for data-warehousing and processing (the following are relevant Redshift, S3, RDS / Aurora, Lambda, SQS, Kinesis)

Scripting tasks using Python (strongly preferred) 

Complex data structure, data processing, data quality and data lifecycle.

Operational management / DBA of DBMS servers (Oracle, MySQL or MSSQL)

Data platforms and data modeling frameworks.

When I applied, the job specified two years hands on experience with the above, but I'm actually a new grad.  They said they're open to junior positions for this role.  AWS is a big part, I was just wondering what sort of questions I would receive?"
1089,2019-10-02 22:01:28,1570042888.0,dataengineering,Here’s What Makes Apache Flink scale,dcepk0,KKcorps,,https://www.reddit.com/r/dataengineering/comments/dcepk0/heres_what_makes_apache_flink_scale/,1.0,0.0,0.0,6895.0,
1090,2019-10-02 23:24:42,1570047882.0,dataengineering,Free/Open Source Data Lineage Tool?,dcfxr6,beaverhair,,https://www.reddit.com/r/dataengineering/comments/dcfxr6/freeopen_source_data_lineage_tool/,13.0,4.0,0.0,6896.0,"Are there any solid options for free data governance tools that ideally include metadata management, data lineage viewers, data catalog, etc.

Looking to build a proof of value for an internal company demo."
1091,2019-10-03 00:13:51,1570050831.0,dataengineering,Snowflake COPY INTO command - Purge not working.,dcgnw1,blazin_wit_bill_ford,,https://www.reddit.com/r/dataengineering/comments/dcgnw1/snowflake_copy_into_command_purge_not_working/,3.0,9.0,0.0,6896.0,"Per the title, I'm using an Azure-hosted Matillion instance to copy files from Blob storage.  (Or rather, a single file).

This file needs to be deleted upon successful completion of my ETL process.  I have the PURGE option set to True, but when I run the job it won't delete the file that it ingested originally.  

Any ideas what I'm doing wrong here and/or what I need to check or change?

Thanks"
1092,2019-10-03 10:01:06,1570086066.0,dataengineering,Data engineer plan (revision needed),dcnay2,9039039,,https://www.reddit.com/r/dataengineering/comments/dcnay2/data_engineer_plan_revision_needed/,17.0,25.0,0.0,6905.0,"This is an updated plan from the ideas shared [on this thread](https://www.reddit.com/r/dataengineering/comments/da0dqt/rate_my_data_engineer_study_plan/)


Once again, what skills would you add/remove? How would you order it?

1. SQL
2. ETL
3. Python
4. Bash
5. AWS 
6. Spark
7. Docker


And realistically, how much of the above would I need to know to land a data-engineering position? I am an MIS undergrad student who already knows some SQL, data structures &amp; algorithms, OOP, and some basic python."
1093,2019-10-03 19:17:18,1570119438.0,dataengineering,I get JSON files dumped into an S3 bucket periodically and need to load this data into Redshift. How do I go about building this pipeline?,dct4ha,robotofdawn,,https://www.reddit.com/r/dataengineering/comments/dct4ha/i_get_json_files_dumped_into_an_s3_bucket/,4.0,13.0,0.0,6911.0,"Hello everyone, 

So as the title says, I get JSON files dumped into S3 frequently and need to load this data into Redshift on a near-realtime basis.
How do I go about this? I have come up with this approach -

1. Publish new files arriving into bucket to an SQS queue using S3 Event notifications. 
2. Write an always running python script that reads from the SQS queue, transforms the data and loads it into Redshift 
3. Put all this into a Docker container and run the container on an EC2 instance.

Would this work out? Also had a couple of questions - 

1. How do monitor and react to failures in the pipeline, do I have to set up my own notification system? If there are failures when processing files, how do I rerun the transformations for the failed messages again?
2. Is there an easy way to ""backfill"" the data? If already have files sitting in S3, is there a way to load them into Redshift without writing a custom script to do this one-time activity?
3. If the volume of the data that comes into S3 increases (and it most likely will), how do I scale this solution? Do I just spin up more containers? Or handle it in python using multi-processing?

Thanks for taking the time to answer the questions in advance!"
1094,2019-10-03 19:28:14,1570120094.0,dataengineering,How can I utilise my current role to set myself up for a data engineering role?,dct9yc,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/dct9yc/how_can_i_utilise_my_current_role_to_set_myself/,12.0,14.0,0.0,6911.0,"I am currently a data/BI reporting analyst and want to move into data engineering.

My current role consists of the following:

* Understand reporting requirements from whoever (finance, marketing, operations etc)
* Extracting the data from SQL database (MS SQL Server, MySQL, Redshift)
* Load into Tableau and build the report
* Repeat

What I've been doing lately:

* Data from different sources e.g. sql databases, flat file CSVs need cleaning/joining/blending/aggregating so I use Python/Pandas to do this sort of stuff then export a csv to be used in Tableau
* Automating this via cron jobs (looking to schedule these scripts into an Airflow workflow?)
* Loading some of that data into a MySQL database which I've got write-access to (perhaps loading data into a staging db and then a final db using a star/snowflake schema?)
* Thinking to rewrite Python scripts from Pandas to PySpark to get some experience in this (not sure if it's a good idea as Pandas seem to be doing the job but hear that data engineers use Spark more than Pandas)

Limitations:

* No write access to Redshift data warehouse (because it's the data warehouse I'm only allowed to read not write)
* Not allowed a dev server to run scripts on because company architecture does not support Python (is that a thing? Assume it is)
* Hosting a db on AWS is expensive therefore not allowed
* Current MySQL which I have write access to is version 5.something that doesn't support window functions or CTEs

Question is, what can I do, and what is the best route to take, in order to spice up my current role in order to set myself up for at least being considered for a data engineer role?

My thoughts are:

* Schedule all scripts via Airflow
* Convert Python scripts from using Pandas to PySpark
* Extract all the data I need for my Tableau reports and then some from the different data sources and drop them into the MySQL db (I have a few tables in here anyways so would be nice to get them all in one place. Only problem, again, is that it doesn't support things like window functions or CTEs)

What are your thoughts? Brutal/savage honesty is more than welcome"
1095,2019-10-03 19:36:25,1570120585.0,dataengineering,What does your data intake look like?,dcte1i,sciencewarrior,,https://www.reddit.com/r/dataengineering/comments/dcte1i/what_does_your_data_intake_look_like/,1.0,4.0,0.0,6912.0,"I've been working in my company's data engineering department before data engineering was a thing, and most of our pipeline slowly converged to industry standards, but our log capture process isn't something I've seen in other companies. We actually connect to each web server and copy the rotating log file via SCP. This decision was made long before projects like Flume and Kafka were available, and it stuck.

While this makes autoscaling trickier, it makes batch processing much simpler. We don't have to worry about waiting for all data to arrive for a given hour, or records arriving late, or duplicates. We just upload everything to a bucket in S3, run a Pig script, and our data warehouse is updated. So I realize I have this knowledge gap. How do you handle duplicates or out-of-order records in your ETL?"
1096,2019-10-03 22:01:25,1570129285.0,dataengineering,Running a scraping platform at Google Cloud for as little as US$ 0.05/month,dcve77,newgermansheperd,,https://www.reddit.com/r/dataengineering/comments/dcve77/running_a_scraping_platform_at_google_cloud_for/,1.0,0.0,0.0,6913.0,
1097,2019-10-03 22:55:19,1570132519.0,dataengineering,Snowflake vs redshift?,dcw67g,doob10163,,https://www.reddit.com/r/dataengineering/comments/dcw67g/snowflake_vs_redshift/,1.0,0.0,0.0,6915.0,Anyone have experience with these two data warehouses? What are some pros and cons of the two?
1098,2019-10-04 02:01:37,1570143697.0,dataengineering,Big Data DevOps Engineer,dcys3y,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/dcys3y/big_data_devops_engineer/,7.0,11.0,0.0,6917.0,"I see lot of company posting DevOps Job that requiring Data engineer backround, is that even a thing ? Data DevOps ?"
1099,2019-10-04 04:58:43,1570154323.0,dataengineering,ETL Whiteboard Interview Tips?,dd0v9f,consoleconsumer,,https://www.reddit.com/r/dataengineering/comments/dd0v9f/etl_whiteboard_interview_tips/,12.0,7.0,0.0,6919.0,"Hi all,

I have an interview coming up next week for a job I am really excited about -- data engineer at a major company in the Bay.   Part of the interview is whiteboarding an ETL case -- they'll give source and target, and want me to write an  ETL (I choose Spark) to get the data from point A to point B (I am assuming it will be OLTP -&gt; OLAP type stuff -- maybe aggregating some kind of metric).

Basically, I have been feeling really overwhelmed trying to prepare for this as I don't have an easy way of verifying whether the nonsense I am coming up with is correct.  Do any of you have any tips for whiteboarding ETL?  Or some resources I can take a look at -- I would love to find some solid tutorials that I can use as a baseline, but so far I haven't had a ton of luck.  Any advice would be really appreciated!

For background, I'm a data engineer at my current role by name, but I mostly end up doing something closer to software dev work (2 years of experience, non-CS but technical major).  I have written some Spark streaming jobs which essentially take in Kafka messages, add a few calculations and put them into HIVE tables.  Our data volume is so low, I have never really run up into the kind of volume issues that I am expecting to be quizzed around.

Thank you so much!"
1100,2019-10-04 10:09:59,1570172999.0,dataengineering,"Noob question: when people say ""learn SQL"", which one are they talking about? MySQL or Postgresql or is it something else?",dd3wyo,BRAGADU,,https://www.reddit.com/r/dataengineering/comments/dd3wyo/noob_question_when_people_say_learn_sql_which_one/,4.0,6.0,0.0,6922.0,Or is SQL different from MySQL/Postgresql?
1101,2019-10-04 10:41:44,1570174904.0,dataengineering,Data Analytics in E-Sports – Future Prospects – Jobs – Everything You Should Know,dd467j,techPackets_005,,https://www.reddit.com/r/dataengineering/comments/dd467j/data_analytics_in_esports_future_prospects_jobs/,18.0,0.0,0.0,6922.0,
1102,2019-10-04 14:02:42,1570186962.0,dataengineering,How often do you get given a schema for your sources?,dd5tmu,ed_elliott_,,https://www.reddit.com/r/dataengineering/comments/dd5tmu/how_often_do_you_get_given_a_schema_for_your/,3.0,6.0,0.0,6922.0,Just interested really how often when you get a data source you also get a schema and how is it defined? I have seen swagger being used but wondered what other people get?
1103,2019-10-04 14:27:18,1570188438.0,dataengineering,"Junior data engineer with no/little technical background, looking for guidence",dd621l,hipsterrobot,,https://www.reddit.com/r/dataengineering/comments/dd621l/junior_data_engineer_with_nolittle_technical/,1.0,2.0,0.0,6922.0,
1104,2019-10-04 18:42:41,1570203761.0,dataengineering,Raspberry Pi Data Eng Projects?,dd936i,beaverhair,,https://www.reddit.com/r/dataengineering/comments/dd936i/raspberry_pi_data_eng_projects/,13.0,13.0,0.0,6925.0,I have a largely unused Raspberry Pi kicking around that I’d like to do something with.. any ideas for projects that could use the hardware with some data engineering related tasks?
1105,2019-10-05 15:17:41,1570277861.0,dataengineering,Web developer or data engineer — which occupation has better long-term prospects?,ddmilm,9039039,,https://www.reddit.com/r/dataengineering/comments/ddmilm/web_developer_or_data_engineer_which_occupation/,17.0,19.0,0.0,6940.0,"which one will be more ""future-proof""


which one has the better work/life balance

which one can help you transition to higher-paying jobs"
1106,2019-10-06 06:59:34,1570334374.0,dataengineering,Unsure what salary to ask for,ddyatx,FearData,,https://www.reddit.com/r/dataengineering/comments/ddyatx/unsure_what_salary_to_ask_for/,0.0,5.0,0.0,6943.0,"I have an interview for a Data Engineering position on Tuesday with a start up company. I have 2 years of experience as a data analyst and 2 years as a data architect. Online it says I am applying to be the first data engineer of their company. I don’t know how much to ask for. Or even interview questions because there is no info on this company online anywhere like Glassdoor. 

Is 95K a good offer? Is it too high for a start up? Is it too low? It says salary is dependent on experience. They say I should have strong SQL and Python skills including data modeling and develop and maintain architecture, automate data, and use Python to process data files."
1107,2019-10-06 21:09:10,1570385350.0,dataengineering,California Consumer Privacy Act CCPA could cost companies $55 billion,de6wb5,jrich8573,,https://www.reddit.com/r/dataengineering/comments/de6wb5/california_consumer_privacy_act_ccpa_could_cost/,19.0,6.0,0.0,6953.0,
1108,2019-10-07 12:09:12,1570439352.0,dataengineering,How do you get started and get work as freelance/consulting Data Engineer?,degx21,chirau,,https://www.reddit.com/r/dataengineering/comments/degx21/how_do_you_get_started_and_get_work_as/,6.0,9.0,0.0,6965.0,
1109,2019-10-07 19:07:45,1570464465.0,dataengineering,Graduate going for DE interview,deljgf,south-african-kev,,https://www.reddit.com/r/dataengineering/comments/deljgf/graduate_going_for_de_interview/,2.0,2.0,0.0,6970.0,
1110,2019-10-07 19:56:42,1570467402.0,dataengineering,What are some good practice projects for data engineering?,dem7ve,chirau,,https://www.reddit.com/r/dataengineering/comments/dem7ve/what_are_some_good_practice_projects_for_data/,12.0,20.0,0.0,6970.0,
1111,2019-10-08 00:09:35,1570482575.0,dataengineering,Setting up Airflow on Azure &amp; connecting to MS SQL Server,deptld,linkerzx,,https://www.reddit.com/r/dataengineering/comments/deptld/setting_up_airflow_on_azure_connecting_to_ms_sql/,1.0,0.0,0.0,6974.0,
1112,2019-10-08 03:15:05,1570493705.0,dataengineering,"How is Python applied to database management, optimizing processes, and cleaning data?",des8hy,chemfreak101,,https://www.reddit.com/r/dataengineering/comments/des8hy/how_is_python_applied_to_database_management/,17.0,14.0,0.0,6976.0,I just started learning Python and want to learn how to apply it to data engineering. How can Python 3 be applied to data quality or DB management or optimizing/automating processes? Is there any specific code from Python I should learn regarding this? Does anyone have any way for me to practice this type of scripting? I don't want to be a developer or anything. Any help would be great thanks.
1113,2019-10-08 05:12:22,1570500742.0,dataengineering,Any good ways to practice python for data engineering ?,detm5j,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/detm5j/any_good_ways_to_practice_python_for_data/,5.0,2.0,0.0,6977.0,I guess to say if I wanted to practice python for data analysis I would load spreadsheets into python and clean csv files via pandas and create  new columns that are derived from calculations.
1114,2019-10-08 05:48:54,1570502934.0,dataengineering,Any Data Engineering Tutors ? I just promoted to a job as a DE from an analyst role at my company and head is spinning. Want to get ahead and impress leadership.,deu20q,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/deu20q/any_data_engineering_tutors_i_just_promoted_to_a/,4.0,3.0,0.0,6977.0,"I tried the Wyzant but it had no tutors. Really want to brush up on making api calls from source data ( marketo or salesforce ), staging to AWS S3  Data lake ( via ec2) and then elt into snowflake dwh. 

Goal to understand AWS and Snowflake and how to tie it all together with python and sql."
1115,2019-10-08 18:37:45,1570549065.0,dataengineering,"Manage, debug &amp; secure Debezium-&gt;Snowflake flows via Kafka with Lenses.io. Hands on",df1o6y,lensesio,,https://www.reddit.com/r/dataengineering/comments/df1o6y/manage_debug_secure_debeziumsnowflake_flows_via/,4.0,0.0,0.0,6982.0,
1116,2019-10-08 21:41:36,1570560096.0,dataengineering,An interview about the architecture of Rockset and how they built a serverless platform for fast and flexible analytics on your semi-structured data,df480y,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/df480y/an_interview_about_the_architecture_of_rockset/,8.0,0.0,0.0,6982.0,
1117,2019-10-09 06:56:04,1570593364.0,dataengineering,Data Profiling Daily Files with Python. How do you store that information for trends and alerts of anomalies?,dfbmdm,tkepongo,,https://www.reddit.com/r/dataengineering/comments/dfbmdm/data_profiling_daily_files_with_python_how_do_you/,10.0,5.0,0.0,6988.0,"We get a ton of files everyday and I’d like to perform data profiling and storing all that information for reporting, tracking trends and sending alerts when there are anomalies or schema changes. 

Is there a best practice of doing this? This is important because every full moon we would get some bad files and process it, which creates a lot of time spent researching and potentially backtracking things in our databases. 

I’d like to do this in Python if possible? Our files are typically less than 1gb so I’d imagine Pandas and other “Non” Big Data technologies would be sufficient."
1118,2019-10-09 13:45:08,1570617908.0,dataengineering,People who work in financial service companies: data engineer. What's your organization pain points?,dff9dk,homchange,,https://www.reddit.com/r/dataengineering/comments/dff9dk/people_who_work_in_financial_service_companies/,2.0,5.0,0.0,6991.0,"Hi, everyone.

I aim to apply for this job as a data engineer. ( [Link](https://careers.bloomberg.com/job/https://careers.bloomberg.com/detail/74658)) however, I'm pretty sure my skills are lacking; Because I had one year in MSc Business Analytics, majorly focusing on machine learning and data modelling; and I'm pretty bad at it.   


Preparing more can make me confident, I reckon. So, for this job,  I need to learn and fill the gaps. Having done research, I found a data engineer course in udacity; not sure whether it can give the skills I need.  


What're you guys thoughts? Can anyone who works in similar companies share your daily activity?  


Any advice would be super helpful!!!  


Thanks!"
1119,2019-10-09 16:16:34,1570626994.0,dataengineering,News Recommendation Algorithm idea and implementation,dfgw8v,gauravc2796,,https://www.reddit.com/r/dataengineering/comments/dfgw8v/news_recommendation_algorithm_idea_and/,2.0,4.0,0.0,6992.0," 

Hi,

As I am still learning data science so there may be some mistakes in my implementation/ approach and I highly appreciate your opinions, points, suggestions, modifications.

I am working on a leading financial services company and for their premium users, I need to implement personalized news.

Through I have found out some idea how to approach the problem, I don't know whether my idea is actually the workable solution or not.

**Problem Statement**: Recommend user's news articles.

**Data**: user's personal information such as age, login time, portfolio information, topics that users have high affinity to. ( this is still WIP still suggestions on data would be beneficial)

**Solution**:

Steps

1. ***Finding readers with similar interests:***

As we have mixed data we will be using [hierarchical clustering algorithm using the Gower](https://stats.stackexchange.com/a/15313/260511) similarity coefficient.

It will help to group similar users who have similar tastes.

**2.** ***Topic modeling***

To understand news articles we will use the [LDA algorithm](https://blog.insightdatascience.com/news4u-recommend-stories-based-on-collaborative-reader-behavior-9b049b6724c4) to identify topics in news.

We can also use it to understand how much a news article is relevant to the topic.

To identify the diversity of topics I will use Jaccard similarity. as we do not want many topics and similar topics.

For mapping news with users, we will use the number of clicks of the user on particular news containing the topic.

**3.** ***Recommendation***

As we have a Group of clusters which has information about their affinity toward the topic,

we can find out similar news article which is similar to the topic using [cosine similarity](https://www.machinelearningplus.com/nlp/cosine-similarity/) and recommend that article to the user group.

Again any doubt/ change/ modifications are appreciated.

Also if you think it is the right solution then please highlight."
1120,2019-10-09 20:54:57,1570643697.0,dataengineering,Why I Recommend My Clients NOT Use KSQL and Kafka Streams,dfkpi9,sciencewarrior,,https://www.reddit.com/r/dataengineering/comments/dfkpi9/why_i_recommend_my_clients_not_use_ksql_and_kafka/,16.0,0.0,0.0,6993.0,
1121,2019-10-10 17:46:07,1570718767.0,dataengineering,Top takeaways from Kafka Summit SF and industry trends,dfz1pn,lensesio,,https://www.reddit.com/r/dataengineering/comments/dfz1pn/top_takeaways_from_kafka_summit_sf_and_industry/,6.0,2.0,0.0,7019.0,
1122,2019-10-12 06:25:19,1570850719.0,dataengineering,Dataframes instead of a database?,dgpqxv,trenchtoaster,,https://www.reddit.com/r/dataengineering/comments/dgpqxv/dataframes_instead_of_a_database/,12.0,20.0,0.0,7043.0,"My main setup includes airflow for scheduling, Postgres for the data warehouse, sqitch for migrations, dbt for creating views (I literally select * from these views, dump the data to csv and stream it to our visualisation platform). All transformation is done with pandas or dbt. 

When I need to get things done quickly, I load the data into pandas, clean it up, and send it to our visualisation tool. From there, other teams create charts or merge the data with other data etc. From this point of view, our third party visualisation tool (Domo) is kind of the data warehouse and anything I have is the staging area. Since I am dealing with so much data from so many sources (many of which change frequently and without warning), I am beginning to feel like putting stuff in the database and managing schemas is just too much overhead - there have been so many times when I just wasn’t made aware of a proper unique constraint or new columns were added and I need to alter the table and backfill data. 

I have been testing just dumping the pure raw data in a container on azure blob, reading it with pandas and outputting a transformed version in parquet format in a different container. It seems to be quick and efficient but i am worried that I might be taking a step backwards that I might regret later. I really do like Postgres and dbt a lot too, but I suppose on azure blob I can eventually move to Spark and still query files. Plus the storage is essentially unlimited. 

Any thoughts?"
1123,2019-10-12 13:24:55,1570875895.0,dataengineering,"How do you determine ""years of experience""?",dgtdj0,LectricVersion,,https://www.reddit.com/r/dataengineering/comments/dgtdj0/how_do_you_determine_years_of_experience/,8.0,7.0,0.0,7046.0,"Specifically in the case where previous professional experience is of little relevance to your current profession?

I'm a Data Engineer, but my first role after I graduated was in 2011 as an IT Consultant - basically ""the IT guy"" in the office that fixes printers, rolls out Windows updates and resets passwords and the like, but also writing the odd SQL statement to pull data out of databases and building the occasional dashboard. It wasn't until 2013 - almost two years to the day in joining - that I got promoted to a reporting analyst position and started doing this full time. It was still nothing too complicated, though, and there was certainly no data modelling or database design involved.

It wasn't really until 2014, when I got a job as a Data Architect, that I really started to do ""Data Engineer"" type things - building OLAP models, creating ETL pipelines etc. I even got to go on a course for it. 2016, when I changed jobs, was really a pivotal point for me as I started designing huge enterprise-level data warehouse solutions for top-tier clients, and all of the ETL processes and architecture that goes with that. I even started learning Python around this point.

With this in mind, how would I specify my ""years of experience""? From the start of my professional career (8 years), or from the start of my ""Data Engineering"" career?  (5 years)?"
1124,2019-10-13 05:39:42,1570934382.0,dataengineering,What do you guys use for source to target mapping specs?,dh4trm,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/dh4trm/what_do_you_guys_use_for_source_to_target_mapping/,12.0,7.0,0.0,7057.0,"Places I’ve worked have used excel to map source to target with logic. The issue is that the specs eventually get out of sync with the actual code. 

They just aren’t easy to maintain. Also,  everyone writes their specs a little differently so there is a lack of standards.

Wondering what’s being used at the places you work. 

Bonus question: what other common meta data do you guys create / maintain and how? Example: data flow diagrams in Visio, data models using Erwin or something.."
1125,2019-10-13 06:12:52,1570936372.0,dataengineering,How do I map a large set of unique values of job professions into categories?,dh56bg,ByMAster2,,https://www.reddit.com/r/dataengineering/comments/dh56bg/how_do_i_map_a_large_set_of_unique_values_of_job/,1.0,0.0,0.0,7059.0,"I am working with a data problem that involves predicting the income of the person.

One of the features of this dataset is 'Job Profession'. Now in this column of the profession, there are a set of distinct unique names. Here are some of the values --&gt; https://pastebin.com/rrD3P1Wr

Now I want to map these jobs into industries which they are representing to reduce the dimensions in my model. So can someone suggest me on how should I go about mapping them?"
1126,2019-10-14 17:57:56,1571065076.0,dataengineering,AWS Athena UI - Issues,dhrptr,saifuddin778,,https://www.reddit.com/r/dataengineering/comments/dhrptr/aws_athena_ui_issues/,7.0,5.0,0.0,7084.0,"People try drawing a comparison of AWS Athena with BigQuery, and usually, end up hating Athena's unpredictable and oftentimes broken UI and go for BigQuery. 

A few things I have noted so far:

* Sessions expire randomly and oftentimes an nginx stacktrace is shown to the user during the active session.
* Query's error stacktrace is generic (gives a similar error for a variety of different unrelated SQL/syntactical errors).
* Switching between tabs and query editor *randomly* erases your query which you were working in the tab.
* Tabs are incoherent: Sometimes query you were working on in tab 1 appears to be in tab 3 after you switch back from the history view.
* Oftentimes, on a misnamed column in query, gives out an error saying Query cancelled by user.  
 (are you serious??)
* The overall UX is so bad: clicking on a query string in a history view sometimes randomly asks you to open the query in a query editor tab, and sometimes doesn't even bother asking and simply navigates to the query editor.

A few other things too, but don't recall them right now. The question really is: Why can't Amazon work on fixing one of its primary products?"
1127,2019-10-15 15:52:59,1571143979.0,dataengineering,An interview about Dataform and how it helps you to keep your data warehouse in good working order,di7gje,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/di7gje/an_interview_about_dataform_and_how_it_helps_you/,13.0,1.0,0.0,7102.0,
1128,2019-10-15 18:30:11,1571153411.0,dataengineering,Secure Self Service Kafka Portal w/ Namespaces+Top Security Features,di9imz,lensesio,,https://www.reddit.com/r/dataengineering/comments/di9imz/secure_self_service_kafka_portal_w_namespacestop/,5.0,0.0,0.0,7102.0,
1129,2019-10-15 19:55:52,1571158552.0,dataengineering,What title would you give this job?,diaqpn,T_house92,,https://www.reddit.com/r/dataengineering/comments/diaqpn/what_title_would_you_give_this_job/,1.0,6.0,0.0,7105.0,"I recently accepted a new position at my company where I will be doing a number of duties. This position is brand new so there is some wiggle room around a title name. I'm wondering if anyone has suggestions on what to call this?

Primary tools / languages:

R, Spark, SQL, Python, javascript.

&amp;#x200B;

What I'll be doing:

* Using my understanding of the many databases my company has to make recommendations around what data we can pull and what we could learn from it
* Develop ETL scripts to combine, clean, and normalize data from multiple internal sources
* Develop and maintain connections to said sources which include cloud storage (AWS, Google) as well as Oracle and MySQL db's 
* Build standardized datamarts and / or functions to quickly pull relevant data for new projects
* Occasional need to build / maintain Shiny web applications
* Assisting in building / validating ML models
* Parsing / mining free text fields using NPL techniques

&amp;#x200B;

Would this fall under a Data Analyst description or does the scope of this go beyond typical Data Analyst duties?"
1130,2019-10-15 23:00:15,1571169615.0,dataengineering,Do you regret migrating to GCP?,didgto,dylancaponi,,https://www.reddit.com/r/dataengineering/comments/didgto/do_you_regret_migrating_to_gcp/,13.0,39.0,0.0,7109.0,"I'm considering a migration from AWS to GCP.  Has anyone done so and regretted their decision?

I'm sure there are annoyances on both platforms.  Are there any I might be overlooking?  Any huge gaps in GCP offerings?"
1131,2019-10-17 02:12:28,1571267548.0,dataengineering,Free sandbox for playing with Spark?,dixkr8,consoleconsumer,,https://www.reddit.com/r/dataengineering/comments/dixkr8/free_sandbox_for_playing_with_spark/,16.0,7.0,0.0,7129.0,"Hi all,

I was wondering if anyone knows of a free (or free trial) of a sandbox type environment for playing with Spark.  At a conference I once had the chance to try Databricks which I really liked, but for idle playing around I'm not sure I can justify paying much for it.  It doesn't seem like there are too many free options out there but I thought I'd ask in case I am missing something!  I wouldn't mind paying a small fee for a month or so but really want to avoid subscription services.  Thank you!"
1132,2019-10-17 17:23:30,1571322210.0,dataengineering,Introduction to NoSQL Databases (Apache Cassandra),dj73bi,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/dj73bi/introduction_to_nosql_databases_apache_cassandra/,3.0,0.0,0.0,7139.0,"Hey all,

I recently completed a small project utilizing Apache Cassandra on a Docker Container as I wanted to gain an appreciation for NoSQL databases and their strengths and weaknesses compared to relational databases. I wanted to share my project and observations/thoughts within an article and I figured I would share the article with you all. I'm just now completing my first year as a data engineer (and as a programmer in the professional setting), so I'm really open to feedback on how I can improve my approach, code skills, and/or enhance this project. I do plan to dive into Spark next as I believe that my current workplace could really benefit from Spark, so I'll likely write up another article about that.

[Article](https://medium.com/@caitlin.ch.johnson/building-a-python-data-pipeline-to-apache-cassandra-on-a-docker-container-fc757fbfafdd?source=friends_link&amp;sk=a69746accea0cdb45c86000062075e01)

Thanks in advance for any feedback!"
1133,2019-10-17 23:17:22,1571343442.0,dataengineering,Google Cloud 100 Gbps Dedicated Interconnect and HA VPN are GA | Google Cloud Blog,djc19b,jrich8573,,https://www.reddit.com/r/dataengineering/comments/djc19b/google_cloud_100_gbps_dedicated_interconnect_and/,1.0,0.0,0.0,7140.0,
1134,2019-10-18 02:06:42,1571353602.0,dataengineering,New to data mapping,djeddh,gecko_electric,,https://www.reddit.com/r/dataengineering/comments/djeddh/new_to_data_mapping/,1.0,3.0,0.0,7145.0,What are some basics I should learn before interviewing for a data mapping position?
1135,2019-10-18 11:41:09,1571388069.0,dataengineering,"Stuck between not enough data to justify HDFS, but need/want certain functions at a local level (ie. not going to the cloud) that appear only available in Hadoop. Any Atlas and Ranger data cataloging and security / auditing analogous systems for local FS, rather than HDFS? Advice?",djkmdb,Anxious_Reporter,,https://www.reddit.com/r/dataengineering/comments/djkmdb/stuck_between_not_enough_data_to_justify_hdfs_but/,10.0,15.0,0.0,7157.0,"Currently have situation between having not much data (mid single-digit to low double-digit TBs (not per data set mind you, but the average total amount)), but desiring tools similar to those that come bundled with hadoop packages, specifically Sqoop, Atlas, and Ranger (which can be easily installed via Ambari with Hortonworks).

Basically, it would cost less to get more drives to store data and RAM to process data right now than to store on HDFS and process with spark, etc than to pay for support from Hortonworks (and in this case not allowed to have HDP without paying for enterprise support), but would still like some way to catalog data sets for end/business user discovery and labeling as well as maintaining audited access to the filesystem. 

Essentially, thinking about how to have a standard data lake architecture like this

[ https:\/\/medium.com\/@rpradeepmenon\/demystifying-data-lake-architecture-30cf4ac8aa07 ](https://i.redd.it/zcjbox2cg9t31.png)

but with Hadoop components (mainly HDFS storage, Sqoop EL, Atlas data catalog, Ranger goverance, and Zeppelin data analysis) replaced with something cheaper and non-Hadoop-based that can still run locally on/across our servers. Most importantly is to have a Sqoop alternative (which we currently use to import 100M+ rows from 100+ column-wide tables each day via virtual tunnel to a remote Oracle DB).

Any general or platform/project/architecture recommendations for this situation?"
1136,2019-10-18 16:59:41,1571407181.0,dataengineering,Online Masters in Data Eng World?,djnwmo,beaverhair,,https://www.reddit.com/r/dataengineering/comments/djnwmo/online_masters_in_data_eng_world/,4.0,9.0,0.0,7158.0,Any suggestions for an online masters somewhere that could be done while working that would be a good fit for data engineering/ data architecture? Even just a solid computer sci/eng masters with a school that is known to have strong data-related courses?
1137,2019-10-18 19:18:44,1571415524.0,dataengineering,Blog: 🚂 On Track with Apache Kafka: Building a Streaming ETL solution with Rail Data,djpt02,rmoff,,https://www.reddit.com/r/dataengineering/comments/djpt02/blog_on_track_with_apache_kafka_building_a/,5.0,0.0,0.0,7162.0,
1138,2019-10-19 01:28:41,1571437721.0,dataengineering,"Science vs Engineering - Remote, Difficulty",djuxod,scikit-Carson,,https://www.reddit.com/r/dataengineering/comments/djuxod/science_vs_engineering_remote_difficulty/,16.0,15.0,0.0,7170.0,"Hello,  


In which specialty is it more common to find remote positions: Data Science or Data Engineering? Which lends itself better to remote work technically?  


Also, many people with non-technical backgrounds are retraining into Data Science careers. Do you think going straight into DE instead is harder or easier?  


Interested to learn your opinions. Thanks!"
1139,2019-10-19 11:24:15,1571473455.0,dataengineering,How realistic is it to build your own data glossary / catalog,dk0xaq,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/dk0xaq/how_realistic_is_it_to_build_your_own_data/,4.0,16.0,0.0,7176.0,
1140,2019-10-19 11:25:36,1571473536.0,dataengineering,Tool to automatically aggregate / summarise data,dk0xo6,WeirdFail,,https://www.reddit.com/r/dataengineering/comments/dk0xo6/tool_to_automatically_aggregate_summarise_data/,1.0,5.0,0.0,7176.0,"I'm a product manager and have access to some (read only) product usage tables which show the number of sessions and actions completed in various web products.

I have manually created scripts which aggregate these into another reporting DB which has simpler aggregated stats - e.g. number of sessions by month etc. 

Are there tools that would take care of this aggregation automatically? Or at least in a less manual way? It would have to be self hosted.

Thanks for any pointers!"
1141,2019-10-19 17:23:48,1571495028.0,dataengineering,Chemist turned Data Engineer,dk4ew3,throwawaygrad001,,https://www.reddit.com/r/dataengineering/comments/dk4ew3/chemist_turned_data_engineer/,14.0,7.0,0.0,7178.0,"I am a PhD chemist that found themselves in a data engineering job. I was hired by a large lab to essentially build their data pipeline from scratch. The way the lab handles data right now is that everyone does their own thing in excel and all the data is stored in excel files or paper reports. The lab has grown to the point where they analyze so many samples that this takes them too long and its difficult for them to manage. The PIs want to grow the lab even more so it will eventually become even more unmanageble for them, that's where I'm suppose to come in. 

Here's the catch- I am the sole person who works on this. There is nobody here who does similar work. Even my supervisor doesn't really know much about what I do- I only see them every couple of days. My background is primarily in chemistry but I also have some background in CS, which is why I got this job. Though my CS background is mostly in terms of data analysis (data cleaning, wrangling, making visualizations, etc...basically things that were relevant to my chem work or data analyst internship I did) and a tiny bit of data science from a DS course I took in grad school. I don't have any experience building data pipelines. I didn't even know how to classify my job for a long time (my actual title is something really generic like ""scientist"") until I did some research and realized the type of work I do seems to fit data engineer/analyst the best.  

I've been in this job for about 2 months now, so far what I've been working on (as requested by my bosses) is making mini python apps (which mostly use pandas) for the lab staff to read in raw instrument data and make a report of final results for them or automatically fill out report templates they have. Basically, trying to get them away from doing dozens of copy-pastes in excel and also add some consistency in terms of format. 

I don't really know what is suppose to come after that. I'm looking for suggestions as to where I should start in terms learning that would be helpful for my job. I've read about lots of different software/packages like spark, airflow, etc, etc. 99% of things I've never heard of before. I'm not sure what would actually be useful for the kind of work I do. This isn't like a large company that produces millions of data so I feel like somethings might be overkill or not applicable for what they need. I also don't have the resources that a large data company would have. Basically, sometimes I feel like I am way over my head and not remotely qualified for this but I want to use this opportunity to learn and gain as much experience as I can. I took this job mostly because they strongly encourage taking time while at work for research/self learning (it is mostly a research lab after all)"
1142,2019-10-20 03:20:22,1571530822.0,dataengineering,Production Airflow cluster deployment,dkc5mm,jrich8573,,https://www.reddit.com/r/dataengineering/comments/dkc5mm/production_airflow_cluster_deployment/,12.0,23.0,0.0,7182.0,"My company is tossing around the idea of moving all data pipelines (60+ pipelines), which are all batch jobs, to Apache-Airflow. Anyone have any advice, gotchas, or “if I could start over, I would change...” related to deploying Airflow in production. 

Thanks for help!"
1143,2019-10-20 17:40:27,1571582427.0,dataengineering,Faster ClickHouse Imports,dkkgir,marklit,,https://www.reddit.com/r/dataengineering/comments/dkkgir/faster_clickhouse_imports/,8.0,2.0,0.0,7191.0,
1144,2019-10-21 23:21:08,1571689268.0,dataengineering,How is Apache NiFi?,dl6ayb,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/dl6ayb/how_is_apache_nifi/,6.0,13.0,0.0,7215.0,"Im a junior DE who just started. Ive been tasked with picking an ETL and building out data pipelines.

Currently, i have a few cron jobs and some simple python scripts to ingest data.

I was considering airflow, but looking at how to install it is giving me a headache. Considering im the only DE, i was considering something like NiFi since its more straightforward, but id really like something python based.

What do you think? Is NiFi a better option?"
1145,2019-10-22 01:08:21,1571695701.0,dataengineering,What are some must-know Spark functions?,dl7ti8,jbnpoc,,https://www.reddit.com/r/dataengineering/comments/dl7ti8/what_are_some_mustknow_spark_functions/,17.0,12.0,0.0,7217.0,"I know this is a pretty granular question, but what are some must-know Spark, specifically PySpark, functions for a data engineer? I need to brush up on it for an interview and after having not used it for a full year, I've admittedly forgotten a lot of it. And thinking about it, I didn't use it very extensively, so my knowledge about Spark is coming up short.

I can do I/O fairly easily, but I'm looking for data-wrangling functions that any data engineer should know. Not sure if I'll have access to documentation during the interview"
1146,2019-10-22 05:49:54,1571712594.0,dataengineering,Take charge of your data: Scan for sensitive data in just a few clicks | Google Cloud Blog,dlbjj9,jrich8573,,https://www.reddit.com/r/dataengineering/comments/dlbjj9/take_charge_of_your_data_scan_for_sensitive_data/,0.0,0.0,0.0,7218.0,
1147,2019-10-22 10:59:02,1571731142.0,dataengineering,advice or best practices for running R/Python jobs and monitoring them?,dlepq1,cntrl_,,https://www.reddit.com/r/dataengineering/comments/dlepq1/advice_or_best_practices_for_running_rpython_jobs/,12.0,8.0,0.0,7220.0,"I hope r/dataengineering is the right place to ask this question.  
My small team and I (we're just three people) are running a couple of data dashboards / web applications (based on R Shiny) to provide various departments of our company with additional data insights.   
Our setup is very minimal and consists of a linux server that runs a postgres database, jobs and also hosts the applications via docker.   
our applications are decently optimized and our userbase is not that huge (currently around 20-30 concurrent users max., but we expect it to grow rapidly in the coming year), therefore performance is fine right now.

each project has various data sources that need to be processed in advance in order to query them from within our applications. This results in a lot of small jobs that produce different outcomes and log files. Since the number of processes is increasing rapidly, we're thinking about a suitable way to monitor them.  
our idea right now is to set up a new app that basically just processes the log files and shows an overview over each job with some basic info. 

since none of us has a real engineering background and we're also rather inexperienced in ""scaling up"" these data processes, we'd love to get some advice regarding our setup.

I'd like to narrow it down to these three questions:  
1. what is a good way to monitor these R / Python Scripts? Are there some tools that basically do what we're trying to achieve with our monitoring app?  
2. is our current approach of script based ""data pipelines"" a good practice in the long run, all of them running as cron jobs?  
3. do you have any recommendations for additional ressources to a better platform?"
1148,2019-10-22 15:19:48,1571746788.0,dataengineering,An interview about the emerging category of data orchestration platforms and how they can be used to bridge the gap between modern and legacy analytics systems,dlh42q,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/dlh42q/an_interview_about_the_emerging_category_of_data/,25.0,0.0,0.0,7224.0,
1149,2019-10-23 00:09:34,1571778574.0,dataengineering,Has anyone user Redis streams for realtime data processing /ingestion,dloiwl,kenaki_the_last,,https://www.reddit.com/r/dataengineering/comments/dloiwl/has_anyone_user_redis_streams_for_realtime_data/,5.0,0.0,0.0,7232.0,
1150,2019-10-23 21:21:32,1571854892.0,dataengineering,Is HAVING a redundant concept ?,dm3cg2,elcric_krej,,https://www.reddit.com/r/dataengineering/comments/dm3cg2/is_having_a_redundant_concept/,1.0,11.0,0.0,7245.0,"Is there a good argument to have \`HAVING\` as part of a SQL dialect ?

It seem to me that most SQL entities have the express role of:

a) Making an operation possible

b) Speeding up and operation

&amp;#x200B;

I don't see \`HAVING\` as having any of those roles.

a)

It seems to me like \`HAVING\` could in all cases be easily abstracted by two extra brackets and a select \*

E.g. \`SELECT COUNT(\*) as cnt, entitiy FROM table GROUP BY entitiy HAVING cnt &gt; x\` translates painlessly into \`SELECT \* FROM (SELECT COUNT(\*) as cnt, entitiy FROM table GROUP BY entitiy) WHERE cnt &gt; x\`.

Is there a situation where such a simplification would not work or would be to cumbersome to write.

&amp;#x200B;

b)

It seems like the optimizations that can be done via \`HAVING\` are minimal and would still be about as easy to implement in the case of only having \`WHERE\`.

&amp;#x200B;

Given a table that's something like:  \`amount, sender, receiver\` and a query of the form:

\`SELECT count(distinct(receiver)) as unique\_receiver, sum(amount) as total, COUNT(\*) as cnt, sender FROM table GROUP BY sender HAVING unique\_receiver &gt; 5 AND total &lt; 6000\`

I can think of two possible optimization:

1. When we are using \`&gt;\` on an expensive aggregated value (in this case \`COUNT + DISTINCT\`), we could use the already computed result of another \`&gt;=\`  aggregated value (in this case \`cnt\`) and eliminate certain rows. So if we figure out \`cnt\` is, say, 3 for a specific \`sender\` in the above query, we know \`unique\_receiver\` &lt;= 3, so we can skip computing it for that \`sender\` since it would be filtered out by the \`HAVING condition\`
2. When using \`&lt;\` the same logic could be used, but for a \`&lt;=\` aggreagted value that's cheaper to compute (though I can't think of a practical case where this might happen)
3. When using \`&lt;\`, given that any aggregated value has already reached a number bigger than what we are comparing it against, we can safely ignore the \`GROUP BY\` combination where this has happened, or finish the query and return nothing if no \`GROUP BY\` is present (in the case above, let's say \`total\` for an account reaches 6001 half way through iterating the rows for a given \`sender\`, when can now ignore any other rows with that specific value for \`sender\` and we can save memory by removing the existing row from the group by)

However, realistically speaking, the above optimizations require knowledge of aggregated metric complexity, when introducing check throughout the query code for certain conditions which would slow things down in most cases and in certain cases (e.g. for SUM or AVG) they only work if all the values in the column's being aggregated are unsigned, or maybe they could work otherwise, but in a reduced scope and with harder check or time spent constructing extra optimization metrics whilst computing the main metric.

Also, realistically speaking, I don't know of any database that actually implements something like this, I can't image that many queries benefiting from this optimization greatly and, if this optimization is being done, I see no reason for not implementing it in the multiple-SELECT scenario where \`HAVING\` is replaced by a \`WHERE\` on the query result.

So... how comes all SQL dialects seem to implement having ? 

Is there some magic behind it that I am missing ?

Is it just legacy standards that kinda stuck  ? (It's not like SQL language that implement it are all ANSI SQL compatible)"
1151,2019-10-24 08:13:51,1571894031.0,dataengineering,Trying to come up with an end to end framework / process from gathering business need to code shipped.,dmbqth,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/dmbqth/trying_to_come_up_with_an_end_to_end_framework/,4.0,2.0,0.0,7253.0,"Anyone have a process tool or any form of reference to implement this. Something that guides or links together user stories / an erd / GitHub , so one can effectively data engineer ?"
1152,2019-10-24 10:44:19,1571903059.0,dataengineering,I would like your feedback on our data engineering homework assignment,dmd2me,MarkiesFredje,,https://www.reddit.com/r/dataengineering/comments/dmd2me/i_would_like_your_feedback_on_our_data/,28.0,9.0,0.0,7254.0,
1153,2019-10-24 20:07:58,1571936878.0,dataengineering,How to use SQL and Lenses Data API to explore Streaming Data in Apache Kafka,dmje7z,lensesio,,https://www.reddit.com/r/dataengineering/comments/dmje7z/how_to_use_sql_and_lenses_data_api_to_explore/,3.0,0.0,0.0,7263.0,
1154,2019-10-25 02:12:08,1571958728.0,dataengineering,Job lineage tool,dmom2p,ihaleem9,,https://www.reddit.com/r/dataengineering/comments/dmom2p/job_lineage_tool/,1.0,0.0,0.0,7274.0,I have a ton of ETL jobs with multiple dependencies. I have this data and I want to visualize it. What open source tools can I use for this?
1155,2019-10-25 04:43:04,1571967784.0,dataengineering,Switching from ms sql developer to data engineering,dmqf5a,ThatFilm,,https://www.reddit.com/r/dataengineering/comments/dmqf5a/switching_from_ms_sql_developer_to_data/,7.0,12.0,0.0,7274.0,"  For last 10 years  I’ve been working with ms sql server database developer/admin, I like to switch now to data engineering role. 

I am looking at data engineering jobs on zip recruiter.

Can you give me any ideas on projects or books that I can read so I can find a new job. 

I’ve been learning so far about python and then will move to Linux scripting, I have a good background but I am not sure how to apply to the new role.

All my previous jobs have used Microsoft products.

Thanks"
1156,2019-10-25 11:24:30,1571991870.0,dataengineering,I want to become a data engineer from a completely non-quantitative background. Critique my plan?,dmuef3,throwawaystickies,,https://www.reddit.com/r/dataengineering/comments/dmuef3/i_want_to_become_a_data_engineer_from_a/,11.0,24.0,0.0,7276.0,"**About me:**
BSN grad internationally, 3.0 GPA, 29 yo, currently working as a medical coder for a large  hospital. My end goal is to become a data engineer in a hospital, working with medical data.

**What I’ve been doing:**

I’ve taken:

• Intro to CS from Udacity

• Programming Foundations in Python from Udacity

• Currently finishing SQL Bootcamp from Jose Portillo in Udemy (we are working on PostgreSQL right now)


**My plan A:**
• If accepted, continue working full-time and follow the 3.5 year plan of taking one course a semester, thereby minimizing student debt as much as possible.

• While in the program, work towards getting a Data Analyst position.

• After graduation, do the Data Engineering nanodegree from Udacity.


**My plan B:**

• Obtain as much Python and SQL skills from udemy then finish the DA nanodegree from Udacity

• Apply for DA jobs and for WGU’s MSDA program

• Finish MSDA and do Udacity’s DE nanodegree

Any thoughts?"
1157,2019-10-25 18:01:01,1572015661.0,dataengineering,How to Migrate Self Managed Kafka to HDInsight via Gitops - Walkthrough,dmyny7,lensesio,,https://www.reddit.com/r/dataengineering/comments/dmyny7/how_to_migrate_self_managed_kafka_to_hdinsight/,2.0,0.0,0.0,7281.0,
1158,2019-10-25 18:52:52,1572018772.0,dataengineering,Getting started in DE as a high school student?,dmzd86,bromideforge,,https://www.reddit.com/r/dataengineering/comments/dmzd86/getting_started_in_de_as_a_high_school_student/,1.0,13.0,0.0,7281.0,"Hey all,

I'm a sophomore(10th grade) who is interested in technology. I've begun getting into programming and the general world surrounding the many different things you can do, and data engineering has caught my eye as an interesting career, given that it's about time to start thinking about the future. 

What could be some things to focus on to get started in a career? There seems to be a lot of different languages/technologies out there(R, Python, Scala, SQL, for technologies/software: Spark, Hadoop, AWS etc.), 

Personally, I'm thinking of starting out with learning some CS fundamentals with Python, then SQL and getting into and playing with databases, analyzing data, but from thereon out there is a lot more to branch out on. Also, I know this is intertwined a bit with data science(I don't like the term but I'm not sure what could be better used), so should statistics also be something to check out a bit?

Thanks!"
1159,2019-10-25 23:24:51,1572035091.0,dataengineering,"Databricks introduces MLflow Model Registry, brings Delta Lake to Linux Foundation",dn38y9,LJefe1511,,https://www.reddit.com/r/dataengineering/comments/dn38y9/databricks_introduces_mlflow_model_registry/,18.0,3.0,0.0,7284.0,
1160,2019-10-26 19:42:33,1572108153.0,dataengineering,New DB internals book released,dngazb,Chr0nomaton,,https://www.reddit.com/r/dataengineering/comments/dngazb/new_db_internals_book_released/,15.0,3.0,0.0,7294.0,"Incase anyone is interested [https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347](https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347). Seems to cover some similar material to Data Intensive Applications, but I'm not sure."
1161,2019-10-26 23:14:08,1572120848.0,dataengineering,Imposter syndome for new/junior DE's,dnj3zj,Omar_88,,https://www.reddit.com/r/dataengineering/comments/dnj3zj/imposter_syndome_for_newjunior_des/,17.0,21.0,0.0,7297.0,"Hi r/dataengineering

&amp;#x200B;

recently transitioned from a DA role into DE, heavily focusing on MS Stack &amp; Apache Spark (python flavour) 

I'm about a month in and I'm making progress but still feel out of my depth (especially when I started going through the process of making a DW)!. So my question is to the veterans who have gone through the whole beginner to expert process, how long did it take you to get over that initial feeling of being a noob.

Logically I know that for the first 6 months in *most* jobs that my productivity will most likely be a net negative number. 

would love to hear views from folk who have come from non-traditional backgrounds (cs/math/engineering)"
1162,2019-10-27 05:58:16,1572148696.0,dataengineering,What does data engineering look like for Microsoft snow leopard,dnofht,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/dnofht/what_does_data_engineering_look_like_for/,7.0,3.0,0.0,7298.0,At what point in the process do data engineers make contribution and what does contribution look like?
1163,2019-10-28 10:03:07,1572249787.0,dataengineering,Faster ZIP Decompression,do5t3t,marklit,,https://www.reddit.com/r/dataengineering/comments/do5t3t/faster_zip_decompression/,2.0,0.0,0.0,7308.0,
1164,2019-10-28 16:01:04,1572271264.0,dataengineering,An interview about the Dagster framework and how you can use it to build testable and maintainable data applications,do98b4,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/do98b4/an_interview_about_the_dagster_framework_and_how/,2.0,2.0,0.0,7308.0,
1165,2019-10-28 18:21:13,1572279673.0,dataengineering,Data Integration with Apache Kafka and Attunity [Podcast],dob3si,vicksyu,,https://www.reddit.com/r/dataengineering/comments/dob3si/data_integration_with_apache_kafka_and_attunity/,11.0,1.0,0.0,7312.0,
1166,2019-10-28 20:11:10,1572286270.0,dataengineering,Choosing the right Tools,docnx0,gh0st_py,,https://www.reddit.com/r/dataengineering/comments/docnx0/choosing_the_right_tools/,3.0,10.0,0.0,7316.0,"Hey everyone, I am wondering how you guys decide which tools to use for each use case. I am currently in this mess of Kafka vs. Kinesis. It seems like Kafka is the right way to go due to the freedom it gives you, but Kinesis just seems easier to set up. I am having a hard time deciding where my use case lands.

I want to send pc data to the stream -&gt; poll/pull from the stream -&gt; analyze -&gt; act on said data. A lot of this data would be published to a dashboard somewhere as well. Any advice?"
1167,2019-10-30 18:19:35,1572452375.0,dataengineering,How do large social network clients track and store user journeys?,dp9028,datadino211,,https://www.reddit.com/r/dataengineering/comments/dp9028/how_do_large_social_network_clients_track_and/,10.0,4.0,0.0,7343.0,"Take instagram or twitter for example: how do analysts know if a user navigated to a post from their feed or by searching? I can imagine a data pipeline that sorts UI events on time and attempts to reconstruct the user journey that way. Or the client can keep track of every previous action that leads up to a UI event in a session. Probably several other ways to do this that I'm not thinking of... 

Are there any industry best practices on these or other approaches? Blog links or papers would be very useful as I'm having trouble coming up with anything on google"
1168,2019-10-30 18:34:13,1572453253.0,dataengineering,The Complete Data Science LinkedIn Profile Guide,dp96xx,BigCloudTeam,,https://www.reddit.com/r/dataengineering/comments/dp96xx/the_complete_data_science_linkedin_profile_guide/,10.0,0.0,0.0,7343.0,
1169,2019-10-30 22:51:35,1572468695.0,dataengineering,Why are more job openings for data engineers than job openings for data scientists?,dpcnhk,nouseforaname888,,https://www.reddit.com/r/dataengineering/comments/dpcnhk/why_are_more_job_openings_for_data_engineers_than/,1.0,6.0,0.0,7346.0,I apologize if this question has been asked before but I am wondering why this is happening at the moment.!
1170,2019-10-31 01:21:04,1572477664.0,dataengineering,Seeking guidance to pick up data engineering skills,dpertd,jackofallfruits,,https://www.reddit.com/r/dataengineering/comments/dpertd/seeking_guidance_to_pick_up_data_engineering/,13.0,10.0,0.0,7347.0,"I'm seeking guidance on how to pick up data engineering skills and possibly navigate my career that direction. 

I don't have background in computer science. I came from civil engineering undergrad whose career slowly steers toward analytics after an MBA and MS in Econometrics. I'm a jack of all trades master of none data scienctist (possibly even data analyst depending on people's definition). 

- My coding skills are basic. I know R from my econometrics program, and I use python because we use GIS quite a lot. 

- Stuff that I do comprises time series (mostly with commercial package called eviews, sometimes R or Python), some predictive analytics with sklearn, spatial statistics and dashboards (tableau/powerBI). 

- Building ETL. **However**, we use commercial packages - FME for spatial data, talend/alteryx for everything else.

- No big data experience (no spark hadoop etc), no cloud experience (aws, GCP, exp). We're federally regulated, so everything is on-prem. 

- I spend like 80% of my time on data hunting, collection and cleaning. Perhaps only 10% of my time is spent building models, and another 10% on communicating the results and documenting them and putting them into our knowledge management system.

- I don't do NLP or computer vision

I'm sorry for the very long post, but I'd appreciate someone pointing me in the direction where I can pick up data engineering skills. 

I thought about doing UPenn's online MCIT because I don't have the CS education. I've also considered GATech's OMSA since I don't think I have the necessary foundation for the OMSCS. 

If at all possible, I'd like to avoid going back to school. I can't stomach another masters, and honestly I'd rather work on practical projects building something and have something to show for it. Not another paper. Thank you much!"
1171,2019-10-31 18:00:07,1572537607.0,dataengineering,Would taking a data engineer role for a year or two help me towards my end goal of becoming a data scientist?,dppkpm,fuzzywunder,,https://www.reddit.com/r/dataengineering/comments/dppkpm/would_taking_a_data_engineer_role_for_a_year_or/,23.0,22.0,0.0,7357.0,"I currently work as a statistician and have employed some data science techniques into my work. In use R, SQL, git and Azure Dev ops and I’ve also been doing a bit of data engineering work in SQL.

I’ve been offered a potential move into a data engineer role. Although it isn’t exactly what I want to be doing (I would eventually like a data science role) it still looks very interesting and I thought it might be a good basis to keep developing my data science skills and programming skills. 

I have applied for a few data science roles but have been pipped to the post by other candidates because of my lack on experience in building ds models.

So my questions is, is this a move in the right direction? Would having a really good understanding of data engineering help me to develop my data science career? 

In some places I’ve read that data engineering and data science cross over a lot so it sounds like it could be a good move.

Any data engineers and data scientists out there that can offer any information?

Thanks!"
1172,2019-10-31 19:42:41,1572543761.0,dataengineering,Chinese users attack Notepad++ app after 'Free Uyghur' release | ZDNet,dpr1f4,jrich8573,,https://www.reddit.com/r/dataengineering/comments/dpr1f4/chinese_users_attack_notepad_app_after_free/,14.0,0.0,0.0,7357.0,
1173,2019-10-31 20:39:13,1572547153.0,dataengineering,Spark Streaming?,dpruey,gh0st_py,,https://www.reddit.com/r/dataengineering/comments/dpruey/spark_streaming/,2.0,2.0,0.0,7358.0,"Hey guys,

So I have log data coming into Kinesis and I am currently pulling that log data with vanilla python, I need to run business logic on this log data and have close to real time results. I noticed spark streaming seems to be something that works well for this. The problem is I am having a hard time understanding why. As far as I can tell it breaks the data into time series chunks? the something below would be the business logic. Any help would be greatly appreciated.

Logger -&gt; Kinesis -&gt; SOMETHING (Spark Streaming?) -&gt; Dashboard/Web page/Database"
1174,2019-11-01 03:15:12,1572570912.0,dataengineering,Are There No Good Airflow Tutorials?,dpxb1v,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/dpxb1v/are_there_no_good_airflow_tutorials/,24.0,38.0,0.0,7361.0,"Im a junior DE who just started using Airflow.

Ive built a couple of dags now im still not sure I get it. Things like data must be at rest between tasks just dont make sense to me. Like cant i just pass a dataframe between tasks?

Are there any good real world examples out there? Things like grabbing files off a ftp and importing, or pulling data from an api. It just hasnt clicked for me yet."
1175,2019-11-01 16:59:57,1572620397.0,dataengineering,Google to acquire Fitbit,dq5dhs,jrich8573,,https://www.reddit.com/r/dataengineering/comments/dq5dhs/google_to_acquire_fitbit/,0.0,2.0,0.0,7371.0,
1176,2019-11-01 20:34:47,1572633287.0,dataengineering,Career path after Data Engineer?,dq8b6z,jwdatascience,,https://www.reddit.com/r/dataengineering/comments/dq8b6z/career_path_after_data_engineer/,21.0,11.0,0.0,7373.0,"I'm thinking about my career and wanted insight into other vets in the DE space on their job title or career path goals...

Since it's a relatively new space, I was wondering what job titles exist currently besides ""Senior Data Engineer"" or ""Data Architect"". What's after that?"
1177,2019-11-02 02:47:14,1572655634.0,dataengineering,OSS Great Expectations just released a Self-Updating Data Dictionary,dqd885,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/dqd885/oss_great_expectations_just_released_a/,13.0,7.0,0.0,7378.0,"Check out this really great tutorial/blog on how to implement the ""Self-Updating Data Dictionary"". Its pretty awesome for how low friction it is to implement. Solid tool to maintain transparency across the data engineering / data science team  and whoever else interacts with them.

[Read here](https://greatexpectations.io/blog/20191004_data_dictionary_plugin/?utm_source=reddit&amp;utm_medium=post&amp;utm_name=user-testing)

[Their Github](https://github.com/great-expectations/great_expectations)"
1178,2019-11-02 04:42:16,1572662536.0,dataengineering,What languages are you using?,dqeia1,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/dqeia1/what_languages_are_you_using/,6.0,12.0,0.0,7378.0,"As a data engineer, what programming languages do you use regularly? 

I am asking because I am almost exclusively python and I am curious if this will limit me. If so what would be your suggestions?"
1179,2019-11-02 23:10:32,1572729032.0,dataengineering,Where to start?,dqqj0b,Bobby_Pine,,https://www.reddit.com/r/dataengineering/comments/dqqj0b/where_to_start/,7.0,4.0,0.0,7387.0,"Hi all,

I’m really new to the concept of data engineering but it intrigues me. I went to school for accounting but in my job I’ve excelled at using SQL and Python to solve issues. I really enjoy creating tools, interfaces, helping others pull data for analysis, etc. Data science isn’t really something that speaks to me, it’s a big buzz word in my work but after reading about data engineering it seemed like something better suited to my interests. 

I know nothing about this field, where can I start at an incredibly high level? I want to learn more."
1180,2019-11-03 00:02:00,1572732120.0,dataengineering,Noob looking for guidance,dqr7rw,araknadash,,https://www.reddit.com/r/dataengineering/comments/dqr7rw/noob_looking_for_guidance/,7.0,9.0,0.0,7386.0,"Hi everyone! 
I am Masters graduate in Computer engineering. I used to work in a Brain machine interfacing lab where we study the relationship between brain waves and motor signals. 
I did few projects on my lab data using python and R to make sense of the raw data we collected. I was wondering if somebody could give me some pointers or suggest me courses that I can do to get started in Data engineering field ! 
I would love any kind of advice!"
1181,2019-11-03 07:19:07,1572758347.0,dataengineering,Apache Airflow Cluster Issues,dqw8zf,theant97,,https://www.reddit.com/r/dataengineering/comments/dqw8zf/apache_airflow_cluster_issues/,5.0,3.0,0.0,7390.0,"Hi All,

We are using Airflow with few complexities:

1. If I create a Airflow cluster(1 Master and 2 Workers) . Logs are written in Elastic file system and Properties file(run id and InstanceID ) are being written in EFS . However EFS is really slow and We switched to EBS . Every hop is failing because the properties isnt being shared in local EBS volume if we have two workers withour EBS. EFS to EBS performance is boosted from 2 hours run to 10 mins run of a dag in Airflow.
2. We have a RDS table to check the dependencies . We tried to replace control-m with Airflow. 
   1. Daily and Weekly(sunday) scheduled dependencies are fine. For example : For a dag to run all the dependencies should be met . In our case weekly dependencies only will be met only on weekly scheduled day(sunday). Remaining days we are manually marking the dependencies as success . What logic should we think or how to tackle this."
1182,2019-11-03 17:30:42,1572795042.0,dataengineering,Currently use Pandas. Should I Switch to Spark?,dr1wom,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/dr1wom/currently_use_pandas_should_i_switch_to_spark/,6.0,19.0,0.0,7394.0,"Currently I use pandas in all my airflow dags/scripts. 
Would it be worth me switching to pyspark? What benefit would it provide?"
1183,2019-11-03 18:30:46,1572798646.0,dataengineering,"Databricks Delta, Apache Hudi, and Apache Iceberg for building a Feature Store for Machine Learning",dr2rni,limmen,,https://www.reddit.com/r/dataengineering/comments/dr2rni/databricks_delta_apache_hudi_and_apache_iceberg/,19.0,3.0,0.0,7394.0,
1184,2019-11-04 06:26:02,1572841562.0,dataengineering,How to really build a career as a data engineer and beyond ?,drci4e,acceptedcitizen,,https://www.reddit.com/r/dataengineering/comments/drci4e/how_to_really_build_a_career_as_a_data_engineer/,1.0,5.0,0.0,7398.0,Currently my first few roles as a data engineer. I want to do my job well and also lay the foundation to be on top of my career in the future. What projects are best for data engineer to complete ( like idk building data warehouses for analytics) and what companies or products to reference as role models. I know DJ Patil was the CDO for Obama so he is a good example. Just ideas like how to be the best you can be
1185,2019-11-04 09:30:14,1572852614.0,dataengineering,Good books/resources for Kappa/Lambda Architecture for Data Pipelines,dreavz,anshulp25,,https://www.reddit.com/r/dataengineering/comments/dreavz/good_booksresources_for_kappalambda_architecture/,9.0,3.0,0.0,7400.0,"I have switched from Build/Release Engineering to Data Engineering and relatively new to it. I am currently tasked with creating Data Pipelines. Can you guys suggest good resources (videos, courses, books, etc)?"
1186,2019-11-04 11:23:33,1572859413.0,dataengineering,Looking for mentorship or remote internship in data engineering,drf8wy,kolaol22,,https://www.reddit.com/r/dataengineering/comments/drf8wy/looking_for_mentorship_or_remote_internship_in/,1.0,0.0,0.0,7403.0,
1187,2019-11-04 14:19:25,1572869965.0,dataengineering,Acquire a Data Engineer position at Google,drgu46,Blackmorse,,https://www.reddit.com/r/dataengineering/comments/drgu46/acquire_a_data_engineer_position_at_google/,9.0,16.0,0.0,7405.0,"Not sure, that I choose right subreddit for this post, but hope this will work :)
My question is how to get this job at Google? Maybe this question is asked too often, but I didn't find a lot information in web about exactly Data Engineer position.
Currently I have an 2,5 years experience at data engineer position ( and 3 years of Java developer before that), also I started to learn algorhitms(books Algorhitms in Java, Cracking the Code Interview), some concepts of programming(functional programming, different languages (Scala, Python, Rust)), different databases (from relational to NoSQL like ClickHouse, Cassandra),  advanced Spark techniques, Cloud Services, Machine Learning Courses, etc...(I can talk about this for hours)

I just want to know , Is it the right direction? Or maybe should I adjust something? (or everything?:) ) I've heard about Google CLoud Platform Certification, Is it must-have for position at Google?

I'm not sure, will the huge amount of information about ""normal"" Google interviews work for the interview of Data Engineer?
It will be great to hear opinion from people who have an interviews at Google, or maybe from ones who was interviewing somebody for this position.

THanks for help"
1188,2019-11-04 15:15:37,1572873337.0,dataengineering,A Definitive Compilation of Apache Airflow Resources,drhgm0,jornaha,,https://www.reddit.com/r/dataengineering/comments/drhgm0/a_definitive_compilation_of_apache_airflow/,35.0,2.0,0.0,7406.0,
1189,2019-11-04 15:18:58,1572873538.0,dataengineering,An interview about how the Ascend platform provides an autonomous data orchestration platform to simplify your production dataflows,drhhx9,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/drhhx9/an_interview_about_how_the_ascend_platform/,0.0,0.0,0.0,7406.0,
1190,2019-11-04 19:08:54,1572887334.0,dataengineering,Am I missing something in regards to BigQuery costs vs Snowflake?,drklwr,Flintron,,https://www.reddit.com/r/dataengineering/comments/drklwr/am_i_missing_something_in_regards_to_bigquery/,12.0,14.0,0.0,7408.0,"We're in the process of evaluating Big Query and Snowflake for a near real time reporting solution we are building

For Snowflake we did a trial run and were able to come up with an estimated yearly cost fairly easily

We are now evaluating Big Query and based on our use case it looks extraordinarily cheap. I'm talking many orders of magnitude cheaper

Our use case is to develop various dashboards and reports for one of our teams. The data the reports will be querying is perishable i.e. after a certain time, we no longer care about it and will move it to another table. This means there is a general maximum table size we can expect on any given day

Using this we have done some, admittedly, rough calculations and BQ is looking a lot cheaper. I would have thought pricing would be somewhat similar. 

Am I missing something obvious about BQ pricing?"
1191,2019-11-04 23:44:59,1572903899.0,dataengineering,Monzo Data Team,drornh,t15k,,https://www.reddit.com/r/dataengineering/comments/drornh/monzo_data_team/,1.0,1.0,0.0,7410.0,
1192,2019-11-05 07:16:04,1572930964.0,dataengineering,How do I master for SQL questions for FAANG in one week?,drunfa,rossofcode,,https://www.reddit.com/r/dataengineering/comments/drunfa/how_do_i_master_for_sql_questions_for_faang_in/,10.0,15.0,0.0,7416.0,"I have a data engineering interview in one week and I am struggling in SQL. I do understand all of it but unable to solve questions on Leetcode. So far, my experience was in etl, data lakes etc but not much in SQL.

I even have issue in visualisation of the flow of control that my potential query should follow."
1193,2019-11-05 07:19:14,1572931154.0,dataengineering,Seeking Toronto data engineers,druokr,mbellm,,https://www.reddit.com/r/dataengineering/comments/druokr/seeking_toronto_data_engineers/,1.0,0.0,0.0,7416.0,"Not sure if this type of post is allowed - my apologies if not.

I work for a high growth VC funded tech startup in downtown Toronto where my team is currently focused on building out our data infrastructure . Specifically, we’re building a reporting database on top of our production database. We’re looking to hire a data engineer to act a part time consultant / mentor to help guide us through this process. 

The time commitment would be once or twice every two weeks, likely outside of regular work to accommodate for your full time job. A lot of the work would be consultative high level guidance, but would also require lower level expertise. We are planning on working with Postgres and Python.

Please send me a direct message if this is something you are interested in, and I’d be happy to provide further details and schedule a call to see if it might be a good fit."
1194,2019-11-05 19:16:05,1572974165.0,dataengineering,Data Analyst wanting to make the move into SWE/Data Engineering. How do I do it?,ds295j,johnnyplatanos,,https://www.reddit.com/r/dataengineering/comments/ds295j/data_analyst_wanting_to_make_the_move_into/,26.0,27.0,0.0,7427.0,"I'm currently a data analyst, and I've been working as one to varying degrees for the past 10 years.  My current role has been the most technical, where I use some SAS, SQL, and Tableau.  A lot of what I do is pulling data from our servers, modeling it with SAS &amp; SQL, and then building dashboards.

Anyway, if you look at my post history, I've been considering moving from this data analyst role to something more technical, e.g. SWE or Data Engineering.  Data Engineering seems to make the most sense since I'm somewhat acquainted with data.  I'm enrolled in Georgia Tech's Masters in Analytics program, but will most likely be dropping out because I don't want to move further into Analytics and Data Science.  

How do I make myself qualified for a role in the data engineering field?  I've done some self-teaching in Python, took a break for a little too long, forgot mostly everything, but will be revisiting that soon.  I'm also going to be taking a few community college courses in data structures, algorithms, etc., but I don't if that'll be that helpful for data engineering.  I'm just trying to get a little more exposure to SWE.  What are some other things I should do to make myself a more qualified applicant?"
1195,2019-11-06 00:03:04,1572991384.0,dataengineering,Is this an example of an ETL pipeline?,ds6cuo,learnthemachines,,https://www.reddit.com/r/dataengineering/comments/ds6cuo/is_this_an_example_of_an_etl_pipeline/,7.0,10.0,0.0,7427.0,"I'm a data scientist who is currently learning some data engineering processes for my role.

Currently, I am running a Jupyter Notebook with Python for implementing time series models in Amazon SageMaker. My current set up:

1. CSV files uploaded to AWS S3 bucket

2. CSVs then pulled into Amazon SageMaker through the Jupyter notebook using boto3, botocore and sagemaker libraries

3. Data manipulation then takes place with pandas in the notebook itself for the analysis to subsequently be carried out

Does this process represent a data pipeline, and an ETL pipeline per se? The data is being transferred from one AWS storage space to another with a view to transforming and then analysing the data but I'm wondering whether this strictly fits the definition of ETL.

Would be grateful for any advice."
1196,2019-11-06 01:35:32,1572996932.0,dataengineering,Not Understanding The Logic,ds7lx0,irixnox,,https://www.reddit.com/r/dataengineering/comments/ds7lx0/not_understanding_the_logic/,1.0,0.0,0.0,7427.0,"This is for Teradata + Job Scheduler

&amp;#x200B;

&amp;#x200B;

In a job scheduler, two steps have been setup for refreshing database.

On the first step, it is called as 'data set ready' script.

The script is on bteq:

SELECT  
Cast(  
CASE  
WHEN 'CONDITION' IN ('','X') THEN '--'  
ELSE ''  
END  
||'CT '  
||Trim(Substring(Databasename From 1 FOR 5))  
||'Tablename\_'  
||Trim(Substring(Databasename From 11 FOR 200))  
||'.'  
||Trim(TableName)  
||' as '||Trim(Databasename)  
||'.'  
||Trim(TableName)  
||' with no data and stats;' AS VARCHAR(1000)) (Title '')  
,'0A'XC(Title '')  
,'.IF ERRORLEVEL != 0 THEN .QUIT 1;' (Title '')  
FROM  
database.tablename  
WHERE Databasename NOT IN ( 'databasename')  
AND 'CONDITION' NOT IN ('','X')  
ORDER BY  
Databasename  
,TableName  
;

SELECT   
Cast(   
CASE   
WHEN 'CONDITION' IN ('','X') THEN '--'   
ELSE ''   
END  
||'INSERTO INTO'  
||Trim(Substring(Databasename From 1 FOR 5))  
||'Tablename\_'  
||Trim(Substring(Databasename From 11 FOR 200))  
||'.'  
||Trim(TableName)  
||' as '||Trim(Databasename)  
||'.'  
||Trim(TableName)  
||' with no data and stats;' AS VARCHAR(1000)) (Title '')  
,'0A'XC(Title '')  
,'.IF ERRORLEVEL != 0 THEN .QUIT 1;' (Title '')   
FROM   
database.tablename  
WHERE Databasename NOT IN ( 'databasename')  
AND 'CONDITION' NOT IN ('','X')  
ORDER BY   
Databasename  
,TableName  
;

Moving to the second step of the schedule, the job name is called that actual name of the database as the step above with the following bteq command:

/\* ARC SCRIPT \*/  
SELECT 'logon host/username,pw;' (Title '') ;

SELECT 'ARCHIVE DATA TABLES' (Title '') ;

/\*GENERATE ARCMAIN SCRIPT \*/  
SELECT '(' || Trim (DATABASENAME ) || '.' || Trim (TableName ) || '),' (Title '')  
FROM DBC . TABLESV  
WHERE 1 = 1  
AND DATABASENAME = 'This.database'  
ORDER BY 1 ASC;

SELECT 'RELEASE LOCK,' (Title '') ;  
SELECT 'FILE=SIM;' (Title '') ;  
SELECT 'LOGOFF;' (Title '') ;  
.EXPORT RESET;  
/\* COPY SCRIPT \*/

SELECT 'logon host/username,pw;' (Title '') ;

SELECT '.SET QUERY\_BAND = ''BLOCKCOMPRESSION=YES;'';' (Title '') ;

SELECT 'COPY DATA TABLES' (Title '') ;

SELECT '(Database' || '.' ||Trim(TableName) || ')'  
|| ' (FROM(' || Trim( DATABASENAME) || '.' ||Trim(TableName) || ')),' (Title '')  
FROM DBC.TABLESV  
WHERE 1=1  
AND DATABASENAME =''  
AND TABLEKIND ='T'  
ORDER BY 1;

SELECT 'RELEASE LOCK,' (Title '') ;  
SELECT 'FILE=SIM;' (Title '') ;  
SELECT 'LOGOFF;' (Title '') ;  
.EXPORT RESET;  
.LOGOFF;

I am lost in between the two steps. 

What is their relation? What is the whole purpose of the first step? 

Wouldn't the second step (this single step) be enough to backup and restore the database?"
1197,2019-11-06 07:03:54,1573016634.0,dataengineering,Migrating to Snowflake,dsbjvm,redd_ross,,https://www.reddit.com/r/dataengineering/comments/dsbjvm/migrating_to_snowflake/,1.0,14.0,0.0,7429.0,"Hi Guys! Long time lurker, first time questionner. Love this sub! Just want to ask for advice what could be the best option for acquiring data from a RDS to Snowflake? Thanks!"
1198,2019-11-06 20:10:20,1573063820.0,dataengineering,Is there something like Fivetran/Stitch but On-Premises?,dsk5n1,underflo,,https://www.reddit.com/r/dataengineering/comments/dsk5n1/is_there_something_like_fivetranstitch_but/,3.0,9.0,0.0,7443.0,
1199,2019-11-06 21:30:12,1573068612.0,dataengineering,Why is CData not more popular?,dsl93l,underflo,,https://www.reddit.com/r/dataengineering/comments/dsl93l/why_is_cdata_not_more_popular/,6.0,10.0,0.0,7443.0,"Fivetran and Stitch are quite popular for ""modern"" ETL (or rather EL). But looking at CData I wonder why it's never mentioned. In terms of connectors (or ""drivers"" in their parlance) they look at least as complete if not more extensive and they have the option for on-premise which neither Stitch nor Fivetran offer. Is there a reason for that? Have you even heard of them?"
1200,2019-11-07 21:54:19,1573156459.0,dataengineering,AWS Glue/Lake Formation or Airflow?,dt379c,sciencewarrior,,https://www.reddit.com/r/dataengineering/comments/dt379c/aws_gluelake_formation_or_airflow/,1.0,15.0,0.0,7470.0,"I have a greenfield data pipeline to build on AWS, and I've narrowed down orchestration to these two options. Lake Formation looks like a good fit for quickly exporting our data on Aurora, but I'm afraid we'll end up fighting against the tool when trying to integrate with external sources like Google Analytics."
1201,2019-11-08 00:57:11,1573167431.0,dataengineering,UX Research,dt5xur,theZeteWhoDied,,https://www.reddit.com/r/dataengineering/comments/dt5xur/ux_research/,1.0,0.0,0.0,7473.0,"Hey, friends! I'm looking for some people with experience handling workflow automation and orchestration to participate in a 1-hour video call. You'll get $50 to talk about your data challenges and provide feedback on a new product I'm working on. If you're interested, take this 1-minute survey to see if you qualify, and make sure you leave your email so I can reach out to you to schedule the session.  [https://www.surveymonkey.com/r/S8GT5R8](https://www.surveymonkey.com/r/S8GT5R8)"
1202,2019-11-08 05:50:39,1573185039.0,dataengineering,How is Databricks Spark different than Spark?,dt9pdq,Phizy,,https://www.reddit.com/r/dataengineering/comments/dt9pdq/how_is_databricks_spark_different_than_spark/,1.0,1.0,0.0,7474.0,
1203,2019-11-08 10:06:35,1573200395.0,dataengineering,Looking for DE Research Papers,dtcbe1,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/dtcbe1/looking_for_de_research_papers/,1.0,8.0,0.0,7479.0,Are there any research papers around Data Engineering? Any links?
1204,2019-11-08 15:43:37,1573220617.0,dataengineering,Advice for total beginner: should i take the bait?,dtfm26,num8lock,,https://www.reddit.com/r/dataengineering/comments/dtfm26/advice_for_total_beginner_should_i_take_the_bait/,1.0,3.0,0.0,7477.0,
1205,2019-11-08 17:51:04,1573228264.0,dataengineering,How to use Lenses as a Secure Data Layer to Produce Data into Apache Kafka,dth9kt,lensesio,,https://www.reddit.com/r/dataengineering/comments/dth9kt/how_to_use_lenses_as_a_secure_data_layer_to/,1.0,0.0,0.0,7478.0,
1206,2019-11-08 21:19:31,1573240771.0,dataengineering,"Re: column databases, thoughts on PostgreSQL w/ store_fdw extension vs Greenplum?",dtk9f3,fail_to_reject_null,,https://www.reddit.com/r/dataengineering/comments/dtk9f3/re_column_databases_thoughts_on_postgresql_w/,1.0,7.0,0.0,7479.0,
1207,2019-11-12 08:47:41,1573541261.0,dataengineering,How do I optimize this code on Pyspark?,dv6a20,metalloidica,,https://www.reddit.com/r/dataengineering/comments/dv6a20/how_do_i_optimize_this_code_on_pyspark/,1.0,1.0,0.0,7544.0,
1208,2019-11-12 14:14:37,1573560877.0,dataengineering,What ETL technologies does the Chinese government use?,dv963w,ethanenglish,,https://www.reddit.com/r/dataengineering/comments/dv963w/what_etl_technologies_does_the_chinese_government/,1.0,7.0,0.0,7544.0,"China has some of the most intense data processing challenges. I’m assuming they don’t use common, open-source ETL tools like Spark, Kafka, Airflow."
1209,2019-11-12 18:26:56,1573576016.0,dataengineering,How do I vizualise the flow of control in some sql queries?,dvcacc,rossofcode,,https://www.reddit.com/r/dataengineering/comments/dvcacc/how_do_i_vizualise_the_flow_of_control_in_some/,1.0,3.0,0.0,7546.0,"I wanted to improve my SQL and for that, I need to understand the flow of control in SQL. I understand the basics but here is one such instance where I am unable to comprehend as to how a column from one table can be used in the where clause of another table in a different query(subquery)?

[This](https://www.hackerrank.com/challenges/the-report/problem) is the problem statement. The query is:

```SELECT IF (Grade &gt;= 8, Name, NULL), Grade, Marks FROM (SELECT Name, (SELECT Grade FROM Grades WHERE (Min\_Mark &lt;= Marks) AND (Marks &lt;= Max\_Mark)) AS Grade, Marks FROM Students) As MyStudents ORDER BY Grade DESC, Name;\`\`\`

My doubt is that in the subquery: (SELECT Grade FROM Grades WHERE (Min_Mark &lt;= Marks) AND (Marks &lt;= Max_Mark)), how come 'Marks' can be used?"
1210,2019-11-12 19:26:36,1573579596.0,dataengineering,Unable to activate Airflow Web authentication:.,dvd4ym,theant97,,https://www.reddit.com/r/dataengineering/comments/dvd4ym/unable_to_activate_airflow_web_authentication/,1.0,0.0,0.0,7546.0,
1211,2019-11-12 22:01:47,1573588907.0,dataengineering,Training plan for data engineering on the job,dvfeet,ElectricalFilm2,,https://www.reddit.com/r/dataengineering/comments/dvfeet/training_plan_for_data_engineering_on_the_job/,1.0,8.0,0.0,7551.0,"I have just joined my company’s analytics team as a data engineer. As of now, it looks like my role will be a hybrid of a database focus and a infrastructure focus, and I will be expected to be skilled in Python, AWS, Snowflake, SQL and a couple of ETL tools. I may also be required to build data pipelines for specific analytics projects. The team is new, so things will evolve, esp over the next 1 year.

**MY QUESTION(s)**

I am creating an on-the-job training plan for myself, for the next 2-3 months, and some feedback/suggestions on these questions would be super helpful:

* **Subject areas**: I think I need to focus on *Python*, *SQL*, *AWS* and *learning to build data pipelines*. Are there any other subject areas I have missed?
* **Resources**: I was planning to go through Dataquest’s Data Engineering track. I don’t expect it to teach me everything, but does it cover all the relevant subject areas?
* **AWS**: What learning paths (and classroom training) within AWS will be appropriate for me here?

**CONTEXT**

Background: Just so you know where I am coming from, I was hired into this role immediately after an internship, where my project involved building a prototype to measure, record and visualize a few data quality metrics. I used Python, SQL, Tableau and AWS (mostly Lambda, with SNS and CloudWatch). This seems to me like the opposite of the norm here, where people are trying to break into data engineering from other roles.

My current skills: I currently have some knowledge of Python and SQL from grad school, but I can improve upon them further. I also got some experience using Snowflake during my internship, and needed to read up a bit on Lambda, SNS, SQS, CloudWatch for what I was doing."
1212,2019-11-12 22:11:52,1573589512.0,dataengineering,A little data scientist needs your help,dvfjm4,amkian,,https://www.reddit.com/r/dataengineering/comments/dvfjm4/a_little_data_scientist_needs_your_help/,1.0,4.0,0.0,7551.0,"Hello data engineering fam! I just graduated college as a machine learning engineer and I’m struggling with finding a job as a data scientist because of my lack of knowledge in data engineering (I live in France and this is what the market needs apparently). All my skills are around using python for machine learning/deep learning and SQL for data collection. Now I’m lost in what to learn first in term of data engineering (technologies for data acquisition, data storage, data deployment, and data viz etc).  Can you give me a small (or big) list of a must know technologies for a junior profil like me and some certification that I should get like coursera or udemy ones. Thanks a lot in advance &lt;3"
1213,2019-11-13 04:59:00,1573613940.0,dataengineering,An interview about data protection regulations and how they can influence the design of your data platform,dvlc4u,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/dvlc4u/an_interview_about_data_protection_regulations/,1.0,0.0,0.0,7561.0,
1214,2019-11-13 23:39:55,1573681195.0,dataengineering,How did you deploy Airflow into production? (AWS),dvyt8c,Shoddy_Researcher,,https://www.reddit.com/r/dataengineering/comments/dvyt8c/how_did_you_deploy_airflow_into_production_aws/,1.0,16.0,0.0,7582.0,"Hi All,

Have committed to rolling out Airflow and now looking at the best way to deploy it on AWS specifically.

Did you do a plain EC2? or run in Docker with ECS? If so, did you split up each service to a different container?

More interested in the pros/cons of both approaches so I can make an informed decision.

&amp;#x200B;

If it's relevant;

Every DAG/task will be an AWSBatchOperator to run our containerised scripts

Volumes will be low, in the 20's per day for now

Logs will go to S3

Will use an RDS instance for the DB

Want to view the webserver/ GUI locally

&amp;#x200B;

Cheers"
1215,2019-11-13 23:40:16,1573681216.0,dataengineering,Non-engineering resources for Data Engineering,dvytew,joshlaird,,https://www.reddit.com/r/dataengineering/comments/dvytew/nonengineering_resources_for_data_engineering/,1.0,3.0,0.0,7582.0,"I'm at the final stage of an interview process for a data engineering role at a startup in London. I have a fair amount of experience with most of the engineering side of data engineering (warehousing, SQL, Spark, have the GCP data eng cert, system design) but as this startup has no data analysts or data scientists yet, I will be advising teams on dimensions, metrics, KPIs, as well as visualising the data. You could say that the role would be spanning the full spectrum of data.

This sub has a load of information on the engineering side but are there any recommended courses for picking up the non-engineering side of data? I've seen that there is a [data warehousing course on Coursera](https://www.coursera.org/specializations/data-warehousing) but I was curious if anyone has any further ideas? I have an annual subscription to Dataquest so I can pick up statistics and data viz via the data analyst course.

Things I reckon I should be looking at (feel free to suggest things I could be missing):

Data modeling (UML diagrams, many to many relationships)
KPIs (customer lifetime value, cost of acquisition etc.)

Or is there a sub with a sidebar which covers the majority of this content?"
1216,2019-11-14 12:10:01,1573726201.0,dataengineering,Community for building data products,dw7iw8,willmachineloveus,,https://www.reddit.com/r/dataengineering/comments/dw7iw8/community_for_building_data_products/,1.0,8.0,0.0,7592.0,"Hey folks, first and foremost there is a fantastic data engineering discord channel that is linked at the top of this subreddit. For the practice of data engineering, career guidance, help, and a host of other things that is the perfect community to join.

However, I'd like to invite you to [this Slack channel](https://join.slack.com/t/growthtesttube/shared_invite/enQtNjc2OTcxMDQ1MjQ5LTIwNWMzZGUzYWFkZTQyNDY1M2VmN2ZlMDQxMTdjY2VmNzUwNTFlYTg5MGY3ZGI2ZWMzZWE4MjdlMDNjN2M4NWY) where we work on launching data products. I used ""products"" and not ""projects"" because our goal is to (eventually) generate revenue for the small squads involved in each. Data products can take many different forms: ebooks, courses, services (e.g. lyrebird ai), resources (e.g. Ahrefs), and physical products. We're interested in all of them and want to hear your ideas.

Though the focus is on launching data products you don't have to be a veteran or professional engineer to join. There will be spots in each group for junior folks/those looking to get some experience to join in and get your hands dirty with some data work. There's also [more information on our site](https://www.growthtesttube.com/#/). 

We've really just launched this community and so are seeing if folks are interested in joining. The next incubator project we have planned starts on January 1 and will consist of a team of no more than 12. That doesn't mean we can't run others simultaneously, especially if there's interest. Anyway, stop by if that sounds interesting and introduce yourself. Thanks!"
1217,2019-11-14 16:51:02,1573743062.0,dataengineering,Tutorial on how to use Airflow without pain,dwajci,shawemuc,,https://www.reddit.com/r/dataengineering/comments/dwajci/tutorial_on_how_to_use_airflow_without_pain/,1.0,0.0,0.0,7598.0,
1218,2019-11-15 09:28:14,1573802894.0,dataengineering,With the advent of telepathy nearing how do data engineers support these solutions. Can a data engineering (reasonably) provide value with existing skills for current demand to support this potential future demand ?,dwnb6c,acceptedcitizen,,https://www.reddit.com/r/dataengineering/comments/dwnb6c/with_the_advent_of_telepathy_nearing_how_do_data/,1.0,2.0,0.0,7610.0,
1219,2019-11-15 10:52:56,1573807976.0,dataengineering,"First data engineer role, what’s the best data engineering with python course I could do over the next few weeks",dwo2qi,fuzzywunder,,https://www.reddit.com/r/dataengineering/comments/dwo2qi/first_data_engineer_role_whats_the_best_data/,1.0,21.0,0.0,7611.0,"Hi all, in a few weeks I’m moving from a stats role using R and SQL to a data engineering role. I can probably use whichever software is best suited and I’ve heard that python is better for data engineering.

Are there any free online courses or blogs that I can follow through over the next few weeks to get a head start on developing my python skills? I’ve had a search myself, just looking for specific recommendations. 

Thanks!"
1220,2019-11-15 12:15:00,1573812900.0,dataengineering,"Data Profiling: Process, Best Practices and Tools (Overview)",dwotiv,cmstrump,,https://www.reddit.com/r/dataengineering/comments/dwotiv/data_profiling_process_best_practices_and_tools/,1.0,0.0,0.0,7612.0,
1221,2019-11-15 12:36:52,1573814212.0,dataengineering,Best practices data warehousing,dwp0r2,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/dwp0r2/best_practices_data_warehousing/,1.0,16.0,0.0,7613.0,"Hello. I am more or less in charge of the data warehouse of my company so I was wondering if there is any good resource about how to create and manage data warehouses. 

Kind of questions that I want to get answered:

* How narrow should be the scope of a schema? Is it better to have a schema called ""users"" with tables about newsletters, interactions and recommendations with some kind of prefix for the tables? Is it better to have three different schemas?
* How many staging areas are needed? We have three schemas: stg, meta and dwh. Is it necessary to have meta also?
* What are the best practices for naming? I have read some of them but not all the cases were covered
* Is it a good idea to have a schema called mix or something like that for tables that are maybe too specific for fit into another schema?
* How important is the column type (i.e varchar length?)
* Is it a good practice to create key restrictions?

I will probably have more but I don't recall right now. I am using Redshift by the way. What do you recommend? 

Thank you very much in advance"
1222,2019-11-15 17:26:06,1573831566.0,dataengineering,How do you move from the ingest layer to serverless layer?,dws9gu,rossofcode,,https://www.reddit.com/r/dataengineering/comments/dws9gu/how_do_you_move_from_the_ingest_layer_to/,1.0,2.0,0.0,7615.0,I was going through the data engineering cookbook's interview questions and found this question. I have some rough ideas but wanted an articulate answer. I read some posts (top google search) but couldn't get much out of it.
1223,2019-11-15 22:04:16,1573848256.0,dataengineering,"Are virtual private databases a thing currently? What are some of the good ones other than Oracle? If they are not so trendy right now, how do we fulfill the needs of security w/o them?",dww31w,rossofcode,,https://www.reddit.com/r/dataengineering/comments/dww31w/are_virtual_private_databases_a_thing_currently/,1.0,9.0,0.0,7618.0,
1224,2019-11-16 02:59:56,1573865996.0,dataengineering,Generated SQL,dwzwxp,tpedar50,,https://www.reddit.com/r/dataengineering/comments/dwzwxp/generated_sql/,1.0,2.0,0.0,7621.0,"We’ve been playing with Stitch with a SQL Sever source and Snowflake destination. 

Is there a post or any info on how it automagically generates the create table statements and insert statements and assigns data types?"
1225,2019-11-16 09:06:14,1573887974.0,dataengineering,The Easy Way to Extend Pandas API,dx3n10,eyaltrabelsi,,https://www.reddit.com/r/dataengineering/comments/dx3n10/the_easy_way_to_extend_pandas_api/,1.0,0.0,0.0,7626.0,
1226,2019-11-16 22:26:17,1573935977.0,dataengineering,PySpark - Recommendation needed for functions,dxc5hy,EggShellBuddyPal,,https://www.reddit.com/r/dataengineering/comments/dxc5hy/pyspark_recommendation_needed_for_functions/,1.0,2.0,0.0,7642.0,"I'm completely new to Spark and was just starting to learn a few things and was wondering how to reliably parse and match keywords in a column. I'm exploring a sample JSON data-set for example with following df schema:

    root
     |-- cookTime: string (nullable = true)
     |-- datePublished: string (nullable = true)
     |-- description: string (nullable = true)
     |-- image: string (nullable = true)
     |-- ingredients: string (nullable = true)
     |-- name: string (nullable = true)
     |-- prepTime: string (nullable = true)
     |-- recipeYield: string (nullable = true)
     |-- url: string (nullable = true)

Now, I'm trying to add a column contains\_chicken based upon if ingredients (separated by "","" and line breaks) list the word ""chicken"" in them, and write to a csv. Here is my sample code:

    from pyspark.sql import SparkSession
    import pyspark.sql.functions as psf
    
    spark = SparkSession.builder.getOrCreate()
    
    sc = spark.sparkContext
    
    path = ""data/recipes.json""
    recDF = spark.read.json(path)
    
    recDF.withColumn(""contains_chicken"", psf.when(psf.col(""ingredients"").rlike(""chicken""), ""true"").otherwise(""false"")).toPandas().to_csv('export.csv')

Upon inspection, it seems like the csv has false negatives for most and ends up only marking 2 values as true, and one of them is incorrect. Am I using the correct function to do this? Should I do some data cleansing on the data-set first? What approach should I take for testing this?

Any direction or response is appreciated and thanks for looking!"
1227,2019-11-16 23:12:09,1573938729.0,dataengineering,Kafka Connect + dbt Sanity Check,dxcsjg,friendofasquid,,https://www.reddit.com/r/dataengineering/comments/dxcsjg/kafka_connect_dbt_sanity_check/,1.0,14.0,0.0,7642.0,"Hi Everyone. I am looking to overhaul our current stack as follows. 

Data ingestion with Kafka (MSK on AWS) and Kafka Connect. We’ll write Avro to S3 and use commercial JDBC drivers with the JDBCSource connector. 

From there, we’ll use Glue Catalog and add external tables point to the S3 location for storage, plus the Kafka schema registry for the Avro schema. 

We’ll then use dbt and SparkSQL (Spark on EMR) to model the data on S3. 

And finally we’ll expose the data with Athena (Tableau) and Presto on EMR (Metabase) for business intelligence. 

I’m wondering if anyone has any thoughts on Kafka Connect generally, dbt generally, the explicit omission of a data warehouse like Redshift in favour of Presto/Athena and the whole end-to-end solution. 

What is likely to go wrong? What might be a better approach?"
1228,2019-11-17 01:14:37,1573946077.0,dataengineering,Insights about Insight Data Engineering Program,dxeef1,aashwin93,,https://www.reddit.com/r/dataengineering/comments/dxeef1/insights_about_insight_data_engineering_program/,1.0,17.0,0.0,7643.0,Has anyone attended the Data Engineering program by Insight in the recent past? I'd like to know your experience as most of what I can find online are specific to the Data Science program. I'd also like to know about their scholarship program and the legal clause about not interviewing with any other company upon acceptance of the offer
1229,2019-11-17 02:17:05,1573949825.0,dataengineering,Career paths for experienced data engineers?,dxf6ft,exact-approximate,,https://www.reddit.com/r/dataengineering/comments/dxf6ft/career_paths_for_experienced_data_engineers/,1.0,0.0,0.0,7644.0,"I am a medium to senior data engineer and being the ""end of year"" and ""review"" season, I am setting out to think about what goals would be good to achieve in the coming months.

The main reason I am asking this is because after 6 years specifically in the data industry, I feel like I will continue to loop around many different tech areas which is resulting in a bit of ""tool fatigue"". 

I want to be strategic with how to develop my career, and frankly I am a bit lost. 
One option is to continue mastering new tech, trying to become more and more ""senior"" to climb the corporate ladder. But I am quite young (late 20s) and I fear that most corps don't look to me for senior and architecture roles just yet. I've seen several older developers who might not be as technical as I am, get into posts which are higher than me simply because they are older and have more generalist experience. I am not that comfortable playing a ""waiting game"".

The other options are to pivot my career into product management or DevOps, which scare me a bit since it will be taking things into a different direction.

My current status:
1. Designed ETL in three different tools and experienced all major ""traditional"" RDBMS and well versed in DWH stuff.

2. Comfortable writing Python, Java and some Scala.

3. AWS certified with a little GCP/Azure Experience.

4. Reasonably comfortable with Docker, Kubernetes, Jenkins and Kafka.

5. Have used the major Apache Projects (Airflow, Hadoop, Nifi, Spark, Flink)

6. Spent quite some time working enterprise dash boarding tools (ex. Qlikview)

7. Decent knowledge of alternative data stores (MongoDB, DynamoDB, Elastic, Redis, Amazon Redshift)

8. I have a maters degree in ML.

**So my question is, what are your professional goals this coming year? What does a career path for an experienced data engineer look like? **

I am mostly interested in the goals of people with 5+ years experience (medium to senior level) but it would be also cool to get the insight from newer people."
1230,2019-11-17 04:13:22,1573956802.0,dataengineering,Break into data engineering w/o technical background,dxgkt2,aerdna69,,https://www.reddit.com/r/dataengineering/comments/dxgkt2/break_into_data_engineering_wo_technical/,1.0,8.0,0.0,7647.0,"I live in Eurooe, graduating in science of communication and trying to become a data engineer in the meantime. I'm good at SQL and I started 2 months ago to learn Python with some books (Think Python and Automate the Boring Stuff) . Would a couple of github ETL projects be enough to break into the field?"
1231,2019-11-18 13:30:47,1574076647.0,dataengineering,String parsing using PySpark,dy20as,EggShellBuddyPal,,https://www.reddit.com/r/dataengineering/comments/dy20as/string_parsing_using_pyspark/,1.0,1.0,0.0,7672.0,"I have a sample dateset:

    |cookTime|datePublished|         description|               image|         ingredients|                name|prepTime|recipeYield|                 url|
    +--------+-------------+--------------------+--------------------+--------------------+--------------------+--------+-----------+--------------------+
    |   PT25M|   2012-08-06|Who doesn't love ...|http://static.the...|1 stick Butter
    1 ...|         Patty Melts|   PT10M|          4|http://thepioneer...|
    |    PT3H|   2012-01-30|My oh my, was thi...|http://static.the...|2 Tablespoons Can...|Spicy Stewed Beef...|   PT20M|         12|http://thepioneer...|
    |   PT25M|   2012-02-08|This was really, ...|http://static.the...|6 whole Pork Chop...|Pork Chops with G...|    PT5M|          6|http://thepioneer...|

From the cookTime and prepTime I'm trying to find a reliable way to extract time string from values such as ""PT25M"" or ""PT3H"" and basically convert all to minutes and calculate total time in mins. Is there a way to do so using PySPark re-ex functions, or should I resort to using Pandas/NumPy instead? In either case, what would be the ideal way to parse string values and extract and convert numerical values using prefix ""PT%"". 

Thanks for looking and for any help!"
1232,2019-11-18 13:56:51,1574078211.0,dataengineering,What is the right way to transfer FTP files to Google Cloud Storage?,dy28p1,ratatouille_artist,,https://www.reddit.com/r/dataengineering/comments/dy28p1/what_is_the_right_way_to_transfer_ftp_files_to/,1.0,1.0,0.0,7673.0,"I have been using rclone to transfer FTP files to google buckets but recently I have been having trouble with the files not appearing in the buckets while rclone ls shows they are there gsutil ls shows they aren't :/  


So I was wondering what is the easiest way to do this simple task."
1233,2019-11-18 18:10:01,1574093401.0,dataengineering,AWS EC2 and EMR Start and stop when the jobs are not running,dy579r,theant97,,https://www.reddit.com/r/dataengineering/comments/dy579r/aws_ec2_and_emr_start_and_stop_when_the_jobs_are/,1.0,0.0,0.0,7679.0,
1234,2019-11-18 22:43:21,1574109801.0,dataengineering,Open source library to perform entity embeddings on categorical variables using Convolutional Neural Networks [+ unit tests and Continuous Integration],dy95qt,CrazyCapivara,,https://www.reddit.com/r/dataengineering/comments/dy95qt/open_source_library_to_perform_entity_embeddings/,1.0,2.0,0.0,7687.0,"In the past 2 years I have been working as a Machine Learning developer, mostly with tabular data, and I've developed a tool to perform entity embeddings on categorical variables using CNN with Keras. I tried pretty much to make it easy to use and flexible to most of the existent scenarios (regression, binary and multi-class classification), but if you find any other need or issue to be fixed, do not hesitate to ask.

I tried to add some cool stuff on the project, such as **unit tests**, **code coverage** with Codacy, **continuous integration** with Travis CI and **auto deployment** to PyPi and **auto-generated documentation** with Sphinx and ReadTheDocs, so if any of you is interested in how to setup your project to have these features, feel free to use it as a base project.

Looking forward to any reviews about the source code. Any tip to improve the readability or even performance, its really welcome and well appreciated.

**Github:** [**https://github.com/bresan/entity\_embeddings\_categorical**](https://github.com/bresan/entity_embeddings_categorical)

PyPi: [https://pypi.org/project/entity-embeddings-categorical/](https://pypi.org/project/entity-embeddings-categorical/)

Code coverage (nowadays reaching 97%): [https://coveralls.io/github/bresan/entity\_embeddings\_categorical?branch=master](https://coveralls.io/github/bresan/entity_embeddings_categorical?branch=master)

Thanks and I hope it can help somebody out there :-)"
1235,2019-11-19 00:25:08,1574115908.0,dataengineering,An interview about data virtualization and data engineering automation with AtScale and the value of abstractions for your data platform architecture,dyaod8,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/dyaod8/an_interview_about_data_virtualization_and_data/,1.0,0.0,0.0,7687.0,
1236,2019-11-19 06:24:57,1574137497.0,dataengineering,Hypothetical scenario: Which job would be the better stepping stone towards becoming a Data Engineer? Data Analyst or Database Administrator?,dyfenq,throwawaystickies,,https://www.reddit.com/r/dataengineering/comments/dyfenq/hypothetical_scenario_which_job_would_be_the/,1.0,20.0,0.0,7689.0,
1237,2019-11-19 10:26:54,1574152014.0,dataengineering,Help review Consultant to Data Engineering CV/Resume?,dyhoit,Carnegie118,,https://www.reddit.com/r/dataengineering/comments/dyhoit/help_review_consultant_to_data_engineering/,1.0,1.0,0.0,7690.0,"CV link: [https://docdro.id/DF74ms6](https://docdro.id/DF74ms6)  


I've currently got 3.5 years' experience working for two big brand technology consulting firms as a data consultant with a specialisation in data engineering. My major selling point previously was that I can do the consulting side but I am also a tech guy. 

However, I want to move away from consulting to a pure big data engineering position and I'd like some advice/input on how I could improve my CV and experience.   


Thank you in advance!"
1238,2019-11-19 11:01:18,1574154078.0,dataengineering,From Data Oops to DataOps: 5 Things You Need to Know - Atlan | Humans of Data,dyhyep,knlph,,https://www.reddit.com/r/dataengineering/comments/dyhyep/from_data_oops_to_dataops_5_things_you_need_to/,1.0,0.0,0.0,7691.0,
1239,2019-11-19 17:11:42,1574176302.0,dataengineering,Checkout our new Kafka CLI,dylnfh,MoCeptor,,https://www.reddit.com/r/dataengineering/comments/dylnfh/checkout_our_new_kafka_cli/,1.0,0.0,0.0,7692.0,"Maybe you've had the same struggles with managing kafka as we did. We just released the first ""beta"" of an open-source kafka tool we have been developing internally for quite some time: [https://github.com/real-digital/esque](https://github.com/real-digital/esque)"
1240,2019-11-19 18:56:16,1574182576.0,dataengineering,what are some easiest scheduling tools similar to Airflow? I know that there is Luigi..any other open source option?,dyn3f0,spectre_S,,https://www.reddit.com/r/dataengineering/comments/dyn3f0/what_are_some_easiest_scheduling_tools_similar_to/,1.0,11.0,0.0,7693.0,
1241,2019-11-19 21:56:47,1574193407.0,dataengineering,Tips on becoming a data engineer,dypqti,agdaman4life,,https://www.reddit.com/r/dataengineering/comments/dypqti/tips_on_becoming_a_data_engineer/,3.0,4.0,0.0,7697.0,"Hi, 

I am going to graduate in May 2020 with a B.S. in Computer Engineering. I am interested in data engineering as a field. Some of my relevant coursework is:

Data Structures, Algorithms, Distributed Systems, Database Organization (Relational DB Design), Operating systems

I have one internship at a startup as a general engineer, some of my roles included: 

● Tested middleware updates on IOT device in order to optimize software workflow  
● Wrote batch scripts for client to maximize local database update efficiency  
● Wrote Linux bash scripts for Build Root Flavor embedded device to fetch memory usage  
● Maintained local SQLite database for client, and wrote scripts to automate updates  
● Drafted workflow for Azure microservice using CosmosDB, ServiceBus Queue, and IOTHub  
bindings  
● Built and designed front end desktop applications using Python Tkinter GUI library

After this semester is over, I am planning on building a fully functional, end to end web scraper as my side project. 

As experienced Data Engineers, does my resume look good for a data engineer? What experience, skills, certs, could be helpful? I am searching for a job in Chicago. 

&amp;#x200B;

Thanks!!!!"
1242,2019-11-19 22:26:36,1574195196.0,dataengineering,First Onsite Data Engineering Interview Advice,dyq74a,creay01,,https://www.reddit.com/r/dataengineering/comments/dyq74a/first_onsite_data_engineering_interview_advice/,1.0,1.0,0.0,7700.0,"I've never done an onsite/white board interview before, and I feel very unsure of how these problems will go.

I'm having a really hard time finding example practice problems for Data Engineering interviews. I was told it will be a SQL/Python based on a white board, with some sort of problem, where I will have to understand the business/product sense, then create data models, and write up an ETL solution. 

As of right now, I have a solid amount of Python and SQL under my belt from courses/internships. I've done similar work to ETL in past experiences, but I don't know how detailed I will go with this on the whiteboard... am I going to literally create a python script, set up database connection credentials and a cursor in python code, add SQL queries to be read by the cursor, maybe memorize some Python Pandas syntax for the Transform, and do all of this on the whiteboard? Are there certain things I can omit?

Also, does anyone have any valuable resources for actual practice problems that tackle the business/product sense, data modeling, and ETL all at once? Or.. am I better off creating my own mock database system, think of my own problems, and then write up solutions?

Thank you!"
1243,2019-11-19 23:48:59,1574200139.0,dataengineering,Here is a question which I tanked answering in a recent interview.,dyrg2p,spectre_S,,https://www.reddit.com/r/dataengineering/comments/dyrg2p/here_is_a_question_which_i_tanked_answering_in_a/,5.0,14.0,0.0,7699.0,"How to you design an optimal build failure/data flow failure notification model without disturbing too many people if you have a pipeline failure off hours.

&amp;#x200B;

I tried saying, I will create thresholds on priority, define several exit codes for my jobs and create a notification model based on that.

&amp;#x200B;

They didn't like this solution."
1244,2019-11-20 07:31:38,1574227898.0,dataengineering,Are there roles that want someone who is great at data science and data engineering?,dyxhdh,nouseforaname888,,https://www.reddit.com/r/dataengineering/comments/dyxhdh/are_there_roles_that_want_someone_who_is_great_at/,1.0,0.0,0.0,7706.0,
1245,2019-11-20 08:58:15,1574233095.0,dataengineering,Anyone a DE in EU and have any opinion on it as a career there ?,dyyd82,acceptedcitizen,,https://www.reddit.com/r/dataengineering/comments/dyyd82/anyone_a_de_in_eu_and_have_any_opinion_on_it_as_a/,1.0,10.0,0.0,7706.0,
1246,2019-11-20 16:20:49,1574259649.0,dataengineering,We should really have an interview questions/experiences thread,dz2oh4,spectre_S,,https://www.reddit.com/r/dataengineering/comments/dz2oh4/we_should_really_have_an_interview/,1.0,3.0,0.0,7708.0,"Hi, I posted one of my interview questions yesterday and I read few exceptional comments which are possibly better answers than what I did in my interview. I think we should have a thread on this so that we can learn from others experiences. Any thoughts?"
1247,2019-11-20 17:33:42,1574264022.0,dataengineering,"Validate data as you receive it so you know your data pipeline will succeed, no matter what changes are made upstream",dz3n2s,ed_elliott_,,https://www.reddit.com/r/dataengineering/comments/dz3n2s/validate_data_as_you_receive_it_so_you_know_your/,1.0,16.0,0.0,7710.0,"I was tired of having to keep re-writing the same validation code for every pipeline I wrote so I wrote this tool to help:

 [https://show.schemaed.com/](https://show.schemaed.com/) 

The idea is that you define a schema which contains, data types, non-null, unique constraints as well as custom constraints which you can write in python so when you process a file you know you are guaranteed that the input file matches your original assumptions.

When data is invalid (according to our schema) each failed row gets a description of exactly why it failed so you don't have to start testing every row and column to find out where incorrect values are

Would love some feedback :)"
1248,2019-11-21 06:16:13,1574309773.0,dataengineering,What data catalogs have you used?,dzdtin,flerkentrainer,,https://www.reddit.com/r/dataengineering/comments/dzdtin/what_data_catalogs_have_you_used/,1.0,20.0,0.0,7718.0,"It seems like data catalog is coming out of the woodwork nowadays with pure vendors like Alation, open source, and even Tableau getting into the mix.

What data catalogs have you had hands on with? What made them great or bad? What do features do you look for?"
1249,2019-11-21 12:33:01,1574332381.0,dataengineering,ETL vs. ELT: A Live Data Integration Showdown,dzhfj3,iqbalahmedalvi,,https://www.reddit.com/r/dataengineering/comments/dzhfj3/etl_vs_elt_a_live_data_integration_showdown/,1.0,0.0,0.0,7723.0,
1250,2019-11-21 18:35:51,1574354151.0,dataengineering,Kafka Tutorial - How to Visualize Data in Apache Kafka Using D3.js,dzlvgw,lensesio,,https://www.reddit.com/r/dataengineering/comments/dzlvgw/kafka_tutorial_how_to_visualize_data_in_apache/,1.0,0.0,0.0,7729.0,
1251,2019-11-22 08:32:17,1574404337.0,dataengineering,"Any good way to think/learn about data with respect to size and fitting it to business needs , like how gbs-terabytes drive thought ?",dzx5oa,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/dzx5oa/any_good_way_to_thinklearn_about_data_with/,1.0,2.0,0.0,7734.0,Not sure if that fully makes sense. To help clarify a simple example of when an analyst can use their computer for data processing vs cloud resources.
1252,2019-11-22 12:15:05,1574417705.0,dataengineering,What is a Data Catalog and Why Should You Even Care?,dzz36s,knlph,,https://www.reddit.com/r/dataengineering/comments/dzz36s/what_is_a_data_catalog_and_why_should_you_even/,1.0,2.0,0.0,7734.0,
1253,2019-11-22 16:35:02,1574433302.0,dataengineering,What do you think of this project idea to build DE skills?,e01s0s,romanX7,,https://www.reddit.com/r/dataengineering/comments/e01s0s/what_do_you_think_of_this_project_idea_to_build/,1.0,17.0,0.0,7737.0,"I'm a DE with almost a year of experience (I also have another 2 years of experience as a software developer).

At work, I mainly use airflow in docker containers for ETL work and build dashboards in Looker.

My reason for wanted to take on a side project is that while I'm familiar with docker, airflow and Looker, i don't feel that I have a truly deep understanding of them because I did not build those tools from the ground up. I'm able to add features, but the lead engineer is usually the one that has to do major work and is consulted on architecture and the really important stuff. On top of that, I want to gain experience in technologies that we don't implement at work (like spark, kubernetes, ML).

In order to build these skills iv come up with a DE focused side project that will touch on most of these technologies. 

The Project:

Monitor and predict power outages using US energy information administration hourly production and consumption api.

The plan is to use dockerized airflow, spark and Amazon RDS to build an ETL pipeline. I will also use spark ML to predict power outages. I will then build an analytics dashboard in tableau to display KPIs and outage predictions and alerts. I'd also like to use kubernetes to manage the docker containers.



My Question:

With the real goal of this project being to gain experience with DE technologies, is this a solid plan? (Given that I just thought of it yesterday) 

I know some of the tools used (like kubernetes) my be overkill for a small side project, but im more focused on learning.

Also, how expensive do you think this could get if I'm running ETL jobs hourly and using cloud storage? Is the budget going to be impractical for a side project?"
1254,2019-11-22 20:00:46,1574445646.0,dataengineering,Streaming data from SQL Server to Kafka to Snowflake ❄️ with Kafka Connect,e04mz2,rmoff,,https://www.reddit.com/r/dataengineering/comments/e04mz2/streaming_data_from_sql_server_to_kafka_to/,1.0,0.0,0.0,7741.0,
1255,2019-11-22 21:02:46,1574449366.0,dataengineering,"Need to use Docker container, unsure where to start",e05jyz,str8cokane,,https://www.reddit.com/r/dataengineering/comments/e05jyz/need_to_use_docker_container_unsure_where_to_start/,1.0,6.0,0.0,7741.0,"I'm doing a technical interview, and at the end it says ""It is also required that your code can be executed in a Docker container (use Docker Compose if you require additional infrastructure)"". I don't really know anything about docker, if someone could give me a run down of what it means, as well as some resources to learn how to use it would be greatly appreciated. THanks1!"
1256,2019-11-22 22:53:55,1574456035.0,dataengineering,Apply for DE jobs in Bay Area,e076l9,ThatFilm,,https://www.reddit.com/r/dataengineering/comments/e076l9/apply_for_de_jobs_in_bay_area/,1.0,6.0,0.0,7742.0,"I have held SQL Developer positions for last 10 years and now would like to switch to DE role or Big Data Engineer role, right now I am focusing on python, algorithms, data structure, JavaScript and node.js and will eventually work in aws developer certification.

I apply to these jobs from indeed.com but I have no luck, I am living in Fresno, should I put a Bay Area address on my resume so employers think I am local.
It is scary to move to Bay Area without a job, I already have a good job here but I feel stuck in a role that’s not going anywhere and I feel I am not technically growing. I am preparing for triple bytes interview by practicing algorithms and DS.

Any tips on projects or job hunting tips will be helpful.
Thank you."
1257,2019-11-24 12:37:54,1574591874.0,dataengineering,How can i learn Data Modeling/Dimensional modeling,e0wyx6,Manyreason,,https://www.reddit.com/r/dataengineering/comments/e0wyx6/how_can_i_learn_data_modelingdimensional_modeling/,1.0,8.0,0.0,7771.0,"Hey guys,

I have recently taken over a data warehouse project and i feel completely overwhelmed with trying to add new sources of data. I have existing dimensions Im wanting to add but i have no idea how to design dimension tables. Is there any good resources i should look into? I've been reading Kimballs data warehousing book in my spare time to try get up to speed. 


Work has also said they would be willing to pay for some training, does anyone have any suggestions? I was looking into some Microsoft certs like 70-767.

Thanks in advance!!"
1258,2019-11-25 17:01:32,1574694092.0,dataengineering,Azure DataFactory copy job copies extension in the name of the destination file,e1h6o1,bachahbar,,https://www.reddit.com/r/dataengineering/comments/e1h6o1/azure_datafactory_copy_job_copies_extension_in/,1.0,0.0,0.0,7865.0,"In Azure Datafactory I copy zipped files from one server and unzip them inside another server.

The source file has the following name and extension ""[sourcefile.zip](https://sourcefile.zip)"" with '.zip' not being in the name and actually the extension (checked it with the option hide extensions in file explorer). When the copy job is finished The folder gets decompressed but keeps the '.zip' in the name but is not a zipped folder.

&amp;#x200B;

Inside my job file I do a copy job from the dataset of the source server and get the ""@item"" that is given by the foreach loop and concatenate the "".zip"" in the wildcardfileName to capture the actual zip file. because the foreach does not give me the full name of the file and if I don't mention the "".zip"" inside the wildcard I get the error \`Could not find file\`

&amp;#x200B;

           {
                                ""name"": ""CopyDataToFileServer"",
                                ""type"": ""Copy"",
                                ""dependsOn"": [
                                    {
                                        ""activity"": ""CopyDataToIRserver"",
                                        ""dependencyConditions"": [
                                            ""Succeeded""
                                        ]
                                    }
                                ],
                                ""policy"": {
                                    ""timeout"": ""7.00:00:00"",
                                    ""retry"": 0,
                                    ""retryIntervalInSeconds"": 30,
                                    ""secureOutput"": false,
                                    ""secureInput"": false
                                },
                                ""userProperties"": [],
                                ""typeProperties"": {
                                    ""source"": {
                                        ""type"": ""BinarySource"",
                                        ""storeSettings"": {
                                            ""type"": ""FileServerReadSettings"",
                                            ""recursive"": true,
                                            ""wildcardFileName"": {
                                                ""value"": ""@{concat(item(),'.zip')}"",
                                                ""type"": ""Expression""
                                            }
                                        }
                                    },
                                    ""sink"": {
                                        ""type"": ""BinarySink"",
                                        ""storeSettings"": {
                                            ""type"": ""FileServerWriteSettings"",
                                            ""copyBehavior"": ""PreserveHierarchy""
                                        }
                                    },
                                    ""enableStaging"": false
                                },
                                ""inputs"": [
                                    {
                                        ""referenceName"": ""IntegrationRuntimeStorageZip"",
                                        ""type"": ""DatasetReference""
                                    }
                                ],
                                ""outputs"": [
                                    {
                                        ""referenceName"": ""FileServer"",
                                        ""type"": ""DatasetReference""
                                    }
                                ]
                            },"
1259,2019-11-25 21:26:01,1574709961.0,dataengineering,Airflow survey,e1l551,serkef-,,https://www.reddit.com/r/dataengineering/comments/e1l551/airflow_survey/,1.0,0.0,0.0,7894.0,"Apache Airflow developers team is running the following survey. Feel free to contribute if you are using airflow.

https://docs.google.com/forms/d/e/1FAIpQLSf20UIdK4GbTMXT_hsPn2iPvH_sNG6QmH8NXGnmJlt7WY_xkg"
1260,2019-11-25 21:49:14,1574711354.0,dataengineering,Interview Questions,e1li0r,tpedar50,,https://www.reddit.com/r/dataengineering/comments/e1li0r/interview_questions/,1.0,10.0,0.0,7895.0,"I had a few interview questions I've run into either in interviews or the application process and I wanted to get some feedback on them.

SQL

* One job description said, ""You must really know SQL and not just know SQL."" and then said, ""Yes there is a difference"".
* Another said rate your SQL skills from 0-10 0 being knows nothing and 10 being expert.

Question: What's the difference between someone who knows SQL and someone who really knows SQL. What are some SQL qualities that someone has that's a 10 vs someone who is an 8 or 7?

ETL

* How would you load 1,000,000+ large (gigabytes) log files into your database?

Question: I joined an established team as a Data Engineer and we working primarily with SSIS I haven't run into a problem like this yet. What's an appropriate answer to this?

Networking

* And understanding of web-based networking concepts, specifically an advanced understanding of HTTP.

Question: What networking knowledge is a Data Engineer expected to have?"
1261,2019-11-25 22:03:36,1574712216.0,dataengineering,Does it get better? Former business/data analyst with less technical bacground,e1lqgy,phenderbender,,https://www.reddit.com/r/dataengineering/comments/e1lqgy/does_it_get_better_former_businessdata_analyst/,1.0,0.0,0.0,7894.0,
1262,2019-11-26 06:07:53,1574741273.0,dataengineering,The Catch-22 Problem All Aspiring Data Engineers Face - Tips ?,e1shik,overweight_neutrino,,https://www.reddit.com/r/dataengineering/comments/e1shik/the_catch22_problem_all_aspiring_data_engineers/,1.0,0.0,0.0,7912.0,"Hello,

I am an aspiring data engineer. The catch-22 problem that people in my position face is as follows: data engineers, by definition, develop infrastructure and solutions for processing large amounts of data (more or less, bear with me here). This is very hard to do using personal projects (since you simply do not have access to, or resources to support, a large amount of data), but how else would you get hired if you have no experience? It seems like a very difficult field to break into. 

I personally got very lucky and landed an internship that allowed be to develop lots of distributed ETL pipelines using cool tools for terabytes of data. BUT, if it were not for this luck I would be at a loss for how to approach entering this interesting field. 

I suppose my question is, how would a regular college student attempt to break into this field? How would you best demonstrate your expertise using certain big-data technologies without access to data thats big enough to necessitate those tools in the first place? Sorry if this comes off as a rant."
1263,2019-11-26 06:54:38,1574744078.0,dataengineering,Python books or courses for Data Engineering,e1t0cn,Abachwani,,https://www.reddit.com/r/dataengineering/comments/e1t0cn/python_books_or_courses_for_data_engineering/,1.0,19.0,0.0,7912.0,"Hi,

I am looking for recommendations on any books or courses which teach data engineering using python. Appreciate any links /resources.

TIA!!"
1264,2019-11-26 16:39:56,1574779196.0,dataengineering,Data science tripped up by data engineering?,e1ysv3,mjgierc,,https://www.reddit.com/r/dataengineering/comments/e1ysv3/data_science_tripped_up_by_data_engineering/,1.0,4.0,0.0,7918.0,"Quick question (or potential rabbit hole). How many data scientists here are regularly tripped up by data engineering and infrastructure issues? I regularly read about 80/20 issue related to cleaning data vs. analyzing it, but want to see if this holds true for the experts in this group. Do you end up wasting lots of time cleaning data and working on ETL, data warehouse and data lake issues?"
1265,2019-11-26 19:52:39,1574790759.0,dataengineering,Repo organization?,e21ky1,parabolic_line,,https://www.reddit.com/r/dataengineering/comments/e21ky1/repo_organization/,1.0,0.0,0.0,7921.0,"I'm looking for a reading list (articles, books, blogs, etc.) for the latest greatest theory and practice for repo organization for a large data warehouse with diverse ETL processes (written in multiple languages and created with multiple applications). I feel like there must be a textbook answer out there somewhere for what my organization is struggling with: organizing a very diverse repo. 

Any tips?"
1266,2019-11-26 20:07:36,1574791656.0,dataengineering,What are the responsibilities of a jr data engineer job and are these jobs common ?,e21t77,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/e21t77/what_are_the_responsibilities_of_a_jr_data/,1.0,4.0,0.0,7921.0,
1267,2019-11-26 21:01:06,1574794866.0,dataengineering,Scaling Apache Airflow for Machine Learning Workflows,e22luc,arimbr,,https://www.reddit.com/r/dataengineering/comments/e22luc/scaling_apache_airflow_for_machine_learning/,1.0,4.0,0.0,7922.0,"Airflow is great for managing ETL pipelines! But what about machine learning workflows?

I was running a few hundred DAGs for all our data engineering with the Celery Executor, but as soon that we started doing machine learning, we realized we needed a solution to launch ML tasks remotely... and get ML version control.

I have seen companies using the *KubernetesPodOperator.* Last week, Dailymotion shared how they [manage to schedule Machine Learning pipelines with Airflow and Kubernetes](https://medium.com/dailymotion/bring-machine-learning-models-faster-to-production-with-airflow-and-kubernetes-e9d47ca3bee5). 

I was working in a much smaller team and we didn't have the resources to maintain a Kubernetes cluster...

So we decided to write an Airflow plugin for Valohai, a machine learning platform built on open standards. I just shared our experience [scaling Apache Airflow for Machine Learning workflows](https://towardsdatascience.com/scaling-apache-airflow-for-machine-learning-workflows-f2446257e495) using the *ValohaiSubmitExecutionOperator.*

Which executor or operators you use to scale Airflow for ML?"
1268,2019-11-26 22:25:29,1574799929.0,dataengineering,An interview about how Sentry used Clickhouse to build an event data warehouse and pay down their architecture debt,e23w2m,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/e23w2m/an_interview_about_how_sentry_used_clickhouse_to/,1.0,0.0,0.0,7921.0,
1269,2019-11-27 00:00:59,1574805659.0,dataengineering,Hi I am Jr DevOps Engineer in a Data Driven Company. How can I improve the company infrastructure (AWS) and provide automation.,e25dzr,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/e25dzr/hi_i_am_jr_devops_engineer_in_a_data_driven/,1.0,3.0,0.0,7924.0,"Here is our tech stack :

  
\- Kafka, Spark, Cassandra, Redshift, Jenkins, Kubernetes, Terraform,Atlantis AWS - GLUE/S3/EMR/DATAPIPELINE/LAMBDA/EC2/ATHENA, Wavefront and Cloudwatch for monitoring even AWS datalake. So my question since I work with several Data engineer, how can jr DevOps help them, what can we automate from a resource and infrastructre level, heck can we help them with even security ?  What level of automation or help does  a Data engineer needs from a DevOps. I really want to improve my skillset and I feel like this is one insight by asking here could help me. Any suggestion or advice is appreciated."
1270,2019-11-27 00:01:34,1574805694.0,dataengineering,Can someone explain MSK?,e25ebo,Phizy,,https://www.reddit.com/r/dataengineering/comments/e25ebo/can_someone_explain_msk/,1.0,0.0,0.0,7924.0,
1271,2019-11-27 09:54:14,1574841254.0,dataengineering,Concepts of Data Preprocessing,e2cstw,mlheadredditor,,https://www.reddit.com/r/dataengineering/comments/e2cstw/concepts_of_data_preprocessing/,1.0,0.0,0.0,7933.0,"Data is truly considered a resource in today’s world. As per the World Economic Forum, by 2025 we will be generating about 463 exabytes of data globally per day! But is all this data fit enough to be used by machine learning algorithms? How do we decide that?

Read this article to find out :  [https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825](https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825)"
1272,2019-11-28 01:16:45,1574896605.0,dataengineering,Simple question. How can I join two data sources in sql if one has a field/column the other lacks?,e2opl4,work_acc_1,,https://www.reddit.com/r/dataengineering/comments/e2opl4/simple_question_how_can_i_join_two_data_sources/,1.0,4.0,0.0,7943.0,"So we get some data off couple of big name APIs and have to merge the data sets. For the most part this is simple, it comes down to a SQL UNION. We've been creating '0' placeholder columns for fields that don't exist in one dataset but do in the other. This is problematic though because in this case '0' implies a value of zero, not NULL. But we can't have NULL because the datatypes for the matching columns must be the same (numeric in this case) in order for the UNION to work. 

Does anyone know of an easy way to deal with this issue if you had to for several dozen columns?"
1273,2019-11-28 07:08:28,1574917708.0,dataengineering,Any good alternative to SAS DQMATCH?,e2t51o,flightcodes,,https://www.reddit.com/r/dataengineering/comments/e2t51o/any_good_alternative_to_sas_dqmatch/,1.0,2.0,0.0,8066.0,"Hi, does anyone here know any good alternative to sas dqmatch? It’s basically a fuzzy match between strings. 

I know of fuzzy wuzzy package for python but this would mean I would still have to build a synonym table as well as the unique identifier along with a reasonable process flow that covers all steps involved. 

Was just hoping if there’s any existing packages already for R or Python that covers this. 

Thanks"
1274,2019-11-28 19:10:33,1574961033.0,dataengineering,Management of Data Lake Raw Zone,e30v7m,beaverhair,,https://www.reddit.com/r/dataengineering/comments/e30v7m/management_of_data_lake_raw_zone/,1.0,9.0,0.0,8259.0,"How are you handling ingestion to the raw zone of your data lake? Specifically for object based storage like S3. Are you copying a new snapshot to a date-named directory each time there is a load? Do you only bring over the deltas? I’m new to data lakes and wondering how best to update the raw zone if we are loading data, say, daily."
1275,2019-12-01 13:31:03,1575199863.0,dataengineering,Looking for some assistance on a project as I've drawn a blank,e4g8qs,pooinmyloo,,https://www.reddit.com/r/dataengineering/comments/e4g8qs/looking_for_some_assistance_on_a_project_as_ive/,1.0,5.0,0.0,8309.0,"I work in a highly regulated industry where each country has its own set of technical and legal regulations. There are both differences and similarities between each countries regulations and I want to develop a system where a user is able to easily identify where there are matches and where requirements are bespoke to each country. 

What I have achieved so far is to break down each set of regulations into categories and laid them out in a spreadsheet in a simple |Country|UniqueID|Regulation| - in another spreadsheet I have mapped each requirement based only on its UniqueID in a table format. So where they match they share the same line, where they don't there is no corresponding match on that line. Something like the following. 

|Denmark|France|Holland|Colombia|Egypt|

|DK001| ------ | ------ | CO011 | EG023 |

The dashes denote an empty cell, so no match for that requirement for that country. 

The end result that I am trying to achieve is where a user can input one or more countries, select the category and the output would display where there are matches and where they are bespoke requirements based on the users choices. It wouldn't need to be fancy, but would need to detail the actual requirement, not the Unique ID. And this is where I'm drawing a blank. I can't quite work out what would be the best way of powering this. I can't even think of the correct search term to use to find resources in google. 

If anybody has an idea on what would be the best way to go about this, or even point me in the right direction in terms of best resources, I would be very grateful."
1276,2019-12-01 22:17:58,1575231478.0,dataengineering,Nifty Pandas Trick: Your dataset has many columns and you want to ensure the correct data types,e4mzq2,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/e4mzq2/nifty_pandas_trick_your_dataset_has_many_columns/,1.0,3.0,0.0,8315.0,https://twitter.com/justmarkham/status/1192794326763474944
1277,2019-12-02 03:50:07,1575251407.0,dataengineering,What tool am I looking for?,e4rogz,mucsc,,https://www.reddit.com/r/dataengineering/comments/e4rogz/what_tool_am_i_looking_for/,1.0,10.0,0.0,8321.0,"I want to send data to Kafka then run some business logic on the data I am receiving from my consumer and spit that data out as fast as possible to a web frontend. Here is my idea so far but I am missing some pieces I feel like.

IoT Device -&gt; Kafka -&gt; Consumer (not sure what I should use for a consumer technology) -&gt; Cassandra -&gt; Flask/Spring boot API that CRUDS Cassandra

Any advice on the overall data flow would be great, or tools I should be using here to help. Especially in that consumer piece, I am not sure what to use."
1278,2019-12-02 13:08:07,1575284887.0,dataengineering,How the Auth0 Data Team Uses R and Python,e4xliv,Ramirond,,https://www.reddit.com/r/dataengineering/comments/e4xliv/how_the_auth0_data_team_uses_r_and_python/,1.0,0.0,0.0,8328.0,
1279,2019-12-02 19:01:26,1575306086.0,dataengineering,DB User Access Control,e51vy5,serkef-,,https://www.reddit.com/r/dataengineering/comments/e51vy5/db_user_access_control/,1.0,4.0,0.0,8332.0,"What are the tools or best practices out there on managing permissions on databases?

I'm looking for something like apache ranger, but for agnostic jdbc connections.

I want to have a flexible and transparent system to control user's permissions in databases at column level. 

I hope this makes sense."
1280,2019-12-02 21:06:22,1575313582.0,dataengineering,Opinions on a project for a CS degree.,e53qbu,Sergivlc,,https://www.reddit.com/r/dataengineering/comments/e53qbu/opinions_on_a_project_for_a_cs_degree/,1.0,4.0,0.0,8335.0,"Good evening,

As part of my degree I need to submit a project which needs to have a substantial coding component. I initially had a pretty clear idea of what I wanted to do, but as I started reading different articles and papers I started having second thoughts about my project idea.

I have always been interested in Data Warehouses, and I thought of creating a ""simple"" Data Warehouse to illustrate the different elements and technical aspects that need to be taken into consideration in building it, and then using some scripts, automate some reporting different departments of the fictious company may need.

Most of the articles I have read so far point out to the idea that whilst Data Warehouse are more needed than ever, the traditional architectures are not fit for purpose in the current environment ([https://www.jamesserra.com/archive/2017/12/is-the-traditional-data-warehouse-dead/](https://www.jamesserra.com/archive/2017/12/is-the-traditional-data-warehouse-dead/)), the main reason being the diversity of data sources, and the increasing importance of real time data.

My idea is gradually evolving to the integration of both relational data and streams using KAFKA, specially after having read Martin Kleppmann's articles, but I am not sure on how to keep the scope of this project manageable, or whether there are more suitable technologies than KAFKA that I should consider.

My tutor for the project can't help me much at the moment since he hasn't worked with Apache before, and for this reason I would appreciate if somebody could give me some advice on whether the project is interesting, and some advice on how to approach it.

Many thanks in advance."
1281,2019-12-03 03:10:25,1575335425.0,dataengineering,How To Set Up Snowflake On Demand (On AWS),e591kn,jakebuilds,,https://www.reddit.com/r/dataengineering/comments/e591kn/how_to_set_up_snowflake_on_demand_on_aws/,1.0,0.0,0.0,8341.0,
1282,2019-12-03 09:28:01,1575358081.0,dataengineering,Change management tools?,e5djwe,Gawgba,,https://www.reddit.com/r/dataengineering/comments/e5djwe/change_management_tools/,1.0,0.0,0.0,8344.0,
1283,2019-12-03 16:58:04,1575385084.0,dataengineering,An interview about building a successful data team and managing their career growth to power a successful financial business,e5i987,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/e5i987/an_interview_about_building_a_successful_data/,1.0,0.0,0.0,8350.0,
1284,2019-12-03 17:22:47,1575386567.0,dataengineering,Commercial ETL tools VS open-source,e5ilne,fgoussou,,https://www.reddit.com/r/dataengineering/comments/e5ilne/commercial_etl_tools_vs_opensource/,1.0,10.0,0.0,8350.0,"Hello,

I'd like to get the input of this great community on their experience with commercial ETL tools. I'm in the process of evaluating Cloverdx (previously cloverETL) for my employer, and while I find it great, I cannot justify the price. Everything it can do can be done using python+Airflow+OS scheduling or some cloud service. Perhaps the visual designer makes it more user friendly for the non-software employees, but that doesn't justify the hefty price tag IMHO. 

Can anyone share their opinions/experiences on switching from a script-based data pipeline to a commercial ETL tool?

Thanks"
1285,2019-12-03 18:07:44,1575389264.0,dataengineering,AWS MSK &amp; Lenses.io - Kafka in Days not Months. New Support for JMX Open Monitoring with Prometheus,e5j8v5,lensesio,,https://www.reddit.com/r/dataengineering/comments/e5j8v5/aws_msk_lensesio_kafka_in_days_not_months_new/,1.0,0.0,0.0,8350.0,
1286,2019-12-03 20:48:23,1575398903.0,dataengineering,Why Airflow,e5llyl,rywalker,,https://www.reddit.com/r/dataengineering/comments/e5llyl/why_airflow/,1.0,0.0,0.0,8351.0,
1287,2019-12-03 21:55:09,1575402909.0,dataengineering,Understanding what is Data Leakage in Machine Learning and how it can be detected,e5mkws,ai_jobs,,https://www.reddit.com/r/dataengineering/comments/e5mkws/understanding_what_is_data_leakage_in_machine/,1.0,0.0,0.0,8351.0,
1288,2019-12-04 00:59:41,1575413981.0,dataengineering,Databricks Opinions?,e5pd0o,Phizy,,https://www.reddit.com/r/dataengineering/comments/e5pd0o/databricks_opinions/,1.0,0.0,0.0,8355.0,
1289,2019-12-04 09:09:29,1575443369.0,dataengineering,Business Benefits of Cross Domain Tracking Using Google Analytics,e5vitq,Countants123,,https://www.reddit.com/r/dataengineering/comments/e5vitq/business_benefits_of_cross_domain_tracking_using/,1.0,0.0,0.0,8357.0,
1290,2019-12-04 09:53:35,1575446015.0,dataengineering,"Metaflow, Netflix's Python framework for data science, is now open source",e5vxuv,thundergolfer,,https://www.reddit.com/r/dataengineering/comments/e5vxuv/metaflow_netflixs_python_framework_for_data/,1.0,3.0,0.0,8357.0,
1291,2019-12-04 09:56:23,1575446183.0,dataengineering,"At bigger companies, which aspects of data engineering are the most employable and worth the most value?",e5vyrq,BostonPanda,,https://www.reddit.com/r/dataengineering/comments/e5vyrq/at_bigger_companies_which_aspects_of_data/,1.0,0.0,0.0,8357.0,
1292,2019-12-04 15:53:49,1575467629.0,dataengineering,Implement Avg Time difference between events for our event analytics pipeline,e5zja2,psinghal20,,https://www.reddit.com/r/dataengineering/comments/e5zja2/implement_avg_time_difference_between_events_for/,1.0,1.0,0.0,8357.0,"Hi,

I am new to this Data Engineering landscape and trying to create a real-time event analytics pipeline for our company. Our clients generate location pings at regular intervals of 5 seconds with their states. We want to calculate the Avg time an order stays in one particular state.  


In our current pipeline, we use Amazon Kinesis to ingest the messages and Kinesis Analytics stream processor to do the streaming joins and calculate aggregates over 1-minute intervals. These aggregates are stored in a PostgreSQL database which can be queried over from the dashboard which is going to be provided to the operations team. 

We are looking to empower the Avg Time difference queries to see how long the product stays in one state."
1293,2019-12-04 16:29:17,1575469757.0,dataengineering,Tutorials vs stack overflow for learning large concepts you have no familiarity with?,e600ga,romanX7,,https://www.reddit.com/r/dataengineering/comments/e600ga/tutorials_vs_stack_overflow_for_learning_large/,1.0,8.0,0.0,8357.0,"I'm currently working on a personal project and have come to the point where I'd like to deploy my airflow ETL pipeline and orchestrate the docker containers using kubernetes.

I have little to no experience with kubernetes and deployment to production environments.

My two choices of how to figure it out are:

1. Google/stack overflow and hack my way to a working solution

2. Take a 20 hour udemy course that will walk me through the process in detail, but probably shelf my project for 2 weeks.

Given that I'm not really on a deadline to complete this, but I also don't want to spend more time than necessary to learn deploy/kubernetes, which approach do you think is best?"
1294,2019-12-04 17:27:43,1575473263.0,dataengineering,Looking for career advice,e60ts6,Earthquake14,,https://www.reddit.com/r/dataengineering/comments/e60ts6/looking_for_career_advice/,1.0,0.0,0.0,8358.0,
1295,2019-12-04 22:12:57,1575490377.0,dataengineering,Must read blog posts recommendations?,e652h3,werdiser,,https://www.reddit.com/r/dataengineering/comments/e652h3/must_read_blog_posts_recommendations/,1.0,0.0,0.0,8369.0,"I know about a handful of engineering blogs that are super high quality and informative (Netflix, Airbnb). I'm mostly interested in your favourite individual blog post(s) by engineering teams related to data engineering.

Also while I'm looking for recommendations, are there any notable people / lists on Twitter in DE worth following?"
1296,2019-12-05 03:21:09,1575508869.0,dataengineering,Is there a way to protect my R code that runs on a AWS account owned by a client?,e69j5a,viniciusvbf,,https://www.reddit.com/r/dataengineering/comments/e69j5a/is_there_a_way_to_protect_my_r_code_that_runs_on/,1.0,2.0,0.0,8384.0,"I just joined a company that, for some weird unimaginable reason, needs to build an ETL pipeline inside an AWS account owned by a client. 
There's one part of the ETL pipeline that runs a code written in R. The problem is, this R code is very important part of our business, and our intelectual property. Our clients can't see this code. 
Is there any way to run this in their AWS environment without them having access to our code? R is not compilable, so we can't just deploy an executable file there. And we HAVE to run this in their environment, I suggested creating an API to run this in our AWS environment, but this is not an option."
1297,2019-12-05 07:28:28,1575523708.0,dataengineering,How Cloud-Based Solutions Are Transforming the Online Gaming Industry,e6cm51,naina_jain1,,https://www.reddit.com/r/dataengineering/comments/e6cm51/how_cloudbased_solutions_are_transforming_the/,1.0,0.0,0.0,8391.0,
1298,2019-12-05 10:55:01,1575536101.0,dataengineering,Career Advice: What should I do to advance my career towards DE and DS ?,e6el4h,code6reaker,,https://www.reddit.com/r/dataengineering/comments/e6el4h/career_advice_what_should_i_do_to_advance_my/,1.0,5.0,0.0,8396.0,"I've been working as QA analyst having automation expertise. For past couple of years, I've moved away from automation to ETL testing and database testing involving large scale enterprise datasets. Right now, I'm in talks with one of product company who develops Data Modeler products to join their team. Is it a good move as I'm not quite connecting Data Modelling with DE/DS ? Please advise."
1299,2019-12-05 18:21:28,1575562888.0,dataengineering,Lost and Need Help with Data Engineering 101,e6jgjk,ElethorAngelus,,https://www.reddit.com/r/dataengineering/comments/e6jgjk/lost_and_need_help_with_data_engineering_101/,1.0,44.0,0.0,8401.0,"A bit of an introduction,

I am a solutions analyst in a marketing company with a degree in Business. I transitioned into the world of data through a retraining program and changed into a hybrid role where I do a bit of everything.

I want to start a data engineering project for me to ease my work and by extension the work of the team by creating a data stack that enables us to ingest data from a variety of platforms like fb ads, double click manager and some crm platforms, transform them to fit our analaytics structure and automate reports and dashboards.

Currently we have nothing in place apart from tools that we pay to use and export data to create our manual reports.

I am intermediate in Python and learning Javascript and my SQL is not the best and I feel this project is a way to derive value for myself and the company grow.

That being said, I don't have the right knowledge to start. I read things about airflow, azure data factory, using cloud functions and its all increasingly a blur to me.

I'm looking at the meltano stack right now but I am wondering if you guys have a better option. Currently I am throwing things but my architecture is shit to say the least (I feel)

I am using StitchData to load my data into BigQuery, then load it up using Google Data astudio for viz work, and now I feel that's less than ideal as I don't have transformation to the extent I need.

Am I at the right place ? Am I sticking myself into the wrong hole ? Is this too ambitious ?

Looking forward to the opinions of my betters here. Cheers !"
1300,2019-12-05 23:52:58,1575582778.0,dataengineering,Profiling the Airflow Scheduler,e6obom,rywalker,,https://www.reddit.com/r/dataengineering/comments/e6obom/profiling_the_airflow_scheduler/,1.0,0.0,0.0,8408.0,
1301,2019-12-06 00:00:09,1575583209.0,dataengineering,Profiling the Airflow Scheduler,e6ofk0,rywalker,,https://www.reddit.com/r/dataengineering/comments/e6ofk0/profiling_the_airflow_scheduler/,1.0,0.0,0.0,8407.0,"Ash is working on making Airflow better for everyone, directly on the open-source project, reviewing and merging PRs, preparing releases, and lately working on [roadmap for Airflow 2.0](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+2.0)  \- one of the big goals is to “improve Scheduler performance and reliability”.

While he's been hacking on it, we convinced him to take a moment to post a progress report:  
[https://www.astronomer.io/blog/profiling-the-airflow-scheduler/](https://www.astronomer.io/blog/profiling-the-airflow-scheduler/)"
1302,2019-12-06 04:45:09,1575600309.0,dataengineering,Blog Article: The Most Underrated Python Packages,e6sbq0,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/e6sbq0/blog_article_the_most_underrated_python_packages/,1.0,3.0,0.0,8424.0,"[https://towardsdatascience.com/the-most-underrated-python-packages-e22bf6049b5e](https://towardsdatascience.com/the-most-underrated-python-packages-e22bf6049b5e)

Love me a good curated list of Python Libraries... [https://github.com/great-expectations/great\_expectations](https://github.com/great-expectations/great_expectations) Has to be my favorite  ;)"
1303,2019-12-06 07:17:18,1575609438.0,dataengineering,"Is there any market or demand for datasets , like peer to peer. Any biz model to do this",e6u5rj,acceptedcitizen,,https://www.reddit.com/r/dataengineering/comments/e6u5rj/is_there_any_market_or_demand_for_datasets_like/,1.0,5.0,0.0,8425.0,So if I am a data engineer and I scrape some days or hit an api and then create a data set can I sell it or monetize that process ? Outside of being a data engineer for a corporate entity.
1304,2019-12-06 16:07:43,1575641263.0,dataengineering,Advice on applying for data engineering jobs?,e6z7b8,statistical_engineer,,https://www.reddit.com/r/dataengineering/comments/e6z7b8/advice_on_applying_for_data_engineering_jobs/,1.0,16.0,0.0,8434.0,"I'm a data scientist that does a lot of data engineering work, and found myself more interested in the data engineering side.




How do you prepare for data engineering interviews?  Software engineers have leet code.  Data scientists also have leetcode and prob/stats/ml questions."
1305,2019-12-06 23:22:01,1575667321.0,dataengineering,Want to switch career from system engineer to data engineer,e74rob,labobina,,https://www.reddit.com/r/dataengineering/comments/e74rob/want_to_switch_career_from_system_engineer_to/,1.0,12.0,0.0,8447.0,"Hi Team,

Need advice about switching career from system engineer to data engineer.  I have a computer science degree and knowledge with Linux, basic AWS, Python, and SQL. 

How possible is to switch or do I need get experience as Software Engineer or developer first? 

Thank you in advanced !"
1306,2019-12-07 12:57:15,1575716235.0,dataengineering,Data Channel IT Services,e7d1vx,Brightcarearmaker,,https://www.reddit.com/r/dataengineering/comments/e7d1vx/data_channel_it_services/,1.0,0.0,0.0,8454.0, Data Channel Marketing reporting tools are used for Integrate your all marketing data under one platform. Data Channel provides Data aggregation tools which helps you to integrate your all Api under one roof. For more-  [https://datachannel-65.webself.net/](https://datachannel-65.webself.net/)
1307,2019-12-08 00:25:16,1575757516.0,dataengineering,"Since Spotify Wrapped came out, wanted to share an article they posted about their event delivery system. Really interesting stuff, apparently they ingest over 350TB of raw data every day",e7kyj7,sparkedkafka,,https://www.reddit.com/r/dataengineering/comments/e7kyj7/since_spotify_wrapped_came_out_wanted_to_share_an/,1.0,1.0,0.0,8469.0,
1308,2019-12-08 07:29:34,1575782974.0,dataengineering,What was your first step towards being a Database Engineer?,e7pw08,lhikary,,https://www.reddit.com/r/dataengineering/comments/e7pw08/what_was_your_first_step_towards_being_a_database/,1.0,0.0,0.0,8472.0,
1309,2019-12-08 09:59:56,1575791996.0,dataengineering,What different strategies do everyone here implement to build your CI/ CD pipelines to test your ETL scripts,e7r5id,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/e7r5id/what_different_strategies_do_everyone_here/,1.0,9.0,0.0,8473.0,
1310,2019-12-08 11:23:13,1575796993.0,dataengineering,What software or program do you guys use to develop graphics of data flows?,e7rrw9,fuzzywunder,,https://www.reddit.com/r/dataengineering/comments/e7rrw9/what_software_or_program_do_you_guys_use_to/,1.0,2.0,0.0,8472.0,That’s the question! I was at a conference the other day and saw some really nice data flow graphics. What’s the best way to do it?
1311,2019-12-08 12:00:45,1575799245.0,dataengineering,Is Udacity still offering monthly subscription on their data engineering course?,e7s1um,kuriouslife,,https://www.reddit.com/r/dataengineering/comments/e7s1um/is_udacity_still_offering_monthly_subscription_on/,1.0,4.0,0.0,8472.0,The current price for the full course (5 months) is $1800 which is insane I think. So I was hoping to finish it off in 2 months  and pay only for 2 months. But I am not able to find a monthly subscription. Did Udacity stop their monthly subscription feature?
1312,2019-12-08 17:21:51,1575818511.0,dataengineering,"Master Data Management, how to match and merge records to unify your data",e7uvy4,linkerzx,,https://www.reddit.com/r/dataengineering/comments/e7uvy4/master_data_management_how_to_match_and_merge/,1.0,0.0,0.0,8472.0,
1313,2019-12-08 19:43:38,1575827018.0,dataengineering,AWS pre:Invent and re:Invent takeaways,e7wpzt,bobhaffner,,https://www.reddit.com/r/dataengineering/comments/e7wpzt/aws_preinvent_and_reinvent_takeaways/,1.0,5.0,0.0,8476.0,"What were your notable, data-related takeaways from re:Invent and the days leading up to it?"
1314,2019-12-08 22:36:53,1575837413.0,dataengineering,"what is a good resource (book, class, etc) to learn about distributed systems?",e7z2ez,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/e7z2ez/what_is_a_good_resource_book_class_etc_to_learn/,1.0,10.0,0.0,8476.0,"as a junior-level data engineer, my knowledge of distributed systems is very poor.  i have a tough time debugging issues when our hadoop or spark clusters go down or have issues scaling. i don't really know how to create a cluster, or take down nodes, allocate the right amount of resources for a spark job, or investigate issues.  


i'd say my python and sql are excellent and i can get around on unix.  


unfortunately, my team is unwilling to teach me. they'd rather just do it themselves.. which kind of sucks.. but i've decided to take it into my own hands and try to figure this out all myself. what is a good place to start?"
1315,2019-12-09 01:54:42,1575849282.0,dataengineering,"What is the difference between Kedro, Prefect and the newly released Metaflow?",e81p3u,OppositeMidnight,,https://www.reddit.com/r/dataengineering/comments/e81p3u/what_is_the_difference_between_kedro_prefect_and/,1.0,1.0,0.0,8481.0,
1316,2019-12-09 05:55:36,1575863736.0,dataengineering,Questions on Data Engineering docs,e84lg9,deployman,,https://www.reddit.com/r/dataengineering/comments/e84lg9/questions_on_data_engineering_docs/,1.0,9.0,0.0,8484.0,How do people typically document their data pipelines within an organization?
1317,2019-12-09 06:09:44,1575864584.0,dataengineering,How do people typically document their data pipelines within an organization?,e84r9g,deployman,,https://www.reddit.com/r/dataengineering/comments/e84r9g/how_do_people_typically_document_their_data/,1.0,4.0,0.0,8484.0,I'm curious how people document the overall data pipelines within an organization (if you even do / wish you did).
1318,2019-12-09 18:00:42,1575907242.0,dataengineering,An interview about how SnowflakeDB was built to provide a performant and flexible data platform for the cloud era,e8bsc7,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/e8bsc7/an_interview_about_how_snowflakedb_was_built_to/,1.0,0.0,0.0,8488.0,
1319,2019-12-09 23:27:40,1575926860.0,dataengineering,"""Data feed streamlining"" learning resources",e8gcic,CelesteKingIII,,https://www.reddit.com/r/dataengineering/comments/e8gcic/data_feed_streamlining_learning_resources/,1.0,2.0,0.0,8492.0,"Hey all

I have a data science internship interview coming up next week. I asked the recruiter what the role would entail, and he mostly mentioned a bunch of stats/ml stuff. Two things that stood out to me however were **(a) data feed streamlining** and **(b) database structure.** 

My background is very much in the ml/stats stuff, so I'm not really familiar with data streaming, etc. Does anyone have any idea what kinda questions I should expect, or how I should prepare? 

Thanks!"
1320,2019-12-10 02:21:55,1575937315.0,dataengineering,200 OK! Error Handling in GraphQL,e8io7y,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/e8io7y/200_ok_error_handling_in_graphql/,1.0,0.0,0.0,8493.0,
1321,2019-12-10 03:53:21,1575942801.0,dataengineering,Which Framework to use to train model and predict online?,e8jtwg,EntropyRX,,https://www.reddit.com/r/dataengineering/comments/e8jtwg/which_framework_to_use_to_train_model_and_predict/,1.0,2.0,0.0,8497.0,"I want to give the users the possibility to request predictions online and I'm evaluating with framework I should leverage for this.

The challenge is: **not only it should generate predictions on demand, but it should also train the model itself.**

For example, I have data for each city in a Country. Obviously I don't want to schedule a job that stores daily/weekly predictions for each city (which probably will never get called). Instead, I want to provide the code for a model and then train it on historical data and make the prediction based on the city selected by the user.

&amp;#x200B;

I was thinking to use Kubeflow, but I have never used it before and I'm not sure this is the right use-case. 

Do you have any frameworks/ approaches to suggest I can look into? 

&amp;#x200B;

Thank you!!"
1322,2019-12-10 06:23:24,1575951804.0,dataengineering,"I got a new job as a DE , my first job coming from an analyst role. I really like what I am doing and learning but I feel like I’m moving at a slower pace than my manager and team lead would like and it really sucks I don’t know if I’ll get fired or If I am just being hard on myself",e8lnlz,acceptedcitizen,,https://www.reddit.com/r/dataengineering/comments/e8lnlz/i_got_a_new_job_as_a_de_my_first_job_coming_from/,1.0,27.0,0.0,8497.0,"I’m not sure if I should look for jr roles or idk , I just wish I could move at a slower pace and feel more comfortable asking questions. I know this is more of an open ended comment so I’m sorry to vent but if anyone has any advice lmk. Even a grasp of what the first 6 months of a DE and reasonable expectations or any uplifting stories of success :)"
1323,2019-12-10 07:53:37,1575957217.0,dataengineering,"Pipedream, a free integration platform built for developers",e8mlxb,dylburger-pipedream,,https://www.reddit.com/r/dataengineering/comments/e8mlxb/pipedream_a_free_integration_platform_built_for/,1.0,1.0,0.0,8500.0,
1324,2019-12-10 14:29:58,1575980998.0,dataengineering,Datacamp Data Engineering Course worth it?,e8q0dg,Viobolt,,https://www.reddit.com/r/dataengineering/comments/e8q0dg/datacamp_data_engineering_course_worth_it/,1.0,0.0,0.0,8502.0,"Hi all,

I started working as a data engineer, from a data analyst recently. I'm reasonably proficient in SQL and R but have little knowledge of python and data architecture so I've been looking at courses. I noticed the datacamp one is onsale but cannot find a single review of it, has anyone completed the course? if not is there any other courses you guys recommend me take, the ones ive found are ridiculously expensive."
1325,2019-12-10 15:38:15,1575985095.0,dataengineering,Minimal WebUI for Prefect workflow management system.,e8qpg1,theslay,,https://www.reddit.com/r/dataengineering/comments/e8qpg1/minimal_webui_for_prefect_workflow_management/,1.0,2.0,0.0,8502.0,I just found a really cool project on GitHub for [Prefect](https://www.prefect.io/). I love prefect but would love to monitor tasks like in Airflow. [https://github.com/roveo/praetor](https://github.com/roveo/praetor)
1326,2019-12-10 20:45:36,1576003536.0,dataengineering,Would I save money migrating from Redshift to Snowflake?,e8uqyy,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/e8uqyy/would_i_save_money_migrating_from_redshift_to/,1.0,3.0,0.0,8506.0,"Hello. I have read about the autoscaling of Snowflake and it's something that could help my company to reduce the costs. Currently, we have 1 year reserved cluster with 10 nodes. The average and maximum CPU consumption within 3 days (almost every day the profile is very very similar) are the following:

https://preview.redd.it/r1nb3fcopu341.png?width=1681&amp;format=png&amp;auto=webp&amp;s=ca73fffde66b8d8f2c7dbb1338bc046e84f00f53

https://preview.redd.it/3e13i3copu341.png?width=1681&amp;format=png&amp;auto=webp&amp;s=6defe688e8cc4c13bc61e54efd62578ade80afc6

With that kind of plot, would be worth to migrate for saving money? I am doubting a lot because despite there are a lot of moments when the consumption is very low, there are others which is 100%. The storage is about 75% and increases very slowly, so that would be an easy calculation to do.

Currently we are not experiencing any kind of issues with Redshift, so it would be just for cost optimization. 

Thank you very much!"
1327,2019-12-10 20:57:37,1576004257.0,dataengineering,Any suggestions on ETL/Data Quality frameworks for managing 100+ pipelines?,e8uwtu,barrontrump2052,,https://www.reddit.com/r/dataengineering/comments/e8uwtu/any_suggestions_on_etldata_quality_frameworks_for/,1.0,13.0,0.0,8506.0,"Hi all,

I work at a python/scala big data company where we use Airflow, AWS, EMR, Redshift, Snowflake, Graph databases like Neptune, transfer terabytes of data back and forth between clusters and S3, that sort of stuff. 

We're looking for a tool to observe data pipeline health checks for 100+ pipelines in the company, spanning multiple tools like the ones listed above. I want to see if stuff is on fire, missing files, changed schemas from our vendors (surprise!), so whenever stuff breaks our developers would be notified about it asap. 

Anyone using a technology that might fit this bill, that you would recommend? 

I was looking at streamsets, apache nifi, or writing some custom code to hook into our airflow jobs. I'm not sure what is the path of least resistance here... of course I would prefer to have open source software, but money isn't an issue. We would also like to be flexible with the tool, so not entirely graphical to the point of being restricted by the software."
1328,2019-12-11 01:03:23,1576019003.0,dataengineering,Where should I start looking for a remote data engineering job?,e8yca8,spectre_S,,https://www.reddit.com/r/dataengineering/comments/e8yca8/where_should_i_start_looking_for_a_remote_data/,1.0,2.0,0.0,8512.0,"I am based in Canada and does anyone had a good success here in getting a remote job which is based out of USA/CANADA?

I had few interviews with US companies but I didn't had enough luck."
1329,2019-12-11 17:48:28,1576079308.0,dataengineering,Do you ever feel like quitting because fuck this is hard?,e98r2q,nfae-v0id,,https://www.reddit.com/r/dataengineering/comments/e98r2q/do_you_ever_feel_like_quitting_because_fuck_this/,1.0,55.0,0.0,8526.0,"I haven’t been doing this long. Still super junior but Jesus this is testing me some days. 

I’ve never been the type of person to want to pack up and just leave but some days I feel like the dumbest person in the room and I want to just go. 

I had a very distinct day last week where I finally felt everything rush over me and I felt buried in stress. That was the first day that I just wanted to cry and hand my badge into my manager. 

I’m not a bad employee. I’m not stupid. I know all of these things. I just wonder if I’m smart enough for this shit. This is fucking hard as hell and I’m struggling. 

I could use some words of wisdom or encouragement please."
1330,2019-12-11 18:51:56,1576083116.0,dataengineering,Building Interactive Dashboards with Dash Bootstrap Components,e99kvz,Soolsily,,https://www.reddit.com/r/dataengineering/comments/e99kvz/building_interactive_dashboards_with_dash/,1.0,0.0,0.0,8527.0,
1331,2019-12-11 21:46:24,1576093584.0,dataengineering,How to master Java transformations in AWS Glue,e9c12t,mite-mitreski,,https://www.reddit.com/r/dataengineering/comments/e9c12t/how_to_master_java_transformations_in_aws_glue/,1.0,0.0,0.0,8529.0,
1332,2019-12-11 22:19:54,1576095594.0,dataengineering,Common Issues faced in Spark,e9cimk,mjfnd,,https://www.reddit.com/r/dataengineering/comments/e9cimk/common_issues_faced_in_spark/,1.0,0.0,0.0,8529.0,
1333,2019-12-12 00:27:54,1576103274.0,dataengineering,I have $500 to Spend on DE Stuff Before Next Year. What do I Buy?,e9ed24,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/e9ed24/i_have_500_to_spend_on_de_stuff_before_next_year/,1.0,4.0,0.0,8535.0,"Title

My job gives $500 each year to spend on personal development. So I can spend it on anything work related. What do I buy?"
1334,2019-12-12 04:50:35,1576119035.0,dataengineering,Reference Data Management and Mapping - Open Source?,e9ho14,MrLewArcher,,https://www.reddit.com/r/dataengineering/comments/e9ho14/reference_data_management_and_mapping_open_source/,1.0,0.0,0.0,8538.0,
1335,2019-12-12 12:13:46,1576145626.0,dataengineering,No CS degree - disadvantaged for applying?,e9ltp6,BudgetShaman,,https://www.reddit.com/r/dataengineering/comments/e9ltp6/no_cs_degree_disadvantaged_for_applying/,1.0,24.0,0.0,8541.0,"I have a degree in business and am thinking of doing a masters in business analytics (data engineering concentration), but many of the data engineer positions on LinkedIn call for a computer science or related degree. Would the MSBA count?

Been coding in SQL for 5 years for various business groups and thinking of going more technical."
1336,2019-12-12 15:24:29,1576157069.0,dataengineering,Preprocessing infected cell images,e9niaw,Temas3D,,https://www.reddit.com/r/dataengineering/comments/e9niaw/preprocessing_infected_cell_images/,1.0,0.0,0.0,8543.0,"Hi, I'm having a lot of problems to preprocess cell images in order to make them different from being infected by virus or not. The thing is both class images are so similar, so I've thought on edit those images to clarify differences, but every tutorial (I don't know how to use a single library for images preprocessing, such as opencv) always difference on foreground and background or detect specific objects or colors. Here, my image is plain and the object is just a dot with no specific form or color (nor the rest of the image).

&amp;#x200B;

Do any of you know about a tutorial, a post or something similar that would be useful?"
1337,2019-12-12 19:39:14,1576172354.0,dataengineering,what are the most important data engineering tools and technologies which you use at work?,e9qs1m,spectre_S,,https://www.reddit.com/r/dataengineering/comments/e9qs1m/what_are_the_most_important_data_engineering/,1.0,23.0,0.0,8551.0,"example:  I use kafka, Nifi, Hive, Sqoop a lot and I am looking to learn few more technologies and wondering where to start?"
1338,2019-12-12 23:45:53,1576187153.0,dataengineering,CDC with Big Data Tools,e9u676,SirGustave,,https://www.reddit.com/r/dataengineering/comments/e9u676/cdc_with_big_data_tools/,1.0,0.0,0.0,8555.0,"Has anyone implemented CDC with (Nifi + Kafka)?
By the way, what alternatives are there when we want to ingest files ( with CDC) or data from relational databases?

Thanks"
1339,2019-12-13 04:50:33,1576205433.0,dataengineering,Data Engineers in a Data Science team,e9xyub,darthvader003,,https://www.reddit.com/r/dataengineering/comments/e9xyub/data_engineers_in_a_data_science_team/,1.0,4.0,0.0,8560.0,"Any Data engineers in a Data Science team?? How does your project look like?

What kind of tools and technologies do you use? 

How do you deploy your model?

How does your team version control?"
1340,2019-12-13 07:47:52,1576216072.0,dataengineering,Can we really do it all with Python?,e9zy6c,cleverchimp,,https://www.reddit.com/r/dataengineering/comments/e9zy6c/can_we_really_do_it_all_with_python/,1.0,7.0,0.0,8565.0,"I'm a data scientist at a small company without a proper engineer. I've learned a whole new respect for y'all as I've inherited a lot of duties I never expected to deal with. I am trying to build the infrastructure we need, and I'm truly overwhelmed by all the tools, libraries and ecosystems out there. Help!!!

End-game everything needs to go to our Snowflake data warehouse. That seems easy enough with a little Python:   
[https://docs.snowflake.net/manuals/user-guide/python-connector.html](https://docs.snowflake.net/manuals/user-guide/python-connector.html)  


We need to ingest from a Postgres database, some JSON, and some CSVs. Our biggest table is about 70 million rows. Big, but nothing too crazy yet. I already wrote some R scripts that were able to do all this, but it took about 20 minutes to write \~500k records with \~50 fields to the warehouse. That was off my aging MacBook Air. It felt ""too slow"" for production so I started Googling away which only confused my more. 

We use tools like Segment and Funnel to ingest data from 3rd-party sources, but they lack connectors for everything, so some custom API scripts are inevitable. Again, my thoughts turn to Python. 

And I need something to run all these scripts for me. Sounds like Supervisor is out, and Airflow is in. Ok. I can adapt. 

So it seems to me this can all be handled with Python, from whatever crazy data source they throw at me, all the way up to Snowflake. But then I stop and ask: am I being too simple minded here? Are there tools that I should know about before I go reinventing wheels again?   


How do y'all handle it? If I'm being stupid, please for the love of God, tell me so."
1341,2019-12-13 08:52:24,1576219944.0,dataengineering,Processing large .gz files efficiently,ea0ka1,lolxorlol,,https://www.reddit.com/r/dataengineering/comments/ea0ka1/processing_large_gz_files_efficiently/,1.0,24.0,0.0,8567.0,"Hi all

As a small side project at work I've found a fun challenge that I thought I might share with you for input. We receive a usb-drive monthly with 2x2tb .gz files with financial data. Each of the two files holds a text file of around 25 tb when unpacked with each row consisting of some delimited base information (date, time, instrument, exchange etc.) and then some price data in a variable number of columns.

The goal for this data is twofold:

* Generate some easily queryable aggregate data (say one row per instrument/minute) - this dataset tends to be around 1/5000 size of the original.
* Store the full dataset in a more optimal way enabling new aggregates, recalculation with new logic, analysis on parts of the raw data etc.

I played around with some sample data a bit and on my local machine and I can load the data into my local Sql Server at around 250gb per hour (after it has been unzipped) but with only transformations being splitting into columns and converting data types from string to date, time, char etc. I'm sure if I did this on a server I could up that speed a few times since I'm limited by ram/cpu and not by disk speeds - however when doing the full size data set the initial processing would need to be on slower disks to unzip and split the files since I don't think we could easily get hold of 50tb NVMe disks. So it seems whatever approach I take on prem working with this sort of data will be unwieldy.

So what are good options that we might also be able to pay? Currently I'm thinking of doing something like:

* Upload compressed data into Azure Blob store (or data lake gen2) where the price for storage is a measly $1.5 per 100tb per month or something (+read/write costs).
* Use data factory (polybase enabled) to either uncompress, then move or to directly move the data into Azure Sql Datawarehouse ([here is an old example from 2016 where they load 1tb of data into Azure sql Datawarehouse in 15 minutes](https://docs.microsoft.com/en-gb/azure/data-factory/v1/data-factory-load-sql-data-warehouse)). (So at current prices around $30 per tb for preprocessing - possibly more before aggregates done etc.).
* Use the Datawarehouse to aggregate the data as needed - then export and either compress or put in an Azure Sql DB and take a backup - push this file back on premise for my analytical data stores. This export via either ADF or Databricks?
* Export raw data to data lake gen(2) but in one file per instrument per day. That way we have it accessible and can access parts of it without having to load the whole dataset. These smaller files should probably again be compressed - maybe parquet files?

As you might see I'm new to working with fairly big data and luckily this is another teams headache - but I'd like to practice a bit and I enjoy the challenge. So what are you pro's take on this? How would you approach it?

**TL:DR:** 25tb individual txt files into aggregates and raw data stored more optimally. How?"
1342,2019-12-13 09:41:04,1576222864.0,dataengineering,Side project,ea0zxh,yongen96,,https://www.reddit.com/r/dataengineering/comments/ea0zxh/side_project/,1.0,5.0,0.0,8568.0,Any advises/ tips to start a data engineered related side project that can build up my profile?
1343,2019-12-13 09:45:05,1576223105.0,dataengineering,SQL — From Intermediate to Superhero,ea114b,eyaltrabelsi,,https://www.reddit.com/r/dataengineering/comments/ea114b/sql_from_intermediate_to_superhero/,1.0,0.0,0.0,8568.0,
1344,2019-12-13 18:30:50,1576254650.0,dataengineering,AzureDevOps,ea6760,Omar_88,,https://www.reddit.com/r/dataengineering/comments/ea6760/azuredevops/,1.0,0.0,0.0,8576.0,"Hey guys,

Month 2 in my data engineering role, trying to get my head around git and devops, has anyone had experience in deploying databricks into uat / prod from Dev? What was your experience if so?"
1345,2019-12-13 18:57:09,1576256229.0,dataengineering,Great Talk about the State of Data Pipeline Testing Today.,ea6jvz,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/ea6jvz/great_talk_about_the_state_of_data_pipeline/,1.0,0.0,0.0,8576.0,
1346,2019-12-14 00:13:16,1576275196.0,dataengineering,Data versioning in your data science projects,eaassl,darthvader003,,https://www.reddit.com/r/dataengineering/comments/eaassl/data_versioning_in_your_data_science_projects/,1.0,22.0,0.0,8581.0,"Hey people! How do you version your data used for your Data Science projects? Let's say you had Model M0.1 released June 2019 using customers.csv and sales.csv ; You had a 3-month cadence for your release.  
Similarly there was another release made in 09-2019., only this time both csvs have been updated.

Now you are trying to rerun your predictions for the old model but data has been updated. How do you version your data for this? Do you keep copies of data each time a release was made? Or use some tools to better organize this? Any thought? Suggestions? Complaints."
1347,2019-12-14 23:21:43,1576358503.0,dataengineering,Looking for guidance on pulling data from an API,eapjoe,rolkien29,,https://www.reddit.com/r/dataengineering/comments/eapjoe/looking_for_guidance_on_pulling_data_from_an_api/,1.0,3.0,0.0,8599.0,"I am connecting to an API that holds a national listing of healthcare providers. The documentation is found here: [https://npiregistry.cms.hhs.gov/registry/help-api](https://npiregistry.cms.hhs.gov/registry/help-api) 

I am able to connect and pull in ***some*** data so that's a start, but say I wanted to pull in everything for the state of Wyoming, how could I do so? The API documentation says: 

"" An API query will return a maximum of 200 results per request. The Skip field in the API will let you skip up to 1000 records. By using these two fields with your search criteria, you can get up to a maximum of 1,200 records over six requests. ""

I'm using Python and my current API query looks something like this:

import requests

response = requests.get('[https://npiregistry.cms.hhs.gov/api/city=&amp;state=WY&amp;postal\_code=&amp;country\_code=US&amp;limit=50&amp;skip=&amp;pretty=true&amp;version=2.1](https://npiregistry.cms.hhs.gov/api/?city=&amp;state=&amp;postal_code=30301&amp;country_code=US&amp;limit=50&amp;skip=&amp;pretty=true&amp;version=2.1)')

&amp;#x200B;

But it requires a zip code to run. To get everything in Wyoming should I just make a list of all Wyoming zip codes and loop through it? How do I get around the limits they place on requests?"
1348,2019-12-15 00:51:01,1576363861.0,dataengineering,How to realistically prepare for a data engineer interview?,eaqnit,stevofolife,,https://www.reddit.com/r/dataengineering/comments/eaqnit/how_to_realistically_prepare_for_a_data_engineer/,1.0,7.0,0.0,8600.0,"What do you guys do to prepare for an interview? Practice writing SQL? Spark? Do System Design? Coding challenges? Put together a pipeline? Read documentations? What can you do?

Any resourceses that you recommend?"
1349,2019-12-15 18:54:22,1576428862.0,dataengineering,An Insight Into How YouTube Stores &amp; Manages Petabytes Of Data Every Single Day,eb196f,techPackets_005,,https://www.reddit.com/r/dataengineering/comments/eb196f/an_insight_into_how_youtube_stores_manages/,1.0,2.0,0.0,8614.0,
1350,2019-12-16 06:44:37,1576471477.0,dataengineering,Anyone use their skills here to create a side hussle ?,ebac02,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/ebac02/anyone_use_their_skills_here_to_create_a_side/,1.0,10.0,0.0,8622.0,"I’m a jr data engineer and would love to get a side hussle going with the skills I use at work ( extract data from apis , store in data lake and create denorm tables in data warehouse )"
1351,2019-12-16 19:17:47,1576516667.0,dataengineering,Security Documentation for Data Products,ebi07h,ethanenglish,,https://www.reddit.com/r/dataengineering/comments/ebi07h/security_documentation_for_data_products/,1.0,2.0,0.0,8637.0,"I work as a data engineer -- creating ETL pipelines for marketing data (e.g. Google Ads, Bing Ads, etc). Are there best practices for documenting the security of our systems that would make it easier for our clients to understand them? For instance, a client says, ""what happens in the case of a security breach?"" There are many very pointed questions that I can answer one-off but I'm curious if there's an industry standard for documenting security.

We use Google Cloud Platform for all of our ETL and follow their best practices. Are there external-facing documents that we would put together for clients? If so, what are they called? Any templates I should follow?"
1352,2019-12-16 19:46:26,1576518386.0,dataengineering,Data Engineer job with AWS Big Data Certification,ebieqm,linuxlicenciado,,https://www.reddit.com/r/dataengineering/comments/ebieqm/data_engineer_job_with_aws_big_data_certification/,1.0,8.0,0.0,8639.0,"Hi,

Can be possible to land a Data Engineer job by passing AWS  Solutions Architect and Big Data Certification plus basic Python and SQL?

&amp;#x200B;

Note: Have working experience as systems admin/eng."
1353,2019-12-16 20:37:24,1576521444.0,dataengineering,"An interview about how the Marquez platform for metadata management powers data lineage tracking, data discovery, and health reporting at WeWork",ebj42a,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ebj42a/an_interview_about_how_the_marquez_platform_for/,1.0,0.0,0.0,8640.0,
1354,2019-12-17 12:11:31,1576577491.0,dataengineering,Data team in a medium sized organization,ebu026,RickyBlurp,,https://www.reddit.com/r/dataengineering/comments/ebu026/data_team_in_a_medium_sized_organization/,1.0,11.0,0.0,8645.0,"Hello guys, I'm new on this sub, which look awesome! I'm coming with a problem :)

We have a brand new data team in my company and we don't know where to put the responsibility cursor (and what would be the best in term of scalability).

Let's say we have a marketplace application with 5 IT teams: ordering, product management, invoicing, logistic &amp; data. The data team is mainly needed to analyze user workflow on the product, working with segment &amp; amplitude - for questions like: ""did the user used this new feature?"", but also analyzing broader metrics like number of orders, total amount of products etc..., working with bigquery &amp; looker - for questions like: ""what is our GMV this month?"". 

Today, we don't know where to draw the line:
* Should the data team be a stakeholder of the other teams? Meaning should they express a need for a specific data and it should pass through grooming in the correct team (invoicing for credit note, ...). Every communication being explicitly put into contracts (API versioning, JSON schema for event listening) with a deprecation warning when something changes.
* Should we have a ""data guy"" in each team having the knowledge of what the data needs, how it works. Basically knowing when we should warn the data team of a change and specify when each team should add a tracking on a new feature.

I would gladly hear how it works in your organizations if you happen to have similar issues :)"
1355,2019-12-17 19:56:02,1576605362.0,dataengineering,Best Practice,ebz7tu,tpedar50,,https://www.reddit.com/r/dataengineering/comments/ebz7tu/best_practice/,1.0,7.0,0.0,8651.0,What is best practice for storing data source connection details in python scripts for data extraction?
1356,2019-12-17 21:57:57,1576612677.0,dataengineering,Can we talk about Dremio,ec0zx7,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/ec0zx7/can_we_talk_about_dremio/,1.0,3.0,0.0,8652.0,"I have been fighting with backing up and restoring a Dremio server for nearly a month now. Went to the community page and asked a question; got auto-moderated for some reason.  I have just found this application to be now where near worth the hassle from a data engineering perspective. It feels like the documentation is sorely lacking and their ""support"" is really just one person kind of answering questions.   


I am curious if anyone has experienced the same. Has it worked for you? If not, what did you move to?   


Feel free to down vote me for just venting but where else are we supposed to discuss vendor selection if not here?"
1357,2019-12-18 00:28:18,1576621698.0,dataengineering,Save attachments from emails,ec35gn,spiki13,,https://www.reddit.com/r/dataengineering/comments/ec35gn/save_attachments_from_emails/,1.0,1.0,0.0,8656.0,"Hi guys, I know that maybe this topic shouldn't be here, but I thought that maybe someone here has face the same problem: to save automatically attachment from emails. There are different solutions like: macros, python or... Any suggestion? 

Thanks!"
1358,2019-12-18 00:45:16,1576622716.0,dataengineering,Honest reviews of Snowflake?,ec3dyk,exact-approximate,,https://www.reddit.com/r/dataengineering/comments/ec3dyk/honest_reviews_of_snowflake/,1.0,33.0,0.0,8656.0,"It seems like there is a great furor about snowflake, but I see very little potential benefit to it apart from price and it being fully managed (which is barely a plus) and there are some serious downsides regarding optimization options.

Can someone give me an honest review of snowflake, including the negative aspects?"
1359,2019-12-18 01:09:25,1576624165.0,dataengineering,An interviewer asked me how do I de-duplicate records in Hive.,ec3q1v,spectre_S,,https://www.reddit.com/r/dataengineering/comments/ec3q1v/an_interviewer_asked_me_how_do_i_deduplicate/,1.0,10.0,0.0,8656.0,"I told them that hql is similar to sql and you can write distinct, unique keywords. They mocked me and then at the end I asked them how they do it in their company..they said they use snowflake instead of Hive and it comes with distinct and unique keywords.

It killed me."
1360,2019-12-18 07:35:56,1576647356.0,dataengineering,How to Analyse Facebook Ads using Google BigQuery Automation,ec8dgy,Countants123,,https://www.reddit.com/r/dataengineering/comments/ec8dgy/how_to_analyse_facebook_ads_using_google_bigquery/,1.0,0.0,0.0,8660.0,
1361,2019-12-18 09:26:04,1576653964.0,dataengineering,Anyone have any good reference or thoughts around adding public data to enhance your source data ?,ec9ecv,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/ec9ecv/anyone_have_any_good_reference_or_thoughts_around/,1.0,7.0,0.0,8659.0,Let’s say I have a source system that has data on homes. I then use the home address to get long / lat data to tie to large public data around demographics. That’s an example. Anyone else do this and have success and care to share how they used public Data to compliment source application data that lives in their warehouse to aid insights ?
1362,2019-12-18 10:44:16,1576658656.0,dataengineering,How do you tackle redundancy in data collection?,eca1lx,tsup4,,https://www.reddit.com/r/dataengineering/comments/eca1lx/how_do_you_tackle_redundancy_in_data_collection/,1.0,2.0,0.0,8661.0,"Many times in my jobs I did the same thing:

* get data from API using python
* parse the data and save it directly to some SQL/NoSQL DB
* query data and use it for analysis

For redundancy reason I want to run collection multiple times on different servers (sometimes data is time sensitive as well - not available at a later date). And as such I need to handle duplicates in data, since multiple servers can write to DB at the same time. 

1. I wonder what is the best solution/architecture for the case (minus just using unique indices or other mechanism of the DB)? 
2. Additionally, could you advise on the architecture if I would start a new small project (e.g. couple of data streams, do some transformation and later show it on a dashboard) that can be scalable as the project goes one without breaking a bank?"
1363,2019-12-18 16:57:22,1576681042.0,dataengineering,Huge join or little statement full of nulls plus a lot of updates for creating fact tables?,ecdo75,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/ecdo75/huge_join_or_little_statement_full_of_nulls_plus/,1.0,2.0,0.0,8662.0,"Hello. Imagine I want to create a fact table for orders that includes the dimensions/metrics product, domain, user\_id, product\_id,revenue and profit. Nowadays, my company has an ETL which is like the following:

&amp;#x200B;

^(INSERT INTO dwh.orders)"
1364,2019-12-18 19:47:54,1576691274.0,dataengineering,ElasticSearch + kibana as a BI stack?,ecfwpq,hyperflow_,,https://www.reddit.com/r/dataengineering/comments/ecfwpq/elasticsearch_kibana_as_a_bi_stack/,1.0,2.0,0.0,8663.0,"I work at a company where we're looking to switch from mongodb (which was very good for prototyping purposes) to a more adapted data store.

A few key points:
- we need a graphical exploration tool (metabase / redash / superset)
- the queries will generally run on data in the 10 million rows range - I guess you could extend it to 100 million for a bit of runway
- even though we used mongodb, most of the data was validated against a jsonschema so it's fairly well structured 

Someone suggested using ElasticSearch + kibana. 

Although I can intuitively say that it doesn't fit our use case since: 
- we have structured data (and using an indexing engine just doesn't feel right)
- SQL and tools in that ecosystem seem to just be the norm.

But it still looks like it could work. 

So I'm wondering: 
1. why would it be a bad idea?
2. why don't people ever compare ES + kibana to the likes of metabase and redash?"
1365,2019-12-18 21:39:21,1576697961.0,dataengineering,"Has anybody deployed a Streamlit ""production"" application for your project",echf59,darthvader003,,https://www.reddit.com/r/dataengineering/comments/echf59/has_anybody_deployed_a_streamlit_production/,1.0,4.0,0.0,8664.0,Was hoping to use Streamlit for our project and was wondering if anyone is using it for any of their prod use cases or potentially considering using it?
1366,2019-12-18 21:43:05,1576698185.0,dataengineering,Header-detail partition scheme in Hive,echgxh,tucanotucano,,https://www.reddit.com/r/dataengineering/comments/echgxh/headerdetail_partition_scheme_in_hive/,1.0,2.0,0.0,8665.0,"Hello,

I need some guidance on how to model the partitions of headers and details of invoices using Hive.

Header tables I plan to store partitioned by year, month and day (e.g., year=2019/month=12/day=2)

Detail table:

Around 200 million records per day) Have invoice number (unique) to relate to headers, but no dates included in the detail per se

I am thinking about 2 options for detail table:

*Partitioning also by year, month, day. This needs a join to be performed in the data ingestion process, before storing. In this scenario, joins between header and details could be penalized because the join is based on the invoice number
*Partitioning by a fixed amount of characters of the invoice number (unique). (e.g., for invoice number 123456789 a partition would be ""12345""). I think this approach could be better because invoice numbers are sequential and thus ""tightly"" mapped with the dates (partition scheme of header table)
*I also need to manage corner (dirty) cases, like invoice details with no valid invoice id: I'm thinking about a dedicated partition for these cases

Does somebody have some suggestions or experience with a related scenario?

Thanks a lot"
1367,2019-12-18 22:53:29,1576702409.0,dataengineering,What IDE for ETL with Python?,ecigmh,Luukv93,,https://www.reddit.com/r/dataengineering/comments/ecigmh/what_ide_for_etl_with_python/,1.0,43.0,0.0,8666.0,"Hi all,

What IDE do you use for your ETL with Python? I have heard you could use jupyter notebook for ETL or VSCode/Pycharm? What are the pro's and con's for your choice?"
1368,2019-12-19 14:44:40,1576759480.0,dataengineering,Query AWS Redshift like a Boss!,ecspi6,mite-mitreski,,https://www.reddit.com/r/dataengineering/comments/ecspi6/query_aws_redshift_like_a_boss/,1.0,0.0,0.0,8682.0,
1369,2019-12-19 19:36:22,1576776982.0,dataengineering,ET(L) with Python,ecwham,Maha_Slug,,https://www.reddit.com/r/dataengineering/comments/ecwham/etl_with_python/,1.0,24.0,0.0,8687.0,"I was wondering if anyone has had much luck doing the L piece of the ETL triangle with Python. I handle millions or hundreds of millions of rows at a time and Python in my experience loads VERY slow. I specifically use MSSQL as my database. I've read a fair bit on the subject and I've tried several ORMS and drivers within Python. I've chunked out the data and other things and achieved some success but for me to really justify using Python over C# I'd need much better results. So I guess my real question, has anyone gotten decent load speeds for 100 million rows using Python?"
1370,2019-12-20 00:15:56,1576793756.0,dataengineering,How much is your current salary as a data engineer and where are you located?,ed0fh4,pauleenah,,https://www.reddit.com/r/dataengineering/comments/ed0fh4/how_much_is_your_current_salary_as_a_data/,1.0,117.0,0.0,8692.0,
1371,2019-12-20 00:54:10,1576796050.0,dataengineering,Building a Twitter Sentiment-Analysis App Using Streamlit,ed0y8y,tcr98,,https://www.reddit.com/r/dataengineering/comments/ed0y8y/building_a_twitter_sentimentanalysis_app_using/,1.0,0.0,0.0,8692.0,
1372,2019-12-20 03:14:26,1576804466.0,dataengineering,Pyspark rdd to SQLite,ed2r33,histogram_of_life,,https://www.reddit.com/r/dataengineering/comments/ed2r33/pyspark_rdd_to_sqlite/,1.0,3.0,0.0,8691.0,"Hello all, How do I insert  rdd into SQLite database without collecting it as rows and inserting row by row ? Thank you"
1373,2019-12-20 13:33:28,1576841608.0,dataengineering,Google Cloud Platform on Coursera worth it?,ed90fq,warosaurus,,https://www.reddit.com/r/dataengineering/comments/ed90fq/google_cloud_platform_on_coursera_worth_it/,1.0,6.0,0.0,8695.0,"I was wondering if anyone had taken the GPC specialization (with certificate) on Coursera and was willing to share their opinion? It's $44 a month just to follow and you'll have to buy credits if you don't have the free trial anymore.

How long did it take you to complete and did you think it was worth the investment (time and money)?"
1374,2019-12-20 14:12:37,1576843957.0,dataengineering,ETL: SSIS/Stored Procedures vs Python,ed9ecf,Luukv93,,https://www.reddit.com/r/dataengineering/comments/ed9ecf/etl_ssisstored_procedures_vs_python/,1.0,23.0,0.0,8695.0,"Hi all,

What are the advantages of using Python for ETL compared to using a combination of SSIS/Stored Procedures? I'm asking this because I am learning Python for ETL and data analysis and I am interested in your arguments."
1375,2019-12-20 16:16:36,1576851396.0,dataengineering,Kubernetes for data-related microservices,edaqzt,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/edaqzt/kubernetes_for_datarelated_microservices/,1.0,5.0,0.0,8697.0,"I wanted to know how/if people use Kubernetes in their organizations, particularly as it pertains to data related applications (models builders/predictors, ETL, microservices). A little background, my user base is entirely internal and no more than 100 users. I am trying to determine if Kubernetes is even worth the effort and energy for a one person data engineering team with a small customer base. I currently use a combination of ECS and manually deployed on prem docker containers stung together with traefik (its kind of janky but it does work).

My big question is, when does the conversion to a Kubernetes platform make sense?"
1376,2019-12-21 00:36:16,1576881376.0,dataengineering,Apache Airflow Summit 2020 in London &amp; North America | Attendees Survey,edh844,kaxil_naik,,https://www.reddit.com/r/dataengineering/comments/edh844/apache_airflow_summit_2020_in_london_north/,1.0,25.0,0.0,8710.0,"We (Apache Airflow PMC &amp; Committers) are planning to organize 2 **Apache Airflow** summits in 2020, one in North America, one in Europe.

These summits will be community events, and an opportunity to bring together users and contributors of Apache Airflow, and collaborate on the development of the project.

We would like to tailor these summits based on what the community wants and expects from it.

So we created a survey as a means to collect this data. If you have 5 minutes, please fill this survey: [https://forms.gle/qDi52z9TY9pT9Lsm6](https://forms.gle/qDi52z9TY9pT9Lsm6)

This is your chance to voice your opinion :) 

This will help us make some key decisions on how we organize it.

#"
1377,2019-12-21 03:40:30,1576892430.0,dataengineering,Career suggestions needed,edjf67,Rkeepwalking,,https://www.reddit.com/r/dataengineering/comments/edjf67/career_suggestions_needed/,1.0,1.0,0.0,8713.0,"I am currently working as a Data modeler overseeing data modeling, Metadata management and database design. What positions should I pursue in the data engineering space? What role can I fit into?
Please suggest."
1378,2019-12-22 06:37:27,1576989447.0,dataengineering,Udacity Data Streaming vs Data Engineer Nanodegrees?,ee0jep,Viva_Uteri,,https://www.reddit.com/r/dataengineering/comments/ee0jep/udacity_data_streaming_vs_data_engineer/,1.0,1.0,0.0,8736.0,"I’m currently a data analyst (mostly excel/tableau/SQL/some R) after doing a boot camp and am interested in data engineering. I am considering the Udacity Nanodegrees but am not sure which one would be best to move into data engineering. 

Thoughts?"
1379,2019-12-22 13:08:56,1577012936.0,dataengineering,Building Data Pre-processing + ML Pipeline for senior year project and needed some advice.,ee3sni,roonishpower,,https://www.reddit.com/r/dataengineering/comments/ee3sni/building_data_preprocessing_ml_pipeline_for/,1.0,0.0,0.0,8737.0,
1380,2019-12-22 15:16:55,1577020615.0,dataengineering,How's 'Data Engineering with GCP' Coursera certification?,ee4s5q,damein11,,https://www.reddit.com/r/dataengineering/comments/ee4s5q/hows_data_engineering_with_gcp_coursera/,1.0,7.0,0.0,8738.0,"I snubbed Udacity nano degree certification due this detailed review thread.
https://amp.reddit.com/r/dataengineering/comments/c11ut4/warning_stay_far_far_away_from_udacitys_data/

Next alternative for a beginners is 
https://www.coursera.org/specializations/gcp-data-engineering

Before starting on it, want to get answers to following questions : 
Does this certification give a beginner good Data engineering foundation skills and introduce to d2d work challenges ?
Is it valued in industry ?"
1381,2019-12-22 20:42:10,1577040130.0,dataengineering,Data Engineering Blog Ideation,ee8h54,ajknzhol,,https://www.reddit.com/r/dataengineering/comments/ee8h54/data_engineering_blog_ideation/,1.0,19.0,0.0,8740.0,"Hi all,

I am Ajay. I have extensive experience in data engineering and distributed systems engineering in cloud. I am planning to start a blog on the same with in-depth articles with theory and practical implementations.

Following are my initial topic list which I have to write about. 

1. Fault Tolerance Explained Mega Series
    - Introduction to Fault Tolerance 
    - Fault Tolerance in Distributed Systems     
    - Buiding Fault Tolerant Architectures in AWS 
2. Apache Spark Explained Mega Series     
    - Overview of Apache Spark     
    - RDD Explained     
    - Spark SQL Explained      
    - A Unified Engine  


These two topic sets would comprise nearly 15 to 20 articles explaining each aspect in as much depth as possible without getting too much technical.

My question is simple. Is there a demand for such blogs and such in-depth articles considering it would take considerable amount of time and effort to write each of them?

Will you be interested in such a series of data engineering topics?

Please let me know if you guys really like a good in-depth blog about data engineering.

Thanks and Regards
Ajay"
1382,2019-12-23 08:31:49,1577082709.0,dataengineering,Analyse Facebook Ads using Google BigQuery Automation,eegtum,naina_jain1,,https://www.reddit.com/r/dataengineering/comments/eegtum/analyse_facebook_ads_using_google_bigquery/,1.0,0.0,0.0,8748.0,
1383,2019-12-23 09:38:16,1577086696.0,dataengineering,does anyone have tips on becoming a data engineer?,eehfrf,Vu004,,https://www.reddit.com/r/dataengineering/comments/eehfrf/does_anyone_have_tips_on_becoming_a_data_engineer/,1.0,2.0,0.0,8748.0,
1384,2019-12-23 16:39:21,1577111961.0,dataengineering,DevOpsRemote.work: Find DevOps Remote Jobs,eeldqe,ai_jobs,,https://www.reddit.com/r/dataengineering/comments/eeldqe/devopsremotework_find_devops_remote_jobs/,1.0,0.0,0.0,8752.0,
1385,2019-12-23 17:35:57,1577115357.0,dataengineering,Experience with ETL tools?,eem2ia,canyouspareadime,,https://www.reddit.com/r/dataengineering/comments/eem2ia/experience_with_etl_tools/,1.0,23.0,0.0,8753.0,"I am doing research on ETL tools and want to understand better when each tool is best. For instance, in what situations is Prefect better than Airflow or Ascend.io better than Argo? I am also interested in Dagster. Do any of you use these tools in your companies and would be willing to speak or chat with me? Thanks for considering."
1386,2019-12-23 21:16:21,1577128581.0,dataengineering,Lambda for ETLs,eep2u9,pdiddy_flaps,,https://www.reddit.com/r/dataengineering/comments/eep2u9/lambda_for_etls/,1.0,6.0,0.0,8757.0,"Hi ladies and gents, happy holidays and all that stuff!

Does anyone have any experience of having used lambdas for their ETLs? We have some python jobs which tigger from a cron on EC2 and it feels like we have outgrown the set up and are running into issues, mainly with insufficient logging, deployment risks, downstream dependency visibility.

Although not having used it in production before my preference is to use airflow with perhaps Data-dog, but as most of the product stack is on AWS our engineering team want to move the data engineering stuff closer to them and have strongly suggested using Lambda.

An example use case would be google big query web events, 2/3m records per day in a single extraction. Also every 15 min extractions from a mongodb instance. 
Not everything is sourced from within S3, however our DW itself is a Postgres instance in Amazon RDS.

If anyone’s tried this and hit any snags, or found it fits the bill please let me know I’d appreciate it! 

Thanks"
1387,2019-12-24 02:00:51,1577145651.0,dataengineering,An episode about building Materialize for interactive analytics on continuously updated streams of data,eesx2y,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/eesx2y/an_episode_about_building_materialize_for/,1.0,0.0,0.0,8759.0,
1388,2019-12-24 18:25:32,1577204732.0,dataengineering,How important are Competitive Programming skills when you are applying for position like that of Data Engineer?,ef3h6f,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/ef3h6f/how_important_are_competitive_programming_skills/,1.0,2.0,0.0,8765.0," Assuming I'm trying to apply for position similar to DE at companies like Google, Facebook, Amazon, How important is the competitive programming skillset?

I'm well aware of different Algorithmic stuff like Time/Space complexity and all different algorithms, but I'm not really into cracking competitive questions. I know this is very important for those Big 3, but given the role I'm interested in, will this be a obstacle for me?

I prefer reading research papers like those of Dremel, implementing different architectures with some DS stuff etc. Should I go completely deep into this or I'll have to do competitive programming to have any chance at cracking these companies?

Really confused here, what to focus on really."
1389,2019-12-25 10:12:28,1577261548.0,dataengineering,Data Science Pulse,efdz3x,ajknzhol,,https://www.reddit.com/r/dataengineering/comments/efdz3x/data_science_pulse/,1.0,9.0,0.0,8770.0,"I am planning to launch this website to make a difference in Data Science field which is crowded by ""how-to"" blogs. My main problem with the ""how-to"" blogs is that they just assume that everyone knows what Data Science is. And people just want to implement Data Science in their business. I come to understand that this is not the case always.

My objective with this website is to clear up, understand and uncover the underlying patterns within Data Science field so that people can come up with objective reasons on WHY they need Data Science.

I have created a site and a landing page with the philosophy behind the idea. Please see below for the link.

 [https://datasciencepulse.com/](https://datasciencepulse.com/) 

I am looking for a healthy discussion regarding this philosophy within the field.

I know this is Data Engineering space but nonetheless I am of opinion that little bit Data Science doesn't hurt as well :)"
1390,2019-12-26 22:22:18,1577391738.0,dataengineering,Swagger Documentation tool for Data Dictionaries?,eg0f3s,tristanjones,,https://www.reddit.com/r/dataengineering/comments/eg0f3s/swagger_documentation_tool_for_data_dictionaries/,1.0,0.0,0.0,8804.0,
1391,2019-12-27 06:52:48,1577422368.0,dataengineering,Basics of big data,eg6kja,itbloggy,,https://www.reddit.com/r/dataengineering/comments/eg6kja/basics_of_big_data/,1.0,0.0,0.0,8815.0,
1392,2019-12-27 13:56:21,1577447781.0,dataengineering,Data Engineer job with Data Warehousing for Business Intelligence,egabun,Zayacvolk,,https://www.reddit.com/r/dataengineering/comments/egabun/data_engineer_job_with_data_warehousing_for/,1.0,0.0,0.0,8820.0,
1393,2019-12-27 22:54:12,1577480052.0,dataengineering,What to focus in Python for Data Engineer Interviews at FAANG,eggqri,iwillgetintofaang,,https://www.reddit.com/r/dataengineering/comments/eggqri/what_to_focus_in_python_for_data_engineer/,1.0,14.0,0.0,8829.0,"Hello All, i am currently working as a Data Engineer(previously ETL Developer)  working in a small company and i am prepping to get a break in  big companies like FAANG. I have 6 YOE in Data Warehousing. My skill set is (sql, ETL Informatica, Basic Python+Pandas,GIT, GCP and Airflow). I understand that most interviews are based on sql+python+data modeling. For Python, I am practicing on Leet Code(easy and medium), I mostly focus on string manipulation. What other areas in python should i focus to get through the interview ? I don't have experience working on huge volume of data, is that going to be an obstacle ?"
1394,2019-12-28 11:24:34,1577525074.0,dataengineering,Looking for sample projects/problems for a data engineer role,egotjy,Rey661199,,https://www.reddit.com/r/dataengineering/comments/egotjy/looking_for_sample_projectsproblems_for_a_data/,1.0,16.0,0.0,8838.0,"Looking for sample projects/problems for a data engineer role (Python + Postgres). 

For example the challenge could be about extraction, transformation and load of data and running sql queries on top to answer business questions. 

: )"
1395,2019-12-29 16:29:03,1577629743.0,dataengineering,Data Science Roadmap: Become Fluent in 8 Stages,eh6hy0,ajknpikachu,,https://www.reddit.com/r/dataengineering/comments/eh6hy0/data_science_roadmap_become_fluent_in_8_stages/,1.0,0.0,0.0,8860.0,
1396,2019-12-29 23:35:59,1577655359.0,dataengineering,Data Engineer job with Data Warehousing for Business Intelligence,ehbx9m,Zayacvolk,,https://www.reddit.com/r/dataengineering/comments/ehbx9m/data_engineer_job_with_data_warehousing_for/,1.0,0.0,0.0,8865.0,
1397,2019-12-30 22:09:22,1577736562.0,dataengineering,What is the best infrastructure with this type of hardware?,ehrbml,v2thegreat,,https://www.reddit.com/r/dataengineering/comments/ehrbml/what_is_the_best_infrastructure_with_this_type_of/,1.0,0.0,0.0,8884.0,
1398,2019-12-30 22:45:57,1577738757.0,dataengineering,Data pipeline discussion thread (Tools used at your workplace).,ehrukl,lnx2n,,https://www.reddit.com/r/dataengineering/comments/ehrukl/data_pipeline_discussion_thread_tools_used_at/,1.0,23.0,0.0,8884.0,"What all tools do you use to design data pipelines? (Hadoop/non hadoop solutions).

1. I work on building enterprise data lakes and we use kafka,sqoop for ingestions, Nifi for transformations, Hive for storage and CA workload automation client for scheduling.
2. What is your tool kit in the order of Ingestion, transformation/processing, storage, visualization?

I hope this helps other newbies."
1399,2019-12-31 00:29:22,1577744962.0,dataengineering,An interview with a DataDog engineer about how they build reliable and highly available systems for processing timeseries data in real time and at massive scale,ehtavt,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ehtavt/an_interview_with_a_datadog_engineer_about_how/,1.0,0.0,0.0,8885.0,
1400,2019-12-31 16:37:40,1577803060.0,dataengineering,Data Science Roadmap: Become Fluent in 8 Stages,ei3m4g,ajknpikachu,,https://www.reddit.com/r/dataengineering/comments/ei3m4g/data_science_roadmap_become_fluent_in_8_stages/,1.0,0.0,0.0,8899.0,
1401,2020-01-01 01:27:50,1577834870.0,dataengineering,Data scientists who switched to data engineering: What compelled you to make the switch? Do you like your current work better?,eialn6,gerradisgod,,https://www.reddit.com/r/dataengineering/comments/eialn6/data_scientists_who_switched_to_data_engineering/,1.0,0.0,0.0,8905.0,
1402,2020-01-01 20:18:22,1577902702.0,dataengineering,Is Data Engineering a very heavy configuration role?,eiljbl,throwaway40420103,,https://www.reddit.com/r/dataengineering/comments/eiljbl/is_data_engineering_a_very_heavy_configuration/,1.0,9.0,0.0,8916.0,I'm not sure how to best word it but does DE involve more time spent setting things up as opposed to writing logic?
1403,2020-01-02 04:35:40,1577932540.0,dataengineering,why do you think most of the companies are moving away from Hadoop?,eiryad,lnx2n,,https://www.reddit.com/r/dataengineering/comments/eiryad/why_do_you_think_most_of_the_companies_are_moving/,1.0,19.0,0.0,8920.0,"I read articles which say Hadoop is dying and all but I am not sure why few companies are moving towards non hadoop solutions. On the top of my head I can say that it eliminates the pain of maintenance.

What are the other reasons?"
1404,2020-01-02 08:22:51,1577946171.0,dataengineering,Fast IPv4 to Host Lookups with ClickHouse and PostgreSQL,eiuj10,marklit,,https://www.reddit.com/r/dataengineering/comments/eiuj10/fast_ipv4_to_host_lookups_with_clickhouse_and/,1.0,0.0,0.0,8922.0,
1405,2020-01-02 10:37:53,1577954273.0,dataengineering,Is there any data cleanup program with the GUI/ease of use of power BI but uses a more robust language? (e.g. python),eivq55,efofecks,,https://www.reddit.com/r/dataengineering/comments/eivq55/is_there_any_data_cleanup_program_with_the/,1.0,6.0,0.0,8921.0,"Hello all,

Disclaimer: I'm an Excel guy, not a data engineer, so this is a shot in the dark. We're a small, not-so-techy company where most of our pipelines need to be ingested from CSVs and SQL databases.

I've recently learned about Power BI and like many ""excel guys"", it's replaced clunky macros for me.  I love how easy it is to use, how it ""records"" your actions like a pipeline, and how I can switch back and forth to editing it using the advanced editor.  

However, what I don't like (at least from what I know) about power query is that:

- It's basically an end user service tool and I have to do everything on my computer.
- M language is super weird, doesn't even look like VB or excel much.  
- It can't really export to anything, I can't even figure out how to use it as a transformation tool and push the output up to an SQL server.

I'm on the lookout for some more robust tools, and while I'm trying to learn python (medium proficiency in pandas), I don't think I'm super ready to move to an all-code stack.  I REALLY love how intuitive the Power Query interface is, except for the aforementioned annoyances.  Is there a program which you guys could point me to that still maintains some semblance of easy GUI?

- Combine, sanitize, merge and transform files, mostly from csvs, SQL, web, or excel files
- can be scheduled
- outputs can be into CSV or pushed into a simple DB
- outputs or errors can be logged
- etc.

Thank you all!"
1406,2020-01-02 16:55:45,1577976945.0,dataengineering,What are some good portfolio project suggestions for aspiring Data Engineers?,eiz6ut,chirau,,https://www.reddit.com/r/dataengineering/comments/eiz6ut/what_are_some_good_portfolio_project_suggestions/,1.0,14.0,0.0,8926.0,
1407,2020-01-02 17:31:04,1577979064.0,dataengineering,"Seasoned DEs, what would you say are the most important technical skills to go beyond the basics and what are some good resources/books/roadmap to take one to the next level?",eizmft,chirau,,https://www.reddit.com/r/dataengineering/comments/eizmft/seasoned_des_what_would_you_say_are_the_most/,1.0,3.0,0.0,8928.0,
1408,2020-01-02 18:08:45,1577981325.0,dataengineering,An Insight Into How Google Services Manage Petabyte-Exabyte Scale Data,ej03us,techPackets_005,,https://www.reddit.com/r/dataengineering/comments/ej03us/an_insight_into_how_google_services_manage/,1.0,0.0,0.0,8929.0,
1409,2020-01-02 18:52:13,1577983933.0,dataengineering,How do you create self healing data pipelines?,ej0ogn,lnx2n,,https://www.reddit.com/r/dataengineering/comments/ej0ogn/how_do_you_create_self_healing_data_pipelines/,1.0,3.0,0.0,8928.0,"I was asked this question in an interview. At my current work place, we see platform errors and we rollback after certain time and try three times before calling it a failure. However, I believe there are better ways to deal with this and I wonder how they look like.."
1410,2020-01-03 06:46:35,1578026795.0,dataengineering,What are some good conferences you recommend for data engineering?,ejaeh7,frodomtbaggins,,https://www.reddit.com/r/dataengineering/comments/ejaeh7/what_are_some_good_conferences_you_recommend_for/,20.0,38.0,0.0,8951.0,"I'm in an entry level data engineer role, primarily using Hadoop, spark scala, and oozie tools. Looking to stay relevant and learn some new things. What do you recommend?"
1411,2020-01-03 13:33:34,1578051214.0,dataengineering,Top Events for Data Professionals in 2020,eje8rb,knlph,,https://www.reddit.com/r/dataengineering/comments/eje8rb/top_events_for_data_professionals_in_2020/,1.0,0.0,0.0,8953.0,
1412,2020-01-03 19:47:15,1578073635.0,dataengineering,Data Engineering career advice,ejiuyt,jwebs91,,https://www.reddit.com/r/dataengineering/comments/ejiuyt/data_engineering_career_advice/,20.0,30.0,0.0,8953.0,"Hi all,

A little bit of context, I've been at my current job (Data Engineer) for around 2 and a half years and am looking to move on as I feel like I'm stagnating there and want to get exposure to some more exciting/up-to-date technologies.

The problem I am having is that other Data Engineer roles want somebody with however many years experience in many things that I have not used before e.g. Spark, Hadoop, Airflow, Pandas, commercial Python experience, etc.

My current role involves heavy use of ETL tools (Talend) and SQL Server for data warehouse development.

I have had some limited exposure to AWS (S3, Redshift, Athena).

I did previously learn Python in my own time and am currently doing another course on Udemy but, without using it at my current role, I'm struggling to retain what I'm learning.

Is there anything else I could do to better position myself in looking for a new DE role? Should I consider dropping down to a junior position in order to get the sort of exposure I would like?

Any suggestions would be much appreciated!"
1413,2020-01-04 20:01:23,1578160883.0,dataengineering,How can I learn enough Software Engineering?,ejzy5b,[deleted],,https://www.reddit.com/r/dataengineering/comments/ejzy5b/how_can_i_learn_enough_software_engineering/,1.0,0.0,0.0,8967.0,
1414,2020-01-04 20:04:41,1578161081.0,dataengineering,Data engineering and python?,ejzzu2,casualphil,,https://www.reddit.com/r/dataengineering/comments/ejzzu2/data_engineering_and_python/,19.0,23.0,0.0,8967.0,"I’m a software engineer. I’ve spent 2 years working for an infrastructure team(ie we were building a framework that other web backend developers could use while writing service code). Overall, i enjoyed building/maintaining the infrastructure (for example using redis, python-rq, activemq). 

I took a deep dive into MachineLearning recently. After a month, I realised that i’d rather not spend my time dwelling in statistics and math to understand the algorithm in and out. I just don’t think that’s my niche. That’s when i encountered “data engineering”.

I like the idea of building architecture and pipelines to enable users(other developers/data scientists/analysts etc) to interact with data. I may be getting ahead of myself, but this feels right to me. 

Oh, also, I’m sort of a python enthusiast, so i’m curious as to what role python plays in this field. Does it have a future? Can i be a python data engineer?

Thanks!"
1415,2020-01-05 00:37:20,1578177440.0,dataengineering,Good examples on how to build Data Pipelines?,ek3pzl,pedrosmv,,https://www.reddit.com/r/dataengineering/comments/ek3pzl/good_examples_on_how_to_build_data_pipelines/,1.0,13.0,0.0,8971.0,"Hello, I am an aspiring data engineer on my company (we only recently are focusing on our data value) and I'm a little bit lost. 

I am pretty familiar with the essencial tools such as Python, SQL, but I wanted some introductory material on the processes of building a data pipeline. Not necesseraly the frameworks but examples and some explaining on each step. 

Do you guys have any material like that? Thanks in advance!"
1416,2020-01-05 04:11:01,1578190261.0,dataengineering,Largest set of data you ever worked on ?,ek6fea,audyoga,,https://www.reddit.com/r/dataengineering/comments/ek6fea/largest_set_of_data_you_ever_worked_on/,1.0,23.0,0.0,8970.0,"Folks, I am  trying to understand the sizing aspect of data based on what everyone has encountered here in real life scenario , expecting valid answers , some background on challenges with type and frequency of data will also be insightful."
1417,2020-01-05 10:43:27,1578213807.0,dataengineering,Udacity Data Engineer Nanodegree Review 2020,ekaiig,akshuvikhe,,https://www.reddit.com/r/dataengineering/comments/ekaiig/udacity_data_engineer_nanodegree_review_2020/,1.0,0.0,0.0,8972.0,
1418,2020-01-05 15:27:54,1578230874.0,dataengineering,"How to generate HTML documentation for an existing database (Oracle, MS SQL Server, MySQL, PostgreSQL, Azure Database, Amazon Redshift, SQLite, Firebird)",ekd01n,Fred_sql,,https://www.reddit.com/r/dataengineering/comments/ekd01n/how_to_generate_html_documentation_for_an/,1.0,0.0,0.0,8974.0,
1419,2020-01-06 10:05:29,1578297929.0,dataengineering,What are you go to technical sources to keep up to date?,ekr18x,Carnegie118,,https://www.reddit.com/r/dataengineering/comments/ekr18x/what_are_you_go_to_technical_sources_to_keep_up/,1.0,1.0,0.0,8993.0,Do you have any journals or particular websites you use to keep up to date?
1420,2020-01-06 15:58:02,1578319082.0,dataengineering,An interview about how the Debezium framework simplifies implementing change data capture for all of your database engines,ekugyt,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ekugyt/an_interview_about_how_the_debezium_framework/,1.0,0.0,0.0,8997.0,
1421,2020-01-07 16:22:47,1578406967.0,dataengineering,The next 2 years for data engineers,elc635,Boozmork,,https://www.reddit.com/r/dataengineering/comments/elc635/the_next_2_years_for_data_engineers/,1.0,57.0,0.0,9019.0,"I've recently absorbed a DBA function into my data engineering team.   
I accepted this because I see a movement from a traditional DBA role to a more data engineering role and I've read a number of blog posts that backed me up on this. That with less managed servers and a move to the cloud, DBAs are moving to more data analyst/data engineer etc responsibilities. 

My question to the hive, where do you see Data Engineering roles in two years?

I personally think there are more specific roles within data engineering to specialise in. Information managers/ data modellers etc."
1422,2020-01-07 17:44:04,1578411844.0,dataengineering,What do you call a group of Data Scientists?,eld65n,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/eld65n/what_do_you_call_a_group_of_data_scientists/,1.0,7.0,0.0,9020.0,"A murder of crows

A caravan of camels

A business of ferrets

A(n) \_\_\_\_\_\_\_\_ of data scientists?

Vote here to decide! [http://allourideas.org/counter\_for\_data\_scientists](http://allourideas.org/counter_for_data_scientists)

Vote multiple times, it is more fun that way. I'm personally campaigning for *n.*

Credit to this tweet for the discourse: [https://twitter.com/chrisalbon/status/1214384871491035136](https://twitter.com/chrisalbon/status/1214384871491035136)"
1423,2020-01-07 19:56:04,1578419764.0,dataengineering,Learnings in 2019 and learning wishlist for 2020,eleyuu,ppipernet,,https://www.reddit.com/r/dataengineering/comments/eleyuu/learnings_in_2019_and_learning_wishlist_for_2020/,1.0,8.0,0.0,9024.0,"Fellow Data Engineers,

Now that 2019 is over and a new decade is beginning, what trends, technologies and tools did you observe and use in 2019 that have come out of the closet and have gained/gaining mainstream adoption?

What big things do you anticipate will make the news in 2020?

I personally worked on Spark, AWS, Snowflake etc last year. I want to learn a few new things and have started learning about dockers a bit. Would like to know about what you all have worked on and learnt in 2019 or willing to learn this year."
1424,2020-01-08 10:17:49,1578471469.0,dataengineering,How to prepare for a DE interview?,elpy27,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/elpy27/how_to_prepare_for_a_de_interview/,1.0,21.0,0.0,9039.0,"Here's what my schedule is for a general DE interview, not specifically for any company

1. Leetcode easy/medium Algorithm questions
2. Leetcode all level SQL questions
3. Reading research papers on DB architecture, spark, data streaming etc., anything related to DE
4. Occasionally doing practical of DE, like playing with dataset on different tools
5. Writing blog around DE practical stuff once in a while 

Is there something I'm missing or should focus on more? Please guide"
1425,2020-01-08 14:06:14,1578485174.0,dataengineering,What are some current open problems in data engineering?,elruw5,overweight_neutrino,,https://www.reddit.com/r/dataengineering/comments/elruw5/what_are_some_current_open_problems_in_data/,1.0,3.0,0.0,9041.0,
1426,2020-01-08 14:08:59,1578485339.0,dataengineering,Ant suggestion to my Data Architecture?,elrvs2,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/elrvs2/ant_suggestion_to_my_data_architecture/,1.0,0.0,0.0,9041.0,"First of all, I love this subreddit. Being a data engineer myself, I find every post interesting, and the expertise of some people answering question is outstanding.

So, I am a (Lead) Data Engineer in a small company. I also work as a data guy for a side-project and have my personal (side-)side-project myself.

Long story short, I am building something very similar to [newsapi.org](https://newsapi.org) \-- a news API that basically allows you to query the news. Why am I doing a clone of it? Well:

1. Because I want to
2. It is quite of a challenge
3. I think I know how to build it
4. I want to make the price for my product 5 times cheaper comparing newsapi

So, in this post I will try to explain my outline of the architecture and I hope that someone will be able to advise me something (because I am not that experienced in the tech stack that I am going to use for this product).

DATA FLOW OUTLINE (no precise tech stuff here)

Most of the news publishers have RSS/Atom feeds (something similar to an API if you do not know what it is) to give a programatical access to the latest published news.

By checking each RSS once in a while you can see new records (here's [the one of the NY Times](https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)). Therefore, by collecting all the endpoints for all the news providers' feeds that I need, I will be able to collect my data. 

Each article itself (to simplify) is title, published date, author, description, etc.

Let's say I have the place where I store all the articles. After that, I would have to query that with my API. 

I plan to store only up to 7 days of recent data in the Elasticsearch.

&amp;#x200B;

CHOSEN TECH STACK

Now, I will describe how I see it being implemented. 

1. Airflow to schedule the process of obtaining new data. 
2. Postgres to store ""operational data"" -- info about feeds, endpoints, last queried time, how many new articles, etc
3. AWS Lambda (serverless function) to read the RSS feed of each news publisher (feedparser package)
4. Elasticsearch to store the article data for the API
5. AWS Lambda + Flask  to deploy API 
6. RapidAPI (an API marketplace) to deliver my solution to the end users
7. EXTRA. S3 + AWS Athena to store raw files and query them if needed

Airflow will schedule a batch (let's say of 100) of feeds to read. It will send 100 asynchronous calls of Lambda function. Each Lambda function will return all the articles that are now in the feed. 

Then, all of those will be deduplicated with those that my solution have seen already. This deduplication will be run against the data in Postgres (not in the Elasticsearch). 

After that, all new records will be inserted into my Elasticsearch cluster. In addition, a CSV file will be moved into the S3 bucket (plus I will configure Athena to query those CSV files if needed). 

My serverless API will make hits to the Elasticsearch. 

There is definitely many things I have looped but I hope you got the idea.

**Any suggestions/questions are more than welcome.** 

I write pretty much everything about this project on my Medium account. For example, here is a [small one about RSS feeds and how to read](https://towardsdatascience.com/collecting-news-articles-through-rss-atom-feeds-using-python-7d9a65b06f70) them with Python posted on Towards Data Science.

Also, if you like you could support me by subscribing to the [Product Hunt prelaunch page](https://www.producthunt.com/upcoming/newscatcher).

If you would like to participate or become a beta tester, you are welcome!

Happy New Year everyone and I wish you all the Best!"
1427,2020-01-08 19:25:17,1578504317.0,dataengineering,Should I leave my job because we use a GUI-based ETL tool?,elvral,NotAClickBot,,https://www.reddit.com/r/dataengineering/comments/elvral/should_i_leave_my_job_because_we_use_a_guibased/,1.0,20.0,0.0,9042.0,"Im about to be the ""data guy"" on my team but executives are incredibly insistent on using a GUI-based ETL tool (Knime, really similar to Alteryx). Should I leave my company for a company that utilizes more python, given that a majority of the industry is going in that direction. Or more so, am I missing out if Im forced to used an GUI based tool opposed to programming?"
1428,2020-01-08 20:26:32,1578507992.0,dataengineering,Analysis of which data engineer tech skills are most in demand,elwmcx,discdiver,,https://www.reddit.com/r/dataengineering/comments/elwmcx/analysis_of_which_data_engineer_tech_skills_are/,1.0,9.0,0.0,9044.0,"I scraped three job listing sites to find which data engineer tech skills are most in demand. I wrote about the results in [Towards Data Science](https://towardsdatascience.com/most-in-demand-tech-skills-for-data-engineers-58f4c1ca25ab?source=friends_link&amp;sk=718920a6a6907224a009600c1720b020). Hope you find it helpful!

Constructive feedback appreciated!"
1429,2020-01-08 21:07:42,1578510462.0,dataengineering,Best practice for loading large CSVs into database?,elx7bv,wolf2600,,https://www.reddit.com/r/dataengineering/comments/elx7bv/best_practice_for_loading_large_csvs_into_database/,1.0,9.0,0.0,9044.0,
1430,2020-01-08 22:03:16,1578513796.0,dataengineering,The Power Of Streaming ETL — Flatten JSON With ksqlDB,elxzff,TheSqlAdmin,,https://www.reddit.com/r/dataengineering/comments/elxzff/the_power_of_streaming_etl_flatten_json_with/,1.0,0.0,0.0,9045.0,
1431,2020-01-09 11:22:32,1578561752.0,dataengineering,Pipeline to the Cloud – Streaming On-Premises Data for Cloud Analytics,em7awj,rmoff,,https://www.reddit.com/r/dataengineering/comments/em7awj/pipeline_to_the_cloud_streaming_onpremises_data/,1.0,0.0,0.0,9054.0,
1432,2020-01-09 13:30:36,1578569436.0,dataengineering,Automation testing of data pipeline,em8eg7,DataAnalyzer_1,,https://www.reddit.com/r/dataengineering/comments/em8eg7/automation_testing_of_data_pipeline/,1.0,4.0,0.0,9057.0,"Hello DataGeeks,

I have been asked to do automation testing of below data pipeline. can somebody suggest/direct me how i can achieve end to end automation testing of this pipeline. 

*I'm looking for framework, tools which i can used.*

**Oracle DW -&gt; Apache Nifi -&gt; AWS S3 (landing area) -&gt; \[Using AWS Lambda step function &amp; cloud watch\] -&gt;  To store data into AWS Redshift -&gt; Power BI**"
1433,2020-01-09 17:22:54,1578583374.0,dataengineering,GameAnalytics + Imply Meet-up in London - January 15th,emb2ek,GameAnalytics,,https://www.reddit.com/r/dataengineering/comments/emb2ek/gameanalytics_imply_meetup_in_london_january_15th/,1.0,0.0,0.0,9062.0,"Hey Engineers! If you’re based in London, come join us at our Apache Druid meetup with Imply next Wednesday.  

We’ll be sharing a few key lessons we learned from migrating our backend systems to Apache Druid. And, there will be 🍕 and 🥤.

You can find all the details here: https://www.meetup.com/Apache-Druid-London/events/267380924"
1434,2020-01-09 19:59:39,1578592779.0,dataengineering,Microsoft Data engineer certification,emdbcz,restie123,,https://www.reddit.com/r/dataengineering/comments/emdbcz/microsoft_data_engineer_certification/,1.0,0.0,0.0,9064.0,
1435,2020-01-09 22:02:44,1578600164.0,dataengineering,How has California Consumer Privacy Act (CCPA) affected your data processing?,emf1oa,srednuos,,https://www.reddit.com/r/dataengineering/comments/emf1oa/how_has_california_consumer_privacy_act_ccpa/,1.0,2.0,0.0,9066.0,"From my personal experience, I am having interesting dilemma, especially since my team have myriad data stores and pipelines. The details on ""do not sell"" aspect also leaves some grey area.

For example, we need to somehow redact the PII data while keeping the raw data in house. It is not easy if the data is later needed needed to pivot to acquire non-PII data.

What are you guys/gals' experience?"
1436,2020-01-10 01:48:09,1578613689.0,dataengineering,Data Engineering and ITIL practices and procedures,emia0x,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/emia0x/data_engineering_and_itil_practices_and_procedures/,1.0,1.0,0.0,9068.0,"Curious what has been others experience when it comes to dealing with the Administrative/ITIL tasks usually associated with the IT department. I'm talking about ServiceNow, Change Control Boards, and IT project managment. Any advice or experiences are welcome."
1437,2020-01-10 04:40:03,1578624003.0,dataengineering,Deploying a simple pandas script to fetch and write csv,emkjp6,histogram_of_life,,https://www.reddit.com/r/dataengineering/comments/emkjp6/deploying_a_simple_pandas_script_to_fetch_and/,1.0,9.0,0.0,9070.0,I am completing a project where I have datasets and ml model in a python package . I started with a sample of dataset to make a prototype now I have to modify it to take in as many csv without including it in the package. To solve this I am thinking to write a fetch dataset script to fetch data and use it to train but the problem is I don’t have clear idea of where to put these raw csv and have pandas clean and merge and write a clean csv so that the script with ml model can fetch the data when training. Where to store Data and deploy the Data cleaning and meeting script?
1438,2020-01-10 06:48:59,1578631739.0,dataengineering,Data Engineering vs. Database engineering role,emm2az,whatwhynotnow,,https://www.reddit.com/r/dataengineering/comments/emm2az/data_engineering_vs_database_engineering_role/,1.0,23.0,0.0,9072.0,"Hello data engineers,

I am about to take my first job as a data person and could really use some advice. I have two offers. First job offer: **data engineer**, no cloud resources, all on prem, Spark, Python, Bash. Second offer: **database engineer** (more of a database administrator role), also no cloud, mostly on prem, heavy SQL and Bash, no data engineering per se, but they are looking to create a data warehouse so possible growth in that direction, also a bit more laid back atmosphere and schedule compared to the first offer but not so much data engineering.

The questions I have are: 

* Is database engineer/administrator a promising career path? How does it compare to data engineer? 
* Which position sounds like a better start?
* What are your thoughts about no cloud usage? I have some experience setting up and using AWS resources on my own, but it won't be a part of either of the two jobs.
* Any other thoughts/suggesting would be appreciated!

Thanks!"
1439,2020-01-10 21:02:25,1578682945.0,dataengineering,"I want to move from a Front-end developer to a data engineer, any path to follow?",emvi18,youssame0,,https://www.reddit.com/r/dataengineering/comments/emvi18/i_want_to_move_from_a_frontend_developer_to_a/,1.0,17.0,0.0,9085.0,
1440,2020-01-10 22:44:27,1578689067.0,dataengineering,Setting Up Multi Node Apache Cluster,emwvva,nitinnks,,https://www.reddit.com/r/dataengineering/comments/emwvva/setting_up_multi_node_apache_cluster/,1.0,0.0,0.0,9089.0,
1441,2020-01-11 06:25:40,1578716740.0,dataengineering,Frustrating Problem With Continuous Sliding Window On Streaming Data,en2r0u,windyslide,,https://www.reddit.com/r/dataengineering/comments/en2r0u/frustrating_problem_with_continuous_sliding/,1.0,21.0,0.0,9099.0,"Hi folks. I'll do my best to explain my problem. Hopefully somebody can give me hand.

So, I'll set the stage. We process perhaps 45k events a day every time a customer buys something. Suppose the event looks something like this:

    {
      customer_id: &lt;string&gt;,
      timestamp: &lt;timestamp&gt;
    }

We'd like to store the count of transactions (meaning count of events) in the last 3 days, for a particular customer in some data store. Preferably Dynamo. 

I'm using a Kinesis stream in AWS to produce the events, and I've explored consuming by either AWS Kinesis Analytics SQL or Flink. 

Here's the problem I'm running into:

I can only ever trigger the window query on the stream when a new event comes in. This becomes an issue as soon as the window moves past an event. I can't re-trigger the calculation so the count is off. Without any sort of TTL the count will be incorrect until a new event for that customer comes in.  


https://preview.redd.it/m6s4sx37s2a41.png?width=893&amp;format=png&amp;auto=webp&amp;s=248f739e7d0cc6bf9158e807492c4a367f1f4f0d

Here's a poor illustration to try to explain what I mean. Imagine each star is a transaction by customer foo. Suppose the current date is 2020-01-03. With Flink, as the event comes in on 03, I have code that will query the stream for the last 3 days and count the number of transactions for foo. I get 3 and update my cache. Perfect! But then as the window advances, the count remains 3. There's no other event to trigger the count on 2020-01-04 or 05, etc, so the count remains 3 until a new event comes in. 

I would like this count to somehow update every time a new event by customer foo leaves or enters the window. 

A couple of thoughts I have:  
1. I need some way to re-trigger the query and cache as an event leaves the window  
2. I need some way to periodically re-query and group all items in the window on some set interval, perhaps once a minute.   


I have ideas about how to do this that feel hacky. For example, perhaps each time I consume an event I push a ""delayed event"" into an SQS queue that becomes visible at current time + window length. Then, every time I eat one of these events I re-query the window for that customer. This isn't that ideal. I'm wondering if there's some canonical way to handle this problem. I'm open to using any software and exploring any options that you think support this in a very nice non-hacky way.  


Thank you!"
1442,2020-01-11 07:53:57,1578722037.0,dataengineering,It would be interesting to see a new take on data enrichment. Anyone have any thoughts ?,en3ni7,acceptedcitizen,,https://www.reddit.com/r/dataengineering/comments/en3ni7/it_would_be_interesting_to_see_a_new_take_on_data/,1.0,1.0,0.0,9100.0,Something that solves more problems and a new way.
1443,2020-01-11 19:03:28,1578762208.0,dataengineering,A Guide To Modern Batch Data Warehousing — Extraction,en9y7d,dmateusp,,https://www.reddit.com/r/dataengineering/comments/en9y7d/a_guide_to_modern_batch_data_warehousing/,1.0,20.0,0.0,9107.0,"Hey! I wrote an opinionated guide to modern batch data warehousing (published to Towards Data Science), based on the research and work I have done when I participated on a migration from a ""classic"" to a ""modern"" data environment

It's heavily inspired by Maxime Beauchemin's Functional Data Engineering principles, and the design philosophy of Apache Airflow (a framework he created)

I hope you like it!"
1444,2020-01-11 19:13:51,1578762831.0,dataengineering,Career Progression ?,ena39p,world_is_a_throwAway,,https://www.reddit.com/r/dataengineering/comments/ena39p/career_progression/,1.0,6.0,0.0,9107.0,"What do you typically see in your data engineering career progression pathway? If any? So far at my last two gigs I was the first of my kind.  So there has always been ambiguity on where/how to promote. 

From what I’ve seen around the industry it can be something like:
Jr. Data Engineer -&gt; Data Engineer -&gt; Senior Data Engineer -&gt; Data Architect ? 

What do you think ? What have you experienced ?"
1445,2020-01-11 20:09:24,1578766164.0,dataengineering,Help narrowing down and fleshing our senior synthesis and project ideas for a soon-to-be graduate,enauf4,t_hood,,https://www.reddit.com/r/dataengineering/comments/enauf4/help_narrowing_down_and_fleshing_our_senior/,1.0,0.0,0.0,9107.0,"Hey guys, first post here but I frequently lurk here and on the datascience sub as well. During my undergrad I've had 3 internships and have had tons of exposure to data engineering, data science and analytics, and data modeling. Because of this, I want my senior project to incorporate at lease some of these techniques into it. Specifically, my project must pertain to my majors (applied mathematics and computer science), but I have a lot of headroom when it comes to how I choose to present my topic and relate it to these areas. I'm still in the brainstorming phase of what I want to do and I'd like help narrowing down my ideas and possibly fleshing out a few. Here's what I have so far:

1. E-Commerce platform that tracks all activity and records it in a DB. 
   1. From here I'd like to use the captured data to drive an analytics program or platform that can easily create dashboards and models. 
   2. I'm not really sure how to begin implementing this though, and I want to try and use methodologies and languages that are actually in demand (i.e. used in production and industry). 
2. IoT device that captures data of some sort of information or data and relays it back to a web server where the user can interact with it.
   1.  I've been exploring TensorFlow, RaspberryPi, NodeMCU, and a few others to see what I can build with these (they seem very versatile). 
   2. For this I'm not sure what specific area I should go for in IoT. Smart cities really intrigue me, but the solutions been discussed for smart cities appear to be on a much larger-scale that what I can feasibly achieve as a student. 
   3. Devices that monitor or track health data also intrigue me. Potential areas I have looked at are blood sugar levels, heart rates, and weight, exercise, and sleep patterns.
3. Anything related to ""buzzwords"". I can use this project to learn about buzzwords that I'm relatively inexperienced with, such as: 
   1. block chain
   2. cryptography
   3. centralized crypto-currencies
   4. AI (specifically autonomous vehicles, image/object recognition)
4. Finally, if it helps at all, here are the languages, tools, and frameworks I am familiar with
   1. Python
      1. Pandas, Numpy, PsycoPG2, Matplotlib, Seaborn, Sklearn, Apache-Airflow
   2. SQL
      1. PostgreSQL, Google Big Query, MySQL, MS SQL Server, GraphQL
   3. R (still pretty novice, but I understand the basics)
      1. RShiny, Caret, Tidyverse
   4. Web (still pretty novice, but I understand the basics)
      1. HTML, CSS, JavaScript, BootStrap
   5. Visualization
      1. RShiny, Tableau, SpotFire, Excel, Matplotlib, Seaborn
   6. Others
      1. C#, C++, Java"
1446,2020-01-11 23:25:49,1578777949.0,dataengineering,"Is an ""Intro to Data Engineering"" Workshop Feasible?",endjuq,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/endjuq/is_an_intro_to_data_engineering_workshop_feasible/,1.0,41.0,0.0,9110.0,"I've always had a passion for teaching, but not enough to become a teacher myself (I love programming way too much and its associated salary). Since there isn't really a traditional way of getting into data engineering, I was thinking of creating and hosting a workshop for people who either (1) know they want to get into data engineering but need a starting point or (2) have the skills to get into data engineering but are interested it what it looks like at a basic level. My goal is for every participant to have a project that they can include within their portfolio that shows that they understand the basics of data engineering (aka I actually want it to be a valuable investment of time).

In my mind, I'd have the participants create a Python script that pulls data from a public API, do some transformation on the data with Pandas, and insert the data into a database (likely SQL). And then have them complete some questions about the data that require knowledge of SQL. I'd also provide instructions ahead of time and make sure that all participants have the necessary environment set up on their laptop before attending the workshop. I would also likely give a dive into additional data engineering concepts that they should explore as a next step, such as Apache Spark, Apache Airflow, and NoSQL databases.

Would something like this be feasible? And/or am I crazy?"
1447,2020-01-12 03:14:44,1578791684.0,dataengineering,What's the best way to learn as much as I can as fast as possible regarding Data Ingestion?,englw3,waterlololololol,,https://www.reddit.com/r/dataengineering/comments/englw3/whats_the_best_way_to_learn_as_much_as_i_can_as/,1.0,5.0,0.0,9117.0,"Preferably involving technologies like Sqoop, Hive, Hadoop, Kafka, NiFi, Syncsort"
1448,2020-01-12 03:38:51,1578793131.0,dataengineering,What's the line Data engineering and business intelligence ?,engwne,amkian,,https://www.reddit.com/r/dataengineering/comments/engwne/whats_the_line_data_engineering_and_business/,1.0,0.0,0.0,9118.0,
1449,2020-01-13 01:23:32,1578871412.0,dataengineering,databricks-connect without using cluster,envela,MenziesTheHeretic,,https://www.reddit.com/r/dataengineering/comments/envela/databricksconnect_without_using_cluster/,1.0,0.0,0.0,9144.0,
1450,2020-01-13 05:09:47,1578884987.0,dataengineering,Advice on the transition from Microsoft to AWS,eny93f,Positive_Archer,,https://www.reddit.com/r/dataengineering/comments/eny93f/advice_on_the_transition_from_microsoft_to_aws/,1.0,5.0,0.0,9147.0,"I work for a company that has historically been a Microsoft shop.  We use SQL server for our data warehouse and vendor application databases.  Our ETL processes (infesting external data and generating extracts for other vendors/apps) are all built using SSIS.  SSRS is our main form of end user reporting (200+ custom reports).  

There has been a push for us to find current MS DW processes that can be migrated to AWS native solutions over the next 12 months.  The end goal being to have the entire DW living in AWS (or as much as possible).  I am currently looking for an AWS friendly platform for our ETL processes.  To me, the first logical choice is AWS Glue.  However, I am having a hard time finding documentation on how to go about implementing ETL in Glue beyond very simple copy examples.  Our DW is a Kimball model with a central normalized data model and data marts that serve different reporting requirements.  Our ETL transformations are mostly looking up business keys in the normalized database to find surrogate keys and then joining relevant entities.  We have tsql stored procedures that SSIS will run against the normalized DB and write the results to denormalized fact/dim data marts for reporting.

I am looking for guidance and advice from anyone with experience with migrating a DW from Microsoft to AWS or experience with the capabilities of AWS Glue/Spark.  Thank you in advance."
1451,2020-01-13 11:50:54,1578909054.0,dataengineering,"What is Hadoop ? Overview of Hadoop Ecosystem ,Architecture and all the components in simple terms !",eo23e3,ashishmg,,https://www.reddit.com/r/dataengineering/comments/eo23e3/what_is_hadoop_overview_of_hadoop_ecosystem/,1.0,0.0,0.0,9152.0,
1452,2020-01-13 15:27:08,1578922028.0,dataengineering,Schema Evolution in Data Lakes,eo43bw,hdanish,,https://www.reddit.com/r/dataengineering/comments/eo43bw/schema_evolution_in_data_lakes/,1.0,0.0,0.0,9153.0,
1453,2020-01-13 20:11:11,1578939071.0,dataengineering,Streams and Tables in Apache Kafka: A Primer,eo7u5n,vicksyu,,https://www.reddit.com/r/dataengineering/comments/eo7u5n/streams_and_tables_in_apache_kafka_a_primer/,1.0,0.0,0.0,9157.0,
1454,2020-01-13 23:14:08,1578950048.0,dataengineering,Data Managed Services,eoagnr,bp_ryan,,https://www.reddit.com/r/dataengineering/comments/eoagnr/data_managed_services/,1.0,3.0,0.0,9162.0,"I work for a manufacturing company that has been using sensor data with SCADA systems since long before “IoT” became a thing, but now we are finally trying to modernize our data architecture in the cloud to mature our analytics strategy. We don’t have the engineering expertise to stand up new pipelines quickly or maintain them. We’re considering outsourcing our data pipelines from SCADA -&gt; EDW/Lake to a managed service rather than build a team.    
   
Anyone have experience or recommendations with data managed services?"
1455,2020-01-14 03:15:12,1578964512.0,dataengineering,Do we need InfoBurst with BusinessObjects and Tableau?,eodszg,fazeka,,https://www.reddit.com/r/dataengineering/comments/eodszg/do_we_need_infoburst_with_businessobjects_and/,1.0,0.0,0.0,9169.0,"I am an architect at a corporation that currently uses Business Objects and Tableau. We have used BO since \~2013 and picked up Tableau later in \~2017.

There's been a investigation into InfoBurst, a tool that does scheduling, bursting, publishing and delivery of reports. My understanding is that BO and/or Tableau do this to a degree.

The thought is that our group wants to leverage InfoBurst as a means to integrate between the two (BO and Tableau).

The current support team challenges in supporting BO and Tableau are noted:

1. have to use BO admin tool to setup/manage WebI and Crystal reports

2. have to create subscriptions, extract refreshes and create manual powershell scripts for certain extracts in Tableau

I presume these tools are capable enough to where I guess I'm not convinced that either tool was set up with best practices when each were stood up, in light of the challenges we seem to be facing. I'm probably wrong, though...

Albeit, all I know is that BO is for data abstraction (and allowing for business rules) through universes and providing reporting, and Tableau is for reporting and visualization.

I'm questioning if we even need BO in the mid to long term, IOW can Tableau interface directly with data source(s) and allow for business rules/logic to be embedded? IOW, can one (Tableau) do the job of the other (BO?)

If so, would there then even be a need for InfoBurst? If not, I believe Orchestrator does posh, could that be leveraged? Surely InfoBurst is not the only way to leverage powershell?

Further, I'm wondering with our current stack:

Can Tableau and/or BO ***not*** dynamically deliver and refresh extracts to Tableau server/desktop, sharepoint, network share or email? Does it ***not*** have the capability of event-based scheduling based on db triggers, ETL schedules, etc.?

Is there ***no*** means to migrate BO universes (i.e., functionality) to Tableau? Is there ***no*** PDF ***nor*** PNG format supported for Tableau dashboard images? ***No*** means to publish to Sharepoint?

Does Tableau ***not*** provide auditing/tracking, alerting based on job execution status? Does Tableau ***not*** support multi- ***nor*** single-pass bursting?

Ultimately, trying to understand if InfoBurst is really a need or if it's a matter of the tools we have access to currently not configured properly."
1456,2020-01-14 08:58:10,1578985090.0,dataengineering,Is Apache Spark suitable for parallel running of a binary?,eoht82,Future-Professional,,https://www.reddit.com/r/dataengineering/comments/eoht82/is_apache_spark_suitable_for_parallel_running_of/,1.0,6.0,0.0,9174.0,"I have a Java or C++ binary that I need to run 100 times in a parallel fashion. The binary contains about 10,000 lines of code doing complex simulations with both numerical and non-numerical computation. 

I know I can use various schedulers (OpenPBS) deployed on a cluster to achieve this. I haven't explored Apache Spark properly yet. Is my requirement within the set of Spark capabilities?"
1457,2020-01-14 11:43:01,1578994981.0,dataengineering,Postgres Upsert - fragmentation issues,eoj6r7,pdiddy_flaps,,https://www.reddit.com/r/dataengineering/comments/eoj6r7/postgres_upsert_fragmentation_issues/,1.0,1.0,0.0,9178.0,"Hi everyone..

Wondering if anyone's got experience with using Postgres UPSERTs for ETLs and if so, have you ever experienced issues with fragmentation and bloat on the tables?

We have hourly batch ETLs upserting into our tables (tables \~ 10s of Millions, upserts \~ 10s of thousands) and we have auto vacuums on AWS, however too often we're having to manually run full vacuums to get the space back and prevent processes from hanging. It feels like we are constantly fighting a battle against it and if left alone for a few days, things would deteriorate to a stand-still.

Has anybody else experienced this issue? Does Upsert fundamentally have a negative impact on fragmentation and if so, what are other people using?

I've done a bit of reading on the issue but nothing conclusive, for example --&gt; [https://www.targeted.org/articles/databases/fragmentation.html](https://www.targeted.org/articles/databases/fragmentation.html)

&amp;#x200B;

Thanks as always!"
1458,2020-01-14 16:28:22,1579012102.0,dataengineering,Most in demand data analyst tech skills,eolymv,discdiver,,https://www.reddit.com/r/dataengineering/comments/eolymv/most_in_demand_data_analyst_tech_skills/,1.0,9.0,0.0,9180.0,"I scraped three job listing sites to find which data analyst tech skills are most in demand. I compared the results to data scientist and data engineer job listings. Here are the top ten results for ""data analyst"":

https://preview.redd.it/1lz5njid5ra41.png?width=700&amp;format=png&amp;auto=webp&amp;s=204dbd09a592b2efb03a187de50e53e70c856745

See the full article here: 

[https://towardsdatascience.com/most-in-demand-tech-skills-for-data-analysts-26d4ea4450f8?source=friends\_link&amp;sk=33f4dbcef7bbda51493608fe53c47ec1](https://towardsdatascience.com/most-in-demand-tech-skills-for-data-analysts-26d4ea4450f8?source=friends_link&amp;sk=33f4dbcef7bbda51493608fe53c47ec1)

Constructive feedback appreciated!"
1459,2020-01-14 17:21:25,1579015285.0,dataengineering,Has anyone read the Facebook storage research paper?,eommb4,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/eommb4/has_anyone_read_the_facebook_storage_research/,1.0,2.0,0.0,9182.0,"https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf

Has anyone read this paper? I had a few questions about this"
1460,2020-01-14 18:23:29,1579019009.0,dataengineering,"Last semester, should I take Data Mining or Machine Learning?",eongag,pmarct,,https://www.reddit.com/r/dataengineering/comments/eongag/last_semester_should_i_take_data_mining_or/,1.0,10.0,0.0,9182.0,"So I am actually a Senior in Mechanical Engineering with a Minor in Computer Science and will be graduating this May.  While I study ME, I have focused mostly on CS the past 2 years and have accepted a solid full time job offer as a data engineer at the startup I currently work at.  I only have to take 3 classes to graduate and am working part time. However, part of me wants to take Data Mining or Machine Learning. At my school these are pretty rigorous courses and I am wondering if you guys think it would actually be VERY beneficial to take one of these courses.  If I do I will be quite slammed this semester as my last three courses and part time data job will occupy roughly 60 hours a week.

What are your guys' thoughts? Would these courses greatly benefit me? Should I take neither and focus on personal projects that I have put on hold and maintain a good work/school/life balance my last semester?"
1461,2020-01-14 18:56:26,1579020986.0,dataengineering,An interview about YugabyteDB and how it was architected to power the new generation of planet scale applications,eonwsv,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/eonwsv/an_interview_about_yugabytedb_and_how_it_was/,1.0,0.0,0.0,9182.0,
1462,2020-01-14 22:07:02,1579032422.0,dataengineering,New to Airflow - Need help with Bashoperator,eoqlws,bigdatanewbee,,https://www.reddit.com/r/dataengineering/comments/eoqlws/new_to_airflow_need_help_with_bashoperator/,1.0,2.0,0.0,9186.0,
1463,2020-01-15 00:36:54,1579041414.0,dataengineering,Final round Interview for a data engineering role with potential team mate. Tips and suggestions are appreciated,eostwd,lnx2n,,https://www.reddit.com/r/dataengineering/comments/eostwd/final_round_interview_for_a_data_engineering_role/,1.0,4.0,0.0,9189.0,"Hello everyone,

I am having a final round technical/culture round with a data engineer. This company builds SaaS platforms for e-commerce. I am desperately looking to move from my current job and would like to stand out among two others who are going to give interviews tomorrow. 

What should I remember and work on?

How would you evaluate a potential team mate when you are interviewing someone?

What are the traits you look for?"
1464,2020-01-15 04:01:38,1579053698.0,dataengineering,SQL Server DBA Being recruited for Postgres and Denodo Role,eovkuz,ChampionThunderGoose,,https://www.reddit.com/r/dataengineering/comments/eovkuz/sql_server_dba_being_recruited_for_postgres_and/,1.0,0.0,0.0,9194.0,
1465,2020-01-15 19:54:27,1579110867.0,dataengineering,Any advice for Netflix onsite data engineer interview?,ep5nok,pojode,,https://www.reddit.com/r/dataengineering/comments/ep5nok/any_advice_for_netflix_onsite_data_engineer/,1.0,11.0,0.0,9208.0,"I will be going thorough 5 round.coding,modeling, design and 2 behavioral. Do I need leetcode practice for coding round?"
1466,2020-01-16 04:07:36,1579140456.0,dataengineering,"We have the results for ""What do you call a group of Data Scientists""!",epcicx,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/epcicx/we_have_the_results_for_what_do_you_call_a_group/,1.0,1.0,0.0,9218.0,"CLUSTER is your winner!

I really wanted kaggle gaggle to win but the people have spoken

Check out the rest of the results here : [https://greatexpectations.io/blog/datasci-counter-poll/](https://greatexpectations.io/blog/datasci-counter-poll/)"
1467,2020-01-16 05:07:19,1579144039.0,dataengineering,Advice for starting on an Agile DE team,epd9vr,jardata,,https://www.reddit.com/r/dataengineering/comments/epd9vr/advice_for_starting_on_an_agile_de_team/,1.0,5.0,0.0,9223.0,"I recently landed a DE role with a team at a very large company in my area. In the interview, the manager mentioned that the team operates using Agile.

I've worked in analytics and data engineering type roles before, but this will be my first time working in Agile and actually my first time working in a Scrum environment in general.

I'm really excited for this role and want to be as prepared as possible. Any advice from others who have worked on Agile teams? Also accepting links to good learning resources about this topic. Thanks!"
1468,2020-01-16 09:45:08,1579160708.0,dataengineering,How to Learn Data Engineering if You're a Data Scientist that specializes in statistical modelling,epg7wy,augustuscauchy1029,,https://www.reddit.com/r/dataengineering/comments/epg7wy/how_to_learn_data_engineering_if_youre_a_data/,1.0,7.0,0.0,9228.0,I'm a Data Scientist with a background in pure mathematics. I have no formal training in Computer Science.
1469,2020-01-16 11:17:02,1579166222.0,dataengineering,30 days of free access to IBM Data Science and AI Programs,epgzkk,frenchdic,,https://www.reddit.com/r/dataengineering/comments/epgzkk/30_days_of_free_access_to_ibm_data_science_and_ai/,1.0,8.0,0.0,9230.0,
1470,2020-01-16 18:46:10,1579193170.0,dataengineering,Any Data Engineers here in education?,eplv63,LetsBeCivilGuys,,https://www.reddit.com/r/dataengineering/comments/eplv63/any_data_engineers_here_in_education/,1.0,10.0,0.0,9242.0,"K-12 in the U.S. more specifically, but curious to hear from others as well. I ask as I currently have the title of Data Analyst but I've had some brief discussions with boss on transitioning role. I already spend a lot of time on database development, ETL, and building of data applications. I also have been implementing Airflow to centralize automations, interact with api's, etc.. So yeah wondering if that's enough to push for 'Data Engineer' title."
1471,2020-01-16 19:35:23,1579196123.0,dataengineering,Business Usecase,epmkte,theant97,,https://www.reddit.com/r/dataengineering/comments/epmkte/business_usecase/,1.0,0.0,0.0,9243.0,
1472,2020-01-16 20:33:07,1579199587.0,dataengineering,Recommendations for Pipeline/Transform Tools for Startup,epnel5,aMusicLover,,https://www.reddit.com/r/dataengineering/comments/epnel5/recommendations_for_pipelinetransform_tools_for/,1.0,0.0,0.0,9248.0,
1473,2020-01-17 05:41:12,1579232472.0,dataengineering,What to expect in Data Modeling interview (Big Data technology)?,epuuug,pojode,,https://www.reddit.com/r/dataengineering/comments/epuuug/what_to_expect_in_data_modeling_interview_big/,1.0,4.0,0.0,9264.0,What steps do you follow to attack data modeling problem ? Do you start with ER diagram or directly start by creating table. I have never gave data modeling interview before so not sure what to expect. I also can't find sample Data Modeling interviews on youtube too.
1474,2020-01-17 12:25:14,1579256714.0,dataengineering,[Recommendation] ETL Tool,epyn22,vrx23,,https://www.reddit.com/r/dataengineering/comments/epyn22/recommendation_etl_tool/,1.0,28.0,0.0,9278.0,"Dear Data Engineers,
The new company I joined a few weeks ago is evaluating ETL Tools to work with their Snowflake Databases. So far, Talend and SnapLogic are leading the ranking. However, I am not sure whether Talend is a good match. I have so far only heard people complaining about it. Arguments were primarily unintuitive and legacy eclipse based user interface, generally old tool (almost 15 years old), basically a blackbox where you cannot really see the underlying code and a lot of bugs which were hard to reproduce and took a lot of time to fix even with the help of the Talend support.

Has anyone of you had similar experiences regarding Talend? Also do you have any considerations regarding SnapLogic? I personally think that Matillion looks quite promising. I have a Spark background myself and I would prefer using it, but a tool is required which is also suitable for non-tech users."
1475,2020-01-17 14:03:19,1579262599.0,dataengineering,Opinions of Apache Airflow,epzhs6,The_Fog_Bandit,,https://www.reddit.com/r/dataengineering/comments/epzhs6/opinions_of_apache_airflow/,1.0,49.0,0.0,9280.0,"I've been looking at Airflow (creating a PoC) to manage some of my work recently, having previously been using an in house tool, and while it clearly has better functionality I am struggling to get along with it.

What are people's opinions of Airflow's user experience? I have read so much about using it that I feel like I'm missing something. Does anyone have any recommendations for reading that has helped them on their journey with Airflow?"
1476,2020-01-17 14:24:42,1579263882.0,dataengineering,What info should be within a schema in a relational database?,epzpc7,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/epzpc7/what_info_should_be_within_a_schema_in_a/,1.0,0.0,0.0,9280.0,
1477,2020-01-18 01:22:36,1579303356.0,dataengineering,Looker's LookML,eq8bp5,tpedar50,,https://www.reddit.com/r/dataengineering/comments/eq8bp5/lookers_lookml/,1.0,1.0,0.0,9295.0,How does LookML work? Is there a library or tool available that generates SQL the way LookML does?
1478,2020-01-18 12:40:06,1579344006.0,dataengineering,How do you handle CDC / Audit (created/modified metadata).,eqexoh,Soft-Degree,,https://www.reddit.com/r/dataengineering/comments/eqexoh/how_do_you_handle_cdc_audit_createdmodified/,1.0,5.0,0.0,9304.0,"Hi All,

Have what I imagine is a common problem.

We're currently redoing some data infrastructure - python shop, use airflow, mostly batch jobs into   
postgres RDBMS (aws rds). 

How do you all handle database changes? I feel like the traditional approach is to have a date\_created, created\_by, date\_modified, modified\_by for every table?

This is pretty painful both from an application code POV and also a table creation POV. Not to mention that you miss information on changes between create/last modification. Surely there's a better way?

&amp;#x200B;

The data size is in GB, is it practical to read the WAL and store it to S3? Basically I'd like to be able to do a full audit on any users/apps/data.

&amp;#x200B;

Thanks"
1479,2020-01-18 17:25:50,1579361150.0,dataengineering,Fast ETL in Python. Ideas?,eqhnmj,skiddadle400,,https://www.reddit.com/r/dataengineering/comments/eqhnmj/fast_etl_in_python_ideas/,1.0,44.0,0.0,9308.0,"I’m working on an etl Pipeline that feeds a bunch on ML models.

At the moment we extract data from a few sql dbs, do some feature extraction and dump everything in another sql dB, where we then read the relevant columns for the model we’re running.

Everything is in Python, mostly pandas. We have a custom optimised sql read (pyodc) but a lot of the merges, groupbys (rolling) could be parallelised. 

Everything is spin up on docker, running on blade server. I’d like to make better use of the 40 cores at my disposal. Especially if we scale this to cloud instances with even more cores and memory.

We’ve tried pandarallel, but it is pretty flaky. I’m experimenting with dask, but it requires quite a lot of changes. Ray / modin don’t speed up merge or groupby according to the docs.

Any experiences with similar problems?

This runs every night, and takes a few hours. The aim is to run it faster."
1480,2020-01-18 18:08:07,1579363687.0,dataengineering,Updating json-records in blob-storage,eqi6eu,akringstad,,https://www.reddit.com/r/dataengineering/comments/eqi6eu/updating_jsonrecords_in_blobstorage/,1.0,1.0,0.0,9309.0,"Hi all. I have a problem I would like some input on. 

I have an incoming stream of events in the form of json records coming from azure event hub. I want to save these. However, these events have a key, so that I want to overwrite an existing record if there is one already with the same key.   


I should probably use a NoSQL database like Azure Tables, cosmos db, but the first is not very compatable with Databricks, and the second is a bit overkill. 

So any tips on how to save these records to blob storage with resonable performance?   


(I need it to be compatible with databricks so I can direct query the data in Power BI using Databricks)"
1481,2020-01-19 01:33:19,1579390399.0,dataengineering,How did you start?,eqoc72,ztheprophet,,https://www.reddit.com/r/dataengineering/comments/eqoc72/how_did_you_start/,1.0,6.0,0.0,9317.0,"I""m just starting my career as a python developer and I'm aiming the position of data engineer.  
It has been less than a year since I first wrote a 'hello world' and I want to know if I could just jump into a DE position and what background should I have"
1482,2020-01-19 01:43:14,1579390994.0,dataengineering,"How to automate the retrieving, extracting, storing of excel data in most fault tolerant way?",eqogvn,papadjo,,https://www.reddit.com/r/dataengineering/comments/eqogvn/how_to_automate_the_retrieving_extracting_storing/,1.0,7.0,0.0,9317.0,"Hey everyone, I need some advice about a project I'm starting for a client.

Basically I need to automate the retrieving, scraping and storing of tens

of excel workbooks which are located on the internet, using python. They are all mostly different

with datapoints I need at different places in the sheets. The process should be repeated monthly

for each workbook but on different dates etc.

&amp;#x200B;

The thing that's bothering me the most is how to automate/schedule all of this and make it as fault

tolerant as possible (and easily recoverable by less technical people). I was looking into tools

such as Airflow and Luigi but I'm not sure if they're an overkill for me and do they fit my needs at all.

Anyone knows are there any other scheduler/automation tools out there for such purposes?  


Any help very appreciated!"
1483,2020-01-19 10:13:48,1579421628.0,dataengineering,What's a good solution to save JSON response so that it can be queried later? -looking for professional advice.,eqtp1y,lnx2n,,https://www.reddit.com/r/dataengineering/comments/eqtp1y/whats_a_good_solution_to_save_json_response_so/,1.0,9.0,0.0,9322.0,"Hello fellow engineers,

I am working on a data pipeline which gets data every 5 secs from an API using Nifi, does some processing. I want to save the data with time stamp somewhere so that this can be used later.

I get 12 responses a minute which is 17,280 records. I need to query them by minute in future.

What's the most efficient way to store them such that retrieval is done with minimal latency?

Thanks.."
1484,2020-01-19 11:33:26,1579426406.0,dataengineering,Why do we need a pipeline?,equbjq,casualphil,,https://www.reddit.com/r/dataengineering/comments/equbjq/why_do_we_need_a_pipeline/,1.0,4.0,0.0,9324.0,"I was learning about Kafka and ETL pipelines which made me ask myself a fundamental question: why do we need a pipeline in the first place? Why can’t we just do all the ETL at one place. 

Here are the points I came up with:

1. Difference in rate of production vs consumption. 
Maybe you have an incoming stream of data and you don have the resources to process the data as fast as it’s coming in. In that case, use some message-oriented middleware and build a pipeline. 

2. High coupling
Without a pipeline, the sources would know about the destinations. They both would be highly coupled. If your sources and destinations are many-to-many, then your communication code could easily get messy. Solution? Use a pipeline. 

3. Scalability and Reliability
This is as a result of high coupling. Without a pipeline, we wouldn’t be able to scale producers and consumers separately. Also, both the producer and consumer will go down together in case of server failure. 



Pls correct if i’m wrong with anything. 
Also, what else? What did I miss?"
1485,2020-01-20 01:33:26,1579476806.0,dataengineering,[Airflow / Snowflake / S3] - is there a better way?,er4d4w,cleverchimp,,https://www.reddit.com/r/dataengineering/comments/er4d4w/airflow_snowflake_s3_is_there_a_better_way/,1.0,0.0,0.0,9335.0,
1486,2020-01-20 05:12:48,1579489968.0,dataengineering,Kafka Hello World,er73im,alexhwoods,,https://www.reddit.com/r/dataengineering/comments/er73im/kafka_hello_world/,1.0,0.0,0.0,9340.0,
1487,2020-01-20 06:57:08,1579496228.0,dataengineering,Thinking about going full throttle in DE aiming to be a freelancer,er8ch9,Darthcolo,,https://www.reddit.com/r/dataengineering/comments/er8ch9/thinking_about_going_full_throttle_in_de_aiming/,1.0,13.0,0.0,9343.0,"Hi all! 

I’m thinking in going deeper into data engineering (I have basic working experience) aiming to achieve some independent work (freelance) in this area in the future.

Do you recommend this over a more traditional path, like web developer? Any thoughts?"
1488,2020-01-20 16:00:26,1579528826.0,dataengineering,SAP cloud Data Warehouse,erdjne,Boozmork,,https://www.reddit.com/r/dataengineering/comments/erdjne/sap_cloud_data_warehouse/,1.0,6.0,0.0,9356.0,"Hi Engineers, 

The company I work for is at a T junction moment, a few different possibilities that I've been helping guide them through.   
A real curve ball has entered the field though SAP cloud data warehouse. 

I have no experience of it, I know no-one who uses it or has even mentioned, obviously that's alarm bells straight away.   
Anyone have any experience and can offer a review?"
1489,2020-01-20 18:01:42,1579536102.0,dataengineering,An interview about how Mayvenn replatformed their production dataflows using Ascend and improved their ability to deliver meaningful analytics to their business,erf2x6,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/erf2x6/an_interview_about_how_mayvenn_replatformed_their/,1.0,0.0,0.0,9357.0,
1490,2020-01-20 23:27:25,1579555645.0,dataengineering,My data collection/processing methods are always very MacGeyver-esque. Is this a problem? Is it normal?,erjznm,Major-Level,,https://www.reddit.com/r/dataengineering/comments/erjznm/my_data_collectionprocessing_methods_are_always/,1.0,9.0,0.0,9369.0,"An example to illustrate my point:

DE is extracting client data from pdfs at the request of another department in their company. While each of these files is visually identical in format, they differ enough under the hood that programmatically parsing the files and extracting all of the information isn't an option. 

A pdf-parser library lets DE consistently extract *some* of the information they need, but not all of it. An OCR library lets DE extract *some* of the information, but, again, not all of it. Between the two, all of the information is available, so DE cleans up the results from each library and merges them. Very Frankenstein.

Is this standard for such a problem? Am I doing something horrifying to more tenured DEs? I'm at a tiny company and the only person who really serves to check-and-balance my decisions is the long-time DBA, so there isn't much standing in the way of me and my own stupidity."
1491,2020-01-21 00:31:32,1579559492.0,dataengineering,Druid vs Alluxio for kubernetes???? Anyone have any experience?,erkx39,OrdinaryRemove,,https://www.reddit.com/r/dataengineering/comments/erkx39/druid_vs_alluxio_for_kubernetes_anyone_have_any/,1.0,1.0,0.0,9371.0,"I want to put an in-memory layer between HDFS and kubernetes pods. I'm looking at Alluxio and Druid but I can't find any documentation on people comparing these two.

From the looks of things, Druid looks far more mature but as I've never used either, I can't say for certain."
1492,2020-01-21 06:18:21,1579580301.0,dataengineering,Creating a Python Poetry Package for PySpark on Kubernetes,erpces,C_athartic,,https://www.reddit.com/r/dataengineering/comments/erpces/creating_a_python_poetry_package_for_pyspark_on/,1.0,2.0,0.0,9377.0,
1493,2020-01-21 11:16:50,1579598210.0,dataengineering,SQLAlchemy won't work under Airflow with Python 3.5.2 on MacOS,ers6ua,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/ers6ua/sqlalchemy_wont_work_under_airflow_with_python/,1.0,0.0,0.0,9383.0,
1494,2020-01-21 13:03:55,1579604635.0,dataengineering,Data engineering jokes,ert3lu,Boozmork,,https://www.reddit.com/r/dataengineering/comments/ert3lu/data_engineering_jokes/,1.0,18.0,0.0,9383.0,"Slow work day here.   


Anyone got any data engineering jokes?"
1495,2020-01-21 14:29:29,1579609769.0,dataengineering,Need help!,ertzvy,Anoop0607,,https://www.reddit.com/r/dataengineering/comments/ertzvy/need_help/,1.0,0.0,0.0,9387.0,
1496,2020-01-21 17:11:29,1579619489.0,dataengineering,How to use the BigQuery Plugin in Grafana ?,ervwi1,Massnsen,,https://www.reddit.com/r/dataengineering/comments/ervwi1/how_to_use_the_bigquery_plugin_in_grafana/,1.0,0.0,0.0,9389.0,[Using BigQuery plugin in Grafana](https://stackoverflow.com/questions/59840012/using-bigquery-plugin-in-grafana)
1497,2020-01-22 00:58:38,1579647518.0,dataengineering,Recommendation for team collab tools,es2pbk,enginerd298,,https://www.reddit.com/r/dataengineering/comments/es2pbk/recommendation_for_team_collab_tools/,1.0,5.0,0.0,9418.0,"Hi everyone! I just became responsible on an engineering team and currently in the process of transforming the workflow, any recommendation on what should I be implementing for efficiency and collaboration? 

I'm planning on using JupyterHub, deploy flask apps, etc.."
1498,2020-01-22 14:07:39,1579694859.0,dataengineering,Snowflake users - how have you overcome not being able to use stored procs (unless you know JS)?,esb0ca,BudgetShaman,,https://www.reddit.com/r/dataengineering/comments/esb0ca/snowflake_users_how_have_you_overcome_not_being/,1.0,36.0,0.0,9612.0,My company uses Snwoflake and I personally utilize DBeaver to query... but I sorely miss stored procs...
1499,2020-01-22 16:25:34,1579703134.0,dataengineering,Airflow vs Appworx,escnib,FrebTheRat,,https://www.reddit.com/r/dataengineering/comments/escnib/airflow_vs_appworx/,1.0,2.0,0.0,9613.0,"We're currently looking at running airflow for process automation and managing data flows.  We have a poc together to show management, but they're concerned about feature parity with our current monolithic automation tool called ""Appworx"".  Has anyone worked with both and can speak the comparison of the two?  Google Fu brings up nothing, probably because Appworx is a legacy application that's not too tier anymore as far as I can tell."
1500,2020-01-22 18:45:55,1579711555.0,dataengineering,Check out my new Post!,esekoq,christolagali,,https://www.reddit.com/r/dataengineering/comments/esekoq/check_out_my_new_post/,1.0,0.0,0.0,9616.0,"Hey Guys! I have recently taken to writing to share my findings  while working with Airflow on AWS. Hoping to get some feedback and possibly claps if you liked it.

[Implement Parallelism in Airflow](https://medium.com/@christo.lagali/implement-parallelism-in-airflow-375610219ba?source=friends_link&amp;sk=6ae3e98d4a43f71495d86cc1073e18e4)"
1501,2020-01-22 22:29:26,1579724966.0,dataengineering,"How to answer the interview question ""describe a pipeline that you've built"" ?",esho60,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/esho60/how_to_answer_the_interview_question_describe_a/,1.0,0.0,0.0,9619.0,
1502,2020-01-23 13:05:54,1579777554.0,dataengineering,"Monitoring Sonos Devices with ksqlDB, InfluxDB, and Grafana",esrlss,rmoff,,https://www.reddit.com/r/dataengineering/comments/esrlss/monitoring_sonos_devices_with_ksqldb_influxdb_and/,1.0,1.0,0.0,9624.0,
1503,2020-01-23 14:32:46,1579782766.0,dataengineering,Kube Explained: Part 2 - Containers,essg9s,vogt4nick,,https://www.reddit.com/r/dataengineering/comments/essg9s/kube_explained_part_2_containers/,1.0,0.0,0.0,9625.0,
1504,2020-01-23 16:57:19,1579791439.0,dataengineering,Best DE certification 2020,esu75g,randm95,,https://www.reddit.com/r/dataengineering/comments/esu75g/best_de_certification_2020/,1.0,9.0,0.0,9629.0,"Hello Guys,

I am working as a data engineer for 1 and a half year and I want to get a certification.

Initially, I wanted to get certified in CCA 175 Hadoop and Spark developer, but I do not find good online material for preparation.

&amp;#x200B;

Do you recommend me another certification?

&amp;#x200B;

Thanks for the help !"
1505,2020-01-23 18:52:56,1579798376.0,dataengineering,Question for anyone who has or is currently mentoring someone?,esvt4i,LDNDataNoob,,https://www.reddit.com/r/dataengineering/comments/esvt4i/question_for_anyone_who_has_or_is_currently/,1.0,3.0,0.0,9630.0,"What do you look for in a mentor / mentee relationship?   
How could someone provide value to a mentor?  


I'm asking because I would really like the ability to work with someone and have them offer me guidance."
1506,2020-01-23 22:27:32,1579811252.0,dataengineering,Better term for non-backfillable?,esyvcy,dylancaponi,,https://www.reddit.com/r/dataengineering/comments/esyvcy/better_term_for_nonbackfillable/,1.0,4.0,0.0,9636.0,"Is there a better term for non-backfillable data?

For example, an API will return data for dates in the past on some metrics.  If your job fails to run for requesting data on a date in the past, you can run the job again for the same date and get the data.

For other metrics it will only return lifetime values at the time of call.  Thus, if your job failed to pull lifetime values at the correct time, that data would be non-backfillable or &lt;insert better term&gt;."
1507,2020-01-24 01:48:48,1579823328.0,dataengineering,ELI5 - Delineation between ETL and ELT,et1s16,que_wut,,https://www.reddit.com/r/dataengineering/comments/et1s16/eli5_delineation_between_etl_and_elt/,1.0,0.0,0.0,9644.0,"Pros and Cons?

Use cases?

Theoretically speaking, wouldn't the lead time between \[ETL\] vs \[ELT\] be the same?"
1508,2020-01-24 07:44:06,1579844646.0,dataengineering,Data Engineering or Software Engineering?,et62uf,OatsBikes,,https://www.reddit.com/r/dataengineering/comments/et62uf/data_engineering_or_software_engineering/,1.0,0.0,0.0,9653.0,"Just about to graduate college and am considering what I want to try to make my life’s work. In your opinion, is data engineering or software engineering the better field to enter?

I suppose things I specifically want to know about how it compares in each field are the following:
1. Competition, or how many compete for a data engineering job v a software engineering job
2. Salary
3. Job satisfaction
4. Future industry stability, or how well the industry is expected to do 20 years down the line"
1509,2020-01-24 13:09:37,1579864177.0,dataengineering,Speeding up SQL Server Data Warehouse Architecture With Automation Procedures – 10 Problem-Solution Scenarios To Jump-Start Your Development,et8z8e,dingopole,,https://www.reddit.com/r/dataengineering/comments/et8z8e/speeding_up_sql_server_data_warehouse/,1.0,0.0,0.0,9656.0,
1510,2020-01-24 14:04:18,1579867458.0,dataengineering,Where do data engineers sit on your org chart?,et9inh,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/et9inh/where_do_data_engineers_sit_on_your_org_chart/,1.0,0.0,0.0,9656.0,
1511,2020-01-24 18:57:10,1579885030.0,dataengineering,Anyone use Greenplum database?,etd2cm,whatwhynotnow,,https://www.reddit.com/r/dataengineering/comments/etd2cm/anyone_use_greenplum_database/,1.0,0.0,0.0,9661.0,"Has anyone used Greenplum? If you did, what client did you use with it (pgAdmin, something else)? Any tips or thoughts about it?"
1512,2020-01-25 04:43:15,1579920195.0,dataengineering,What is a decent salary with a couple years of experience?,etkz2i,BostonPanda,,https://www.reddit.com/r/dataengineering/comments/etkz2i/what_is_a_decent_salary_with_a_couple_years_of/,1.0,0.0,0.0,9670.0,
1513,2020-01-25 15:53:16,1579960396.0,dataengineering,open source data catalog tools,etr0pu,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/etr0pu/open_source_data_catalog_tools/,1.0,5.0,0.0,9680.0,"Can you please share your experiences with open source data catalogs like ckan, dkan etc? I would like to use one of them in my org."
1514,2020-01-25 20:49:24,1579978164.0,dataengineering,Best way to design Bucketing for Hive table (Spark on Hive),etuqoc,pojode,,https://www.reddit.com/r/dataengineering/comments/etuqoc/best_way_to_design_bucketing_for_hive_table_spark/,1.0,4.0,0.0,9684.0,"What is the best way to design hive table for best performance for below scenario? 

Scenario

1. Columns : user\_id,load\_dt/timestamp,content\_id,content\_url,referral\_url,session\_id
2. Each day wiki user activity logs are received with Kafka
3. Approx 100 M events are received in 5 min window
4.  these logs are processed by Spark
5.  number of users/number of events are not fix for interval 
6. Facts needs to be calculated : 
   1. number of user  visited page by each day
   2. number of users by country (join with another table = user profile info)
   3. how many users came through google url (referral url column)
   4. how many users revisited the page (find out based on diffferent session id)

&amp;#x200B;

My thoughts on design

Activity table

1) partition table by load\_dt

2) create 10 bucket with cluster by  session\_id 

&amp;#x200B;

User table

1) partition by country

2) index by user id

&amp;#x200B;

Any feedback on this design ?"
1515,2020-01-26 01:11:53,1579993913.0,dataengineering,A Data Engineer's Naive Foray Into Data Science,etyas4,shanghaiclown,,https://www.reddit.com/r/dataengineering/comments/etyas4/a_data_engineers_naive_foray_into_data_science/,1.0,0.0,0.0,9686.0,
1516,2020-01-26 16:14:39,1580048079.0,dataengineering,Google just published 25 million free datasets,eu764d,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/eu764d/google_just_published_25_million_free_datasets/,1.0,4.0,0.0,9695.0,
1517,2020-01-26 20:12:40,1580062360.0,dataengineering,Daily Engineering Daily Resources,euaaus,Matt__92,,https://www.reddit.com/r/dataengineering/comments/euaaus/daily_engineering_daily_resources/,1.0,3.0,0.0,9697.0,"How are you keeping up to date with all the new technologies in the data engineering topic? So many tools, so many possibilities. What resources you read on daily basis to get to know the most about DE? What are most-worthy blogs, sites which is worth to subscribe?

I would like to create a complete feedly list with these resources. Every tips are appreciated."
1518,2020-01-27 02:38:49,1580085529.0,dataengineering,How to learn data science “best practices” if you’re the only data scientist on the team?,eug02t,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/eug02t/how_to_learn_data_science_best_practices_if_youre/,1.0,7.0,0.0,9702.0,"I know this is /r/dataengineering but I think y’all can give me valuable insight as well. 

 I’m a grad student in my final year. 

I just accepted a spring internship at a well-known tech company that  doesn’t have a data scientist in the particular group I’ll be working in. If I do well, the plan is to be brought on full time post graduation later this summer. 

I know a lot about stats, ML, A/B testing etc. However, I’m less familiar with putting things in production or writing “production level code”. 

Are there any books/learning resources I should look into before I start? 

At the moment, I’m considering [Clean Code](https://www.amazon.com/dp/0132350882/ref=cm_sw_r_cp_awdb_t1_wyIlEb93NCPQF), [Designing Data-Intensive Applications](http://shop.oreilly.com/product/0636920032175.do), and [Geurilla Analytics](https://guerrilla-analytics.net/). Which (if any) if these should I read?

Any other recommendations/words of advice are much appreciated!"
1519,2020-01-27 11:40:03,1580118003.0,dataengineering,Kicking the tires on BigQuery – Google’s Serverless Enterprise Data Warehouse,eulqvd,dingopole,,https://www.reddit.com/r/dataengineering/comments/eulqvd/kicking_the_tires_on_bigquery_googles_serverless/,1.0,0.0,0.0,9704.0,"Part 1 - [http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-1/](http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-1/)

Part 2 - [bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-2](https://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-2)"
1520,2020-01-27 11:49:50,1580118590.0,dataengineering,Multi Matrix Deep Learning with GPUs,eulu16,Albertchristopher,,https://www.reddit.com/r/dataengineering/comments/eulu16/multi_matrix_deep_learning_with_gpus/,1.0,0.0,0.0,9704.0,
1521,2020-01-27 12:45:40,1580121940.0,dataengineering,Apache Kafka message encoding and schema management,eumcx5,DhiaTN,,https://www.reddit.com/r/dataengineering/comments/eumcx5/apache_kafka_message_encoding_and_schema/,1.0,0.0,0.0,9704.0,
1522,2020-01-27 13:50:37,1580125837.0,dataengineering,Is the future being a mix of Data Engineer and Data Scientist?,eumx17,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/eumx17/is_the_future_being_a_mix_of_data_engineer_and/,1.0,13.0,0.0,9705.0,"Hello. I am currently working in a position which involves both. I research new ML models, I code tools to automate things, I add new variables to the BI tool, I create new reports, I deploy ML models into production... And I make an extensive use of AWS.

I am amazed on how easy are certain things with AWS and how much work it saves comparing to only a few years ago. Even though, there are some areas in which it could be better, but other companies as Snowflakes arises to cover that niches. In Snowflake pretty much everything is fully self administrated and you don't have to do anything, and AWS has services like Personalize or Forecast, which enables a non technical person to create decent models; or things like Sagemaker, which makes ML developing and deployment very easy. 

As time passes, everything is getting easier and easier and requires less work in both sides, data engineering and data science so I am wondering that maybe in the future those specialicites doesn't exists and the only thing will be Full Stack Data Scientist or Data Scientist Engineer or whatever. 

What do you think?"
1523,2020-01-27 15:29:09,1580131749.0,dataengineering,Do you know Lightbend Cloudflow ?,eunwhh,Alant3k,,https://www.reddit.com/r/dataengineering/comments/eunwhh/do_you_know_lightbend_cloudflow/,1.0,0.0,0.0,9709.0,"Hello,

Has anyone tried Lightbend's Cloudflow ? It looks quite polished, I'm curious of it. Does it integrates well with Spark Structured Streaming and the Dataflow model ?"
1524,2020-01-27 16:22:42,1580134962.0,dataengineering,An interview about how the Great Expectations framework helps you add meaningful tests and validation to your data pipeline to drive down technical debt,euoixv,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/euoixv/an_interview_about_how_the_great_expectations/,1.0,0.0,0.0,9709.0,
1525,2020-01-27 18:11:50,1580141510.0,dataengineering,Airflow DAGs inside of Docker image?,eupxrw,romanX7,,https://www.reddit.com/r/dataengineering/comments/eupxrw/airflow_dags_inside_of_docker_image/,1.0,8.0,0.0,9710.0,"Iv noticed most people who use Airflow and Docker do not copy their dags folder into their Docker image and instead mount it externally or through github.

Is this best practice and what are the pros and cons?"
1526,2020-01-27 20:01:44,1580148104.0,dataengineering,Writing a self-contained ETL pipeline with python,eurman,voorloopnul,,https://www.reddit.com/r/dataengineering/comments/eurman/writing_a_selfcontained_etl_pipeline_with_python/,1.0,4.0,0.0,9714.0,
1527,2020-01-27 20:11:43,1580148703.0,dataengineering,Checkout my new Post!,eurrxz,christolagali,,https://www.reddit.com/r/dataengineering/comments/eurrxz/checkout_my_new_post/,1.0,0.0,0.0,9715.0,"Hey Guys! I have recently taken to writing to share my findings  while  working with Glue on AWS. Hoping to get some feedback and possibly claps if you liked it. 

Your insights will absolutely help me in tweaking my writing style.

[Leveraging Glue as a Central Meta Store](https://medium.com/@christo.lagali/leveraging-glue-to-act-as-a-central-metadata-store-402c753b14e?source=friends_link&amp;sk=91d17a6a15e4609bea50e25904afd535)"
1528,2020-01-27 22:02:55,1580155375.0,dataengineering,Rapid Processing of Large JSON and CSV Files on the Command-Line,eutgbb,Yord13,,https://www.reddit.com/r/dataengineering/comments/eutgbb/rapid_processing_of_large_json_and_csv_files_on/,1.0,0.0,0.0,9716.0,
1529,2020-01-27 22:56:50,1580158610.0,dataengineering,Automation and framework development in data engineering,euua0q,GoodJobMate,,https://www.reddit.com/r/dataengineering/comments/euua0q/automation_and_framework_development_in_data/,1.0,0.0,0.0,9718.0,
1530,2020-01-27 23:05:20,1580159120.0,dataengineering,Trying to architect a new dataset be like...,euuelg,Not-NedFlanders,,https://www.reddit.com/r/dataengineering/comments/euuelg/trying_to_architect_a_new_dataset_be_like/,1.0,6.0,0.0,9718.0,
1531,2020-01-28 00:31:38,1580164298.0,dataengineering,How to store Tensorflow or Keras model as a JSON?,euvq9y,Zelly77,,https://www.reddit.com/r/dataengineering/comments/euvq9y/how_to_store_tensorflow_or_keras_model_as_a_json/,1.0,0.0,0.0,9719.0,"How can I convert an hdf5 weights file to a json to store it in a database? 

How can I convert it back to use it again?

Key points:

Using ArangoDB

Willing to consider hdfs + spark

Need to store weights and architecture

I am using tensorflow in python

HDF5 is terrible

&amp;#x200B;

Thanks!"
1532,2020-01-28 08:56:18,1580194578.0,dataengineering,Snowflake auto commit and Duplicate,ev21z9,theant97,,https://www.reddit.com/r/dataengineering/comments/ev21z9/snowflake_auto_commit_and_duplicate/,1.0,0.0,0.0,9724.0,
1533,2020-01-28 10:11:41,1580199101.0,dataengineering,onTimer bug in Flink ProcessorFunctions on Kinesis Analytics?,ev2puv,windyslide,,https://www.reddit.com/r/dataengineering/comments/ev2puv/ontimer_bug_in_flink_processorfunctions_on/,1.0,0.0,0.0,9725.0,"Hey folks,  


Some strange behaviour I've encountered that I'm wondering if anybody has run into.   


We are using the ProcessorFunction ([https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process\_function.html ](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html)) functionality of Flink in order to do some stateful processing of events. In particular, we're leveraging the timer callback [https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process\_function.html#timers ](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html#timers). For some reason, these timers perform perfectly well locally but the identical code loaded into Kinesis Analytics won't fire the ""onTimer"" function. I've logged everything up until then and it works perfectly well. It's just that the onTimer functions won't seem to fire. Is there some limitation of Kinesis Analytics that might prevent this? Any recommendations you might have?"
1534,2020-01-28 16:39:13,1580222353.0,dataengineering,How do you organise your Airflow dags/scripts?,ev6ebl,Forumpy,,https://www.reddit.com/r/dataengineering/comments/ev6ebl/how_do_you_organise_your_airflow_dagsscripts/,1.0,13.0,0.0,9726.0,"I'm new to Airflow and want to know some best practises for organising scripts and dags. For example, in a somewhat complicated and multi-step DAG, I can imaging having this all in a single file would get unmanageable. How do you handle situations like this? Factor them out into a common ""scripts"" directory or something?

Some ""real-world"" example DAGs would be much appreciated too."
1535,2020-01-28 19:52:55,1580233975.0,dataengineering,How do you organize/document the re-architecture of an existing data pipeline?,ev8twm,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/ev8twm/how_do_you_organizedocument_the_rearchitecture_of/,3.0,10.0,0.0,9732.0,"Hello DE Community,

I started a new role in October 2019 for a healthtech startup on their analytics team and all of my work the first 3 months has been data engineering which is new for me. We're a small team of 6 and for the most part all generalists. 

Our team is in the process of building a new data warehouse in Redshift and my job to-date has been building jobs to move the source data from our application databases (Postgres) and push it into Redshift. I just got assigned a new project which is to recreate part of our current data pipeline that send data to our BI tool.

The work right now is just documentation and analysis… mapping dependancies, mapping which .sql scripts produce specific outputs that get picked up etc. How do you all go about documenting and organizing a project like this? I'm essentially pulling apart the pieces that make up the job now and putting them back together using the data that we migrated from our application databases.

For context if it matters, we're an AWS shop and use Matillion as our ETL tool."
1536,2020-01-28 23:37:06,1580247426.0,dataengineering,What are good questions to ask your DE interviewer?,evc2ut,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/evc2ut/what_are_good_questions_to_ask_your_de_interviewer/,2.0,13.0,0.0,9740.0,
1537,2020-01-29 13:02:59,1580295779.0,dataengineering,Want to switch career from Security Analyst to data engineer,evlga1,sep03,,https://www.reddit.com/r/dataengineering/comments/evlga1/want_to_switch_career_from_security_analyst_to/,1.0,16.0,0.0,9757.0,"Hi Guys,

&amp;#x200B;

I just need advice from experienced folks here, im currently a Soc analyst and handling vulnerability assessment project (gathering raw vulnerabilities report on qualys and transferring the useful data to MS Excel -&gt; and making a dashboard report on Power BI.) I realized that im enjoying it rather than the security side stuff. However i do have a little background with SQL and python. Do you think i can transition to Data engineer with just self study? 

&amp;#x200B;

Thanks in advanced"
1538,2020-01-29 21:59:41,1580327981.0,dataengineering,GCP Cloud Composer and AI Platform Notebooks,evs7op,Kubacka,,https://www.reddit.com/r/dataengineering/comments/evs7op/gcp_cloud_composer_and_ai_platform_notebooks/,1.0,0.0,0.0,9791.0,
1539,2020-01-29 23:18:40,1580332720.0,dataengineering,Fully Remote - Scala Data Engineer - PropTech - Salary: $170-240K DOE,evth40,PhilipAlldus,,https://www.reddit.com/r/dataengineering/comments/evth40/fully_remote_scala_data_engineer_proptech_salary/,1.0,4.0,0.0,9794.0,"Helping a PropTech client look for a fully remote (US) Scala Data Engineer!

**Why work here?**

* \-Fully Remote -
* Salary: $170-240K DOE
* They put cutting edge tech at the forefront of their business developing advanced algorithms!
* They recently received $60M in funding to scale and grow their platform as well as investing in R&amp;D to scale new products.
* Unparalleled work culture to achieve the most collaborative/innovative environment possible!

**What They're looking for?** Our client are looking for a Scala based data engineer who is proficient in using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.

Contact: [philip@alldus.com](mailto:philip@alldus.com)"
1540,2020-01-30 02:42:13,1580344933.0,dataengineering,Rule Proposal: No Job Listings,evwiir,dixicrat,,https://www.reddit.com/r/dataengineering/comments/evwiir/rule_proposal_no_job_listings/,1.0,15.0,0.0,9802.0,"There was a recent post that is just a straight job listing, and as far as I can tell there’s no rule against it. I generally like the career transition advice posts. I’d even be cool with an “I’m a headhunter and these are the skills we’re looking for in data engineers” type post. But headhunters posting straight job listings could really dilute the value I (and I’d imagine others) get from this sub. Thoughts?"
1541,2020-01-30 12:26:46,1580380006.0,dataengineering,What are the advantages of dbt against running SQL queries?,ew3csd,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/ew3csd/what_are_the_advantages_of_dbt_against_running/,1.0,10.0,0.0,9836.0,"Hello. I have read some posts about dbt but it is not very clear to me why would I use dbt instead of running SQL queries directly (using Airflow or other tool). As far as I understood, the main advantage is that you can code things that in SQL are not possible, like loops. But beyond that, is there anything else?

Right now, for me it seems that it is not worth the time you have to spend to learn the tool comparing with the advantages. 

Could you give me some insights on this? Thanks"
1542,2020-01-30 13:37:31,1580384251.0,dataengineering,Honest reviews I found about DataCamp,ew4389,akshuvikhe,,https://www.reddit.com/r/dataengineering/comments/ew4389/honest_reviews_i_found_about_datacamp/,1.0,2.0,0.0,9838.0,
1543,2020-01-30 14:13:48,1580386428.0,dataengineering,Use Azure Blob-storage as a simple document-store.,ew4ij9,akringstad,,https://www.reddit.com/r/dataengineering/comments/ew4ij9/use_azure_blobstorage_as_a_simple_documentstore/,1.0,0.0,0.0,9838.0,"Hi, I had a thread earlier about writing data to blob-storage without having to worry about duplicate records. I have now made some python code that allows for using blob-storage as a simple NoSQL database. Data can be stored in json or csv and read as regular blob-data. Documents can be up to 100MB in size.  
Hope some of you get to use it. (And please contribute, code should be ported to newest version of azure python sdk)

 [https://github.com/maka89/Document-Blob](https://github.com/maka89/Document-Blob)"
1544,2020-01-30 18:25:07,1580401507.0,dataengineering,Implicit vs Explicit data sources,ew7xnj,ethanenglish,,https://www.reddit.com/r/dataengineering/comments/ew7xnj/implicit_vs_explicit_data_sources/,1.0,0.0,0.0,9845.0,"Would an example of an implicit data source be reporting data like Google Ads (e.g. search term reports that are read only) and explicit be categorizing a domain as news, entertainment, etc?

Implicit is read-only whereas explicit you have to write.

The definitions for explicit are not as clear so I wanted to get your thoughts on if I'm thinking about it correctly."
1545,2020-01-30 19:04:19,1580403859.0,dataengineering,My company are hiring a Lead Data Engineer - AI driven Start up Proptech company in NYC. 180-220k salary plus equity.,ew8kip,colmfitz92,,https://www.reddit.com/r/dataengineering/comments/ew8kip/my_company_are_hiring_a_lead_data_engineer_ai/,1.0,0.0,0.0,9847.0,"Hi Guys 

I am working for an AI driven PropTech company in New York City and we are hiring a Lead Data Engineer to take ownership of the Data Team. 

My manager is looking to speak to Senior Data Engineers based in New York with 5+ years Python programming experience and experience with Airflow and AWS redshift! 

Happy to share more details if you want to email me at [colm@alldus.com](mailto:colm@alldus.com) with CV. 

&amp;#x200B;

All the best"
1546,2020-01-30 20:07:26,1580407646.0,dataengineering,Data Vault Modeling Primer for Heterogeneous Data,ew9kpe,audyoga,,https://www.reddit.com/r/dataengineering/comments/ew9kpe/data_vault_modeling_primer_for_heterogeneous_data/,1.0,3.0,0.0,9851.0,"Pretty neat reference document for Data Vault modeling something which guys snowflake advocate. 

[https://elib.uni-stuttgart.de/bitstream/11682/10311/1/Integration\_of\_Heterogeneous\_Data\_in\_the\_Data\_Vault\_Model.pdf](https://elib.uni-stuttgart.de/bitstream/11682/10311/1/Integration_of_Heterogeneous_Data_in_the_Data_Vault_Model.pdf)

Has anyone been using Data Vault Modeling at work ?"
1547,2020-01-30 21:48:40,1580413720.0,dataengineering,Planning to take Udacity Data Engineer Nanodegree after reading reviews,ewb39z,akshuvikhe,,https://www.reddit.com/r/dataengineering/comments/ewb39z/planning_to_take_udacity_data_engineer_nanodegree/,1.0,13.0,0.0,9853.0,
1548,2020-01-31 03:02:07,1580432527.0,dataengineering,Spoken Querying with SQL,ewfzxi,pvn251,,https://www.reddit.com/r/dataengineering/comments/ewfzxi/spoken_querying_with_sql/,1.0,1.0,0.0,9857.0,"We have built this system that enables practitioners to dictate SQL queries with multimodal interaction (speech- and touch-based correction) on emerging environments such as smartphones/tablets. This allows users to compose ad hoc queries over arbitrary tables and even slice and dice their data on the go. Although the current project is done in an academic research environment, we want to understand how this can be useful in an industrial setting. It would greatly help our research project if you can fill out this questionnaire: [https://docs.google.com/forms/d/e/1FAIpQLSe14TTwGyaLtK0DL80h3OY20rLrQoEPpHu5hyRr9HVdxGZEuQ/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSe14TTwGyaLtK0DL80h3OY20rLrQoEPpHu5hyRr9HVdxGZEuQ/viewform?usp=sf_link). We can send you the link to our system if you would like to try it out."
1549,2020-01-31 05:58:57,1580443137.0,dataengineering,How to be agile when sick?,ewiftg,BostonPanda,,https://www.reddit.com/r/dataengineering/comments/ewiftg/how_to_be_agile_when_sick/,1.0,11.0,0.0,9858.0,"I've been sick for the entire sprint and my burn down sucks. Everyone sees it and it makes me feel bad. Agile just feels hard sometimes, coming from a scrum master. Helppppp :(

P.S. Customers don't care if I'm sick lol"
1550,2020-01-31 07:17:49,1580447869.0,dataengineering,What should be the roadmap in 2020 to learn data engineering and make a career switch by 2021?,ewjeqw,aakhri_paasta,,https://www.reddit.com/r/dataengineering/comments/ewjeqw/what_should_be_the_roadmap_in_2020_to_learn_data/,1.0,26.0,0.0,9861.0,"Currently working as a web developer.
Need suggestions on technologies, courses, books and learning path.
Although the path could lead to Data Science and ML but right now I have no interest in pursuing those except studying mathematics for foundation."
1551,2020-02-01 01:46:16,1580514376.0,dataengineering,Open Data Engineering positions,ewxa41,spark58510,,https://www.reddit.com/r/dataengineering/comments/ewxa41/open_data_engineering_positions/,1.0,3.0,0.0,9883.0,"We still have two open DE positions open for a growing team.  Team will soon be moving from an on-prem stack.

Looking for experience with AWS native, Spark/Python/Scala, Talend and/or Streamsets would be nice and building pipelines end-to-end

Must have strong SQL background and preferably legacy ETL experience.

Must be willing to relocate to Roanoke VA

Salary range is in the 85-120k range depending on experience

DM me if you’re interested in talking more."
1552,2020-02-01 13:43:01,1580557381.0,dataengineering,"Join Joyn, the next Netflix",ex4zot,shawemuc,,https://www.reddit.com/r/dataengineering/comments/ex4zot/join_joyn_the_next_netflix/,1.0,2.0,0.0,9887.0,To all data engineers: come and join an awesome company and an equally awesome Team https://jobs.lever.co/joyn/8e588223-4fc7-46da-befb-84cde214ee6e we have anything from Junior to lead and this is just an example Position
1553,2020-02-02 14:09:43,1580645383.0,dataengineering,Monitoring DB trends in AWS,exnka9,data-david,,https://www.reddit.com/r/dataengineering/comments/exnka9/monitoring_db_trends_in_aws/,1.0,5.0,0.0,9905.0,"I was wondering about what the best workflows/tools are for a following scenario.

Imagine you receive data from \`N\` restaurants, on a daily basis, like how many drinks, dishes of certain type, total order count etc etc, a restaurant made. All these entries go into a postgres DB, with few colums {datetime, restaurant, type\_record, count}. Number of restaurants is in the 100's, so I need something that does not need to be updated with a CONFIG file every time a restaurant is added to the system.  


Now I want to run a daily script that:

1) can query the DB, 

2)make some basic calculations

3) catch something like \`number of drinks for today for restaurant X\` is 15% higher than its daily average\` and push an alert to slack or pagerduty .  


All I can think of is to run this code on a simple lambda function. This implementation would mostly suffice but I was wondering if there are smarter/better ways to achieve this.

&amp;#x200B;

Details:

Latency of the query (steps 1 and 2) are not a problem.

The main problem is how to have such a monitoring system on the DB that is as simple as possible (easy to maintain). 

&amp;#x200B;

Any ideas?"
1554,2020-02-02 18:05:16,1580659516.0,dataengineering,Building a Modern Batch Data Warehouse Without UPDATEs,exqjl4,dmateusp,,https://www.reddit.com/r/dataengineering/comments/exqjl4/building_a_modern_batch_data_warehouse_without/,1.0,9.0,0.0,9909.0,"Hi there!

I wrote about adapting the Star Schema to a modern data stack and ""functional data engineering"" in the below blog post.

It's the second time I share a data engineering blog post here, and last time we had really constructive discussions, looking forward to it! 

https://towardsdatascience.com/building-a-modern-batch-data-warehouse-without-updates-7819bfa3c1ee?source=email-2fa68a257a2-1580481817235-layerCake.autoLayerCakeWriterNotification-------------------------351515d1_83b5_49b5_8676_27a5370a26d9&amp;sk=821068431a519e522ab37fe664eb14cd"
1555,2020-02-02 22:45:17,1580676317.0,dataengineering,Ways to efficiently store large amounts of simulation data for my PhD (**Any** feedback appreciated !!),exuvv9,The_Bundaberg_Joey,,https://www.reddit.com/r/dataengineering/comments/exuvv9/ways_to_efficiently_store_large_amounts_of/,1.0,1.0,0.0,9915.0,
1556,2020-02-03 04:02:37,1580695357.0,dataengineering,Is there a reason almost all big data tools are written in Java?,exzhwi,sixilli,,https://www.reddit.com/r/dataengineering/comments/exzhwi/is_there_a_reason_almost_all_big_data_tools_are/,1.0,8.0,0.0,9921.0,
1557,2020-02-03 11:43:19,1580722999.0,dataengineering,How would you ETL this one? Processing Flat Files with custom delimiters,ey4r27,WranglingData,,https://www.reddit.com/r/dataengineering/comments/ey4r27/how_would_you_etl_this_one_processing_flat_files/,1.0,9.0,0.0,9928.0,"Hey Folks.  


I've been looking into methods on how to process a series of daily data snapshots into ""upsert"" daily files.

These files were originally extracted from SQL Server using the BCP utility.  The thing that is annoying about them is they have custom field (a string pattern of |&lt;&gt;|) and custom record delimiters (string of \~&lt;&gt;\~).  If you open these files in a text editor, they are one long line, regardless of how many records they are.  There is no header details, but the schema is known.

I was thinking of processing these files into a PostgreSQL database, but I cannot use the COPY command.

The next thought was to use Python and either Pandas or PySpark and get the files into a dataframe and then into PostgreSQL.  While I can see that this would be possible, I am not 100% sure on the actual procedure.  My initial thoughts are to use `df = SQLContext.read.text('/File/path.dat', lineSep='\~&lt;&gt;\~')` and then split each record (still not 100% sure on how to achieve this yet).  From there, I would stage into the PostgreSQL database for further processing

Can anyone give any other suggestions?

Cheers"
1558,2020-02-03 17:24:11,1580743451.0,dataengineering,Database versioning and schema migration tools,ey8ny6,penciltwirler,,https://www.reddit.com/r/dataengineering/comments/ey8ny6/database_versioning_and_schema_migration_tools/,1.0,3.0,0.0,9935.0,"Hi y'all, I'm looking into how to properly version my postgres database so that I can say ""migrate existing database to this state, or this state is bad, lets rollback to a previous version"". Basically git for databases.

I'm looking at alembic right now because I am more familiar with Python. However, I see a lot of posts about Flyway.

What do you guys think? Thanks!"
1559,2020-02-03 22:58:52,1580763532.0,dataengineering,Has someone implemented Data Vault 2.0 on Hadoop/Hive/Impala?,eydty0,tucanotucano,,https://www.reddit.com/r/dataengineering/comments/eydty0/has_someone_implemented_data_vault_20_on/,1.0,12.0,0.0,9940.0,"At my company, we are researching a lot the DV 2.0 data model and making some PoCs, but there isn't a lot of experiences on the web. I'm concerned about data replication (keeping data history in the Enterprise layer, almost replicating data in our data lake). Even though is not exclusively DV related, joins are costly and time-consuming with Hive and even with Impala. We already developed pyspark applications to reduce the time of this joins, getting interesting improvements, trying to get better times for constructing the staging, enterprise and data access layers. We are already using parquet files and partitioning.

I would appreciate any experience you could share with me"
1560,2020-02-04 01:40:07,1580773207.0,dataengineering,RocksDB Is Eating the Database World,eygend,ssb61,,https://www.reddit.com/r/dataengineering/comments/eygend/rocksdb_is_eating_the_database_world/,1.0,1.0,0.0,9942.0,
1561,2020-02-04 12:09:40,1580810980.0,dataengineering,"Hi, we are hiring Senior Data Engineer @ Chartbeat in Remote or New York, NY, USA",eynyc5,tanya2615,,https://www.reddit.com/r/dataengineering/comments/eynyc5/hi_we_are_hiring_senior_data_engineer_chartbeat/,1.0,0.0,0.0,9952.0,
1562,2020-02-04 14:54:31,1580820871.0,dataengineering,Architecting IoT Data Pipeline in Azure,eypmff,Fireedit,,https://www.reddit.com/r/dataengineering/comments/eypmff/architecting_iot_data_pipeline_in_azure/,1.0,2.0,0.0,9955.0,"Hi all,

Does anyone have experience in designing and implementing data pipeline from IoT sensors in Azure platform? 

My knowledge is bit limited on this matter so need some guidance please.

What I am thinking is something like this. Please note that this is based on my current knowledge. Feel free to criticise, challenge, give alternatives, suggestions etc.

Source: IoT sensors
Ingestion layer: Azure IoT Hub
Processing layer: Azure Streaming Analytics
Then, the output from this layer is branched off to speed layer in form of real time streaming to power BI.
The other branch is to batch layer in form of Azure Data Lake Gen2, then onto Azure SQL Database for easy querying.

Some questions I have regarding design considerations are:
1. Ingestion layer
     I believe instead of IoT Hub, another tool which can be used here is Kafka via Azure HDInsight.
     What will be the pro &amp; cons, design considerations to choose Kafka vs IoT Hub (and vice versa)?

2. Processing layer
     I mentioned Azure Streaming Analytics (ASA) above, but I believe I can replace it with SparkStreaming via Azure Databricks, although I am not sure how to present real time streaming in Power BI using SparkStreaming ( ASA is supported as streaming source in Power BI).
What will be the pro &amp; cons, design considerations to choose ASA vs SparkStreaming (and vice versa)?

3. Historical data storage
     Above I choose Azure Data Lake Gen2 as I believe it's a better choice than Azure Blob storage as the data maybe used for analytics in the future. Not sure if this is  valid or good enough reason to choose ADLSGen2 vs Blob though?

Also, from ADLSGen2, I am thinking to ETL the data to Azure SQL Database using Azure Data Factory  as most people in the company is familiar with SQL database, therefore will be easy for them to run query against the historical IoT data. 
However, this makes me think - should I then remove the ADLSGen2 altogether, so that output from ASA go straight to Azure SQL Database?
Or, should I still keep ADLSGen2 so that when in the future, we need to run some (advanced) analytics using distributed platform eg. Spark/Databricks, we can get the data from ADLSGen2 as the source and this will be more efficient and fast as it supports parallel processing, while Azure SQL Database doesn't?

If keeping ADLSGen2 layer, what will be the best format to store the data in? Json, csv, parquet, or something else?
Is it better to keep the data as raw as possible in there, i.e. no aggregation done in the previous layer (ASA or SparkStreaming) or the data should be aggregated in the processing layer eg. Count of events in 1 minute, etc?

Sorry for long post and lots of questions. I am just a newbie willing to learn!
I will appreciate your input a lot!

thanks"
1563,2020-02-04 17:24:01,1580829841.0,dataengineering,An interview about the BrightHive platform for building data trusts and the complexities that are inherent in sharing data across organizations,eyrlhw,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/eyrlhw/an_interview_about_the_brighthive_platform_for/,1.0,0.0,0.0,9960.0,
1564,2020-02-04 20:25:22,1580840722.0,dataengineering,How to convert a big .json file to .csv format?,eyugsd,harsht07,,https://www.reddit.com/r/dataengineering/comments/eyugsd/how_to_convert_a_big_json_file_to_csv_format/,1.0,20.0,0.0,9965.0,I am a complete noob(student). It's a 12GB .json file. I tried doing it in python with pandas but my system crashes. Help is much appreciated.
1565,2020-02-05 15:14:39,1580908479.0,dataengineering,Data Engineering Wiki,ez9drx,RobDaBigSpoon,,https://www.reddit.com/r/dataengineering/comments/ez9drx/data_engineering_wiki/,1.0,18.0,0.0,9982.0,"I don't see a wiki for this sub-reddit. Or am I just missing it? If not, I think this sub-reddit could use a wiki."
1566,2020-02-05 22:28:00,1580934480.0,dataengineering,Any experienced/senior data engineers have any tips or advice for an senior undergraduate interested in data engineering roles?,ezftno,e2ee_for_all,,https://www.reddit.com/r/dataengineering/comments/ezftno/any_experiencedsenior_data_engineers_have_any/,1.0,11.0,0.0,10000.0,"
In NYC, experience with Java and Python. I have conceptual knowledge of Hadoop and Spark. What should I know/learn for entry-level data engineer roles?"
1567,2020-02-05 22:46:51,1580935611.0,dataengineering,Where to start: Data Analyst or Data Engineer? Ultimate Goal: Data Scientist,ezg4rd,RogerSmithII,,https://www.reddit.com/r/dataengineering/comments/ezg4rd/where_to_start_data_analyst_or_data_engineer/,1.0,20.0,0.0,10000.0,"My background is in economics / finance and I've taken the math needed for an engineering degree (to prep for grad school). I recently learned PostgreSQL via a MOOC. I want to eventually become a Data Scientist but I'm not sure where to start. I also want to have exposure to Data Engineering tasks because I want to know what DE's go through, in case I get a job at a smaller firm. My logic is that the person who is well rounded has a better chance of being employed in a downturn. Given this, does it make sense to learn Data Engineering first and then move onto Data Analysis and then Data Scientist? Is there a better path?"
1568,2020-02-06 16:46:32,1581000392.0,dataengineering,For hire: Talend developer (Remote / Relocation / Gigs),eztby1,navds,,https://www.reddit.com/r/dataengineering/comments/eztby1/for_hire_talend_developer_remote_relocation_gigs/,1.0,5.0,0.0,10014.0,"I don't know if I can post it here but I am looking for a job. For lack of local demands for these skills, I am considering to switch entirely to web development but that would be my last ressort. Advices are welcomed too.

I have 2 years experience in Talend Open Studio for DI and ESB, have worked on the enterprise version for 6 months. 

* Proficient in Linux
* Know enough shell &amp; scripting to be autonomous: Bash, Python, Ruby
* Web development background
* Have been exposed to: Google Cloud Platform, Jenkins, Spring, Odoo, Magento"
1569,2020-02-06 21:06:57,1581016017.0,dataengineering,How do you balance the need for validation and standardisation with the need to support diversity?,ezx5of,aisingiorix,,https://www.reddit.com/r/dataengineering/comments/ezx5of/how_do_you_balance_the_need_for_validation_and/,1.0,0.0,0.0,10020.0,
1570,2020-02-06 23:37:04,1581025024.0,dataengineering,How to handle sensitive data ?,ezzl61,The_Grim_Flower,,https://www.reddit.com/r/dataengineering/comments/ezzl61/how_to_handle_sensitive_data/,1.0,14.0,0.0,10022.0,"Im reading about how to handle sensitive data as a data engineer for a position as a DE for a cyber sec company , any additional info would be great."
1571,2020-02-07 13:44:06,1581075846.0,dataengineering,In Need Of A Senior Data Engineer In Barcelona or Madrid!,f09czh,GregElliot,,https://www.reddit.com/r/dataengineering/comments/f09czh/in_need_of_a_senior_data_engineer_in_barcelona_or/,1.0,4.0,0.0,10035.0,"I am currently growing a team of Data Analysts &amp; Data Scientists and looking for a Senior Data Engineer to lead this team in a really exciting new project.

Message me for more details if you think you could be perfect for this opportunity!"
1572,2020-02-07 23:51:50,1581112310.0,dataengineering,Onsite interview,f0hkt6,BGT_freedom,,https://www.reddit.com/r/dataengineering/comments/f0hkt6/onsite_interview/,1.0,25.0,0.0,10046.0,I have an on-site technical interview for a Data Engineer role coming up and was looking for insight on typical types of questions and what “gotcha” questions people have encountered.
1573,2020-02-09 15:50:50,1581256250.0,dataengineering,Not getting a good grasp of CIF architecture,f18v6t,Filmboycr,,https://www.reddit.com/r/dataengineering/comments/f18v6t/not_getting_a_good_grasp_of_cif_architecture/,1.0,2.0,0.0,10108.0,"i am currently trying to make a basic OLAP for a fictional beer company, but the problem is that i have to use the Inmon Corporate Information Factory and i have looked it up but i am not really getting a good grasp or i'm not sure of what it really is.

From what i have understand, the CIF architecture in a basic form is a regular 3NF database model with data that you can load to those tables and you create the cube with it for example with SQL Server.

Thanks"
1574,2020-02-09 17:34:28,1581262468.0,dataengineering,Using ODBC to connect any database directly to Jupyter notebook.,f1a68q,gauravc2796,,https://www.reddit.com/r/dataengineering/comments/f1a68q/using_odbc_to_connect_any_database_directly_to/,1.0,0.0,0.0,10111.0,
1575,2020-02-09 20:51:32,1581274292.0,dataengineering,Upskill as Data Engineer,f1d235,vanthar686,,https://www.reddit.com/r/dataengineering/comments/f1d235/upskill_as_data_engineer/,1.0,6.0,0.0,10118.0,"Hi folks,

I've spent quality time in data platform teams from past 3 years (5+ years of exp in total). I've worked on ingestion and ETL pipelines involving streams and batches. I've designed and developed rule engine to work on near real time events. I'm more of a developer than an analyst. I'm proficient in Java. I'm considering to upskill myself in my free time. Could you guys suggest me the things that I need to focus on in short term and Long term to stay in the game of competitive career."
1576,2020-02-09 22:05:54,1581278754.0,dataengineering,Scaling Flink Timers,f1e6zd,windyslide,,https://www.reddit.com/r/dataengineering/comments/f1e6zd/scaling_flink_timers/,1.0,0.0,0.0,10119.0,
1577,2020-02-09 23:09:36,1581282576.0,dataengineering,Poor man full-text search with django and postgres,f1f5p3,voorloopnul,,https://www.reddit.com/r/dataengineering/comments/f1f5p3/poor_man_fulltext_search_with_django_and_postgres/,1.0,0.0,0.0,10121.0,
1578,2020-02-09 23:11:41,1581282701.0,dataengineering,Poor man's full-text search with django and postgres,f1f6td,voorloopnul,,https://www.reddit.com/r/dataengineering/comments/f1f6td/poor_mans_fulltext_search_with_django_and_postgres/,1.0,0.0,0.0,10121.0,
1579,2020-02-10 18:36:29,1581352589.0,dataengineering,Cherre Series A Funding Round Announcement!,f1stzw,jdCherre,,https://www.reddit.com/r/dataengineering/comments/f1stzw/cherre_series_a_funding_round_announcement/,1.0,0.0,0.0,10140.0,
1580,2020-02-10 18:42:25,1581352945.0,dataengineering,An interview about the data vault method of data modeling and how it simplifies integrating the evolving data sources that you are dealing with in your enterprise data warehouse,f1sx65,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/f1sx65/an_interview_about_the_data_vault_method_of_data/,1.0,0.0,0.0,10140.0,
1581,2020-02-10 20:14:00,1581358440.0,dataengineering,Love this no frills publication for Python links &amp; news,f1u9ug,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/f1u9ug/love_this_no_frills_publication_for_python_links/,1.0,0.0,0.0,10143.0,
1582,2020-02-10 20:34:20,1581359660.0,dataengineering,(AWS) Moving data from postgresql to redshift in (near) real-time?,f1uky9,themikep82,,https://www.reddit.com/r/dataengineering/comments/f1uky9/aws_moving_data_from_postgresql_to_redshift_in/,1.0,18.0,0.0,10143.0,"I can write and schedule a batch job using a few different tools to do this, but batch jobs have the issue of data becoming stale almost immediately. Any suggestions on a broader strategy to approach this? Don't need the nitty-gritty, just a bit of a north star to start planning a strategy.

Thanks!"
1583,2020-02-11 15:23:30,1581427410.0,dataengineering,How much data engineering can be learnt at home?,f2893x,LetoileBrillante,,https://www.reddit.com/r/dataengineering/comments/f2893x/how_much_data_engineering_can_be_learnt_at_home/,1.0,10.0,0.0,10165.0,"I am a noob at data engineering, and more of a scientist. I wish to be more hands-on and get into data engineering. My question is: how much is it possible to learn from home with a Macbook Air. Asking this because data engg. is all about dealing with big volumes of data which are found at an industry scale. Things like algorithms and python coding can be learnt from home as they are only about logic. Does the same apply to big data applications?"
1584,2020-02-11 17:58:21,1581436701.0,dataengineering,Interesting questions and answers on big data modeling (with Cloudera supported tools bias),f2aacq,sib_n,,https://www.reddit.com/r/dataengineering/comments/f2aacq/interesting_questions_and_answers_on_big_data/,1.0,0.0,0.0,10168.0,
1585,2020-02-11 20:08:17,1581444497.0,dataengineering,How to assign withColumn based on value of multiple columns,f2c8p4,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/f2c8p4/how_to_assign_withcolumn_based_on_value_of/,1.0,4.0,0.0,10170.0,"I have a pyspark dataframe (cannot convert to pandas as it is huge) and I need to create a new column and assign a value of ""y"" or ""n"" based on whether any of 10 columns  xval1....xval10 contain the string '48'. How would I do this?"
1586,2020-02-11 20:45:50,1581446750.0,dataengineering,Data Engineering Twitter Accounts?,f2ctzj,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/f2ctzj/data_engineering_twitter_accounts/,1.0,14.0,0.0,10170.0,"For those of you that are active on Twitter, what Twitter accounts do you follow that provide updates or useful insights within the data engineering world? Or, do you have a Twitter account that you use to post insight/updates relating to data engineering? I recently got more involved on Twitter and I'd really like to start engaging with other data engineering folks on there."
1587,2020-02-11 22:31:23,1581453083.0,dataengineering,crash course in Eclipse/Scala/Spark workflow?,f2egpo,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/f2egpo/crash_course_in_eclipsescalaspark_workflow/,1.0,0.0,0.0,10171.0,"Hi. I work on a data team develops Scala in Eclipse, exports JARs to an EMR cluster, and then runs Spark jobs. 

Anyone know of a good resource to quickly learn the work flow or development cycle? Because they are different components, it's hard to find a tutorial that neatly combines them all. 

I have gotten bits and pieces from teammates and Udemy courses, learned basic Scala and get the gist of Spark. I'm not a Java programmer so I'm not used to that development life cycle. 

Thanks for any advice."
1588,2020-02-12 02:08:32,1581466112.0,dataengineering,Data lake on AWS,f2huau,mrcool444,,https://www.reddit.com/r/dataengineering/comments/f2huau/data_lake_on_aws/,1.0,8.0,0.0,10175.0,"Hello All,

I am interested in knowing how you guys built your datalake on AWS.
What technologies did you use to ingest various sources?
Our source files are csv, JSON and Parquet and we are ingesting to raw and curated.

Thanks,
Mc"
1589,2020-02-12 05:15:18,1581477318.0,dataengineering,Data engineer,f2kip8,chay1308,,https://www.reddit.com/r/dataengineering/comments/f2kip8/data_engineer/,1.0,7.0,0.0,10174.0,What are the mandatory skills to become a data engineer
1590,2020-02-12 20:57:14,1581533834.0,dataengineering,Hiring a Data Engineering contractor,f2w2rl,MattDamonsTaco,,https://www.reddit.com/r/dataengineering/comments/f2w2rl/hiring_a_data_engineering_contractor/,1.0,23.0,0.0,10195.0,"My company and DS team has a need for a long-term data engineering contractor. The company has existing relationships with some offshore vendors (about whose employees I have received mixed reviews) and I'm wondering if there are other reputable vendors with whom any of you have either a. worked with in the past or b. currently work with. 

I am not in HR and would certainly prefer to just do work and manage my team; right now, managing my team means hiring a contractor and I would *love* some guidance on who has great engineer contractors."
1591,2020-02-12 22:04:13,1581537853.0,dataengineering,Studying Data Analysis/Engineering Concurrently,f2x4fd,MrFourSeasons,,https://www.reddit.com/r/dataengineering/comments/f2x4fd/studying_data_analysisengineering_concurrently/,1.0,0.0,0.0,10199.0,
1592,2020-02-13 01:38:01,1581550681.0,dataengineering,Big Data Architectures Survey,f30fwy,glpaz,,https://www.reddit.com/r/dataengineering/comments/f30fwy/big_data_architectures_survey/,1.0,0.0,0.0,10203.0,"I'm  going to write a survey about Big Data architectures, but I don't know  if that would be useful or how difficult it would be. I mean, is there a  science behind architectural choices?

In  traditional software development we have a lot of books, papers and  protocols on software architecture, but it isn't true on Big Data  architectures. I see a lot of articles and posts about the Lambda and  Kappa architectures, but how exactly do big companies choose their Big  Data architectures?

Where can I find references and articles about that? Where do you find that?

Thanks."
1593,2020-02-13 07:18:23,1581571103.0,dataengineering,All things GCP: Machine Learning Decision pyramid - Understand which Google Cloud tools matches best for you.,f34ym4,gauravc2796,,https://www.reddit.com/r/dataengineering/comments/f34ym4/all_things_gcp_machine_learning_decision_pyramid/,1.0,0.0,0.0,10209.0,
1594,2020-02-13 09:00:41,1581577241.0,dataengineering,When can I start data engineering on my iPhone. ex. Manage data warehouse and query from my iPhone,f363iz,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/f363iz/when_can_i_start_data_engineering_on_my_iphone_ex/,1.0,0.0,0.0,10216.0,
1595,2020-02-13 09:01:29,1581577289.0,dataengineering,Anyone build a data warehouse of their mobile application data from their iPhone ?,f363ty,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/f363ty/anyone_build_a_data_warehouse_of_their_mobile/,1.0,0.0,0.0,10216.0,
1596,2020-02-13 09:20:52,1581578452.0,dataengineering,What are the good data engineering conferences to join?,f36b78,pollyanna__,,https://www.reddit.com/r/dataengineering/comments/f36b78/what_are_the_good_data_engineering_conferences_to/,1.0,0.0,0.0,10216.0,
1597,2020-02-13 12:37:30,1581590250.0,dataengineering,"Understanding ADAM optimization Algorithm, The Easy Way",f384h8,AnishAndTheBoys,,https://www.reddit.com/r/dataengineering/comments/f384h8/understanding_adam_optimization_algorithm_the/,1.0,0.0,0.0,10220.0,
1598,2020-02-13 12:56:21,1581591381.0,dataengineering,Decompose your Monolithic ML Pipeline with a Feature Store,f38akj,jpdowlin,,https://www.reddit.com/r/dataengineering/comments/f38akj/decompose_your_monolithic_ml_pipeline_with_a/,1.0,0.0,0.0,10220.0,"This blog post talks about how data engineering can manage feature pipelines feeding a feature store - and how this is essentially DevOps. While data scientists can run ML pipelines, training models - these pipelines start from the feature store. Both sets of pipelines can run at different cadences. Also,  how you can version data with Apache Hudi in your feature store.

&amp;#x200B;

  [https://www.logicalclocks.com/blog/mlops-with-a-feature-store](https://www.logicalclocks.com/blog/mlops-with-a-feature-store)"
1599,2020-02-13 14:56:41,1581598601.0,dataengineering,DBND - a Python library for building and tracking data pipelines.,f39kt6,decay1775,,https://www.reddit.com/r/dataengineering/comments/f39kt6/dbnd_a_python_library_for_building_and_tracking/,1.0,0.0,0.0,10225.0,
1600,2020-02-13 16:24:39,1581603879.0,dataengineering,"Choosing a batch orchestration tool. Looking into airflow vs glue, open to others.",f3ap4l,doob10163,,https://www.reddit.com/r/dataengineering/comments/f3ap4l/choosing_a_batch_orchestration_tool_looking_into/,1.0,2.0,0.0,10228.0,"At my company, we were using databricks to productionize our pipelines and ETL jobs. After running into some pain points, we realize that this is not going to be scalable or easy to work with in the way that we like. We are doing some research regarding orchestration tools and came up with two candidates for this process - airflow and glue. I was wondering if anyone had experience with both and could tell me the pros and cons of each workflow orchestration tool to better suit my needs (or just in general for arguments sake, perhaps)."
1601,2020-02-13 16:44:57,1581605097.0,dataengineering,An Awesome List of Open-Source Data Engineering Projects,f3az0z,gunnarmorling,,https://www.reddit.com/r/dataengineering/comments/f3az0z/an_awesome_list_of_opensource_data_engineering/,1.0,6.0,0.0,10229.0,
1602,2020-02-13 16:47:33,1581605253.0,dataengineering,"Is Spark widely used in ""Windows-stack"" companies?",f3b0ax,timlee126,,https://www.reddit.com/r/dataengineering/comments/f3b0ax/is_spark_widely_used_in_windowsstack_companies/,1.0,1.0,0.0,10229.0,
1603,2020-02-13 19:10:53,1581613853.0,dataengineering,Can I get a Jr. Data Engineering Job in 3 months?,f3d4nr,MrFourSeasons,,https://www.reddit.com/r/dataengineering/comments/f3d4nr/can_i_get_a_jr_data_engineering_job_in_3_months/,1.0,14.0,0.0,10232.0,"* Graduating in May w/ B.S. Information Technology
* SQL: Can perform basic queries
* Python:  Understand basics + numpy, pandas, matplotlib. I have not done any scripting yet.
* I have basic knowledge of relational databases and their designs as well as a couple management systems.

I'm asking because I am currently studying data analysis but that is not my goal career so if at all possible I would prefer to be an employable Jr. DE than a DA by this summer."
1604,2020-02-13 23:27:46,1581629266.0,dataengineering,"Automating Tax Department's Calculations (Small, static data)",f3h7h4,SgtSlice,,https://www.reddit.com/r/dataengineering/comments/f3h7h4/automating_tax_departments_calculations_small/,1.0,0.0,0.0,10243.0,
1605,2020-02-14 00:25:10,1581632710.0,dataengineering,Best free online resource to learn Data Engineering skills?,f3i3bv,freebird348,,https://www.reddit.com/r/dataengineering/comments/f3i3bv/best_free_online_resource_to_learn_data/,1.0,0.0,0.0,10245.0,
1606,2020-02-14 00:45:32,1581633932.0,dataengineering,Pigeon Hole “DE” Role. Need help on what to do!,f3idxw,AgileFold,,https://www.reddit.com/r/dataengineering/comments/f3idxw/pigeon_hole_de_role_need_help_on_what_to_do/,1.0,7.0,0.0,10246.0,"Hello, I was hired six months ago for a data engineering position. I made sure it wasn’t a BI role that I was previously at. The manager I interviewed with talked a lot about AWS, airflow, python, Azure cloud, redshift that they’re working with. Boy, was I stoked! coming from an SSIS, Microsoft, SQL background, finally getting my foot into DE. 

Into about a couple of months, I realized they do everything they can to get out of coding, the only client that they’re working with used python for Airflow but I won’t be working on that project. They love drag and drop and I’m very disappointed because I’ve been put on projects using SSIS, SQL server, Azure data Factory and lots of SQL and no python. 

Fast forward to now, this is where I think might be the last straw for me and will try to find another job. They are putting me 100% on a very long project to query adhoc data and export it to flat files. It’s not creating data pipelines, it’s not DE, while all my team members get to work on this huge, high visibility project using redshift and python while I get put onto this crap. I get it that I’m the new kid but damn.

I’ve communicated this very well with my manager before they’ve decided to put me on this project that I wanted to pursue DE roles, not being a sql dev or a data extract person. He is well aware of my career aspirations. 

Now, I wanted to ask for opinions on anyone who have been through this situation. How do you get out? I’m working on side python projects and implement python scripting in anything I can  relating to work. What else can I do? Start polishing my resume again? Any advice would be greatly appreciated. I’m really depressed about where this all is headed. Thanks for reading/advice in advance."
1607,2020-02-14 00:49:17,1581634157.0,dataengineering,"Apache Airflow: Variables, Macros and Templating",f3ify1,marclamberti,,https://www.reddit.com/r/dataengineering/comments/f3ify1/apache_airflow_variables_macros_and_templating/,1.0,0.0,0.0,10246.0,
1608,2020-02-14 01:47:29,1581637649.0,dataengineering,Wondering what the best online free resource is to learn Data Engineering skills?,f3j9bs,freebird348,,https://www.reddit.com/r/dataengineering/comments/f3j9bs/wondering_what_the_best_online_free_resource_is/,1.0,3.0,0.0,10248.0,"Hey everyone! I currently work as a technology consultant (basically it’s a buzz word that means we work on consulting projects that require somewhat technical knowledge). Over the past 6 months, I’ve been working as a “Web Data Analyst” for a company that handles millions of site views per day. My role consists of a few things, but a significant portion of it is handling and maintaining ETL pipelines to report on large subsets of data. This includes creating workflows in SQL using our internal data warehouse software.  

I’ve been enjoying my work recently and from my understanding, what I do currently is similar to what a data engineer does. I understand that there are a few skills that I am lacking – for example, I do not know Hadoop. However, I do know some scripting languages such as Python. 

I was wondering which is the best online resource I can use in order to up my skillset to become a full time Data Engineer. I would preferably like my training to be free and reputable to a future employer so I can get a good job within the industry. Are they any good options on Coursera or any similar platform?

It is worth noting that I may be able to get my work to pay for a short training course. For example, if there is a 5 week Hadoop bootcamp I’m sure it’s possible for them to fund it. 

Thanks in advance!"
1609,2020-02-14 05:59:09,1581652749.0,dataengineering,How can I break into data engineering,f3mil8,askewbutton,,https://www.reddit.com/r/dataengineering/comments/f3mil8/how_can_i_break_into_data_engineering/,1.0,11.0,0.0,10253.0,Can anyone give me tips for breaking into data engineering. Currently working as an IT analyst I have experience with python and sql.
1610,2020-02-14 09:05:52,1581663952.0,dataengineering,How Bosch Does Real-time Analytics on IoT Event Streams,f3okga,ssb61,,https://www.reddit.com/r/dataengineering/comments/f3okga/how_bosch_does_realtime_analytics_on_iot_event/,1.0,1.0,0.0,10257.0,
1611,2020-02-14 11:29:04,1581672544.0,dataengineering,How can I get hands on Spark?,f3pu7s,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/f3pu7s/how_can_i_get_hands_on_spark/,1.0,4.0,0.0,10260.0,"Hello. I would like to learn Spark using it for some real case in my company. After searching on Google I have not very clear how could I implement it. 

The typical examples are SQL, Machine Learning, Streaming... But for ML we are good with Sagemaker+Lambda+API Gateway, the ETL is running okay with Redshift and for the streaming we have some Kinesis streams and I don't think we can use Spark Streaming for adding more, because that depends (I guess) on the backend team to implement it and it's not going to happen.

Is there any other possibility than moving some parts of the ETL to Spark? 

Thank you"
1612,2020-02-14 16:47:03,1581691623.0,dataengineering,How would a business incorporate Kafka + pandas with Oracle databases?,f3t60o,pandadata,,https://www.reddit.com/r/dataengineering/comments/f3t60o/how_would_a_business_incorporate_kafka_pandas/,1.0,0.0,0.0,10266.0,
1613,2020-02-14 17:54:34,1581695674.0,dataengineering,Comprehensive Guide To Approximate Nearest Neighbors Algorithms,f3u3dx,eyaltrabelsi,,https://www.reddit.com/r/dataengineering/comments/f3u3dx/comprehensive_guide_to_approximate_nearest/,1.0,0.0,0.0,10266.0,
1614,2020-02-14 23:37:32,1581716252.0,dataengineering,What do you use for data analysis?,f3z8uq,enginerd298,,https://www.reddit.com/r/dataengineering/comments/f3z8uq/what_do_you_use_for_data_analysis/,1.0,12.0,0.0,10271.0,"What do you use for data analysis thats sitting on your data lake/Database/Raw files etc.., is there an efficient way to combine all streams of data into one analytical portal?"
1615,2020-02-15 03:53:04,1581731584.0,dataengineering,Airflow for managing transient clusters on EMR,f42mqr,lovestodonothing,,https://www.reddit.com/r/dataengineering/comments/f42mqr/airflow_for_managing_transient_clusters_on_emr/,1.0,0.0,0.0,10275.0,
1616,2020-02-15 07:32:07,1581744727.0,dataengineering,Building Big Data Pipelines with PySpark + MongoDB + Bokeh,f454gv,Edwinb60,,https://www.reddit.com/r/dataengineering/comments/f454gv/building_big_data_pipelines_with_pyspark_mongodb/,1.0,0.0,0.0,10282.0,
1617,2020-02-15 11:18:01,1581758281.0,dataengineering,Recommendations for good professional training courses?,f4758p,ManBearHybrid,,https://www.reddit.com/r/dataengineering/comments/f4758p/recommendations_for_good_professional_training/,1.0,14.0,0.0,10286.0,"My company hired me as a data engineer, with a tiny bit of actual DS/ML mixed in. 
Everything I know about this field is self-taught - my real background is in mechanical engineering, but I have been working as a coder for about 2 or 3 years now. I was supposed to be working under the senior data engineer, but he recently resigned. It looks like the company is hoping that I'll step into his shoes.

My company has asked that I look into training courses, since exec has allocated a generous budget for professional development.

Does anyone have any recommendations?

For added info, I work in med-tech, so data security and governance are important. For the last few months, I've been working on developing a template for producing serverless web tools within the AWS ecosystem, for consumption within our company. These are largely to support other teams, such as Manufacturing and or Quality. In the near future, we will be building customer-facing applications, obviously dealing with patient data. So best practices around scalability, security and prevention of data leakage are important to us."
1618,2020-02-15 16:27:52,1581776872.0,dataengineering,How are jobs chained together in MapReduce?,f4a2ra,timlee126,,https://www.reddit.com/r/dataengineering/comments/f4a2ra/how_are_jobs_chained_together_in_mapreduce/,1.0,0.0,0.0,10290.0,
1619,2020-02-15 17:49:19,1581781759.0,dataengineering,How many of you struggle with drafting a DE-focused resume?,f4b4rk,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/f4b4rk/how_many_of_you_struggle_with_drafting_a/,1.0,1.0,0.0,10292.0,"This is primarily for those that want to jump into data engineering, but obviously feel free to chime in if you're not in that category.

I'm still pulling together my first tutorial that I believe will help serve as a starting point for those who want to jump into data engineering, so hopefully that will be released pretty soon. I recently wondered if those who want to jump into data engineering have struggles with creating a DE-oriented resume as well, especially if you're trying to transition from a non-tech field (which was my situation). I could easily pull together a comprehensive guide on things to highlight on the resume that ""translate"" into DE-related skills, and I could perhaps offer reviews/feedback of resumes as well (obviously the latter part wouldn't be free).

Thoughts? Would tips for enhancing LinkedIn profiles be helpful as well? Any other areas relating to professional development that people seem to struggle with as they try to transition into another field?"
1620,2020-02-16 00:50:22,1581807022.0,dataengineering,"what differences and similarities are between ""dataflow engines"" and ""stream processing systems""?",f4h7dy,timlee126,,https://www.reddit.com/r/dataengineering/comments/f4h7dy/what_differences_and_similarities_are_between/,1.0,1.0,0.0,10297.0,
1621,2020-02-16 13:09:07,1581851347.0,dataengineering,"Differences between datawarehouse, datamart and OLAP cube",f4pbds,Luukv93,,https://www.reddit.com/r/dataengineering/comments/f4pbds/differences_between_datawarehouse_datamart_and/,1.0,1.0,0.0,10314.0,
1622,2020-02-16 18:12:43,1581869563.0,dataengineering,Which framework should I use? streaming app reading(JSON) data from stdIn,f4spwc,sam_butt,,https://www.reddit.com/r/dataengineering/comments/f4spwc/which_framework_should_i_use_streaming_app/,1.0,6.0,0.0,10316.0,"I'm having a bit of difficulty in choosing a framework on how to go with a problem at hand.

The application should be streaming. And the data(JSONS) provided to it would be from stdin

&gt;e.g authorize &lt; operations

Furthermore, there's some functionality that requires some window operation. e.g. do something with the incoming data over 5 minutes span and don't have to use a database and rather an in-memory data structure.

I'm thinking of doing it in Scala but I can do it in either Scala/Python.

* The first thing that comes to my mind is Spark/Pandas because of Streaming, Window functionality, and data frames as the in-memory data structure.
* Also thought about scalas fs2 for streams and circle for JSON

**Any suggestions on how to go about it? What framework can be used or not at all?**

And if Spark, How to read data from StdIn?

And is it still in memory when in SparkSQL we do createorReplaceTable("""")?

Thanks"
1623,2020-02-16 20:16:41,1581877001.0,dataengineering,Spark slow read from MS SQL Server,f4ukix,pistonman94,,https://www.reddit.com/r/dataengineering/comments/f4ukix/spark_slow_read_from_ms_sql_server/,1.0,0.0,0.0,10317.0,
1624,2020-02-17 08:02:22,1581919342.0,dataengineering,What does your data eng team spend their time on?,f548yf,PelicanIO,,https://www.reddit.com/r/dataengineering/comments/f548yf/what_does_your_data_eng_team_spend_their_time_on/,1.0,6.0,0.0,10326.0,"In our case, I find we spend a lot more time dealing with bugs and feature requests from data scientists than what I imagine the average team does. Internally we're split on this- most of us want to spend more time on tooling and cost optimization, reduce reactivity etc, but I don't see a clear way forward to do that. 

Is this normal? What are other people doing to move away from reactivity and more towards tooling?"
1625,2020-02-17 15:11:58,1581945118.0,dataengineering,Can we set default schemas for AWS Redshift roles to virtually achieve different databases under ONE AWS Redshift instance?,f58jjw,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/f58jjw/can_we_set_default_schemas_for_aws_redshift_roles/,1.0,0.0,0.0,10335.0,
1626,2020-02-17 15:16:18,1581945378.0,dataengineering,Best DE Books,f58le7,randm95,,https://www.reddit.com/r/dataengineering/comments/f58le7/best_de_books/,1.0,15.0,0.0,10335.0,"Hello,

I am a junior DE (\~1.5 years exp), and I am wondering what are the best books about Data Engineering.

&amp;#x200B;

I am currently reading **Designing Data-Intensive Applications** which I discovered from another post on this subreddit.

&amp;#x200B;

Do you recommend another one?

&amp;#x200B;

Thank you very much :)"
1627,2020-02-18 14:15:35,1582028135.0,dataengineering,Data Lake buckets: store latest version of records only in bucket or include timestamp in name to keep history?,f5qwhj,today_is_tuesday,,https://www.reddit.com/r/dataengineering/comments/f5qwhj/data_lake_buckets_store_latest_version_of_records/,1.0,5.0,0.0,10361.0,"I'm working for an insurance company who have policy and quote information in a Mongo DB. Both quotes and policies can be modified over time and no history of changes is kept in the datasets I'm using. When a record in Mongo is created or changed I copy it to a bucket as a json then insert it into an append only landing table in BigQuery, our data warehouse.

&amp;#x200B;

The thing is that because I use the record ID as a filename when uploading to the bucket, if a record is modified it just overwrites the file already in the bucket. This means that the landing table is building up a history of changes, but the bucket only has the latest version of the files.

&amp;#x200B;

The history of changes is useful for reporting but if I want to change the schema of the table and keep the history I have to create a query that'll transform the table from the old schema to the new. This is time consuming for me as there's a lot of nested data and the schema changes are relatively frequent. It'd be a lot easier for me to just delete the table and reprocess all the files in the bucket into it.

&amp;#x200B;

So I was considering uploading the Mongo jsons to the bucket with a name of \`&lt;ID&gt;\_&lt;timestamp&gt;.json\`.   


I'm wondering is it normal to do this? It feels odd to keep a history of changes outside the source database so I'm unsure if this is a bad pattern. Storing a history in the data warehouse, but not the data lake feels odder still though.  


If anyone had advice or knows useful resources that'd be much appreciated. I'm struggling to find anything in my googling."
1628,2020-02-18 18:26:58,1582043218.0,dataengineering,Distributed Data for Microservices — Event Sourcing vs. Change Data Capture · Debezium,f5ubbl,gunnarmorling,,https://www.reddit.com/r/dataengineering/comments/f5ubbl/distributed_data_for_microservices_event_sourcing/,1.0,0.0,0.0,10366.0,
1629,2020-02-18 18:35:38,1582043738.0,dataengineering,Can someone recommend a great resource for learning Data Governance primer and best practice?,f5ugff,SgtSlice,,https://www.reddit.com/r/dataengineering/comments/f5ugff/can_someone_recommend_a_great_resource_for/,1.0,5.0,0.0,10367.0,"I am currently working for a tax department in a large Multinational and their data governance and controls are terrible. I can automate a few simple processes for them, but without a strong governance framework it will be like putting a band aid over a large wound.

Can anyone recommend some online resources or books on data governance?

Thanks"
1630,2020-02-18 18:36:41,1582043801.0,dataengineering,Checking Table Updates,f5uh06,DuckDuckFooGoo,,https://www.reddit.com/r/dataengineering/comments/f5uh06/checking_table_updates/,1.0,5.0,0.0,10367.0,Is there a best practice for finding out when tables have been updated? Is it specific to each type of database? Do you use metadata? Do you query the table for time stamps? Is this even a data engineers job or what title would this most closely align with?
1631,2020-02-18 21:35:43,1582054543.0,dataengineering,An interview with Snowplow Analytics tech lead about how they manage data infrastructure for streaming events across multiple clouds,f5xbv0,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/f5xbv0/an_interview_with_snowplow_analytics_tech_lead/,1.0,0.0,0.0,10372.0,
1632,2020-02-18 23:02:17,1582059737.0,dataengineering,Data Vault integrated with a Data Lake - thoughts,f5ypmv,AndresAngel,,https://www.reddit.com/r/dataengineering/comments/f5ypmv/data_vault_integrated_with_a_data_lake_thoughts/,1.0,8.0,0.0,10375.0,"I have been exploring this new data modeling Data Vault 2 and how this can easily get along with Data Lakes. Ideally the Data Vault design might require a raw stage to maintain the natural state of the data before any ETL/ELT and keep the 100% reliability on data pulled from any source.

&amp;#x200B;

[Data Vault 2 architecture](https://preview.redd.it/tom4ottkwqh41.png?width=2054&amp;format=png&amp;auto=webp&amp;s=f115e961de82d8e9f61445135f437d5629ae9de2)

But writing this down over the Data Lake concept my first layer it's also a raw data before bring the data into a curated area to be consumed by the Data Vault/Tranditional DWH model. I feel here we could have a redundancy on design and likely the right approach to refactor this is maintain a single raw layer on the data lake and push my DV to start from the raw vault and consume the Data Lake raw stage to built Hubs , Links and Satellites from Raw Vault.

&amp;#x200B;

[Data Lake - Data Vault with only one raw stage](https://preview.redd.it/9xlj6bu3yqh41.png?width=1932&amp;format=png&amp;auto=webp&amp;s=1ef6dfc4031e411bb3c6f814ffbb14a02b1bbe5f)

Would my assumption make sense?"
1633,2020-02-18 23:52:09,1582062729.0,dataengineering,In memory geospatial querying...,f5zi9f,romanX7,,https://www.reddit.com/r/dataengineering/comments/f5zi9f/in_memory_geospatial_querying/,1.0,0.0,0.0,10377.0,
1634,2020-02-19 01:46:18,1582069578.0,dataengineering,Anybody here switch from data scientist to data engineer? I'm considering making the switch and would love to hear from others who have traveled this path!,f61b9a,gerradisgod,,https://www.reddit.com/r/dataengineering/comments/f61b9a/anybody_here_switch_from_data_scientist_to_data/,1.0,0.0,0.0,10382.0,
1635,2020-02-19 02:32:11,1582072331.0,dataengineering,ETL monitoring dashboard,f61zl1,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/f61zl1/etl_monitoring_dashboard/,1.0,34.0,0.0,10384.0,"Does anyone here use a dashboard to monitor ETL jobs? Either a vendor or homegrown solution?

I joined a new company in October and it's amazing to me that we do not have a solution ready to answer simple questions about our own data processes and pipelines.

Questions like:

1. How many jobs ran successfully yesterday?
2. How many rows were written?
3. How is our data growing? (size of tables)
4. What is the average load time?
5. What jobs failed and what was the reason?

We had an outage last week and my director discovered that some tables hadn't been refreshed in a few weeks and he couldn't answer questions to leadership about what was going on. We have notifications set up through Slack but not a solution that brings together the entire process.

Has anyone else gone through this exercise? Any advice or thoughts on getting this organized and off the ground. I'm going to volunteer to take this project on."
1636,2020-02-19 21:43:41,1582141421.0,dataengineering,How to find duplicate based upon multiple columns in a rolling window in pandas?,f6g5ek,sam_butt,,https://www.reddit.com/r/dataengineering/comments/f6g5ek/how_to_find_duplicate_based_upon_multiple_columns/,1.0,6.0,0.0,10399.0,"Also, posted it on SO

[https://stackoverflow.com/questions/60285964/how-to-find-duplicate-based-upon-multiple-columns-in-a-rolling-window-in-pandas](https://stackoverflow.com/questions/60285964/how-to-find-duplicate-based-upon-multiple-columns-in-a-rolling-window-in-pandas)

As the data is streaming. I want to check if a duplicate record(based upon some columns) arrives withing two minutes so I discard it as and do no processing on it. print it as a duplicate.

I have tried a variety of things to no avail.

Any help would be appreciated.  
Thanks"
1637,2020-02-19 23:08:32,1582146512.0,dataengineering,Is it worth using PySpark for writing 50 rows/min?,f6hf87,JumpyCookie,,https://www.reddit.com/r/dataengineering/comments/f6hf87/is_it_worth_using_pyspark_for_writing_50_rowsmin/,1.0,8.0,0.0,10401.0,"Hey guys, I need to write an ETL for taking data from Azure (through their REST API) and putting it into a separate Postgres db. The data we would write would be around 50 rows per minute. Wondering whether I should be learning/using PySpark for this for speed, or should I just use the general python SQL API. Thanks for any help!"
1638,2020-02-19 23:45:37,1582148737.0,dataengineering,Looking for fast and efficient start with Spark,f6hyuo,sebigboss,,https://www.reddit.com/r/dataengineering/comments/f6hyuo/looking_for_fast_and_efficient_start_with_spark/,1.0,4.0,0.0,10402.0," Hi everyone,

I'm currently in transition for a more technical role at my company and one thing that seems very useful and close to my expertise seems to learn more about Apache Spark.  
I'm got a master's in mathematics, phd in social sciences and am self-taught quite proficient with all the python pandas-related stuff as well as some R, Matlab and KNIME. My company employs Spark with scala so I'm going to start more from the data-side and not the implementation (we have scala developers, so no need to try to pile onto somthing that is farther away from my expertise anyways).  
Given that background, I'm not too keen on starting any random Spark101 course as I fear that it will waste my time with a lot of stuff that I A) might have already understood or B) am not afraid of anyways because mathematical complexity is a non-issue for me.  
Do you have any (paid or free) recommendations where to kickstart hard into understanding the Spark way of doing things?

  
Thanks in advance!!!"
1639,2020-02-20 16:30:46,1582209046.0,dataengineering,How to combine fact tables?,f6tuvk,Darth_Data1,,https://www.reddit.com/r/dataengineering/comments/f6tuvk/how_to_combine_fact_tables/,1.0,11.0,0.0,10414.0,"I was tasked with creating some proofs of concepts for some BI solutions at my company. I want to start with creating OLAP cubes in a data warehouse, of sorts. A project I want to focus on will use a Produced Quantity and a Returned Quantity given a date range. Both of these, I thought, would be two different fact tables; however, I want to show them both in the same table along with a Return Rate (Returned / Produced Quantities). Does anyone have any suggestions of how to go about doing this? Do I need to create one fact table that shows a Produced Quantity and Returned Quantity along with the Returned Rate, or do I need to create two separate fact tables and somehow relate them to one another and do the calculation in a BI tool (If so, how?)?"
1640,2020-02-20 18:27:49,1582216069.0,dataengineering,Tips and advice on getting entry level data engineering job?,f6viet,pcorn81,,https://www.reddit.com/r/dataengineering/comments/f6viet/tips_and_advice_on_getting_entry_level_data/,1.0,4.0,0.0,10414.0,"I’ve been following this sub for around a year now and I realize I would really like to enter the data engineering world now. From what I’ve seen on this sub, it seems like most data engineers are software engineers that have focused specifically on data flows and processes. Would anyone be willing to critique me and advise me how to make the leap?

A little about me:
* 2 years exp in tech implementation in IT consulting firm, but not a software engineer
* Masters in engineering (non SWE)
* Currently working on portfolio project for sports analytics. Created a script to scrape football data, clean, and visualize. Originally just a pure python project, but now trying to make a website out of it to be able to show anyone - so now learning how to use flask, heroku, Postgres, and other tools involved in deploying a small website
* Familiar with SQL, **very** comfortable with python now. However, all my programming skills are from googling and YouTubing everything for the last 5 years, so I do have a bit of imposter syndrome here, because I’ve never written production level code. No AWS exp but currently working on solutions architect cert

I would really love to get into this field but I find myself being paralyzed because I don’t even know where to start. Any and all constructive criticism would be appreciated!"
1641,2020-02-20 22:46:57,1582231617.0,dataengineering,Do you write test cases for each of your data pipelines?,f6zg1m,x1084,,https://www.reddit.com/r/dataengineering/comments/f6zg1m/do_you_write_test_cases_for_each_of_your_data/,1.0,0.0,0.0,10417.0,
1642,2020-02-20 22:50:30,1582231830.0,dataengineering,Change-request mechanic for structured data,f6zhzb,amirouche,,https://www.reddit.com/r/dataengineering/comments/f6zhzb/changerequest_mechanic_for_structured_data/,1.0,0.0,0.0,10417.0,
1643,2020-02-21 00:32:21,1582237941.0,dataengineering,ETL with NiFi?,f712qb,infazz,,https://www.reddit.com/r/dataengineering/comments/f712qb/etl_with_nifi/,1.0,0.0,0.0,10417.0,
1644,2020-02-21 01:06:03,1582239963.0,dataengineering,Is anyone using AWS Step Functions for data engineering workflows?,f71kp5,soamv,,https://www.reddit.com/r/dataengineering/comments/f71kp5/is_anyone_using_aws_step_functions_for_data/,1.0,9.0,0.0,10418.0,"Hey all, Step Functions seems like a good workflow tool.  I was wondering if anyone's around here is using it for driving ETL or other data engineering workflows.  If you use it, how's your experience with it so far?  If you considered it but chose not to use it, why?"
1645,2020-02-21 03:00:57,1582246857.0,dataengineering,Skills relevant to data engineering,f735s1,Ckothwal,,https://www.reddit.com/r/dataengineering/comments/f735s1/skills_relevant_to_data_engineering/,1.0,4.0,0.0,10420.0,"Hi guys,

I am a grad student and have taken several courses related to data engineering like Big Data that made me learn about the Hadoop and the spark ecosystem of tools. Though we didn't have huge data sets to deal with, I have learn what each tool in the ecosystem does. But, I am confused on what are tools and concepts are used in the industry?. And what kind of projects can I put up on my resume to get into Junior data engineering jobs? I am comfortable with python (Django) and SQL as well."
1646,2020-02-21 05:07:19,1582254439.0,dataengineering,Tips for landing a 2 Month internship,f74t7k,Mhayc_en,,https://www.reddit.com/r/dataengineering/comments/f74t7k/tips_for_landing_a_2_month_internship/,1.0,2.0,0.0,10422.0,"Hello fellow seniors , i'm currently a second year Data Engineering student (Bachelor's Degree) , it's mandatory to pass an internship of 2-3 months during the summer 2020 , i was wondering about the requirements or some tips on what to include in my resume so i can land an internship .
A good internship will be my launching point , i'm really looking to learn from it alot since all i have seen during school is merely theoretical , my current skill chart doesn't look so promising but there is much room for progress:

2 Scala projects : 
- A Library that generates JSON data files ( Page View events) and (Recommention click events).
- A program that transfroms a database from SQL server to Mongodb server (Both hosted on Virtualbox locally ).

Hadoop : Programmed a WordCount for a text file
Spark : used pyspark for a cleaning task 
I know how to setup hadooo on virtual-machine. 

I'm looking for advice on what to do as a project next, so i can land the internship and please if anyone here is on the look for trainees i'll be more than happy to pass any kind of assignments . I feel like the Data Engineering field is made for people who already have work experience that know how data is managed traditionally and conventionally , for this reason i highly value the chance that internships give !

Thank you in advance."
1647,2020-02-21 05:26:01,1582255561.0,dataengineering,What are my chances of getting a Junior Data Engineering job? Also are there lot of remote Data Engineer Jobs out there?,f751ws,astropydevs,,https://www.reddit.com/r/dataengineering/comments/f751ws/what_are_my_chances_of_getting_a_junior_data/,1.0,10.0,0.0,10421.0,"I’m currently working as a Data Analytics and Database Admin for a big company in California. My daily work usually consists of doing SQL (PostgreSQL) to create views, validating stuff and using Python once in a while to create scripts. I don’t do any database setups or anything, just sql work. We also have data engineers so I don’t do any ETL kind of work. I don’t really do any analytics either although my title has it, since my boss does most of the analytics and my coworker does the reporting on SAS. 

I would like to transition into something more technical and something I can work remote in the future as I would like to travel. I do have a degree in STEM, basically physics. I’ve learned that Spark is the big tool being used for ETL and although I don’t know it, I’m planning on learning it. Would my background be enough to transition to a Junior Data Engineering role or would I need to get Spark and AWS down before applying for jobs? Also are there lot of remote work available for data engineering so I can transition to a Remote Data Engineer role within few years? 

Any advices or recommendations would be appreciated."
1648,2020-02-21 05:42:55,1582256575.0,dataengineering,Dimensionalizing,f759tj,tpedar50,,https://www.reddit.com/r/dataengineering/comments/f759tj/dimensionalizing/,1.0,0.0,0.0,10421.0,
1649,2020-02-21 17:02:44,1582297364.0,dataengineering,Is SMACK the most popular data engineering tech stack?,f7caqy,timlee126,,https://www.reddit.com/r/dataengineering/comments/f7caqy/is_smack_the_most_popular_data_engineering_tech/,1.0,0.0,0.0,10432.0,
1650,2020-02-21 21:49:11,1582314551.0,dataengineering,AWS Glue Crawler - Read single column file,f7gini,nariver1,,https://www.reddit.com/r/dataengineering/comments/f7gini/aws_glue_crawler_read_single_column_file/,1.0,1.0,0.0,10433.0,"Hi there!

I would like to know if anyone had succeed in create use AWS Glue Crawler with a file with ""single column"". This file has to be split by position and has no delimiter.

We tried to create a custom classifier with no luck.

Thank you!"
1651,2020-02-21 22:08:23,1582315703.0,dataengineering,Roles in a data engineering team?,f7gt05,tucanotucano,,https://www.reddit.com/r/dataengineering/comments/f7gt05/roles_in_a_data_engineering_team/,1.0,1.0,0.0,10433.0,
1652,2020-02-21 22:09:19,1582315759.0,dataengineering,Is Google Cloud Data Engineering Certificate worth a try to transition into DataEngineering?,f7gthm,ptndoss,,https://www.reddit.com/r/dataengineering/comments/f7gthm/is_google_cloud_data_engineering_certificate/,1.0,5.0,0.0,10433.0,"Hello All,

Am a Master students(Software Engineering) in US with 5yrs experience in ETL development(informatica). I would like to get into Data Engineering field. I don’t have experience working in Big Data. Have done academic projects in AWS and GCP. My questions here is, does Google Cloud Data Engineer help me to transition to Data Engineering role? I am in my final semester and confused whether to prepare myself to be a Software Engineer(Data Structure and Algo) or Data Engineering(GCP Data Engineering certification)

Looking for your valuable views."
1653,2020-02-21 22:54:04,1582318444.0,dataengineering,"I interviewed the technical founder and CEO of Vector Space AI, Kasian Franks on AI/ML, data, blockchain technology, crypto, startups, and more (podcast)",f7hi18,Teshercohen,,https://www.reddit.com/r/dataengineering/comments/f7hi18/i_interviewed_the_technical_founder_and_ceo_of/,1.0,2.0,0.0,10437.0,"I interviewed the technical co founder and CEO of Vector Space AI, Kasian franks on developing On-Demand Correlation Matrix Datasets for Hidden Relationship Detection in Data &amp; Training in Artificial Intelligence (AI) Systems, blockchain technology, NLP/NLU, and more!

I believe the community may find the conversation to be interesting as we discuss various topics that tie into artificial intelligence, blockchain, and machine learning. Kasian Franks is a serial tech entrepreneur with decades of experience in the bio and life science industries as a computer engineer.

I hope everyone enjoys the conversation!

[Libsyn](http://directory.libsyn.com/episode/index/id/10467551) // [iTunes](https://podcasts.apple.com/us/podcast/1-kasian-franks/id1465687933?i=1000444105556) // [Google Play](https://play.google.com/music/m/Dlgsqozn2ax2uyg2zrjr7fibxqy?t=1_Kasian_Franks-All_Things_Interesting) // [Spotify](https://open.spotify.com/episode/25LFf48ku301iyibw53RBh?si=AiLU0mpqTVuK4MTU9kIOiA) // [Overcast](https://overcast.fm/+Sm_1Kd7Uk)

The episode is also available on most platforms as well."
1654,2020-02-22 00:35:51,1582324551.0,dataengineering,Processing 1 billion records locally from Hive metastore(parquet format) takes forever 6 hours. How to speed it up?,f7j0ur,sam_butt,,https://www.reddit.com/r/dataengineering/comments/f7j0ur/processing_1_billion_records_locally_from_hive/,1.0,0.0,0.0,10439.0,
1655,2020-02-22 01:38:42,1582328322.0,dataengineering,List of non-English subreddits - Used to filter out non-English Reddit content in data projects,f7jxcg,berzark,,https://www.reddit.com/r/dataengineering/comments/f7jxcg/list_of_nonenglish_subreddits_used_to_filter_out/,1.0,0.0,0.0,10439.0,
1656,2020-02-22 04:07:55,1582337275.0,dataengineering,What exactly does a DE do?,f7lvdc,RogerSmithII,,https://www.reddit.com/r/dataengineering/comments/f7lvdc/what_exactly_does_a_de_do/,1.0,28.0,0.0,10440.0,"I've watched many YouTube videos and while I understand the concept, I have yet to see a tutorial of the actual work that is performed. Can someone share what coding in DE looks like? Perhaps a video demonstration?"
1657,2020-02-22 04:49:35,1582339775.0,dataengineering,Ruby pipeline tools?,f7mdn8,SearchAtlantis,,https://www.reddit.com/r/dataengineering/comments/f7mdn8/ruby_pipeline_tools/,1.0,2.0,0.0,10440.0,"Looking at a company that's using map-reduce/java (cloudera) and Ruby for their current processing with a future shift to spark. 

Are there any Ruby based pipeline and orchestration tools? Everything I'm aware of like airflow, Luigi, etc. are python frameworks.

I suspect given the legacy Ruby code base a switch to python is a tough sell. And given the pain of bash scripts I'd rather avoid that if at all possible.

Thanks!"
1658,2020-02-22 05:12:21,1582341141.0,dataengineering,Local setup or cloud is better ?,f7mnid,Ckothwal,,https://www.reddit.com/r/dataengineering/comments/f7mnid/local_setup_or_cloud_is_better/,1.0,12.0,0.0,10441.0,"Hey everyone,

I am a beginner to the field of data engineering. I am now comfortable with several tools such as HDFS, MapReduce, Hive and Pig. However, I have been just using AWS EMR for my practice. Are there any benefits to install the Hadoop ecosystem on my mac and configure them locally or is AWS the best way to go about ?"
1659,2020-02-22 08:06:04,1582351564.0,dataengineering,Currently a Data Engineer need help interviewing for other positions,f7olb5,LordCommanderStannis,,https://www.reddit.com/r/dataengineering/comments/f7olb5/currently_a_data_engineer_need_help_interviewing/,1.0,7.0,0.0,10441.0,"So I started off as a SQL Developer, mostly working with tools such as SSIS and Informatica and using mostly SQL to do ETL. I would say my SQL skills are very good, probably at least a 9/10. Recently my company has started to use Spark and I've learned Scala but someone who didn't major in computer science, what would you recommend for interview prep? All the interviews I've landed so far ask for algorithms during the coding tests."
1660,2020-02-22 19:37:10,1582393030.0,dataengineering,Unit testing pipelines?,f7vs1d,PelicanIO,,https://www.reddit.com/r/dataengineering/comments/f7vs1d/unit_testing_pipelines/,1.0,14.0,0.0,10447.0,"Has anyone does this/are doing this successfully? Are there any tools you are using? How were you able to get other teams to buy into better swe practices for data? It's ongoing problem we are facing and the only hurdle to implementation imo is the work to build it, maybe some cultural changes, and it still doesn't solve the problem of understanding if output changes are good or bad. Thoughts?"
1661,2020-02-22 19:49:37,1582393777.0,dataengineering,Airflow set up with k8s executor,f7vyo5,frank998,,https://www.reddit.com/r/dataengineering/comments/f7vyo5/airflow_set_up_with_k8s_executor/,1.0,0.0,0.0,10448.0,"I am using a PVC to mount dags. The configs don't have examples and I am running to a very strange issue.

    ## name of dags pvc
    dags_volume_claim = dags_pvc

    ## I don't know what below is supposed to be
    ## if PVC is mounted at /opt/app and airflow is in /opt/app/airflow 
    ## and dags should be /opt/app/airflow/dags
    ## then does this sound right?
    dags_volume_subpath = airflow/dags

Please correct me if the above is wrong.

Now the 2nd issue is that on this PVC I have all my dags, parsers and other scripts.
There is a python script on this PVC under airflow directory that is supposed to access a file inside hidden directory on root of PVC.

So for us root of PVC is /opt/app
The file is in /opt/app/.hidden/file1

When my webserver pod is coming up, it complains it cannot access 
    /opt/app/.hidden/file1


Any ideas?

Thanks!"
1662,2020-02-23 00:38:53,1582411133.0,dataengineering,Streaming data changes to a Data Lake with Debezium and Delta Lake pipeline,f805jw,usefulcalamity,,https://www.reddit.com/r/dataengineering/comments/f805jw/streaming_data_changes_to_a_data_lake_with/,1.0,5.0,0.0,10455.0,"A client has approached me with a use case of capturing data changes in multiple instances of a Microservices Application (they run an instance per customer), to update a Data Lake for analytics.  
We took on the challenge to assemble a changed data capture pipeline with Debezium and Delta Lake combo  
High Level Strategy Overview:

* Debezium reads database logs, produces json messages that describe the changes and streams them to Kafka
* Kafka streams the messages and stores them in a S3 folder
* Using Spark with Delta Lake we transform the messages to INSERT, UPDATE and DELETE operations, and run them on the target data lake table. This is the table that holds the latest state of all source databases
* Next we can perform further aggregations on the latest table for analytics

Below is the summary post of the process along with an example project  
Would love any feedback  
1. [https://medium.com/@yinondn/streaming-data-changes-to-a-data-lake-with-debezium-and-delta-lake-pipeline-299821053dc3](https://medium.com/@yinondn/streaming-data-changes-to-a-data-lake-with-debezium-and-delta-lake-pipeline-299821053dc3)  
2. [https://github.com/tikal-fuseday/delta-architecture](https://github.com/tikal-fuseday/delta-architecture)"
1663,2020-02-23 00:46:30,1582411590.0,dataengineering,Sample coding / tutorial for DE,f809g0,RogerSmithII,,https://www.reddit.com/r/dataengineering/comments/f809g0/sample_coding_tutorial_for_de/,1.0,3.0,0.0,10455.0,"Yesterday, I asked the following question: 
https://www.reddit.com/r/dataengineering/comments/f7lvdc/what_exactly_does_a_de_do/ Unfortunately, I think people misinterpreted what I was asking. 

If I wanted to see an example of web development. There are tons of resources on YouTube and elsewhere where I can see what coding in web dev loops like. Here's one for Javascript: https://www.youtube.com/watch?v=hdI2bqOjy3c&amp;t=18m21s 

**I can't find the same thing for DE.** Does anyone have an example of what coding looks like for DE?"
1664,2020-02-23 02:25:24,1582417524.0,dataengineering,Data engineer Vs senior data engineer responsibilities?,f81lgq,pbj800100,,https://www.reddit.com/r/dataengineering/comments/f81lgq/data_engineer_vs_senior_data_engineer/,1.0,9.0,0.0,10456.0,"What would you say is the difference between a data engineer and a sr data engineer? I've been in an engineering role for almost a year and talking to my employer about my next career step... He doesn't believe in promoting just for the sake of promoting, but promotions must come with increased or different responsibilities (makes perfect sense). I'm not interested in managing people and we agreed that's fine, but then I was asked to research what the main difference is for an engineer with 1-2 years experience Vs an engineer with more experience. What has changed for you personally the longer you've been in a role, if anything? All the job descriptions I look at seem to be more or less the same, regardless if it is called a senior engineer or not. TIA"
1665,2020-02-23 13:57:45,1582459065.0,dataengineering,Need Feedback: Developing a design for a search tool that holds millions of records,f88keg,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/f88keg/need_feedback_developing_a_design_for_a_search/,1.0,19.0,0.0,10463.0,"I have a large set of keywords, basically one word or multi-word phrases and millions of them. I want to create a efficient tool to search across them. Here are two approaches I can think of:

1. Store these in SQL DB, shard horizontally, add Full-text Index to the column
2. Store data as text files, basically a million documents, partitioned by hash function and stored in different folders and use the concept of inverted-index, built individually for each of these folders

Which approach seems more logical in terms of implementation and performance?"
1666,2020-02-23 19:54:15,1582480455.0,dataengineering,GitHub - ploomber/ploomber: A workflow management tool to accelerate DS/ML pipeline development,f8cy19,ploomber-io,,https://www.reddit.com/r/dataengineering/comments/f8cy19/github_ploomberploomber_a_workflow_management/,1.0,6.0,0.0,10467.0,
1667,2020-02-23 19:56:14,1582480574.0,dataengineering,How do you deal with DE FOMO?,f8cz34,exact-approximate,,https://www.reddit.com/r/dataengineering/comments/f8cz34/how_do_you_deal_with_de_fomo/,1.0,0.0,0.0,10467.0,
1668,2020-02-24 04:27:58,1582511278.0,dataengineering,Thoughts on Data Catalogues like Alation?,f8kanf,PelicanIO,,https://www.reddit.com/r/dataengineering/comments/f8kanf/thoughts_on_data_catalogues_like_alation/,1.0,8.0,0.0,10475.0,"Has anyone ever used them before? How was it? Is it worth the money?

[https://www.alation.com/](https://www.alation.com/)"
1669,2020-02-24 06:06:44,1582517204.0,dataengineering,What are the qualities of a world class data engineer?,f8ljkl,craicongoing,,https://www.reddit.com/r/dataengineering/comments/f8ljkl/what_are_the_qualities_of_a_world_class_data/,1.0,0.0,0.0,10479.0,"There is obviously no objective measure for this, as people's perception of what is ""world class"" will differ.

However, by your own subjective estimation, what are the qualities that you believe a junior (or senior) data engineer should strive for to become a top notch data engineer?"
1670,2020-02-24 07:04:01,1582520641.0,dataengineering,Data Scientists vs Data Engineers: Which one is for you?,f8m8in,drecklia,,https://www.reddit.com/r/dataengineering/comments/f8m8in/data_scientists_vs_data_engineers_which_one_is/,1.0,2.0,0.0,10481.0,
1671,2020-02-24 15:22:09,1582550529.0,dataengineering,Need help on our server setup!!!!,f8r4qq,dreamer2177,,https://www.reddit.com/r/dataengineering/comments/f8r4qq/need_help_on_our_server_setup/,1.0,18.0,0.0,10485.0,"Hi! I  badly need some advice.

Currently, we experienced delays (1 day turnaround time) in processing large   
amounts of data (200GB-300GB)   
Our company bought a server with ff. specs:  
\- 2x Intel Xeon Gold 5218  
\- 512GB RAM  
\- 8TB SSD 

And here's our current process:

1. We use Elasticsearch to store raw data  
2. Retrieve 200GB-300GB of data (from ES) to preprocess it using Python  
3. Save data to DB (PostgreSQL)  
4. Visualize data using Tableau  
(Note: we are using commodity laptops in our current setup)

Can you give me an advice on how can we maximize the specs of our server?  
What are the best setup with our scenario?"
1672,2020-02-24 17:43:16,1582558996.0,dataengineering,Looking for someone with Trading Platform experience!,f8t1t7,philcor123,,https://www.reddit.com/r/dataengineering/comments/f8t1t7/looking_for_someone_with_trading_platform/,1.0,1.0,0.0,10486.0,"Hey everyone - An upcoming podcast guest asked me to reach out to some networks about a Data Engineer position they have available and are struggling to find some people for, I thought this might be a good place to start. 

They are an online trading platform going through some high growth right now, the team itself creates quantitative products &amp; analyzes platform activity. 

They are looking for someone who has experience with Python and SQL, distributed computing like Kubernetes or Mapreduce and experience with additional technologies: Docker, PostgreSQL, Kafka, Airflow. 

If anyone knows someone that might be interested feel free to message me or reach out to me at philip@alldus.com"
1673,2020-02-24 18:00:21,1582560021.0,dataengineering,Don't let Apache Kafka steal your weekends!,f8tan2,lensesio,,https://www.reddit.com/r/dataengineering/comments/f8tan2/dont_let_apache_kafka_steal_your_weekends/,1.0,0.0,0.0,10486.0,
1674,2020-02-24 18:37:01,1582562221.0,dataengineering,Cleaning large datasets on your local (memory-constrained) machine?,f8tuf6,General_Example,,https://www.reddit.com/r/dataengineering/comments/f8tuf6/cleaning_large_datasets_on_your_local/,1.0,24.0,0.0,10486.0,"Hi folks,

I'm wondering how to approach the problem of cleaning/transforming a dataset on my local machine, when the dataset is too large to fit into memory.

My first thought is to stream it line by line using a Python generator and perform my cleaning steps that way. Is there any existing library or framework that is built around this concept? Or is there a better way to approach this problem?

Thanks."
1675,2020-02-24 19:02:27,1582563747.0,dataengineering,"Developing Airflow Plus (modern typed, open source, testable Airflow wrapper with some magic)",f8u8rd,biellls,,https://www.reddit.com/r/dataengineering/comments/f8u8rd/developing_airflow_plus_modern_typed_open_source/,1.0,0.0,0.0,10486.0,
1676,2020-02-24 21:20:36,1582572036.0,dataengineering,How much does an entry level (&lt; 1yr exp) DE make and where are you located?,f8wgpr,RogerSmithII,,https://www.reddit.com/r/dataengineering/comments/f8wgpr/how_much_does_an_entry_level_1yr_exp_de_make_and/,1.0,22.0,0.0,10490.0,How much does an entry level (&lt; 1yr exp) DE make and where are you located?
1677,2020-02-24 23:32:46,1582579966.0,dataengineering,Azure Data Engineer Cert,f8ylnl,Grangering,,https://www.reddit.com/r/dataengineering/comments/f8ylnl/azure_data_engineer_cert/,1.0,0.0,0.0,10492.0,
1678,2020-02-25 00:22:49,1582582969.0,dataengineering,"What skills/knowledge are expected from entry-level, fresh out of undergraduate data engineers??",f8ze9y,e2ee_for_all,,https://www.reddit.com/r/dataengineering/comments/f8ze9y/what_skillsknowledge_are_expected_from_entrylevel/,1.0,0.0,0.0,10494.0,
1679,2020-02-25 01:42:40,1582587760.0,dataengineering,Groupby list,f90mbr,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/f90mbr/groupby_list/,1.0,0.0,0.0,10493.0,"I have a data operation and I need to groupby a list which may be by one or two features.. How can I execute this?

My first thought was to create a list of lists and pass it to the groupby operaiton, but I get an error:

&amp;#x200B;

&gt;TypeError: Invalid argument, not a string or column: \['record\_educ\_lvl\_desc'\] of type &lt;class 'list'&gt;. For column literals, use 'lit', 'array', 'struct' or 'create\_map' function.

How do I make this work?

&amp;#x200B;

     record_fields = [['record_educ_lvl_desc'], ['record_home_construction_desc'],['record_rf_cost_grp'],['record_bsmnt_typ_grp_desc'], ['record_shape_desc'],
    ['record_sqft_dec_grp', 'record_sqft_dec_grp_10_flag'],['record_home_age'],
    ['record_home_age_grp','record_home_age_missing']]
    
    
    for field in record_fields:	
    	df_group = df.groupBy('year', 'area', 'state', 'code','field).sum('net_contributions')"
1680,2020-02-25 06:06:49,1582603609.0,dataengineering,Is moving from a DE role where writing Spark pipeline jobs is the main focus to a new company where DE works on Snowflake pipeline jobs the main focus a downgrade?,f94bou,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/f94bou/is_moving_from_a_de_role_where_writing_spark/,1.0,11.0,0.0,10496.0,"I'm interviewing for a DE role where their architecture and skill requirements is quite a bit different than my current DE role. Can someone help me determine if this is a downgrade or lateral movement?  


**This is what I'm currently working on at my current company:**

* Python/Pandas
* PySpark 
* sometimes Scala Spark
* Apache Pig/ Hive
* BigQuery (Data Warehouse)
* HDP distribution of Hadoop
* Very little data modeling or data cleansing
* Vertica (Business Intelligence DW)

  
**Potential new role is more focused on the following:**

* Python/Pandas
* Airflow (orchestration/scheduling)
* AWS Snowflake (Data Warehouse)
* A lot of Data Modeling
* A lot of Data Cleansing
* Vertica (Business Intelligence DW)

From my understanding, a lot of the big data processing and transformations happens within the Snowflake data warehouse. Is this considered a more watered down DE role?

I want my next role to be **at least a lateral move** that will sharpen my Big Data skills over the next 3-5 years and be more marketable. 

The reason I'm leaving is because my company sucks. There is no growth available in the next 1-2 years at least. And I'm mostly self-teaching right now. There's no leadership. Due to a number of layoffs - we're fighting fires instead of being proactive and building out new features."
1681,2020-02-25 10:06:05,1582617965.0,dataengineering,Pyspark - how do I use groupby with lists?,f970ee,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/f970ee/pyspark_how_do_i_use_groupby_with_lists/,1.0,2.0,0.0,10499.0,"I'm running a groupBy operation on a dataframe in Pysaprk and I need to groupby a list which may be by one or two features.. How can I execute this?

My first thought was to create a list of lists and pass it to the groupby operaiton, but I get an error:

&amp;#x200B;

&gt;TypeError: Invalid argument, not a string or column: \['record\_edu\_desc'\] of type &lt;class 'list'&gt;. For column literals, use 'lit', 'array', 'struct' or 'create\_map' function.

How do I make this work?   I'm open to other ways I could do this.

&amp;#x200B;

     record_fields = [['record_edu_desc'], ['record_construction_desc'],['record_cost_grp'],['record_bsmnt_typ_grp_desc'], ['record_shape_desc'],
    ['record_sqft_dec_grp', 'record_renter_grp_c_flag'],['record_home_age'],
    ['record_home_age_grp','record_home_age_missing']]
    
    
    for field in record_fields:	
    	df_group = df.groupBy('year', 'area', 'state', 'code', field).sum('net_contributions')"
1682,2020-02-25 10:22:20,1582618940.0,dataengineering,Data model for mobile game analytics,f975vv,korhner,,https://www.reddit.com/r/dataengineering/comments/f975vv/data_model_for_mobile_game_analytics/,1.0,0.0,0.0,10499.0,
1683,2020-02-25 10:30:50,1582619450.0,dataengineering,Data model for mobile game analytics,f978oc,ikorhner,,https://www.reddit.com/r/dataengineering/comments/f978oc/data_model_for_mobile_game_analytics/,1.0,10.0,0.0,10499.0,"Hello guys,

I am trying to redesign a huge table we use for analytics of our mobile game. It is a snapshot table where each day we copy all users (with their dimensions and their measures for that day), partitioned by date. It powers our BI tools and allows ad hoc queries with minimal joins.

Problem is, we have almost 200 millions users (much of those are inactive) and copying all those each day becomes expensive and slow. One idea we had was to keep only users who are active in the last 90 days or so but that makes some queries (1 year retention for example) difficult to calculate.

Did you ever had a similar problem and how did you solve it? We also have some redundant tables aggregated by some dimensions to allow fast aggregated queries but we also need the table at the user level for ad hoc queries."
1684,2020-02-25 15:03:40,1582635820.0,dataengineering,A conversation about the conflicts that lead to shadow IT in data and analytics projects and how to work toward resolving those tensions.,f99xzz,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/f99xzz/a_conversation_about_the_conflicts_that_lead_to/,1.0,0.0,0.0,10500.0,
1685,2020-02-25 16:22:58,1582640578.0,dataengineering,Microsoft To Build First Cloud Data Center Region In Mexico,f9ayw5,TheTesseractAcademy,,https://www.reddit.com/r/dataengineering/comments/f9ayw5/microsoft_to_build_first_cloud_data_center_region/,1.0,0.0,0.0,10502.0,
1686,2020-02-25 18:15:12,1582647312.0,dataengineering,"Data engineering: what skills/knowledge are expected from entry-level, fresh out of undergraduate??",f9cluz,e2ee_for_all,,https://www.reddit.com/r/dataengineering/comments/f9cluz/data_engineering_what_skillsknowledge_are/,1.0,13.0,0.0,10504.0,"Undergraduate in NYC graduating in May and interested in data engineering.     

Would it be a better idea to get a data analyst or software engineering job for a year before jumping into data engineering?"
1687,2020-02-25 22:04:16,1582661056.0,dataengineering,Has anyone here completed any certifications that gave you a significant salary boost?,f9g764,pauleenah,,https://www.reddit.com/r/dataengineering/comments/f9g764/has_anyone_here_completed_any_certifications_that/,1.0,1.0,0.0,10511.0,"What certifications did you complete and where are you currently located, and how many YOE do you have?"
1688,2020-02-25 23:09:46,1582664986.0,dataengineering,Would this project come under Data Engineering?,f9h9gd,infiniteAggression-,,https://www.reddit.com/r/dataengineering/comments/f9h9gd/would_this_project_come_under_data_engineering/,1.0,0.0,0.0,10512.0,
1689,2020-02-26 03:19:45,1582679985.0,dataengineering,Want to learn Data Engineering? Here are some Example Projects to get your hands dirty.,f9l209,sanchit089,,https://www.reddit.com/r/dataengineering/comments/f9l209/want_to_learn_data_engineering_here_are_some/,1.0,5.0,0.0,10515.0,
1690,2020-02-26 06:09:27,1582690167.0,dataengineering,Thoughts on this school project?,f9nf17,MadLadPatty,,https://www.reddit.com/r/dataengineering/comments/f9nf17/thoughts_on_this_school_project/,1.0,4.0,0.0,10518.0,"Hi! Very new and interested in the Data Engineering &amp; Data Management space, and wanted to ask you guys for your thoughts on a project for one of my classes. Here's some context (the actual question is at the bottom of the post).

In an analytics program, and this is from a class about Data Management. For our final project, our professor set up a partnership with a small startup. They're soon going to share a snapshot of their MySQL RDS containing data that we're going to perform analysis &amp; create a sales &amp; fulfillment dashboard for them on later on.

Overview: we need to do some dimensional modeling, pipeline their data into a Data Warehouse (we've been working with Fivetran and Snowflake in class), then create our dashboards using Tableau after connecting them to the DW. The dashboards we give them won't be ""deployed"" per se, they will be used as a reference for them to build their own charts/dashboard in their customer-facing app using React (I think). 

They currently only use Amazon RDS and are willing to invest in a DW and pipelining/streaming solution that we're going to pitch. Here's where it gets tricky: we need to come up with a solution to that fits within these asks...

* They want a 5-10 min latency from the transaction to when it can be available in their dashboards
* The solution should be scalable (they currently have 30 customers + 300 daily users and are expecting 5-10x more in the next few years)
* They want to allocate $100/month tops

I don't have any experience, but I've done a bit of research (no cost analysis yet), and here's what I've come up with so far...

* For the pipelining solution, I'm thinking of researching Amazon's Kinesis or Apache Kafka
* For the DW solution, I know Snowflake charges by usage, so it might be too expensive for the budget, so maybe Redshift might be a good choice with the added benefit of keeping their systems within the AWS ecosystem.

Thoughts/suggestions?"
1691,2020-02-26 08:17:22,1582697842.0,dataengineering,Any opinions on what companies are preferred to build a career as a DE?,f9ox9h,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/f9ox9h/any_opinions_on_what_companies_are_preferred_to/,1.0,5.0,0.0,10520.0,
1692,2020-02-26 18:48:26,1582735706.0,dataengineering,How to use array elements in Pyspark sql statement?,f9w42a,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/f9w42a/how_to_use_array_elements_in_pyspark_sql_statement/,1.0,6.0,0.0,10534.0,"When working with Pyspark, I need to select fields from an array that gets updated periodically

&amp;#x200B;

    arr = ['age', 'occupation', 'id']

how do I use the array in the sql query statement?

    df = spark.sql(""""""select &lt;fields from arr&gt; from tablename"""""")"
1693,2020-02-26 21:54:21,1582746861.0,dataengineering,Why You Need More than a Schema Registry to Become an Event-Driven Enterprise,f9z2yn,DigitalBackbone,,https://www.reddit.com/r/dataengineering/comments/f9z2yn/why_you_need_more_than_a_schema_registry_to/,1.0,0.0,0.0,10540.0,
1694,2020-02-28 02:03:09,1582848189.0,dataengineering,Data Engineering Interview Process,fall30,doyouevenbayes,,https://www.reddit.com/r/dataengineering/comments/fall30/data_engineering_interview_process/,1.0,0.0,0.0,10564.0,
1695,2020-02-28 06:01:09,1582862469.0,dataengineering,"I know there a lot of click baits and websites that list / sell trade shows to data leaders, but anyone here know of any good trade shows for a junior data engineer ? Besides AWS reinvent",faorso,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/faorso/i_know_there_a_lot_of_click_baits_and_websites/,1.0,16.0,0.0,10567.0,
1696,2020-02-28 06:39:44,1582864784.0,dataengineering,How AppsFlyer uses Apache Airflow to run more than 3.5k daily jobs,fap8p1,shar1z,,https://www.reddit.com/r/dataengineering/comments/fap8p1/how_appsflyer_uses_apache_airflow_to_run_more/,1.0,1.0,0.0,10568.0,
1697,2020-02-28 09:17:06,1582874226.0,dataengineering,"How to progress in DE career, want to switch jobs",faqywx,ordinary_guy1,,https://www.reddit.com/r/dataengineering/comments/faqywx/how_to_progress_in_de_career_want_to_switch_jobs/,1.0,3.0,0.0,10570.0,"I have been in software engineering for the past 6 years, it has been a mix of different domains/technologies. 

For the first 4 years, I had been into full-stack development with a little bit of PHP and later on large part into React, Nodejs stack.

Then I moved into Data Engineering in the same company, extensively worked on setting up Kafka pipeline, worked on writing ETL jobs on Apache Spark using Scala. The entire team was new into DE, so learned about things in DE majorly from the internet, trial and error, nobody had any prior experience, a lot of times burnt our hands while trying out different things. The data pipeline is set up and quite stable, later on, have been writing a lot of ETL jobs in Spark and Scala and using Oozie for scheduling. I am looking for a job where I can learn from people who are experienced and know DE. But the technologies that I am working on seem to be far less in demand. Everyone is asking for Python/Airflow. I have worked on Spark majorly using Scala API. I am not proficient in general Scala, like Actors/Cats/Akka and other libraries. I can't say I actually know Scala better since scala is a lot bigger than just Spark. I feel stuck in the current job right now. I have applied to many companies but it seems they don't care about the breadth of work that I have done. All I get is rejections. How do I proceed in DE career?"
1698,2020-02-28 16:20:13,1582899613.0,dataengineering,Advice,favdks,rwilldred27,,https://www.reddit.com/r/dataengineering/comments/favdks/advice/,1.0,0.0,0.0,10579.0,
1699,2020-02-28 18:07:01,1582906021.0,dataengineering,Advice for a daily fantasy baseball data pipeline on AWS,fawxps,rwilldred27,,https://www.reddit.com/r/dataengineering/comments/fawxps/advice_for_a_daily_fantasy_baseball_data_pipeline/,1.0,0.0,0.0,10579.0,
1700,2020-02-28 21:45:45,1582919145.0,dataengineering,"Data Engineering, Big Data and Machine Learning on Google Cloud Platform Specialization",fb0e1d,lwilson747747,,https://www.reddit.com/r/dataengineering/comments/fb0e1d/data_engineering_big_data_and_machine_learning_on/,1.0,0.0,0.0,10584.0,
1701,2020-02-28 23:07:59,1582924079.0,dataengineering,Dumb question: when do you know to use rank and/or partition by?,fb1ncj,ima666,,https://www.reddit.com/r/dataengineering/comments/fb1ncj/dumb_question_when_do_you_know_to_use_rank_andor/,1.0,6.0,0.0,10585.0,
1702,2020-02-29 12:05:36,1582970736.0,dataengineering,Distributed ML Pipeline,fbaisr,IsaGoksu,,https://www.reddit.com/r/dataengineering/comments/fbaisr/distributed_ml_pipeline/,1.0,0.0,0.0,10598.0,
1703,2020-02-29 21:07:58,1583003278.0,dataengineering,Opinions on a Kafka infra tutorial,fbhcir,mech_monkey,,https://www.reddit.com/r/dataengineering/comments/fbhcir/opinions_on_a_kafka_infra_tutorial/,1.0,5.0,0.0,10611.0,"Hey all! I have been data engineer for over ten years now, still trying to learn something new everyday. I have recently started to log my personal experiments as I go along, here's some sample: [https://blog.sogdian.co.uk/posts/kafka-cluster-using-aws-cdk.html](https://blog.sogdian.co.uk/posts/kafka-cluster-using-aws-cdk.html). I know it reads quite dry and I don't have the expertise to do video, but any advice/help/questions would definitely be appreciated!"
1704,2020-02-29 22:05:11,1583006711.0,dataengineering,Does anyone here start in supply chain and end in data engineering or vice versa and ever try and apply abstract thinking from their past experiences( let’s say in supply chain ) to data engineering? I feel like there are a lot of parallels and surprised I have not seen this conversed.,fbi7fk,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/fbi7fk/does_anyone_here_start_in_supply_chain_and_end_in/,1.0,0.0,0.0,10610.0,
1705,2020-03-01 09:09:42,1583046582.0,dataengineering,Anyone work in Data Engineering at a consulting company,fbqjpx,REorganize009,,https://www.reddit.com/r/dataengineering/comments/fbqjpx/anyone_work_in_data_engineering_at_a_consulting/,1.0,0.0,0.0,10610.0,
1706,2020-03-01 17:48:43,1583077723.0,dataengineering,"Airflow read line on file, wait 1 min, read next line",fbvt2r,derzemel,,https://www.reddit.com/r/dataengineering/comments/fbvt2r/airflow_read_line_on_file_wait_1_min_read_next/,1.0,1.0,0.0,10615.0,
1707,2020-03-01 18:16:00,1583079360.0,dataengineering,Does this qualify as a Data Engineering Project?,fbw7rx,infiniteAggression-,,https://www.reddit.com/r/dataengineering/comments/fbw7rx/does_this_qualify_as_a_data_engineering_project/,1.0,13.0,0.0,10615.0,"I'm an undergraduate Software Engineering sophomore and I've been reading up as much as I can about DE. From what I've understood, I've set out to carry out a small project that involves some DE concepts (or at least I think they are) and I wanted to ask whether what I'm planning to do comes under the vast umbrella of Data Engineering.

This is how I've laid the blueprint out and what ""concepts"" I would like to implement. I understand that all these things are a massive overkill for the project but that's the main reason I'm doing them, so I can focus on the DE aspect of it more than the functionality itself.  

**Deployment**:

* Writing unittests to validate data
* Containerizing the API using Docker
* Creating a CI/CD pipeline
* Deploying the project on AWS

**A cron job that carries out the following tasks:**

* Parsing/cleaning data sets from the NORAD page
* Storing/removing parsed data into a SQL/NoSQL database

**API**:

* Flask RESTful API that returns a json object containing only the data that was stored in the db. This data will be used by said ""tracker"".

**Tracker**:

* Plots the data/coordinates onto a world map and updates in real time as the position changes. Shows positions in the past and predicted path as well.

I really appreciate any pointers/suggestions you may have. Please let me know if you'd like me to elaborate. Thanks!"
1708,2020-03-01 20:13:00,1583086380.0,dataengineering,Checking for Data Issues,fbxzqs,bigboss393,,https://www.reddit.com/r/dataengineering/comments/fbxzqs/checking_for_data_issues/,1.0,1.0,0.0,10618.0,"Hi guys,

Let's say I've got some data from Google analytics and want to model the data.

What checks should I do to check the data quality/checking for any issues?

New to engineering, thanks!"
1709,2020-03-01 23:44:47,1583099087.0,dataengineering,Batching data to S3 with fluentd,fc186i,adrianulbona,,https://www.reddit.com/r/dataengineering/comments/fc186i/batching_data_to_s3_with_fluentd/,1.0,1.0,0.0,10623.0,
1710,2020-03-02 02:08:08,1583107688.0,dataengineering,Help understanding airflow,fc3a29,ImBatmanWhoAreYou,,https://www.reddit.com/r/dataengineering/comments/fc3a29/help_understanding_airflow/,1.0,15.0,0.0,10626.0,"I haven’t used it but I’ve seen a lot of people talking about automating tasks using Apache Airflow. 

I’m trying to make sure I understand the differences between it and cron or APScheduler. 

My understanding is it’s a cron job with a built In GUI (using flask to serve it up) and some error handling to retry tasks when they fail. Also having some extra sauce as well to come up with complex dependencies between different tasks. 

Is that about right?

————

Really I’m curious as to how it compare to an app that I have in production on a relatively small scale. My app does scheduled data pulls from an API that stores them in a postgres database using APScheduler. The API calls are quite slow so I don’t do it in real time (eventually I could async it maybe). And then create what amount to dashboards in a flask app. I have all of this deployed in a single Heroku app. 

The way I am understanding it, if I wanted to do the same thing with airflow I would need to separate apps. Is that right? Is there a way I could integrate airflow into my current Flask app as a blueprint?

Thanks. I hope this is a reasonable question to ask here."
1711,2020-03-02 03:11:33,1583111493.0,dataengineering,Develop python packages for Databricks,fc44b9,MenziesTheHeretic,,https://www.reddit.com/r/dataengineering/comments/fc44b9/develop_python_packages_for_databricks/,1.0,9.0,0.0,10627.0,"A common struggle that I have seen in several companies is the way teams manage tons of code in Databricks notebooks.

I’ve tried to introduce a better way of working by converting notebooks into python packages. The advantages are worth it in my opinion, but my team is less enthousiastic. 

I wrote a blog post about this way of working, and I thought I’d share it here to get some feedback.

https://menziess.github.io/howto/enhance/your-databricks-workflow/

The advantages I see using python packages over Databricks notebooks:
- No complicated cross dependencies between notebooks anymore (someone editing some utility notebook could cause multiple other notebooks to fail)
- Full git control in your IDE
- Iterating faster by developing locally with small data
- Being able to run code directly on Databricks from your ide to prove that your code scales
- Package versioning over notebook copying
- Linting
- Tests

Any advice on how to motivate my team to try this is also appreciated :)"
1712,2020-03-02 11:30:05,1583141405.0,dataengineering,Resourcing a data engineering team,fc9n5t,Boozmork,,https://www.reddit.com/r/dataengineering/comments/fc9n5t/resourcing_a_data_engineering_team/,1.0,0.0,0.0,10639.0,
1713,2020-03-02 13:24:31,1583148271.0,dataengineering,The drivetrain approach to create data product,fcaomu,TheTesseractAcademy,,https://www.reddit.com/r/dataengineering/comments/fcaomu/the_drivetrain_approach_to_create_data_product/,1.0,0.0,0.0,10640.0,
1714,2020-03-03 04:31:03,1583202663.0,dataengineering,An interview about the ksqlDB platform and the unified experience that it provides for building stream processing applications on top of Kafka with SQL.,fcnows,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/fcnows/an_interview_about_the_ksqldb_platform_and_the/,1.0,0.0,0.0,10649.0,
1715,2020-03-03 18:47:58,1583254078.0,dataengineering,Career Advice to switch to Data engineering.,fcxec1,jrwrita,,https://www.reddit.com/r/dataengineering/comments/fcxec1/career_advice_to_switch_to_data_engineering/,1.0,15.0,0.0,10661.0,"Hello , first time poster here, I need some career advice.

I currently have been a ""business analyst"" for 5 years now, 28 y/o.
Apart from the domain knowledge/business knowledge I have skills in VBA, SQL, Python(mainly pandas and numpy), and BI tools(tableau and some others).

I currently make below avg salary in my city and figure I need to switch elsewhere where I can grow. I believe there is little growth in my current role.

I have been applying to data engineer/data analyst roles with no bites. I feel like I would be good in a data analytics type of role.

Is their anything else I can be doing/learning to better my shot at landing something? I kind of am opposed to doing my masters at this point and need some advice.

Thank you."
1716,2020-03-03 21:49:29,1583264969.0,dataengineering,I’m hiring a Data Engineer III,fd0a54,magnusboletus,,https://www.reddit.com/r/dataengineering/comments/fd0a54/im_hiring_a_data_engineer_iii/,1.0,14.0,0.0,10668.0,"Hi Everyone, 

I’m an IT recruiter, and I am looking for a Data Engineer III with USA work authorization who is in Seattle or will be willing to relocate to Seattle. This position is a 6 month contract. 

The ideal candidate has skills with: 

- ETL 
- Python 
- Data Modeling 
- 5+ years SQL, SQL Tuning, Oracle, OLAP Big Data technologies 
- Proficiency with Linux 
- Experience with Hadoop based-technologies like HBase, Pig, Hive, and Spark 
- Experience with Amazon Web Services 

If you’re interested please reply to this post. Here is the link to the website of the company I work for - S.com-USA.com"
1717,2020-03-04 02:58:47,1583283527.0,dataengineering,Have any of you folks used Pentaho kettle/ PDI?,fd4w2t,drumkeys,,https://www.reddit.com/r/dataengineering/comments/fd4w2t/have_any_of_you_folks_used_pentaho_kettle_pdi/,1.0,3.0,0.0,10673.0,Started a new position at a FAANG company using it. Not sure how I feel about it and wondering what others’ thoughts are.
1718,2020-03-04 03:43:43,1583286223.0,dataengineering,Working with image data,fd5hrx,tgalchemy,,https://www.reddit.com/r/dataengineering/comments/fd5hrx/working_with_image_data/,1.0,0.0,0.0,10673.0,Does anyone have any advice or resources for working with large image data? New to the field and just looking to catch-up
1719,2020-03-04 04:52:35,1583290355.0,dataengineering,Data Engineering course: trying to gauge interest,fd6eo5,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/fd6eo5/data_engineering_course_trying_to_gauge_interest/,1.0,94.0,0.0,10673.0,"Hi all, I am a data engineer with 6 years of experience. One thing I have noticed is the lack of a course that covers the basics of data engineering for beginners (the ones that do are very expensive). I have always been interested in teaching technical concepts, and wanted to see if there would be any interest in such a course. Did not put a lot into design as I wanted to get feedback only on the **CONTENT**.

Would really appreciate any feedback on  [https://josephs-blank-site-4bcce3.webflow.io/sde1](https://josephs-blank-site-4bcce3.webflow.io/sde1) 

Thank you all :)"
1720,2020-03-04 05:54:41,1583294081.0,dataengineering,Beginner Project ideas recommendations ?,fd77qv,Ckothwal,,https://www.reddit.com/r/dataengineering/comments/fd77qv/beginner_project_ideas_recommendations/,1.0,3.0,0.0,10674.0,"Hi all, I am a beginner to the field to data engineering. I have learnt how DFS works and navigating through HDFS, applying mapreduce jobs to files on HDFS, write Hive and Pig queries for the same. I am also now familiar using spark (pySpark) to work with data in AWS S3 using AWS EMR. But I haven't been able to think of a project that can integrate all of this together. Any recommendations of ideas to implement that is capable enough to be put on the resume for entry-level jobs?"
1721,2020-03-04 13:24:09,1583321049.0,dataengineering,Understanding the Customer Journey Through Data,fdbmdw,TheTesseractAcademy,,https://www.reddit.com/r/dataengineering/comments/fdbmdw/understanding_the_customer_journey_through_data/,1.0,0.0,0.0,10677.0,
1722,2020-03-04 16:08:52,1583330932.0,dataengineering,Data warehouse logical organization,fddicv,ikorhner,,https://www.reddit.com/r/dataengineering/comments/fddicv/data_warehouse_logical_organization/,1.0,3.0,0.0,10680.0,"Would anyone be open to discuss or share some resources regarding designing logical units in data warehouse? More precisely, I am thinking about different layers and who has access to which data: 

1)  Data lake - we import all of our raw data here. Should we allow any external tool or report to access this layer? What about ad hoc queries? 

2) Transformed raw data - we clean raw data and convert it to columnar formats. Should BI tools access this data? 

3) Various transformed data - this is basically data transformed for easier querying - star schemas. 

4) Reporting data - tables used for various BI tools, usually aggregated 

5) Serving data - data used by product teams -i.e. product recommendations, churn prediction, etc  

What I am unsure about is: 

\- For example, BI tools sometimes need data from transformed and raw layers. What should I do, allow that or copy that data to reporting? We are talking about large tables so copying would increase costs and using them directly breaks encapsulation - we can't easily modify that tables without breaking lots of reports.

 \- Does this separation make sense, is there something I am missing?"
1723,2020-03-04 18:55:10,1583340910.0,dataengineering,Breaking up the Airflow DAG monorepo,fdfvzq,houqp,,https://www.reddit.com/r/dataengineering/comments/fdfvzq/breaking_up_the_airflow_dag_monorepo/,1.0,0.0,0.0,10689.0,
1724,2020-03-04 20:53:57,1583348037.0,dataengineering,Hiding database credentials in python script,fdhq8s,Luukv93,,https://www.reddit.com/r/dataengineering/comments/fdhq8s/hiding_database_credentials_in_python_script/,1.0,16.0,0.0,10689.0,"Hello,

Please your advise on how to hide database credentials  when pulling data out of a sql database using python?"
1725,2020-03-05 03:10:39,1583370639.0,dataengineering,Loading Data,fdnaqd,tpedar50,,https://www.reddit.com/r/dataengineering/comments/fdnaqd/loading_data/,1.0,9.0,0.0,10696.0,"When loading data from a source system in your data lake(we’re staging data to s3 and accessing it via snowflake):

Do you load all tables from your source system into S3 or do you go through and identify the ones that you think will be used and only load those?"
1726,2020-03-05 11:14:34,1583399674.0,dataengineering,Start Learning Apache Airflow! 12 hours of videos from basic to advanced,fdsp8w,marclamberti,,https://www.reddit.com/r/dataengineering/comments/fdsp8w/start_learning_apache_airflow_12_hours_of_videos/,1.0,0.0,0.0,10705.0,
1727,2020-03-05 17:59:19,1583423959.0,dataengineering,"How to Process and store data over time(ex: per hour, week, month, year)",fdxak7,Mac_Attack18,,https://www.reddit.com/r/dataengineering/comments/fdxak7/how_to_process_and_store_data_over_timeex_per/,1.0,7.0,0.0,10718.0,"I want to process a raw log file and get some info. For example unique device id/account id/request id connected over the last hour. I would run that every hour and build a model of how our app would be used.

How could I apply that to say 24 hours, a week, a month. If I just added the total of each category for each hour it wouldn't give me unique devices or accounts. As they would be counted multiple times. Say a device is connected for 5 hours. It would be counted 5 times.

My current idea would be store each unique device id/account id/request id per hour and then have a job run once a day/week/month that takes those lists for each hour and computes the data per specified time frame. That could take up a lot of space though. Is there a better way to do what I want?"
1728,2020-03-05 18:40:32,1583426432.0,dataengineering,Got 3 offers in 4 days for data engineer after an year's stint of Jr.Data engineer role-AMA,fdxxmz,spectre_S,,https://www.reddit.com/r/dataengineering/comments/fdxxmz/got_3_offers_in_4_days_for_data_engineer_after_an/,1.0,0.0,0.0,10718.0,
1729,2020-03-05 20:13:03,1583431983.0,dataengineering,Route to data engineering,fdzdfu,mycrappycomments,,https://www.reddit.com/r/dataengineering/comments/fdzdfu/route_to_data_engineering/,1.0,10.0,0.0,10721.0,"So I’ve been a BI developer for about 10 years now. I’ve been pretty much in the Microsoft stack. I’ve been trying to get away from visualization and focus more on the data side of things like etl, data warehousing. 

How do I get into data engineering? I’m studying for the Azure data engineering certification to cover some knowledge gaps and because of my affinity to Microsoft my entire career. 

Couple of things I’ve noticed. 
As a BI developer, people want me for my visualizations which I’m trying to get away from. It’s just not for me. 
Companies hiring for data engineers won’t give me he time of day because I haven’t had that title even though my experience is almost similar to what they’re asking for.
Also companies that hire data engineers tend to do stuff on prem instead of in the cloud. Is my Azure certification going the wrong path?"
1730,2020-03-06 07:14:07,1583471647.0,dataengineering,I Created A Free SQL 101 Course For Those of You Looking to Get Into Data Engineering,fe8obj,coyne_operated,,https://www.reddit.com/r/dataengineering/comments/fe8obj/i_created_a_free_sql_101_course_for_those_of_you/,1.0,1.0,0.0,10733.0,
1731,2020-03-06 09:40:55,1583480455.0,dataengineering,Big Data Analytics with PySpark + Tableau Desktop + MongoDB,fea5r6,Edwinb60,,https://www.reddit.com/r/dataengineering/comments/fea5r6/big_data_analytics_with_pyspark_tableau_desktop/,1.0,0.0,0.0,10734.0,"If you want to, check it out below:

[https://www.udemy.com/course/big-data-analytics-with-pyspark-tableau-desktop-mongodb/?referralCode=348A25E57F2654D3F0DA](https://www.udemy.com/course/big-data-analytics-with-pyspark-tableau-desktop-mongodb/?referralCode=348A25E57F2654D3F0DA)

https://preview.redd.it/4769we4ta0l41.png?width=2559&amp;format=png&amp;auto=webp&amp;s=69d740fbbc307382ec260d47c0cab7e53c2bee06"
1732,2020-03-06 17:09:55,1583507395.0,dataengineering,Data Modeling for Personal Project,feerov,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/feerov/data_modeling_for_personal_project/,1.0,15.0,0.0,10743.0,"Hello DE Community,

I'm working on a personal project to build a relational database for some sports data and I'm confused about some of the relationships between the entities. I've attached an image with the model.

Where I'm getting confused is with the relationship between Teams and Games (business rules below).

If I wanted to query for all games for a specific team I'd have to join the games table twice, because a team could be home or away. In terms of cardinality, for a game to exist there needs to be a pair of teams, and only one pair of teams can play in a single game (one-to-one). 

Is there an approach I could take so that I could make the querying of games for a specific team simplified?

Business Rules:

* A team is a franchise during a specific season
* A team cannot exist without a franchise
* A franchise can have a minimum of one team and a maximum of many teams
* Teams play each other in games
* A single game is played by exactly two teams
* Pairs of teams can play each other multiple times
* Games are played in venues
* For a game to exist is needs to be played in a venue
* A venue could be used for a minimum of one game or many games

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/fpxef3u2g2l41.png?width=705&amp;format=png&amp;auto=webp&amp;s=5368709b65a1996e91f9b78027ca235ac3a51200"
1733,2020-03-06 20:17:54,1583518674.0,dataengineering,Airflow + Load Balancer (for tasks),fehm86,mikomono,,https://www.reddit.com/r/dataengineering/comments/fehm86/airflow_load_balancer_for_tasks/,1.0,6.0,0.0,10747.0,"Hey all,

I am experimenting with Apache Airflow and the different executors it provides. Particularly, I am interested in the Dask executor, and pretty recently in the Kubernetes executor.

I've opted in for the Dask executor because of its load balancing capabilities;  to distribute the jobs depending on the current load of the worker nodes. From a layman's observation (just using htop), the jobs are distributed like fine, but, even when a node's CPUs are all occupied, it will receive new tasks (some lightweight, and some high CPU-bound).

I think this behaviour is governed by the `parallelism` setting in airflow.cfg.

My question is, is there a configuration parameter/setting either in Airflow or Dask that can put the tasks on ""hold"" (I know Dask doesn't support queues) until the resources are freed? Is switching to the Kubernetes executor a better solution to utilize the resources of the cluster\*?

&amp;#x200B;

\*  I am setting the Kubernetes cluster right now (first time also :P)

&amp;#x200B;

Any ideas and comments are most welcomed! :-)"
1734,2020-03-07 12:56:42,1583578602.0,dataengineering,Label your data with machine learning,fetopu,AaronWard_,,https://www.reddit.com/r/dataengineering/comments/fetopu/label_your_data_with_machine_learning/,1.0,0.0,0.0,10759.0,
1735,2020-03-07 13:13:41,1583579621.0,dataengineering,Airflow vs Azkaban,fettq8,spd2019b,,https://www.reddit.com/r/dataengineering/comments/fettq8/airflow_vs_azkaban/,1.0,12.0,0.0,10760.0,What’s the max scale one has reached with Airflow in terms of number if DAGs.. I have read somewhere Azkaban scales really well and can run thousands if DAGs in parallel.. Has anyone tried both and what do you think puts Airflow in the advantage
1736,2020-03-07 15:16:43,1583587003.0,dataengineering,Just landed my dream data engineering job!,feuzrf,jeanshanchik,,https://www.reddit.com/r/dataengineering/comments/feuzrf/just_landed_my_dream_data_engineering_job/,1.0,0.0,0.0,10762.0,
1737,2020-03-07 16:37:36,1583591856.0,dataengineering,Should I jump ship for a hedge fund?,fevwvk,romanX7,,https://www.reddit.com/r/dataengineering/comments/fevwvk/should_i_jump_ship_for_a_hedge_fund/,1.0,0.0,0.0,10764.0,"TLDR: would you leave your comfortable DE role to work for a hedge fund offering a significantly higher salary?

I'm fairly happily employed, id say an 8 out of 10. I do still give recruiters the time of day if they bring a position that sounds amazing to my attention. This week, a recruiter messaged me about a DE role at a hedge fund that primary focuses on Apache airflow with a total compensation almost double what I make now. 

Airflow is the tool I use the most at my current position and iv really taken the time to become the ""airflow specialist"" on my team so this seems like a great match. Also....the money!

My question is, what's the catch? I consider myself a mid level DE and have a masters in analytics. They seem to think I'm qualified for this job that pays significantly higher than my already six figure salary. Are the hours going to be crazy? Is there lots of stress and pressure working at hedge funds? Is the overall quality of life much worse?"
1738,2020-03-08 02:11:18,1583626278.0,dataengineering,What is your opinion on modern data engineering Traditional ETL tools like Informatica/Talend etc?,ff46c1,whelping_monster,,https://www.reddit.com/r/dataengineering/comments/ff46c1/what_is_your_opinion_on_modern_data_engineering/,1.0,2.0,0.0,10775.0,
1739,2020-03-08 03:36:10,1583631370.0,dataengineering,Accidental Data Engineer Looking for Advice,ff5aaa,Nova-Lord,,https://www.reddit.com/r/dataengineering/comments/ff5aaa/accidental_data_engineer_looking_for_advice/,1.0,0.0,0.0,10775.0,"Hello!

I started working as a data analyst for a relatively new company. Funny thing is that the company does not have a handle on their data. We have multiple APIs (around 15 APIs) that work as the back-end for our main product and almost every API has an associated database. I have convinced the higher-ups that we need to create an analytics environment. That means extracting data from the production databases and moving it into an environment that is analytics friendly. Luckily I have some experience with ETL and other data engineering tasks, but I am still unsure of the best option or what is common practice.

What I need advice on are tools that exist to facilitate extracting data from multiple sources and moving it into one location. I imagine that the data would have to be moved into a staging area, transformed into an analytics friendly format, and then moved to some sort of data warehouse. All the production databases are on Amazon Aurora, so using tools that are AWS friendly will make it an easier sell to the engineering team (AWS Glue is on my radar). Their main concern is going to be how will it affect the performance of their live services. Any advice is appreciated.

Another thing to note is that there are DBAs who work for this company, but they are all in another country. Initially I sought their support with this, but I hit some roadblocks that I will just describe as ""company bureaucracy"". I do have the support of the local engineers, but if I do not lead these efforts and push for it to be done, then no one will and it will not get done."
1740,2020-03-08 07:38:08,1583645888.0,dataengineering,How to best prepare for DE onsite interviews,ff8860,jtwilly1127,,https://www.reddit.com/r/dataengineering/comments/ff8860/how_to_best_prepare_for_de_onsite_interviews/,1.0,8.0,0.0,10779.0,"Hi everyone!

This is my first reddit post so if the format isn't 100% I'm sorry :3

For a while, I've been working at a startup and managing everything related to data . I began as their intern who didn't have extensive knowledge in CS. I had taken a OOP, Algorithms (a very bad one) and Database 1 class before the internship. I knew close to nothing when I started and learned everything I know about data engineering on the job (aka google &amp; reddit). Didn't have a lot of mentorship since the team was was super busy and small. Felt like I was a chicken with its head cut the whole time I worked there but grateful for the experience because i learned a lot FAST. 

I'm currently on the job hunt and seem to be doing an OK since I have several onsite interviews coming up. I have about a week of preparation before they start and not sure what the experience is going to be like because I've never had a formal onsite interview before! These are what I believe are my current strengths and weaknesses:

*Weaknesses: big data pipelines, data modeling best practices and BI analytics and tools.*

*Strengths: SQL, Python, AWS S3 &amp; EC2, linux, OOP/algorithms, web crawlers and reg. sized pipelines*

Does anyone have suggestions on how to prepare based off the weaknesses I listed? Does anyone want to talk about their onsite interview experience? Does anyone have nice words of encouragement? 

Anything is appreciated!"
1741,2020-03-09 04:15:09,1583720109.0,dataengineering,Poll: Anyone here work for a company that has a SQL style guide and enforces it?,ffnj92,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/ffnj92/poll_anyone_here_work_for_a_company_that_has_a/,1.0,29.0,0.0,10799.0,
1742,2020-03-09 07:56:32,1583733392.0,dataengineering,Personal experience with Dagster,ffq6ez,MrMosBiggestFan,,https://www.reddit.com/r/dataengineering/comments/ffq6ez/personal_experience_with_dagster/,1.0,3.0,0.0,10800.0,"Hey all, 

Just wanted to give my experience on dagster, as I 've seen a lot about it, but not a lot of personal experience using it. I wanted to try it on a simple mock data warehouse architecture I built out. Basically, a Postgres instance hosted on RDS, with some sample data, extracting it to S3, and loading into Redshift. From there, maybe some dbt models to create some additional views and tables.

Overall, I really like the approach and architecture of the project, but I don't think it's something I would use on the job in a production setting just yet. The tutorial goes in-depth, but still manages to skip some important things, such as how file storage/filehandlers work. There's zero documentation on their AWS plugin, and many of their classes are still only roughly documented. Some of the tutorials lack clarity as well. They might introduce an entire code block, but not point out which line they are referencing to in their description of  a process. Worse still, their airlines example jumps around from file to file, without giving a clear overview of an entire file. So many parts of their demo are not even covered. 

I really like the idea of being able to abstract out resources. Being able to point to a local Postgres instance vs a Redshift warehouse in production and having these configured appropriately is really appealing and makes testing so much easier. The problem is that it's not immediately clear to me how do properly configure resources. The documentation could use more explicit examples. There's configs, inputs, outputs, etc. In one case, the keys for an S3 object are Bucket/Key, in others, they are bucket/key (no caps). Again, it's really hard to look up documentation on these. I think part of the problem is there are many different ways to do things, that it gets hard to keep a good mental model of what I should be doing.

In the end, I got a very small working example  working of pulling data from postgres, to a local csv, and then uploading that to S3. I didn't continue any further, since I felt I had spend so much time on it already. With all that said, I'll be keeping an eye on this project, and really hope for its success. It hopes to solve a lot of pain points I have with Airflow, and I expect it'll continue to improve.

Let me know if you have any questions!"
1743,2020-03-09 08:48:46,1583736526.0,dataengineering,Apache Beam: How to input data from a URL instead of a file or other source?,ffqog5,pknerd,,https://www.reddit.com/r/dataengineering/comments/ffqog5/apache_beam_how_to_input_data_from_a_url_instead/,1.0,0.0,0.0,10800.0,"Hi

&amp;#x200B;

I am fiddling with Apache beam, is there any way to feed html scraped data directly into Apache beam pipelines instead of CSV files?"
1744,2020-03-09 08:55:02,1583736902.0,dataengineering,Language server protocol for query languages?,ffqqpn,eleijonmarck,,https://www.reddit.com/r/dataengineering/comments/ffqqpn/language_server_protocol_for_query_languages/,1.0,0.0,0.0,10800.0,"In recent time, there have been a surge of language server protocol and rightfully so! It is amazing to aligned the tooling around a language.

I would love to see the same for sql/database standards where you can just download the psql extension or the redis extension on any ide and start smashing away.

Is there such a thing as a lsp for query languages?"
1745,2020-03-09 17:09:48,1583766588.0,dataengineering,Any production feedback on Prefect workflow management ?,ffw2qa,sib_n,,https://www.reddit.com/r/dataengineering/comments/ffw2qa/any_production_feedback_on_prefect_workflow/,1.0,8.0,0.0,10808.0,
1746,2020-03-09 17:47:07,1583768827.0,dataengineering,An interview about how a data hub architecture can reduce the overhead of managing data governance and compliance across an organization,ffwnal,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ffwnal/an_interview_about_how_a_data_hub_architecture/,1.0,1.0,0.0,10809.0,
1747,2020-03-10 03:07:40,1583802460.0,dataengineering,BI Manager transitioning to Data Engineering Manager,fg5l6t,HansSlinger,,https://www.reddit.com/r/dataengineering/comments/fg5l6t/bi_manager_transitioning_to_data_engineering/,1.0,0.0,0.0,10826.0,
1748,2020-03-10 11:45:55,1583833555.0,dataengineering,Need a tool to run queries over multiple sources of data?,fgbb5q,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/fgbb5q/need_a_tool_to_run_queries_over_multiple_sources/,1.0,3.0,0.0,10833.0,"What are some tools that can let me run queries like sql on multiple sources of data. These sources are aws rds, excel and Google analytics.

Do I have to manually combine these sources first and then only can run analysis?"
1749,2020-03-10 16:50:24,1583851824.0,dataengineering,Do I have the skills for data engineering? What can I work on? Resume attached,fgeywy,ROC2021,,https://www.reddit.com/r/dataengineering/comments/fgeywy/do_i_have_the_skills_for_data_engineering_what/,1.0,2.0,0.0,10846.0,
1750,2020-03-10 16:54:12,1583852052.0,dataengineering,GCP or AWS for building portfolio and getting cert ?,fgf0z8,bia1999,,https://www.reddit.com/r/dataengineering/comments/fgf0z8/gcp_or_aws_for_building_portfolio_and_getting_cert/,1.0,1.0,0.0,10846.0,
1751,2020-03-11 07:37:23,1583905043.0,dataengineering,Need feedback on a project,fgsa7o,vishalbilagi,,https://www.reddit.com/r/dataengineering/comments/fgsa7o/need_feedback_on_a_project/,1.0,7.0,0.0,10856.0,"I am working on my first data engineering project that reads data from 3 different map service APIs and writes that raw API data to a Kafka broker with 3 topics, one for each of those APIs.

Here is what I want to achieve

1. Fetch API data as is and write it to Kafka
2. Use Spark to clean and transform, then write it to a database
3. Query DB / visualize data / make a recommendation engine

I have completed \[1\] of the 3 above. I want some feedback before moving forward and implementing this whole idea. Should I clean data before writing to Kafka (in that case, will I really need Kafka?). Should I choose a graph based database? Since I want to be able to derive meanings between two points."
1752,2020-03-11 08:07:20,1583906840.0,dataengineering,Is there any better ways to study distributing tools?,fgslce,wasabinoodle,,https://www.reddit.com/r/dataengineering/comments/fgslce/is_there_any_better_ways_to_study_distributing/,1.0,11.0,0.0,10856.0,"I am a newbie in data engineering.

After building a simple data pipeline with apache spark, I have a strong feeling that there's so many tools to study in DE.

We need to learn both SQL and NoSQL databases, batch processing tools like Hadoop, Saprk, streaming tooks, Kafka, spark streaming, flink, scheduler tools like airflow, etc.

One thing is most of these tools needs a distributing architecture to have full functionality for processing big data, but how can we get a hands-on learning with these tools without a cluster? I mean of course we can use AWS but it still costs a fair amount of money.

Just want to know if there's better ways to learn these tools, especially see how they perform in a distributing enrironment."
1753,2020-03-11 09:50:28,1583913028.0,dataengineering,Data Infrastructure Setup in Your Firm,fgtkye,Electric_pokemon,,https://www.reddit.com/r/dataengineering/comments/fgtkye/data_infrastructure_setup_in_your_firm/,1.0,3.0,0.0,10858.0,
1754,2020-03-11 15:17:40,1583932660.0,dataengineering,Database/model schema synchronization has arrived in ERBuilder 3.4.,fgx039,Fred_sql,,https://www.reddit.com/r/dataengineering/comments/fgx039/databasemodel_schema_synchronization_has_arrived/,1.0,0.0,0.0,10860.0,
1755,2020-03-11 18:13:16,1583943196.0,dataengineering,Last minute crunch for technical interview,fgzmaw,romanX7,,https://www.reddit.com/r/dataengineering/comments/fgzmaw/last_minute_crunch_for_technical_interview/,1.0,0.0,0.0,10862.0,
1756,2020-03-11 22:47:47,1583959667.0,dataengineering,questions about an application,fh42qx,DarkDoomDoom,,https://www.reddit.com/r/dataengineering/comments/fh42qx/questions_about_an_application/,1.0,3.0,0.0,10862.0,"It's not really related to the work of data engineer but I don't know where to ask this question. 

I applied for a Hadoop data engineer position

The first interview went very well I met a data scientist

The second interview was technical, I had Python and ... data science questions.

According to the feedback from the data scientist, I succeeded very well in python questions but not in data science. I tried to justify myself by saying that I had prepared myself for Hadoop, SQL, Spark, Scala questions etc.

I had this interview last Wednesday and she told me that I would have an answer on Friday next week.

First of all what does this mean? Did they not like my profile and tried to give themselves as much time as possible before making a decision and if possible seeing the other candidates?

And today I just saw that the offer for the job has resurfaced in the recruitment sites. Does that mean they even prefer to find other people? I don't know if they recruit several people but I think only one."
1757,2020-03-12 00:34:22,1583966062.0,dataengineering,Amazon Redshift launches pause and resume,fh5se2,soamv,,https://www.reddit.com/r/dataengineering/comments/fh5se2/amazon_redshift_launches_pause_and_resume/,1.0,3.0,0.0,10864.0,
1758,2020-03-12 04:43:56,1583981036.0,dataengineering,Aws vs Azure for data engineering,fh9fi0,zainuikhan,,https://www.reddit.com/r/dataengineering/comments/fh9fi0/aws_vs_azure_for_data_engineering/,1.0,7.0,0.0,10867.0,"Hello everyone 

I joined reddit a few days ago as I have started to train for data engineering. Recently passed the AWS Cloud Practitioner exam but am torn between choosing Azure vs AWS as the primary cloud for data engineering. From my research it appears that Azure is easier to work with since it’s GUI based and is heavy on T-SQL. Besides Microsoft has always been the top vendor for business intelligence and data warehousing. However AWS is the dominant cloud platform but is more Linux and Python based as it’s more open sourced. I have a background in business intelligence on sql server (around 2 years). I know python but don’t have comparable proficiency to sql. 

Just wanted some advice on which platform to choose which will be easier to adopt to as well as be job safe

Thanks!"
1759,2020-03-12 07:34:01,1583991241.0,dataengineering,[Academic Survey] Decision making &amp; Data Analysis,fhbhie,evadimara,,https://www.reddit.com/r/dataengineering/comments/fhbhie/academic_survey_decision_making_data_analysis/,1.0,0.0,0.0,10869.0,
1760,2020-03-12 07:37:30,1583991450.0,dataengineering,[Academic Survey] Decision making &amp; Data Analysis,fhbitq,evadimara,,https://www.reddit.com/r/dataengineering/comments/fhbitq/academic_survey_decision_making_data_analysis/,1.0,0.0,0.0,10869.0,
1761,2020-03-12 14:45:07,1584017107.0,dataengineering,Sending data from BigQuery to REST API endpoints with Google Cloud Functions,fhfrko,dwl285,,https://www.reddit.com/r/dataengineering/comments/fhfrko/sending_data_from_bigquery_to_rest_api_endpoints/,1.0,1.0,0.0,10873.0,
1762,2020-03-12 18:41:16,1584031276.0,dataengineering,Fav tools/resources for marrying systems/data integration?,fhj8o8,SpicyBiker23,,https://www.reddit.com/r/dataengineering/comments/fhj8o8/fav_toolsresources_for_marrying_systemsdata/,1.0,0.0,0.0,10876.0,"Hi there! 

got a super exciting interview for a data scientist job. Going to be working with older school IT and senior programmers. I am a data scientist - modeler and statistician at heart - so not super versed in data engineering. 

Going to have to confront flat files, old Oracle databases, billing databases, and huge amounts of time series from  automated meters (it's a medium big electric utility) likely out of a commercial product. 

Any advice on tools or processes I should look into and brush up on for data systems integration? I need a crash course. I won't have to do all this work myself, but I *really* need any kind of guidance so I have some familiarity.  

Thanks!"
1763,2020-03-12 18:43:25,1584031405.0,dataengineering,Fav. tools/resources for system marrying/data integration?,fhj9xf,pythagorasshat,,https://www.reddit.com/r/dataengineering/comments/fhj9xf/fav_toolsresources_for_system_marryingdata/,1.0,4.0,0.0,10876.0,"Hi there!

got  a super exciting interview for a data scientist job. Going to be  working with older school IT and senior programmers. I am a data  scientist - modeler and statistician at heart - so not super versed in  data engineering.

Going to have to  confront flat files, old Oracle databases, billing databases, and huge  amounts of time series from  automated meters (it's a medium big  electric utility) likely out of a commercial product. I have a hunch that this utility is literally drowning in data and doesn't know what is useful/not. 

Any  advice on tools or processes I should look into and brush up on for  data systems integration? I need a crash course. I won't have to do all  this work myself, but I *really* need any kind of guidance so I have some familiarity.

Thanks!"
1764,2020-03-12 19:57:01,1584035821.0,dataengineering,Brooklyn based Start-Up looking for a Lead Data Engineer,fhkhru,philcor123,,https://www.reddit.com/r/dataengineering/comments/fhkhru/brooklyn_based_startup_looking_for_a_lead_data/,1.0,0.0,0.0,10877.0,"A Brooklyn based Startup is looking for a Lead Data Engineer as a first hire to build their in-house real time streaming platform! 

This role will have a significant impact on the firm's success and will have a direct path to leadership as the project scales. 

Tech stack: Python primary coding language, spark, hadoop, Kafka for data streaming, Snowflake, Airflow / Luigi  

Team: Total engineering is a team of 12 at the moment that will offer support to various aspects of the project but this is the first hire on the team directly responsible for building this platform. 

I would love to hear from anyone who wants to learn more about the position! 

Contact: philip@alldus.com"
1765,2020-03-13 08:58:24,1584082704.0,dataengineering,Looking for a little advice,fhvlwd,Resident_Author,,https://www.reddit.com/r/dataengineering/comments/fhvlwd/looking_for_a_little_advice/,1.0,7.0,0.0,10888.0,"I've been working as a software engineer for about 8 years on full stack, though I concentrated mostly on the server side. About a year and a half ago while working for a start up I joined their data engineering team in a ""why not"" moment and now I lead the team. I still feel like I have some huge holes in my resume, but am wanting to pursue further into data engineering as I'm enjoying the mix of technologies.

I'm currently maintaining a pipeline on AWS that consists of kinesis/firehose, s3, lambda, rds, elasticsearch/kibana, quicksight, node js on docker ec2, a react front end application, step functions and some glue (pyspark). I'm working right now on designing a new data lake using AWS lake formation with glue workflows and event bridge for operational analytics.

Most of the work I've been doing has been in pyspark and terraform, with some typescript in lambdas and such.

My question is.....is what I've been working on technology wise valuable? Because I'm pretty much solely responsible for our data pipelines, I don't really have any other viewpoint and I have nothing to go off of. I started taking a data engineering multi-quarter certificate at a local university trying to fill in any gaps, but it's been hard to meet the commitment with my work responsibilities. What am I missing in my knowledge base and what skills should i be trying to shore up?"
1766,2020-03-13 13:18:41,1584098321.0,dataengineering,Discord Server for Engineers,fhy224,div20181,,https://www.reddit.com/r/dataengineering/comments/fhy224/discord_server_for_engineers/,1.0,0.0,0.0,10892.0,"Hi,

Check out this engineering study discord server, there are many computer engineers on there, and on top of that there are other engineers such as electrical engineers, computer engineers, aerospace engineers, mechanical engineers, civil engineers and so on!

https://discord.gg/UCbmAyv

I found this server the other day, and I use it to ask for careeer-advice. I'm currently in my third year in uni studying to become an engineer. There was a conference meeting on this server, which was pretty cool and the best thing about it was that the conference meeting was held by a professor in engineering. I asked him careeer-advice and much more, and I really learnt a lot."
1767,2020-03-16 01:45:26,1584315926.0,dataengineering,[Newbie Question] Need help with public S3 bucket,fjb3h6,jc-de,,https://www.reddit.com/r/dataengineering/comments/fjb3h6/newbie_question_need_help_with_public_s3_bucket/,1.0,7.0,0.0,10938.0,"Hi all, 

Beginner DE here. I am having trouble finding a dataset \~100gb to mimic real big data processing. 

I found the AWS Open Data Portal and a dataset I want to explore further; link here: [https://registry.opendata.aws/ichangemycity/](https://registry.opendata.aws/ichangemycity/) 

My issue: 

The readme says the data is available to access for free on S3. I've created an AWS free tier account and I am on the S3 console but have no idea how to extract the above dataset. Any help is appreciated!"
1768,2020-03-16 18:46:28,1584377188.0,dataengineering,Big Data and Analytics in the COVID-19 Era,fjnzzm,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/fjnzzm/big_data_and_analytics_in_the_covid19_era/,1.0,1.0,0.0,10951.0,
1769,2020-03-16 20:27:11,1584383231.0,dataengineering,"Been in data science for years, want to know more about data engineering. What should i learn?",fjpq4e,loct989,,https://www.reddit.com/r/dataengineering/comments/fjpq4e/been_in_data_science_for_years_want_to_know_more/,1.0,5.0,0.0,10952.0,"Basically for years, ive been writing python and r code on my local computer doing regression modeling and some occasional machine learning techniques.

I mostly just have used basic sql queries with a sql server database or accessing redshift thru sql workbench to query.  Nothing more advanced.

So, obv have a basic understanding of databases. SQL, python, and R.

What do i need to learn to become more knowledgeable about data engineering?"
1770,2020-03-16 20:28:36,1584383316.0,dataengineering,What is a good resource to learn about spark/Hadoop configuration,fjpr22,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/fjpr22/what_is_a_good_resource_to_learn_about/,1.0,5.0,0.0,10952.0,"I recently started my DE job but still have not had a good grasp of topics on Hadoop/spark such as

&amp;#x200B;

1. Clusters
2. Memory allocation
3. executor memory
4. executor cores
5. driver memory
6. memory overhead
7. memory.fraction
8. dynamic allocation

&amp;#x200B;

what is  a one/two stop resource I can use to gain a good understanding of these?"
1771,2020-03-17 11:11:12,1584436272.0,dataengineering,Flexible and rapid data modelling,fk254f,Oct04,,https://www.reddit.com/r/dataengineering/comments/fk254f/flexible_and_rapid_data_modelling/,1.0,9.0,0.0,10958.0,"Hey guys,

We're designing our DWH stack from the ground up pretty much. This is an environment which sees rapid change both in terms of data sets and data sources (but mostly data sets and shifting user requirements). Were intending to use Snowflake with an ELT tool such as FiveTran or Matillion.

We're currently deciding how to model, at the moment our legacy DWH uses the fact/dimension star schema model and an ETL process. This isn't very flexible and it's time consuming.

We've considered Data Vault 2.0 as it seems a lot of the benefits align for us, I'm just concerned that whilst it may be flexible, its actually not going to be quick, and it'll take ages to implement!

What are you guys using? - The key points here are very rapid time to insights and flexibility above everything else... tempted to just use Views in Snowflake tbh...!"
1772,2020-03-17 13:25:13,1584444313.0,dataengineering,COVID-19 vs History - R Script (If you have questions or want the script just ask me),fk3m1h,Iam_an_80s_guy,,https://www.reddit.com/r/dataengineering/comments/fk3m1h/covid19_vs_history_r_script_if_you_have_questions/,1.0,2.0,0.0,10960.0,
1773,2020-03-17 19:07:18,1584464838.0,dataengineering,SageMaker: automatically stop your instances when idle,fk8uvl,derivablefunc,,https://www.reddit.com/r/dataengineering/comments/fk8uvl/sagemaker_automatically_stop_your_instances_when/,1.0,0.0,0.0,10962.0,
1774,2020-03-17 21:20:20,1584472820.0,dataengineering,An interview about the CouchDB document database and the work being done to rearchitect it to run on top of FoundationDB,fkb8xe,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/fkb8xe/an_interview_about_the_couchdb_document_database/,1.0,0.0,0.0,10966.0,
1775,2020-03-17 23:40:54,1584481254.0,dataengineering,"A blog for those that have to handle vague and constant stakeholder requests: ""Just One More Stratification! Or: How to say “no” as a data person""",fkdolr,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/fkdolr/a_blog_for_those_that_have_to_handle_vague_and/,1.0,2.0,0.0,10966.0,
1776,2020-03-18 00:30:11,1584484211.0,dataengineering,Real-time Stream Processing on AWS personal project,fkehuf,60ri,,https://www.reddit.com/r/dataengineering/comments/fkehuf/realtime_stream_processing_on_aws_personal_project/,1.0,12.0,0.0,10966.0,"Hi,

I'm looking to get my hands on data stream processing tools a gain some experience in cloud computing as well. At the same time I should pick my master thesis topic in the next couple of weeks. It would be really awesome if I could come up with some topic which will also help me to get experience I want. That's why I think it would be nice to build some data pipeline on AWS, which will allow me to achieve both goals. Considering my inexperience with both AWS platform and processing tools I'm interested in, it would be nice to hear your remarks concerning my plan. Feel free to address any flows or give me any advice. I will be very grateful.

My first concern is project size. My solution should be big enough (meaning I'd like it to be distributed and working with enough data so use of big data tools could be justified). On the other hand I want to keep project price as low as possible ($100 a month tops during next 6 months). I thought I could take a look into real time streaming both because it's interesting and could be potentially cheaper since I won't need to keep large amount of data stored and could anytime spin up and down my project cluster.

Real time streaming brought me to my next problem. What data could I use? There is obviously twitter streaming API, which seems like a possible choice. Do you guys have any idea where I could get some interesting, free streams which will have high enough volume? Any examples where people produced the data from IoT devices or any APIs I failed to google?

My other concern is tool selection considering I only have experience with spark batch processing from the tool set I thing I could use. I bumped into blog on AWS and one of their use cases describing [real time stream processing Apache Spark Streaming and Apache Kafka](https://aws.amazon.com/blogs/big-data/real-time-stream-processing-using-apache-spark-streaming-and-apache-kafka-on-aws/) . My project prototype will then  probably store the data somewhere (not sure what DB I will choose yet, since there is a big question mark concerning data source) and then I wanted to create some simple web app where some basic info/charts will be displayed to illustrate pipeline functionality (let's say some word trends if it's twitter data). Would Apache Kafka make sense for both using with Twitter stream or let's generating my own data through Kafka producer app? Is Spark Streaming on EMR wise choice for data processing? What should I consider when choosing DB? Do you know any good examples of personal projects that leverage some of the mentioned tools? I know this question is difficult to grasp, but I feel like I need some feedback after initial googling couple of sessions. Thanks in advice!"
1777,2020-03-18 12:34:12,1584527652.0,dataengineering,Orchestration for running real time API polling and Websocket jobs,fknhbn,FX-Macrome,,https://www.reddit.com/r/dataengineering/comments/fknhbn/orchestration_for_running_real_time_api_polling/,1.0,8.0,0.0,10974.0,"Hi Guys,

&amp;#x200B;

So I want to pull data from many external web sources. Examples might be a stock market data feed (using WebSockets) or polling an API every X seconds. I'm having trouble finding a tool which will manage and run these jobs for me and then publish into Apache Kafka topics.

&amp;#x200B;

Simply put I want to do these things:

* Collect data from multiple data sources (either via scripts written in Golang/Python or direct integration into the platform)
* Automatic restarting of ingesting jobs on failure
* Act as a producer into Apache Kafka Topics
* Dashboard where I can manage jobs and see any metrics about failures etc.
* Optional: flexibility in running real-time and batch processing jobs

&amp;#x200B;

I've done some research and there's a lot of different approaches: Kafka producers directly (no idea how these are run asynchronously), Jenkins to manage script jobs (although WebSockets would be long standing tasks with no end unless there's an error), Apache NiFi (had some difficult experiences with this in the past), Apache Airflow (seems to be more batch processing), Azkaban, Apache Flume etc.

&amp;#x200B;

Any help would be great, thanks."
1778,2020-03-18 17:15:21,1584544521.0,dataengineering,"I’m a Data Scientist, Not Just The Tiny Hands that Crunch your Data",fkr3ob,ahmedbesbes,,https://www.reddit.com/r/dataengineering/comments/fkr3ob/im_a_data_scientist_not_just_the_tiny_hands_that/,1.0,0.0,0.0,10980.0,
1779,2020-03-18 20:13:11,1584555191.0,dataengineering,"Brown columbia is offering two micro-grants for projects that help inform the public about #COVID19. Each is $5,000, and is for journalists, technologists, health researchers, data and social scientists —any and all communities involved in covering the virus. More at 👇",fku6j6,Gangadhar_s,,https://www.reddit.com/r/dataengineering/comments/fku6j6/brown_columbia_is_offering_two_microgrants_for/,1.0,0.0,0.0,10982.0,
1780,2020-03-18 20:44:14,1584557054.0,dataengineering,airflow k8s executor vs k8s operator,fkupzw,frank998,,https://www.reddit.com/r/dataengineering/comments/fkupzw/airflow_k8s_executor_vs_k8s_operator/,1.0,0.0,0.0,10982.0,
1781,2020-03-19 08:33:19,1584599599.0,dataengineering,Why It’s Vital for Companies to Focus on Data Engineering?,fl5dyp,anishilluzz,,https://www.reddit.com/r/dataengineering/comments/fl5dyp/why_its_vital_for_companies_to_focus_on_data/,1.0,0.0,0.0,10988.0,
1782,2020-03-19 09:17:03,1584602223.0,dataengineering,Why It’s Vital for Companies to Focus on Data Engineering?,fl5v1e,anishilluzz,,https://www.reddit.com/r/dataengineering/comments/fl5v1e/why_its_vital_for_companies_to_focus_on_data/,1.0,3.0,0.0,10987.0,
1783,2020-03-20 02:42:37,1584664957.0,dataengineering,What's the difference between Data Analyst and Data Engineering?,fllfrd,DaniBarro,,https://www.reddit.com/r/dataengineering/comments/fllfrd/whats_the_difference_between_data_analyst_and/,1.0,8.0,0.0,11002.0,
1784,2020-03-20 12:27:48,1584700068.0,dataengineering,Big Data Analytics with PySpark + Power BI + MongoDB,flss2l,Edwinb60,,https://www.reddit.com/r/dataengineering/comments/flss2l/big_data_analytics_with_pyspark_power_bi_mongodb/,1.0,0.0,0.0,11011.0,"If you want to, check it out below:

[https://www.udemy.com/course/big-data-analytics-with-pyspark-power-bi-mongodb/?referralCode=F8D077DD33FFBF7B7077](https://www.udemy.com/course/big-data-analytics-with-pyspark-power-bi-mongodb/?referralCode=F8D077DD33FFBF7B7077)

https://preview.redd.it/wwyvxkfc1tn41.png?width=2560&amp;format=png&amp;auto=webp&amp;s=e345d7138bb746fcfec293574b7f2889dd6a71fa"
1785,2020-03-20 21:40:51,1584733251.0,dataengineering,Do data engineers work on the field?,fm1akc,marcato53,,https://www.reddit.com/r/dataengineering/comments/fm1akc/do_data_engineers_work_on_the_field/,1.0,6.0,0.0,11023.0,"I saw a diagram that showed the hierarchy of data needs, the very bottom is data recording like all the recording that would be done on field like volumes, quality tests etc.  And then the second is reliable data flow, infrastructure, storage and pipelines etc.  So I am wondering as far as organizing all of the logistics for gathering data at the lowest level, are any of you working a position where you are partially responsible both for the project management of the testing equipment etc, AS WELL as the more technically computer work of organizing all the data itself?  I don't necessarily mean hiring the crew/supervising to install the testing equipment, but in terms of being the person who oversees that all of that testing equipment is indeed installed etc?"
1786,2020-03-21 18:06:57,1584806817.0,dataengineering,"Tips/tutorials to build skills in core networking concepts (proxy, NAT, ssh tunnels, port forwarding...)",fmh9ey,phgolard,,https://www.reddit.com/r/dataengineering/comments/fmh9ey/tipstutorials_to_build_skills_in_core_networking/,1.0,4.0,0.0,11035.0,"Even though I've got 7 years exp. in Big data (Hadoop, spark) &amp; Micro Svc. (k8s, docker, node.js), my background is Economics. Sometimes, I feel uncomfortable with core networking topics.
Would be great if you could share tutorials, starting from basics to advanced topics. Ideally hands on (Linux/Mac os). Thx in advance"
1787,2020-03-22 01:12:18,1584832338.0,dataengineering,Digitalisierte Papierunterlagen Sparen Platz &amp;amp; Geld - Staub Scanning,fmp59r,nichollejirasek,,https://www.reddit.com/r/dataengineering/comments/fmp59r/digitalisierte_papierunterlagen_sparen_platz_amp/,1.0,0.0,0.0,11042.0,
1788,2020-03-22 01:25:03,1584833103.0,dataengineering,Data engineer laptop setup on MacBook 13.3,fmpd1y,lnx2n,,https://www.reddit.com/r/dataengineering/comments/fmpd1y/data_engineer_laptop_setup_on_macbook_133/,1.0,0.0,0.0,11042.0,
1789,2020-03-22 22:02:20,1584907340.0,dataengineering,"Delta Lake, Iceberg, Hudi and Hive: Which can actually reshape Data Lake?",fn647j,data-orchestration,,https://www.reddit.com/r/dataengineering/comments/fn647j/delta_lake_iceberg_hudi_and_hive_which_can/,1.0,0.0,0.0,11057.0,"* Delta Lake has the best momentum
* Iceberg has best design
* Hudu has awesome performance
* Hive is fading away

[https://medium.com/@eric.sun\_39815/rescue-to-distributed-file-system-2dd8abd5d80d](https://medium.com/@eric.sun_39815/rescue-to-distributed-file-system-2dd8abd5d80d)"
1790,2020-03-22 22:56:09,1584910569.0,dataengineering,Building your first Cadence Workflow,fn71t8,MaximFateev,,https://www.reddit.com/r/dataengineering/comments/fn71t8/building_your_first_cadence_workflow/,1.0,0.0,0.0,11059.0,
1791,2020-03-23 16:17:00,1584973020.0,dataengineering,An interview with the project lead for Linode's recently released object storage service about the challenges involved in building a provider grade S3 compatible service.,fnkoam,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/fnkoam/an_interview_with_the_project_lead_for_linodes/,1.0,0.0,0.0,11071.0,
1792,2020-03-23 20:31:28,1584988288.0,dataengineering,Data Engineer Macbook setup- Essential tools and software,fnp54h,lnx2n,,https://www.reddit.com/r/dataengineering/comments/fnp54h/data_engineer_macbook_setup_essential_tools_and/,1.0,19.0,0.0,11073.0,"Hello everyone,

I move to work as a data engineer at a start-up and they gave a mac book pro 13.3 inch with touch. I used windows 10 in my previous role for 2 years.

Can you suggest me tools which you use to make daily job responsibilities much faster?

How does your machine setup look like?

Thanks"
1793,2020-03-23 22:56:32,1584996992.0,dataengineering,"Is blockchain useful for data engineering, big data and data science?",fnrs5p,timlee126,,https://www.reddit.com/r/dataengineering/comments/fnrs5p/is_blockchain_useful_for_data_engineering_big/,1.0,12.0,0.0,11077.0,"Is blockchain useful for data engineering, big data and data science? Or will it be?

I am considering whether to study its concepts, among and relative to the others (NLP, Spark, ...).
Thanks."
1794,2020-03-23 23:55:33,1585000533.0,dataengineering,I need data engineering learning resources,fnsuse,akeebismail,,https://www.reddit.com/r/dataengineering/comments/fnsuse/i_need_data_engineering_learning_resources/,1.0,4.0,0.0,11079.0,"Hi there, can someone list any resources could be books or course to learn data engineering"
1795,2020-03-24 01:24:45,1585005885.0,dataengineering,New data intern looking for some advice!,fnuczn,Filmerandeditorguy,,https://www.reddit.com/r/dataengineering/comments/fnuczn/new_data_intern_looking_for_some_advice/,1.0,3.0,0.0,11082.0,"Hello,

First post on a subject such as this, so please guide me to a different place if needed. I have a situation I could really use some suggestions with. I graduated college with my degree in management information systems and now I got a job as a data intern. I do not have anyone in my company to teach me as i am the first data position this company has had. 

What the company does now:

Has forms which internal clients request a job on. These forms store data in SharePoint list data. On the employee side they can continue the form to add other information needed. I have not created a data model yet but I think I may make a relational data model to help with solutions. Right now there is a JobID and a whole bunch of attributes as the data is very flat. An employee has an automatically updating excel file with this data but it takes 40 min to update and it is so impractical to work with due to how unorganized and slow it is. 

&amp;#x200B;

What I have been thinking:

Set up an azure SQL database and use flows to move the data from SharePoint into SQL and then from there into powerBI. This way I can query useful data way quicker. However the data is still very flat and unstructured and I have not done this before so I am trying to learn. I am looking for suggestions on some sort of data pipeline. I was told the data must stay in SharePoint but I can move it from there. For reference the excel file that is being used has 120,000 cells. I know this isn't that many comparatively but for how unstructured this system is it is very difficult to deal with."
1796,2020-03-24 01:32:05,1585006325.0,dataengineering,Overwhelmed :/,fnuh7a,Enough-Word,,https://www.reddit.com/r/dataengineering/comments/fnuh7a/overwhelmed/,1.0,2.0,0.0,11083.0,"I recently got my first job in both a data analyst and data engineering capacity. I'm also the first grad student they've hired,  so the onboarding was :/. 

I've completed a couple of other internships (as data analyst)  but data engineering seems like a whole new ball game. 

I got given the opportunity through a connection in the company. I'm also on a 6 months contract so if I want to get it extended I'm gonna prove to have held my weight come review time. 

Working from home because of COVID-19 definitely hasn't made my transition smoother but oh well, it is what it is, I guess.

The initial task I've been given is to understand the data pipeline and learn how to use some of the services we use on Azure (BLOB, Logic Apps, Data lake, data factory, etc). 

So my fellow Redditors, if you have any tips for me to be mindful of or any resources I could use, it'd be much appreciated."
1797,2020-03-24 03:23:48,1585013028.0,dataengineering,"Where shall I install HDFS filesystem: on my existing hard drive, partition, filesystem or ...?",fnw8v0,timlee126,,https://www.reddit.com/r/dataengineering/comments/fnw8v0/where_shall_i_install_hdfs_filesystem_on_my/,1.0,1.0,0.0,11084.0,
1798,2020-03-24 04:45:05,1585017905.0,dataengineering,"For those of you who manage data engineers , what’s your day to day ?",fnxgeq,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/fnxgeq/for_those_of_you_who_manage_data_engineers_whats/,1.0,11.0,0.0,11085.0,
1799,2020-03-24 13:20:24,1585048824.0,dataengineering,Big data or data engineering for NLP?,fo3hmw,timlee126,,https://www.reddit.com/r/dataengineering/comments/fo3hmw/big_data_or_data_engineering_for_nlp/,1.0,0.0,0.0,11095.0,
1800,2020-03-24 13:59:47,1585051187.0,dataengineering,Is Hadoop still the way to go?,fo3xnr,owtdata,,https://www.reddit.com/r/dataengineering/comments/fo3xnr/is_hadoop_still_the_way_to_go/,1.0,36.0,0.0,11096.0,"I’m a graduate data scientist at a company that is trying to be more innovative with their data.
There’s a lot of research here bringing in a lot of data and the current management system isn’t ideal

I’ve been looking into setting up Hadoop as a way to manage the data. We have data coming in in 10 minute chunks from a variety of sources 

What I was wondering is if it is still worth going down the Hadoop route? Or should I be looking more into Databricks and things like that?

I’m really new to this stuff so any help at all would be appreciated"
1801,2020-03-24 14:05:59,1585051559.0,dataengineering,Airflow Kubernetes Executor on AWS Help,fo40is,goobdin,,https://www.reddit.com/r/dataengineering/comments/fo40is/airflow_kubernetes_executor_on_aws_help/,1.0,0.0,0.0,11096.0,"Hi everyone, 

I am in need of some guidance with regards to scaling our airflow deployment at my company. Does anyone have any good resources for deploying Airflow with the Kubernetes Executor on AWS?

Thank you in advance!"
1802,2020-03-24 16:55:53,1585061753.0,dataengineering,What kind of calculations can you do with ticker data?,fo6fpn,enigmatic_user,,https://www.reddit.com/r/dataengineering/comments/fo6fpn/what_kind_of_calculations_can_you_do_with_ticker/,1.0,7.0,0.0,11098.0,"Hey everyone, 

I'm currently building a streaming pipeline using Pub/Sub and Beam to ingest and stream ticker data into BigQuery for analysis as a personal project.

I thinking of also adding calculations in my Beam pipeline to add some complexity to it but not quite sure what kind of calculations I can do with this kind of data. 

The following link shows the type of data that I'm currently pulling:

 [https://docs.pro.coinbase.com/#the-ticker-channel](https://docs.pro.coinbase.com/#the-ticker-channel)"
1803,2020-03-25 17:33:13,1585150393.0,dataengineering,Self-Hosted or Standalone Spark Server?,fosa3a,LexaIsNotDead,,https://www.reddit.com/r/dataengineering/comments/fosa3a/selfhosted_or_standalone_spark_server/,1.0,15.0,0.0,11120.0,"It looks like my company won't be approving expenses anytime soon given that our revenue is being impacted by the COVID-19 situation. However, I still want to move forward with transitioning some of our ELTs to Apache Spark (and thus convert them to ETLs), as there is actually a need for it. Does anyone know of any good tutorials or guides that they've used to set up a standalone Apache Spark instance, preferably on Docker? I've already done some searching but there doesn't seem to be a comprehensive guide on how to do so, with some steps missing here and there."
1804,2020-03-25 17:36:04,1585150564.0,dataengineering,Best practice for scheduling Python ETL scripts,fosbqi,P_H_I_L_L_Y,,https://www.reddit.com/r/dataengineering/comments/fosbqi/best_practice_for_scheduling_python_etl_scripts/,1.0,28.0,0.0,11120.0,"We currently utilize Snowflake as our DW and would like use the Snowflake connector for our Python scripts. 

We are completely on AWS for our environment and would like to schedule some Python scripts for daily ETL. Without having someone run the scripts manually on their laptops, what is best practice to get things running on a regular basis? 

  
Let me know if I could provide any additional information."
1805,2020-03-25 18:39:56,1585154396.0,dataengineering,SparklyClean - efficient data deduplication and supervised learning pipeline using Apache Spark,fotgbs,CaptainKamina,,https://www.reddit.com/r/dataengineering/comments/fotgbs/sparklyclean_efficient_data_deduplication_and/,1.0,0.0,0.0,11119.0,"Hi all!

I built this framework based on a VLDB paper ([http://www.vldb.org/pvldb/vol9/p864-chu.pdf](http://www.vldb.org/pvldb/vol9/p864-chu.pdf)), for the specific purpose of parallelizing data deduplication tasks. It is my very first time building something like this, so I would very much appreciate any feedback/suggestions!

[https://github.com/david-siqi-liu/sparklyclean](https://github.com/david-siqi-liu/sparklyclean)

Cheers!"
1806,2020-03-25 21:22:22,1585164142.0,dataengineering,how do I compare two pandas dataframes in Spark?,fowd0t,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/fowd0t/how_do_i_compare_two_pandas_dataframes_in_spark/,1.0,3.0,0.0,11122.0, I need to compare two dataframes and point out columns which are different between the two dataframes. How would I accomplish this?
1807,2020-03-26 00:02:29,1585173749.0,dataengineering,COVID-19 calculator and data,foz8g3,JonNexoid,,https://www.reddit.com/r/dataengineering/comments/foz8g3/covid19_calculator_and_data/,1.0,0.0,0.0,11126.0,"The medical research side of this pandemic is advancing quickly. Sadly the same can not be said for political and strategic understanding. The more we know about the geographic distribution of medical conditions, the better decisions our governments can make. By filling in the COVID-19 Survival Calculator, you get an insight into your own personal risk as well as providing data that will be used to save lives.

Your privacy is important. We do not need identifiable information. You can fill in the calculator anonymously. 

Find out more visit.

[https://www.covid19survivalcalculator.com/](https://www.covid19survivalcalculator.com/)

Fill in the calculator. 

[https://www.covid19survivalcalculator.com/calculator](https://www.covid19survivalcalculator.com/calculator)

Download the dataset.

https://www.covid19survivalcalculator.com/data/download.csv

This project is being run by Nexoid, United Kingdom. We are a data and business systems company, usually working in the finance sector. We have pulled our resources to assist with the COVID-19 pandemic.

Find out more about Nexoid 

[https://www.nexoid.com/](https://www.nexoid.com/)"
1808,2020-03-26 00:34:41,1585175681.0,dataengineering,Managing JSON data (+ integration with AWS S3),fozs52,vladproex,,https://www.reddit.com/r/dataengineering/comments/fozs52/managing_json_data_integration_with_aws_s3/,1.0,6.0,0.0,11126.0,"Hope I'm in the right corner to ask this. I have around 150 GB of data organized in thousands of JSONL files. Altogether, there are hundreds of millions of items. The files will be downloaded from an S3 bucket and stored on the work server. 

I need to perform fast queries on these items. The ultimate goal is to get the relevant JSON objects. I usually work with Python. I should also say that I am in a research context, not a production one. However, speed is important.

What is the best, fastest solution for this? I heard MongoDB would be the right solution. I don't think SQL would work, because the objects have nested fields and the fields coverage varies quite a lot. Are there drawbacks to MongoDB? How fast would the queries be?"
1809,2020-03-26 03:27:49,1585186069.0,dataengineering,Tokenization in a Data Lake environment,fp2mic,databass09,,https://www.reddit.com/r/dataengineering/comments/fp2mic/tokenization_in_a_data_lake_environment/,1.0,6.0,0.0,11126.0,"Given that there is a lot of momentum around privacy laws (GDPR, CCPA), I am thinking that I'll have to start building in ways to tokenize PII data before I store it. Has anyone developed any patterns that they use to tokenize PII when developing data pipelines? Off of the top of my head, it seems that you would have to:

1. Read the PII value
2. Apply a hash function to the PII value
3. Store the PII value and resulting hash value into a low latency key/value database
4. Store the resulting dataset without PII in the data lake

I'd love to hear ideas on this!"
1810,2020-03-26 09:50:45,1585209045.0,dataengineering,"Data Teams Going ""Remote"" - Challenges, Learnings &amp; Observations",fp7k2d,ss1222,,https://www.reddit.com/r/dataengineering/comments/fp7k2d/data_teams_going_remote_challenges_learnings/,1.0,0.0,0.0,11130.0,
1811,2020-03-26 12:41:51,1585219311.0,dataengineering,Need review about SimpliLearn Big Data Engineer masters program,fp9dxs,mortycodes,,https://www.reddit.com/r/dataengineering/comments/fp9dxs/need_review_about_simplilearn_big_data_engineer/,1.0,0.0,0.0,11131.0,
1812,2020-03-26 16:17:20,1585232240.0,dataengineering,What Order to Learn?,fpc8h4,SatoriSlu,,https://www.reddit.com/r/dataengineering/comments/fpc8h4/what_order_to_learn/,1.0,14.0,0.0,11133.0," 

Hello!  I was hoping to get some general guidance on what to learn first.  I recently accepted a position at a software company as a Site Reliability Engineer.  I will be supporting their analytics product which heavily relies upon the following technologies:  

&amp;#x200B;

\- Map Reduce

&amp;#x200B;

\- Spark

&amp;#x200B;

\- Hive

&amp;#x200B;

\- Kafka

&amp;#x200B;

\- Druid

&amp;#x200B;

\- HDInsights

&amp;#x200B;

  My question is: Given the tools/technologies above, what order should I learn these in to best understand how they all work together?  

&amp;#x200B;

I have found some good resources to learn the majority (with the exception of Hive and Druid) but need some guidance on what to learn first.  Essentially, I am just looking to establish familiarity with these products so I have a good foundation going in.  I have about 10 days to study these topics before I start so I'm trying to be as efficient as possible.  

&amp;#x200B;

Thank you!"
1813,2020-03-26 18:05:29,1585238729.0,dataengineering,I am lost - need help designing a beginner DE project,fpe39w,wooojc,,https://www.reddit.com/r/dataengineering/comments/fpe39w/i_am_lost_need_help_designing_a_beginner_de/,2.0,6.0,0.0,11136.0,"Hi DE, 

As a current Analyst,  I am trying to make a switch to DE and now that we are WFH, I have all the time in the world to level up. I want to design a *reasonable* capstone project and eventually put it on GitHub that shows that I can use DE/cloud tools and also SWE best practices. 

My details: 

\- I know SQL quite well. My work situation does not necessarily accommodate my role to venture out into DE-related tasks. (I know a fair amount about: postgresql, SQLalchemy, SQLITE3) 

\- I know python at basic - intermediate level (continuing to learn as I go) 

\- Was doing the data engineering course on DataQuest for a few months (found it quite frustrating tbh; but learned stuff nonetheless) 

\- learning Spark  

\- learning AWS through free udemy course as well 

I found Common Crawl an interesting source of data (since it's TBs in size) but after days of trying to connect their S3 bucket to pyspark, it's showing that it's perhaps not beginner friendly. 

**What are some reasonable DE projects I can do with my skills above, to show what I've learned in DE, cloud tools and show SWE best practices?**"
1814,2020-03-26 22:12:01,1585253521.0,dataengineering,Understanding the Concept of an Event Portal – An API Portal for Events,fpip0f,DigitalBackbone,,https://www.reddit.com/r/dataengineering/comments/fpip0f/understanding_the_concept_of_an_event_portal_an/,1.0,0.0,0.0,11140.0,
1815,2020-03-26 22:12:35,1585253555.0,dataengineering,3 Steps to Becoming an Event-Driven Enterprise,fpipep,DigitalBackbone,,https://www.reddit.com/r/dataengineering/comments/fpipep/3_steps_to_becoming_an_eventdriven_enterprise/,1.0,6.0,0.0,11140.0,
1816,2020-03-27 05:13:41,1585278821.0,dataengineering,Does database replication require some routing for client to read or write a replica?,fpplxa,timlee126,,https://www.reddit.com/r/dataengineering/comments/fpplxa/does_database_replication_require_some_routing/,1.0,3.0,0.0,11146.0,
1817,2020-03-27 14:54:33,1585313673.0,dataengineering,Webinar on The Emergence of Big Data and Its Solutions,fpwfsb,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/fpwfsb/webinar_on_the_emergence_of_big_data_and_its/,1.0,0.0,0.0,11157.0,
1818,2020-03-27 19:20:24,1585329624.0,dataengineering,Are the servers behind a load balancer replicas or partitions ?,fq147u,timlee126,,https://www.reddit.com/r/dataengineering/comments/fq147u/are_the_servers_behind_a_load_balancer_replicas/,1.0,3.0,3.0,11164.0,"When talking about load balancing https://en.wikipedia.org/wiki/Load_balancing_(computing),
do people assume the servers behind a load balancer are replicas or partitions or something else? 

- With replication, load balancer can direct a request to any server.

- With partitions, load balancer can direct a request only to one server. I imagine a need to adjust what data are stored on each server, so as to balance loads between the servers.


Thanks."
1819,2020-03-27 21:35:05,1585337705.0,dataengineering,Learning buddy needed,fq3wod,wise2wiz,,https://www.reddit.com/r/dataengineering/comments/fq3wod/learning_buddy_needed/,1.0,92.0,0.0,11165.0,"Hello ,
   I would like to learn data engineering skills and like to join or build a group of learning buddies that helps to work on projects and bring up ideas that solve smaller probelms in data engineering focussing on databases, ETL and building data lakes and workflows using spark and airflow like tools. Any leads are appreciated. 
Thanks"
1820,2020-03-28 00:29:32,1585348172.0,dataengineering,Webinar - Conducting Remote Interviews: Engineering Edition,fq7e3h,hszafarek,,https://www.reddit.com/r/dataengineering/comments/fq7e3h/webinar_conducting_remote_interviews_engineering/,1.0,0.0,0.0,11168.0,"Register here: [https://zoom.us/webinar/register/WN\_tuBKtmc1Tm67dMJcfina0A](https://zoom.us/webinar/register/WN_tuBKtmc1Tm67dMJcfina0A?fbclid=IwAR1XosLKnR-Sq4KADlWRXtuaCPvBZwo1NPZTdNSrTDpcKzHzvoXpQNpbulk)

https://preview.redd.it/i5nu9txckap41.png?width=1999&amp;format=png&amp;auto=webp&amp;s=fd41642a36b95564e21e7affb79945963b3b9df0"
1821,2020-03-28 12:44:35,1585392275.0,dataengineering,Ideas for designing a data pipeline.,fqhufk,ordinary_guy1,,https://www.reddit.com/r/dataengineering/comments/fqhufk/ideas_for_designing_a_data_pipeline/,1.0,1.0,0.0,11179.0,"Let's say I have a set of microservices (let's assume 5) which are emitting events for each transaction. Let's say microservice 1 emits an INIT event with ID1. This event is consumed by the service 2 and adds its own meta data and maintaining the ID sent by the service 1. Likewise, all events flow through all the services if a transaction is successful otherwise a service which marks the transaction as failed, those events will be ignored by further services. All the events are emitted in Kafka. I wish to assign a unique id to all the events belonging to a particular transaction and stream them to hadoop using spark/flink. Which one will work the best for this use case? 

Also, I assume I will need a temporary cache to store all the events of a transaction for some time until a unique id is not assigned to all the events of a transaction. Is there a better way of doing this using streaming engine like spark or flink? I read about stateful streaming and windowing. Will that help here?"
1822,2020-03-28 15:53:41,1585403621.0,dataengineering,Do the things (files?) being replicated included in the state of a server?,fqkkr7,timlee126,,https://www.reddit.com/r/dataengineering/comments/fqkkr7/do_the_things_files_being_replicated_included_in/,1.0,1.0,0.0,11178.0,"&gt; The central  role of the file service in distributed systems makes it 
&gt; essential that the service continue to operate in the face of client
&gt; and server failures.  Fortunately, a moderately fault-tolerant design
&gt; is straightforward for simple servers. To  cope with transient
&gt; communication failures, the design can be based on at-most-once
&gt; invocation  semantics  (see  Section  5.3.1);  or  it  can  use  the 
&gt; simpler  at-least-once semantics with a server protocol designed in
&gt; terms of idempotent operations, ensuring  that duplicated requests do
&gt; not result in invalid updates to files. **The servers can be  stateless**,
&gt; so that they can be restarted and the service restored after a failure
&gt; without any  need to recover previous state. **Tolerance of
&gt; disconnection or server failures requires file  replication**, which is
&gt; more difficult to achieve and will be discussed in Chapter 18.

What is the state of the server?

Do the things (files?) being replicated not included in the state of the server?

Thanks."
1823,2020-03-28 19:25:44,1585416344.0,dataengineering,Change data capture,fqoe3a,babuuz,,https://www.reddit.com/r/dataengineering/comments/fqoe3a/change_data_capture/,1.0,29.0,0.0,11180.0,"Our company is buying CDC tool. Few companies approached and introduced us their product. Our main goal of this tool is to get incremental update from Oracle database. They all have their own advantages. But I would like to hear the feedback from the users  who used this tool. Could you share your feedback on products such as Striim, attunity, oracle etc."
1824,2020-03-29 00:53:35,1585436015.0,dataengineering,I created a discord server specifically for those starting out or interested in the field of Data Engineering!,fquk0i,wooojc,,https://www.reddit.com/r/dataengineering/comments/fquk0i/i_created_a_discord_server_specifically_for_those/,4.0,4.0,0.0,11181.0,"I am sure there are similar people out there looking for help/resources/a place to ask silly questions/ find study buddies, etc in this field! Let's help each other out...here is the link: [https://discord.gg/aUawNPm](https://discord.gg/aUawNPm) 

It would also be awesome if experienced Data Engineers joined and helped us out. 

Disclaimer: 

1. I am not a Data Engineer; just pushing this cause along to get into this field myself. 
2. This is my first discord server - bare with me while I figure things out."
1825,2020-03-29 21:04:55,1585505095.0,dataengineering,Advice for someone starting out?,fra6uq,frankieatlantic,,https://www.reddit.com/r/dataengineering/comments/fra6uq/advice_for_someone_starting_out/,3.0,10.0,0.0,11201.0,"Couple of things about me: 

\-Out of college, I'm working at a mid-sized tech company in a role called ""Data Science Practice"" but it is mostly a combo of Data Engineering/Analytics, I don't work w/ algorithms. 

\-I really enjoy the engineering side of it and want to pursue that, but I'm not a CS major so I do lack certain fundamentals in things like Data Sturctures and Algorithms. 

\-I'm proficient in SQL and getting better w Python but no other technical skills. 

&amp;#x200B;

Basically, with the whole lockdown, I'm trying to create a plan to both learn the theory behind the field while also having projects or certificates to put on my resume outside of work. I want to try and move to a more focused DE position in a year. What would you guys suggest?"
1826,2020-03-29 21:18:32,1585505912.0,dataengineering,"How to implement HDFS in user space, so that FUSE can work with it?",frafcu,timlee126,,https://www.reddit.com/r/dataengineering/comments/frafcu/how_to_implement_hdfs_in_user_space_so_that_fuse/,1.0,0.0,0.0,11200.0,"https://en.wikipedia.org/wiki/Filesystem_in_Userspace says 

&gt; Filesystem in Userspace (FUSE) is a software interface for Unix and Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by **running file system code in user space** while the FUSE module provides only a ""bridge"" to the actual kernel interfaces. 

*Hadoop The Definitive Guide* says:

&gt; Filesystem in Userspace (FUSE) allows **filesystems that are implemented in user space**
to be integrated as Unix filesystems. Hadoop’s Fuse-DFS contrib module allows HDFS
(or any Hadoop filesystem) to be mounted as a standard local filesystem. Fuse-DFS is
implemented in C using libhdfs as the interface to HDFS. At the time of writing, the
Hadoop NFS gateway is the more robust solution to mounting HDFS, so should be
preferred over Fuse-DFS.

When using FUSE with HDFS, does it require  implementing HDFS in user space?

Given HDFS is a distributed parallel file system, what does ""implementing HDFS in user space"" mean specifically?

Thanks."
1827,2020-03-30 00:11:33,1585516293.0,dataengineering,"CALLING ALL DATA ENGINEERS! Four minutes of your time can save thousands, if not millions, of lives!",frdf0l,AILaunchpad,,https://www.reddit.com/r/dataengineering/comments/frdf0l/calling_all_data_engineers_four_minutes_of_your/,1.0,1.0,0.0,11205.0,Appreciate your feedback and support on how to engineer the data extracted from this survey: [https://www.surveymonkey.com/r/QFQJH7V](https://www.surveymonkey.com/r/QFQJH7V)
1828,2020-03-30 03:50:02,1585529402.0,dataengineering,Is data engineering a highly lucrative career to pursue?,frh28v,Perceptive_Person,,https://www.reddit.com/r/dataengineering/comments/frh28v/is_data_engineering_a_highly_lucrative_career_to/,1.0,16.0,0.0,11209.0,"I'm a software engineer undergrad, considering to specialise in something rather than being a generalist

I looked at stack overflow 2019 list of highest paying skills and Scala is #1

Dice also has a 2020 report which puts Cloudera and MapReduce as 2 of the 4 highest paying skills

Everyone keeps talking about data science and machine learning being the next big thing but for that to machine learning to occur, we need data engineers to build the pipelines first

Should I pursue data engineering? My aim is to get a really high paying job\* like working at Jane Street/ hedge fund. If I aim to be a quant atm, I don't think it'll work out because I need to learn a lot of maths and stats and finance before I graduate and also get amazing grades somehow. However, I already have big data work experience and could double down on that.

\*I know I might get backlash for saying that and people telling me to not aim for money but computer science is my passion and I'd rather work at a hedge fund and get 300k for doing the same work I'm doing at a tech company.  I dont have rich parents so building a financial foundation is really important to me"
1829,2020-03-30 04:59:31,1585533571.0,dataengineering,Small-medium business - is snowflake overkill?,fri4bo,we_are_ananonumys,,https://www.reddit.com/r/dataengineering/comments/fri4bo/smallmedium_business_is_snowflake_overkill/,1.0,8.0,0.0,11209.0,"Hi

I’ve been working with a small but successful production business who have a need to overhaul their reporting - initially to automate what they already have, but with a view to increasing their data science in the near future. They have several application databases that data needs to be drawn from, and it’s currently wrangled manually in excel. They’re interested in Tableau/powerBI. 

I’m wondering whether you would generally advocate for implanting a basic cloud warehouse eg Snowflake, to land and conform the data from the apps and model it before using it in the BI tool. My instinct tells me that it’s be better to start to implement this layer of separation and storage now. Agree/disagree?

Thanks"
1830,2020-03-30 07:42:21,1585543341.0,dataengineering,Data Engineer at Amazon,frkeft,iwillgetintofaang,,https://www.reddit.com/r/dataengineering/comments/frkeft/data_engineer_at_amazon/,1.0,12.0,0.0,11211.0,"I applied for Data Engineer role at Amazon. The requirement is mostly Data Modeling, Data Warehousing, building ETL pipelines and Sql. Recruiter didn't provide much information or prep materials but mentioned the first phone round is scripting. I would like to prep hard and do whatever it takes to make it in the first round. 

Any suggestions on what to expect and specific areas to focus ?"
1831,2020-03-30 13:34:57,1585564497.0,dataengineering,Data architecture guidelines,frodwr,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/frodwr/data_architecture_guidelines/,1.0,0.0,0.0,11212.0,
1832,2020-03-30 14:05:23,1585566323.0,dataengineering,Kafka Connect JDBC Sink: tips &amp; tricks (video walkthrough),fror98,rmoff,,https://www.reddit.com/r/dataengineering/comments/fror98/kafka_connect_jdbc_sink_tips_tricks_video/,1.0,0.0,0.0,11213.0,
1833,2020-03-30 15:18:43,1585570723.0,dataengineering,Webinar on Hopsworks Feature Store for Databricks,frpr15,jpdowlin,,https://www.reddit.com/r/dataengineering/comments/frpr15/webinar_on_hopsworks_feature_store_for_databricks/,1.0,3.0,0.0,11215.0,
1834,2020-03-30 16:56:21,1585576581.0,dataengineering,Webinar on The Emergence of Big Data and Its Solutions,frr8n0,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/frr8n0/webinar_on_the_emergence_of_big_data_and_its/,1.0,0.0,0.0,11214.0,
1835,2020-03-30 18:20:31,1585581631.0,dataengineering,Do we really need Open Source Time Series Databases?,frsp95,Timbo2020,,https://www.reddit.com/r/dataengineering/comments/frsp95/do_we_really_need_open_source_time_series/,1.0,1.0,0.0,11216.0,
1836,2020-03-30 20:00:19,1585587619.0,dataengineering,Robust Apache Airflow Deployment,fruk1e,darthvader003,,https://www.reddit.com/r/dataengineering/comments/fruk1e/robust_apache_airflow_deployment/,1.0,0.0,0.0,11217.0,
1837,2020-03-31 00:06:54,1585602414.0,dataengineering,Robust Apache Airflow Deployment,frz9s1,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/frz9s1/robust_apache_airflow_deployment/,1.0,0.0,0.0,11219.0,
1838,2020-03-31 00:07:07,1585602427.0,dataengineering,Easy way to manage your Airflow setup,frz9x7,akhilanandbv003,,https://www.reddit.com/r/dataengineering/comments/frz9x7/easy_way_to_manage_your_airflow_setup/,1.0,0.0,0.0,11219.0,
1839,2020-03-31 00:22:23,1585603343.0,dataengineering,Prefect Now Fully Open Source,frzjru,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/frzjru/prefect_now_fully_open_source/,1.0,13.0,0.0,11219.0,
1840,2020-03-31 02:50:23,1585612223.0,dataengineering,How important is domain expertise/knowledge for data engineering?,fs259a,___24601,,https://www.reddit.com/r/dataengineering/comments/fs259a/how_important_is_domain_expertiseknowledge_for/,1.0,2.0,0.0,11220.0,
1841,2020-03-31 03:42:36,1585615356.0,dataengineering,Monitoring Databrick Jobs with Datadog,fs305s,CesQ89,,https://www.reddit.com/r/dataengineering/comments/fs305s/monitoring_databrick_jobs_with_datadog/,1.0,0.0,0.0,11220.0,"Does anyone here have any experience with using Datadog to monitor Databricks jobs? If so, what's your experience like? 

My team and I are currently using it but we find it very limiting in regards to monitoring individual Notebooks.  Datadog seems better at monitoring ""general"" infrastructure metrics such as if the cluster is up or down, overall Job/Stage counts, etc but we need more depth, down to the notebook cell running and read/write speeds.

Prior to Datadog we used Slack Webhooks and the Jobs UI to ""monitor"" individual job/cells for failure and what not but our boss wants us to use start using Datadog for whatever reason. 

Just curious about other people's experience."
1842,2020-03-31 12:53:18,1585648398.0,dataengineering,Can I install and use Apache-airflow in Windows?,fsa64z,_ibleedgreen_,,https://www.reddit.com/r/dataengineering/comments/fsa64z/can_i_install_and_use_apacheairflow_in_windows/,1.0,4.0,0.0,11225.0,"Hello,
I've been using Airflow in my couple of projects for a while. I have always used Linux for that. Recently I had to switch on Windows and couldn't install Airflow. So, is it possible to install airflow on Windows and if yes , does it make it's usage any different than in Linux."
1843,2020-03-31 14:56:34,1585655794.0,dataengineering,An interview with Tyler Colby about his experiences working as a data professional in the non-profit sector and the challenges that are unique to that domain,fsbmfs,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/fsbmfs/an_interview_with_tyler_colby_about_his/,1.0,0.0,0.0,11225.0,
1844,2020-03-31 16:10:13,1585660213.0,dataengineering,Webinar on The Emergence of Big Data and Its Solutions,fscmut,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/fscmut/webinar_on_the_emergence_of_big_data_and_its/,1.0,0.0,0.0,11225.0,
1845,2020-03-31 17:34:44,1585665284.0,dataengineering,Ways to automate reading emails and attachments - discussion thread.,fsdyvh,lnx2n,,https://www.reddit.com/r/dataengineering/comments/fsdyvh/ways_to_automate_reading_emails_and_attachments/,1.0,1.0,0.0,11227.0,"Hello, How do you automate reading emails and attachments? I know that you can do it with Nifi and that would be my first try since I have some knowledge on it.

What are other ways and tools which you used or would use?

Thanks."
1846,2020-03-31 19:03:09,1585670589.0,dataengineering,Cohesion: Rethinking Workflow Development,fsfjql,soamv,,https://www.reddit.com/r/dataengineering/comments/fsfjql/cohesion_rethinking_workflow_development/,0.0,0.0,0.0,11228.0,
1847,2020-03-31 20:53:54,1585677234.0,dataengineering,"Google Cloud Data Engineer, Professional Services",fshmqc,_bvic_7,,https://www.reddit.com/r/dataengineering/comments/fshmqc/google_cloud_data_engineer_professional_services/,1.0,0.0,0.0,11232.0,
1848,2020-04-01 09:08:00,1585721280.0,dataengineering,Using federated queries on GCP from AWS??,fstzgg,dataduncan,,https://www.reddit.com/r/dataengineering/comments/fstzgg/using_federated_queries_on_gcp_from_aws/,1.0,7.0,0.0,11243.0,"hello all, our company stack uses AWS S3 for our storage of raw events logs which we then process and transfer over to GCS and finally to BQ.

We want to try and reduce costs in terms of storage on the BQ side, so we are looking at using federated queries in BQ to get the data we need from AWS within our ETL pipeline and, therefore, avoid the need to transfer some data over from AWS.

Has anyone tried using federated queries, or used them to take data from AWS? In the Google documentation I can only find guides on mysql databases, but nothing specifically on AWS?

Any help would be much appreciated!"
1849,2020-04-01 11:19:33,1585729173.0,dataengineering,How do you keep development and production environments separated?,fsvo6h,Crolle,,https://www.reddit.com/r/dataengineering/comments/fsvo6h/how_do_you_keep_development_and_production/,1.0,19.0,0.0,11244.0,"I manage data pipelines at a company that has been scaling up over the last months. At first, I was asked to design a very flexible architecture to match the product evolution, but now this flexibility comes at the cost of reliability. When I had only a few customers, changing a thing to the pipe and rebuilding dependencies was not a big deal, but  as the product gets more and more complex, screw ups become more visible. Besides, the business now wants releases, just like any other software. So, enough with the YOLO.

I would like to know from your experience how your manage to keep development and production environments separated. Do you keep versions of your code along with versions of your data? Are there tools around that help you do that ? (I'm working with Dataiku DSS for the moment, but I'm open to alternatives)."
1850,2020-04-01 14:27:54,1585740474.0,dataengineering,Anyone working with Ruby?,fsy0b4,blockchan,,https://www.reddit.com/r/dataengineering/comments/fsy0b4/anyone_working_with_ruby/,1.0,5.0,0.0,11244.0,"Python is everywhere in data, but I was wondering if anyone uses Ruby?

I find it much more pleasant for writing anything custom and wrangling data.

Does Python rules due to network effects (and subsequent libraries) only? Or is it objectively better than Ruby?

Do I hinder my career opportunities by leveraging Ruby over Python?"
1851,2020-04-01 17:24:11,1585751051.0,dataengineering,Having a data warehouse vs Querying data lake directly- what do you prefer and why?,ft0prb,lnx2n,,https://www.reddit.com/r/dataengineering/comments/ft0prb/having_a_data_warehouse_vs_querying_data_lake/,1.0,18.0,0.0,11245.0,"Hello,

Did any one evaluate querying data lake directly in contrast to having a need of a designated data warehouse?

What do you prefer?"
1852,2020-04-01 21:22:43,1585765363.0,dataengineering,Validating data science pipelines,ft5k61,sasjurse,,https://www.reddit.com/r/dataengineering/comments/ft5k61/validating_data_science_pipelines/,1.0,4.0,0.0,11251.0,"How do you validate your data pipelines? 

From the engineering side, I see some push for testing on synthetic data. However, creating synthetic data is hard ( and might look completely different from your real data).

From the data science side, I see that a lot of ""data scientist testing"" is actually inspecting the results or creating visualizations. Which is kinda hard to replicate on daily batch runs of pipelines. 

I've preferred doing tests on the results, (even wrote a post about it here: [https://medium.com/me/stats/post/952c5985e070](https://medium.com/me/stats/post/952c5985e070) ), but curious about other approaches"
1853,2020-04-01 22:01:19,1585767679.0,dataengineering,Need Guidance towards learning Cassandra,ft6apa,Dminor77,,https://www.reddit.com/r/dataengineering/comments/ft6apa/need_guidance_towards_learning_cassandra/,1.0,6.0,0.0,11253.0,"Hi, I started learning Cassandra a week ago from linkedIn learning. Completed the Essentials of Apache Cassandra that covered: Architecture, Data Modeling, Data Types, Table Designing, Consistency level, and Materialized Views.

I want to deep dive further. Can anyone please guide me what resources I should see and what projects I should implement to learn more and experience the power of Cassandra?

Thank you."
1854,2020-04-01 22:54:39,1585770879.0,dataengineering,"Why You Need to Look Beyond Kafka for Operational Use Cases, Part 1: The Need for Filtering and In-Order Delivery",ft7aoy,DigitalBackbone,,https://www.reddit.com/r/dataengineering/comments/ft7aoy/why_you_need_to_look_beyond_kafka_for_operational/,1.0,0.0,0.0,11256.0,
1855,2020-04-01 23:27:43,1585772863.0,dataengineering,"What are the best choices to build Stream Processing pipeline, today?",ft7x3w,__Julia,,https://www.reddit.com/r/dataengineering/comments/ft7x3w/what_are_the_best_choices_to_build_stream/,1.0,0.0,0.0,11256.0,
1856,2020-04-02 03:00:52,1585785652.0,dataengineering,What does a data engineer/scientist in a political party do? - thoughts,ftbq41,lnx2n,,https://www.reddit.com/r/dataengineering/comments/ftbq41/what_does_a_data_engineerscientist_in_a_political/,1.0,0.0,0.0,11265.0,
1857,2020-04-02 10:07:56,1585811276.0,dataengineering,Apache Airflow Nested Task List,fthsav,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/fthsav/apache_airflow_nested_task_list/,1.0,0.0,0.0,11270.0,"So I have an airflow `dag` like this
```
task_list = [task1, task2]
start &gt;&gt; task_list &gt;&gt; end
```

How it works is that after `start`, list of tasks in `task_list` runs in parallel.
Now I want to do something like this:
```
task_list = [(task1a &gt;&gt; task1b), (task2a &gt;&gt; task2b)]
start &gt;&gt; task_list &gt;&gt; end
```
Which should make the dag look like pairs of tasks are run in parallel like [here](https://ibb.co/fdzDDqV).
But instead, it's looking like [this](https://ibb.co/9hZm2T4).

Is there any way to achieve what I am trying to achieve? If yes, what am I doing wrong?

Thanks."
1858,2020-04-02 16:23:31,1585833811.0,dataengineering,Webinar on Data Science in 2020: Myths &amp;amp; Facts,ftm9aa,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/ftm9aa/webinar_on_data_science_in_2020_myths_amp_facts/,1.0,0.0,0.0,11271.0,
1859,2020-04-02 16:23:58,1585833838.0,dataengineering,Webinar on Data Science in 2020: Myths &amp; Facts,ftm9h2,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/ftm9h2/webinar_on_data_science_in_2020_myths_facts/,1.0,0.0,0.0,11271.0,
1860,2020-04-02 20:23:30,1585848210.0,dataengineering,Must follow list on twitter,ftqf4i,infernaz91,,https://www.reddit.com/r/dataengineering/comments/ftqf4i/must_follow_list_on_twitter/,1.0,6.0,0.0,11280.0,"Hey guys!

Noob Question, but what do you suggest to follow on twitter?"
1861,2020-04-02 21:23:10,1585851790.0,dataengineering,Job Interview! SQL Server Integration Services Projects needed!,ftrje8,mhalnasser,,https://www.reddit.com/r/dataengineering/comments/ftrje8/job_interview_sql_server_integration_services/,1.0,4.0,0.0,11283.0,"Hello Everyone,

&amp;#x200B;

I applied for an internet providing company for data engineering. I'm mostly an expert with front-end development but I'm willing to transfer my career to data engineering. In my interviewer gove me one week to learn about data engineering specifically  SQL Server Integration Services and show him some projects.

&amp;#x200B;

Here is my plan!

1. Subscribe to a [Udancity](https://www.udacity.com/course/data-engineer-nanodegree--nd027) course for data engineering. ( I feel it would teach how to be a data engineer and put a good impression about how serious I'm about this job ).
2. Make a weather data engineering project where I'm going to collect data from weather APIs and do something with it.

&amp;#x200B;

Any suggestions. Any comment would be helpful!"
1862,2020-04-03 00:11:50,1585861910.0,dataengineering,How much ML should a data engineer know?,ftuo86,ed_elliott_,,https://www.reddit.com/r/dataengineering/comments/ftuo86/how_much_ml_should_a_data_engineer_know/,1.0,0.0,0.0,11288.0,
1863,2020-04-03 06:53:14,1585885994.0,dataengineering,Help with understanding simple pipline,fu16ra,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/fu16ra/help_with_understanding_simple_pipline/,1.0,2.0,0.0,11294.0,"I'm currently learning about DE and I need help understanding my data ""pipeline"". So, I've had simple exposure to what AWS is and we've just begun learning Apache Airflow. 

When I watch videos on pipelines, they get a little complex because they go over programs that I don't know about. For example, Spark, Hive, etc. 

I'm supposed to a simple project where I get data from the web (either a .csv from Kaggle or via an API) and then we I have to analyze the data. Because I don't know about the other products, my ""pipeline"" looks like this: 

Source (.csv) &gt; Collect (?) &gt; ETL (Airflow) &gt; Analytics (?) 

I don't know what to put in the Collect step. Since my data is already in a CSV, would this be placing it in a SQL server, for example? And for the Analytics step, would that be in Jupyter? I'm a bit confused with how Amazon would fit in here."
1864,2020-04-03 09:20:42,1585894842.0,dataengineering,ELI5 the job description of a Data Engineer,fu35k2,data_analist,,https://www.reddit.com/r/dataengineering/comments/fu35k2/eli5_the_job_description_of_a_data_engineer/,1.0,14.0,0.0,11295.0,"I'm a Data Engineer by trade, and I constantly find myself struggling to explain what I do to non-technical friends and family. When I'm asked about my job, most of the time I just say I'm a Software Engineer, and leave it at that. If I'm asked to explain further, most of my explanations follow something close to the following, obviously being more concise and without the technical jargon:

&amp;#x200B;

&gt;Data Engineers are responsible for the creation and maintenance of analytics infrastructure that enables almost every other function in the data world. They are responsible for the development, construction, maintenance and testing of architectures, such as databases and large-scale processing systems. As part of this, Data Engineers are also responsible for the creation of data set processes used in modeling, mining, acquisition, and verification.  
&gt;  
&gt;(from  [https://blog.panoply.io/how-to-become-a-data-engineer-a-guide](https://blog.panoply.io/how-to-become-a-data-engineer-a-guide))

&amp;#x200B;

So I challenge my fellow DE's, how would you succinctly explain to a friend / family member what we do for work?"
1865,2020-04-03 10:49:53,1585900193.0,dataengineering,How to properly use Apache Airflow to transform data?,fu46io,melesigenes,,https://www.reddit.com/r/dataengineering/comments/fu46io/how_to_properly_use_apache_airflow_to_transform/,1.0,9.0,0.0,11295.0,I have a set of text data that I wrote a python script program (it has to call some sklearn models located in a directory within the python module). Am I supposed to call import in the DAG and place the python module with the DAG in the dags folder? Cuz airflow for some reason doesn't recognize the import and I have no idea why. I followed a bunch of stack overflow questions that said to either zip the file which still doesn't work or add airflow home and python home to path or use sys to make sure the path is set but that doesn't work either. Am I doing something fundamentally wrong?
1866,2020-04-03 16:47:31,1585921651.0,dataengineering,Webinar on Data Science in 2020: Myths &amp; Facts,fu8pn4,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/fu8pn4/webinar_on_data_science_in_2020_myths_facts/,1.0,0.0,0.0,11299.0,
1867,2020-04-04 01:49:15,1585954155.0,dataengineering,Airflow DAG produces no result despite 'success',fuibpq,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/fuibpq/airflow_dag_produces_no_result_despite_success/,1.0,0.0,0.0,11302.0,
1868,2020-04-04 20:55:06,1586022906.0,dataengineering,"[Question] What are the choices you pick if you want to build Stream Processing pipeline, today?",fuy3cu,__Julia,,https://www.reddit.com/r/dataengineering/comments/fuy3cu/question_what_are_the_choices_you_pick_if_you/,7.0,13.0,0.0,11315.0,"Hi,

I am working on a new pipeline and I would like to hear from some of you about the best choices to develop Stream Processing pipeline. I am getting incoming data every on milli second intervals.

I have built similar pipeline before. I used the following technologies: 

0/ Listening from RabbitMQ in older days, and applying micro-batching operations

1/ Kafka-Spark. something like this [one](https://www.baeldung.com/kafka-spark-data-pipeline) 

2/ benchmarked  AWS Kinesis. We didn't have good performance using it. 

3/ we tested Flink, but it is mainly designed for faster streaming.

&amp;#x200B;

I would like to hear about new ideas and emerging technologies in this places.

What would you use today, in April 2020, if you build a new pipeline? I would like to hear your thoughts."
1869,2020-04-04 21:04:32,1586023472.0,dataengineering,Is a junior DE position with cloudera good enough?,fuy9ex,bigbear02,,https://www.reddit.com/r/dataengineering/comments/fuy9ex/is_a_junior_de_position_with_cloudera_good_enough/,1.0,4.0,0.0,11315.0,"Hi,

I'll be starting a job as a junior DE for a online gambling company in central Europe. The stack I'll be using will be:
Spark structured streaming, Apache, nifi and Apache kudu all on cloudera on azure cloud. Is this a good entry to becoming a cloud DE? I have my doubts because Cloudera is slowly becoming less important than aws.
Any thoughts?
Thanks."
1870,2020-04-05 06:31:44,1586057504.0,dataengineering,Project Ideas for Beginner DEs,fv74x7,djurisic_luka,,https://www.reddit.com/r/dataengineering/comments/fv74x7/project_ideas_for_beginner_des/,1.0,12.0,0.0,11328.0,I’m a data analyst who is striving to become a data engineer. What are some aws project ideas that I could start working on and gain some DE experience?
1871,2020-04-05 06:33:51,1586057631.0,dataengineering,My Journey Towards Data Engineering,fv75za,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fv75za/my_journey_towards_data_engineering/,1.0,11.0,0.0,11328.0,"Dear all,

I am a newbie without Data or IT related background. I suddenly fell in love with Data and decided to kick start a career in Data Engineering and progress from there. So far, i have been learning python and SQL (since couple of months ago) on my own. While i still feel very shaky with what i have learnt so far, the only way to progress is by doing, breaking, making things. I am ready to push myself. I really want to learn.

I hereby seek opportunities to engage in projects for knowledge purpose. I am available 100% for free. I need you to support my career by engaging me. Plus, i will be more than happy to hook with someone who can also mentor me along the way.

Thank you"
1872,2020-04-05 07:15:49,1586060149.0,dataengineering,Data engineering blog,fv7plv,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/fv7plv/data_engineering_blog/,1.0,0.0,0.0,11328.0,
1873,2020-04-05 19:17:05,1586103425.0,dataengineering,The role of external data in navigation through crises,fvgbmf,DvirPer,,https://www.reddit.com/r/dataengineering/comments/fvgbmf/the_role_of_external_data_in_navigation_through/,2.0,1.0,0.0,11334.0,
1874,2020-04-06 00:20:15,1586121615.0,dataengineering,CDC pipeline with reconciliation for MongoDB data,fvlmz1,vanthar686,,https://www.reddit.com/r/dataengineering/comments/fvlmz1/cdc_pipeline_with_reconciliation_for_mongodb_data/,1.0,12.0,0.0,11341.0,"I'm working on ingestion pipelines. My company has multiple database sources used to store OLTP data. MongoDB is one of them. We need to bring data from MongoDB to Datalake for analysts to run reports on. I tried plugging Debezium - MongoDB connector for this use case. Debezium gives me complete row for create and delete records. However, for updated records I get only the changed attributes along document ID. This is causing problem in reconciling change data into snapshot. As an alternative I'm running **mongoexport** tool to fetch collections' data incrementally. But I'm looking for a solution involving CDC and reconciliation.   
Have you guys tried bringing MongoDB data into DataLake through CDC? If so, could you please help me understand how that can be achieved?"
1875,2020-04-06 01:13:59,1586124839.0,dataengineering,Simple SQL Query (Teach a Beginner),fvmktm,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fvmktm/simple_sql_query_teach_a_beginner/,2.0,12.0,0.0,11341.0,"**SELECT** [r.**name**](https://r.name), w.channel, **COUNT**(\*) num\_events **FROM** accounts a **JOIN** web\_events w **ON** a.**id** = w.account\_id **JOIN** sales\_reps s **ON** s.**id** = a.sales\_rep\_id **JOIN** region r **ON** r.**id** = s.region\_id **GROUP** **BY** r.**name**, w.channel **ORDER** **BY** num\_events **DESC**;

&amp;#x200B;

I am just trying to reconfirm my thoughts. Based the above query;

1. COUNT(\*) is applied to the column just before it ie [w.channel](https://w.channel). If yes, is this generally applicable?
2. Will it make a difference if i instead write COUNT(w.channel)?"
1876,2020-04-06 14:43:48,1586173428.0,dataengineering,Data processing with Akka Actors: Part I,fvxafp,skrbic_a,,https://www.reddit.com/r/dataengineering/comments/fvxafp/data_processing_with_akka_actors_part_i/,1.0,0.0,0.0,11354.0,"Here is my first blog post, where I described how to design actor based apps with a simple yet representative example. Anyone interested in Scala or Akka feel free to check it out.

[https://aleksandarskrbic.github.io/akka-actors-1/](https://aleksandarskrbic.github.io/akka-actors-1/)"
1877,2020-04-06 17:10:31,1586182231.0,dataengineering,Webinar on Data Science in 2020: Myths &amp; Facts,fvzeog,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/fvzeog/webinar_on_data_science_in_2020_myths_facts/,1.0,0.0,0.0,11363.0,
1878,2020-04-06 19:08:48,1586189328.0,dataengineering,Open Source Data Lineage App in Python,fw1fk1,haltingwealth,,https://www.reddit.com/r/dataengineering/comments/fw1fk1/open_source_data_lineage_app_in_python/,2.0,10.0,0.0,11366.0,
1879,2020-04-06 22:52:39,1586202759.0,dataengineering,Director of Engineering opportunity - Media company - NYC,fw5o6z,philcor123,,https://www.reddit.com/r/dataengineering/comments/fw5o6z/director_of_engineering_opportunity_media_company/,1.0,0.0,0.0,11368.0,"An AI podcast guest has asked for our help with an engineering role they are working on so I thought I would reach out to this group! 

They are a media company based in New York City. They are seeking a Director of Data Engineering to play a crucial leadership role, responsible for planning and executing the engineering needs as they go through rapid transformation. 

Their core team is made up of Machine Learning (ML) engineers and data scientists who are building and deploying ML solutions - They're looking for someone with:

* 4+ years of professional software engineering and programming experience (Java, C++, Scala, Python)
* 3+ years of architecture and design infrastructure (reliability, scalability, quality)
* 4+ years of experience as a people manager and hands-on experience leading. 

You can reach me at [philip@alldus.com](mailto:philip@alldus.com)"
1880,2020-04-07 00:34:54,1586208894.0,dataengineering,Data Horror Story Time: Rookie Data Science Mistake Invalidates a Dozen Medical Studies,fw7jj4,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/fw7jj4/data_horror_story_time_rookie_data_science/,1.0,2.0,0.0,11370.0,
1881,2020-04-07 02:54:59,1586217299.0,dataengineering,How much data engineering should a data analyst know?,fw9wzn,NotAClickBot,,https://www.reddit.com/r/dataengineering/comments/fw9wzn/how_much_data_engineering_should_a_data_analyst/,1.0,2.0,0.0,11373.0,"Im currently working in the marketing (media agency) space but would like to transition into a more analytics management position over time. But with that, how much data engineering should I know to be well-off? When engaging with analytics management (or even data analysts) what do you see as something they should be more aware of that affects your job?"
1882,2020-04-07 03:51:36,1586220696.0,dataengineering,"Regular PLC Programmer, Advice Greatly Appreciated",fwatel,aganoth,,https://www.reddit.com/r/dataengineering/comments/fwatel/regular_plc_programmer_advice_greatly_appreciated/,1.0,6.0,0.0,11374.0,"Background:
I have been tasked with a huge Data Engineering project at work: Connecting our many PLCs (computers that run Industrial Machines) to some sort of database. Coming from supporting PLCs in the middle of the night, I want to keep the project as simple as it needs to be while still being usable to the people who will play with the data. This is challenging as there's a ton of software (MQTT protocol, Kafka protocol, CoAP protocol, etc...), so I would appreciate input from some people who do this on a regular basis.

The other task I really want is to use this edge device to allow others to look at / trend this data, but they would not have any SQL knowledge whatsoever.

Current Layout:
PLC -&gt; Server -&gt; Postgres / TimescaleDB &lt;- Visualization Application

I am designing the PLC &amp; Server software, so I have control over what protocol to use (Currently TCP since I'm just testing and sending the binary data over TCP is less packets than HTTP).

Data Rate: 150 - 300mb / s.  I want to be able to support ~500kb @ 20ms cycle with 6 - 12 clients to 1 server / edge device (but in all reality, this will probably be more like 100kb @ 50-100ms -- This is a project goal but I think storage costs will limit this).

Database:
Postgres with TimescaleDB extension. The table layout is column for time, column for module, and then each value from the PLC (~1000) of them is also a column (I've been asked by the team to avoid NoSQL databases like InfluxDB due to the lack of structure). The data will be analyzed by another team with AWS, but they will pick and choose the data they want to move there to use their tools on, so I figure postgres should be super easy for them to copy from.

Specific Questions:
1) Do I need a message broker like Kafka? I don't think so, since I know exactly where data needs to go, and a static data source so it should be easy to plan.
2) Does my database layout below sound normal? I know it approaches the max columns in postgres, but columns as tag names seems the simplest for any visualization software to display to the user.
3) Is there any good Data Visualization tools already built? I need it to be super super simple: Select a few values (With a description, if possible), select a time range, then submit. Get a chart that you can zoom in/out and see the values at certain times, like an oscilloscope. I have tried Grafana and it doesn't seem designed for what I need.

A goal of mine is to remain as flexible as possible. The amount of data proposed here boggles my mind, so I worry that testing works good with approach A, but then having implemented on half the lines we run into problems so we can still switch to approach b or so on."
1883,2020-04-07 04:06:36,1586221596.0,dataengineering,Part time projects,fwb1jm,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/fwb1jm/part_time_projects/,1.0,0.0,0.0,11375.0,
1884,2020-04-07 05:31:42,1586226702.0,dataengineering,Real-time / near real-time data warehouse,fwcc0w,ordinarydude,,https://www.reddit.com/r/dataengineering/comments/fwcc0w/realtime_near_realtime_data_warehouse/,1.0,30.0,0.0,11377.0,"Hi DataEngineers,

I am trying to figure out how people build real-time data warehouse solution.  It seems to me that everyone is talking about it but I am unable to find a good reference that can help me understand how it can be done.

I come from the traditional data warehouse where we build dimension and fact tables in daily night job and the result is consumed. Now it seems that everyone is saying that this data warehouse should be updated in real-time with the current data integration technology landscape.

Is it really being done? How do you guys do it?

&amp;#x200B;

\*the real-time here refers to &lt; 5 minutes SLA from source to target"
1885,2020-04-07 11:42:56,1586248976.0,dataengineering,I Need Explanation on this SQL Query Please,fwh0cy,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fwh0cy/i_need_explanation_on_this_sql_query_please/,1.0,12.0,0.0,11384.0,"Question

How many **accounts** have more than 20 orders?

Solution Query Provided by My Class

`SELECT` [`a.id`](https://a.id)`,` [`a.name`](https://a.name)`, COUNT(*) num_orders`

`FROM accounts a`

`JOIN orders o`

`ON` [`a.id`](https://a.id) `= o.account_id`

`GROUP BY` [`a.id`](https://a.id)`,` [`a.name`](https://a.name)

`HAVING COUNT(*) &gt; 20`

`ORDER BY num_orders;`

&amp;#x200B;

My questions are

1. while i can understand the reason for joining both account and order table, there was no column selected from the joined order table. So what is the essence of joining order table in this case beyond the joining
2. Even though the question ask the number of orders which seems obvious that the column to give us the number of order will be in the order table. The solution doesn't however select such column from the order table.
3. Lastly, i feel the magic here is in the COUNT(\*). My understanding of what it does here is to count the number of time an [a.id](https://a.id) appears for each name (or is it the other way?), i will appreciate a bit clarification on this.

I will appreciate explanation like i am 5 years old"
1886,2020-04-07 17:19:43,1586269183.0,dataengineering,An interview about how Cherre builds and maintains a knowledge graph of commercial real estate data and how it enables them to answer valuable questions,fwlb7z,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/fwlb7z/an_interview_about_how_cherre_builds_and/,1.0,0.0,0.0,11389.0,
1887,2020-04-07 21:32:58,1586284378.0,dataengineering,Between LPI Linux essentials and System Administrator's Guide to Bash Scripting,fwputu,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fwputu/between_lpi_linux_essentials_and_system/,1.0,0.0,0.0,11396.0,
1888,2020-04-08 01:32:25,1586298745.0,dataengineering,DBA to Data Engineer,fwu92h,LAPORTS,,https://www.reddit.com/r/dataengineering/comments/fwu92h/dba_to_data_engineer/,1.0,13.0,0.0,11405.0,"Has anyone made the transition from DBA to Data Engineer?
 
Was it a big leap or an easy move?
 
Did your DBA experience help?

What other skills did you find you needed to pick up quickly?"
1889,2020-04-08 07:17:22,1586319442.0,dataengineering,Would a data app marketplace work?,fwzkxi,nnd-nnguyen,,https://www.reddit.com/r/dataengineering/comments/fwzkxi/would_a_data_app_marketplace_work/,1.0,16.0,0.0,11409.0,"If there were some platform that allowed data engineers to write apps and sell them would you potentially use it?   Would you become a ""Data App Developer""?  I think there are some marketplaces that are specific to things like Salesforce, Hubspot etc.. but it feels like this could be done in more of a technology or company agnostic fashion. 

Build a web scraper for a particular site - sell it for $50 a month, 

Build a Mongo to MySql converter for $25 a month ( I'm making up prices. Could be free. ) 

Whatever tools you've ever built would it be interesting to potentially let people subscribe to your tooling? 

If something like this were to exist what would the platform have to do in terms of delivering the ""apps""?"
1890,2020-04-08 15:43:11,1586349791.0,dataengineering,Webinar on Data Science in 2020: Myths &amp; Facts,fx5t1b,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/fx5t1b/webinar_on_data_science_in_2020_myths_facts/,1.0,0.0,0.0,11417.0,
1891,2020-04-08 22:59:26,1586375966.0,dataengineering,Assessing &amp; interviewing data engineers from a distance,fxdluo,hszafarek,,https://www.reddit.com/r/dataengineering/comments/fxdluo/assessing_interviewing_data_engineers_from_a/,2.0,9.0,0.0,11428.0,
1892,2020-04-08 23:27:15,1586377635.0,dataengineering,SQL and Pandas,fxe5bi,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fxe5bi/sql_and_pandas/,1.0,27.0,0.0,11431.0,"As i progress in my Data Engineer career, i only continue to see competition between SQL and Pandas rather than being complementary. What i imply is that they both seems to do exactly the same thing. I am yet to discover any major difference. I checked google but did not really find anything. 

1. What exactly is the difference between the two?
2. As a Data Engineer, when exactly do you use one instead of the other and why?

I will be glad if anyone can point out how they complement or why i actually need both"
1893,2020-04-09 01:07:16,1586383636.0,dataengineering,Data bricks self paced training courses.,fxfzje,welschii,,https://www.reddit.com/r/dataengineering/comments/fxfzje/data_bricks_self_paced_training_courses/,1.0,8.0,0.0,11434.0,"Has anyone tried these? They are about 75 USD each. Are they worth while doing? I have experience using other tools, but recently I've had recruiters asking about Apache spark/data bricks. Thanks."
1894,2020-04-09 01:53:40,1586386420.0,dataengineering,Thoughts on Udacity's Data Engineering Nanodegree?,fxgrxd,scuba-steve-0,,https://www.reddit.com/r/dataengineering/comments/fxgrxd/thoughts_on_udacitys_data_engineering_nanodegree/,1.0,12.0,0.0,11435.0,I saw some mixed reviews on here of it from 8-10 months back. Wondering if anyone has taken it more recently and has insights on whether it's improved or not.
1895,2020-04-09 03:34:12,1586392452.0,dataengineering,Bash Scripting and Python Automation for Data Engineer,fxiefe,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fxiefe/bash_scripting_and_python_automation_for_data/,1.0,0.0,0.0,11436.0,
1896,2020-04-09 06:40:47,1586403647.0,dataengineering,Remote Internships,fxl5n6,Mhayc_en,,https://www.reddit.com/r/dataengineering/comments/fxl5n6/remote_internships/,1.0,4.0,0.0,11437.0,"Hello , i'm a CS student currently on the data engineering road . I'm wondering about the availablity of remote Internships ? Is there any companies open to this option , considering the problem of confidentiality ! ?
And if yes what are good platforms to land one ? 
Thank you !"
1897,2020-04-09 12:20:19,1586424019.0,dataengineering,Help creating sql alchemy conn without password showing,fxpbc9,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/fxpbc9/help_creating_sql_alchemy_conn_without_password/,1.0,0.0,0.0,11440.0,"In the .cfg file, I connected sql alchemy to Postgres with user: airflow_admin and password: pass: 
`sql_alchemy_conn = postgresql+psycopg2://airflow_admin:pass@localhost:5433/airflow_backend`

How do I anonymize this so that the password doesn't show? Do I create a `.env` file and store the password as a variable and then reference that variable in .cfg? 

I read the following but I still don't understand: https://airflow.readthedocs.io/en/stable/howto/set-config.html"
1898,2020-04-09 13:03:57,1586426637.0,dataengineering,Kafka connect cluster,fxptcw,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/fxptcw/kafka_connect_cluster/,1.0,0.0,0.0,11441.0,"Hi,

I have a confluent kafka cloud cluster running on the cloud. I have taken their basic plan. Now I need to setup my kafka connect cluster. Is there a easy way ( ansible, terraform ) which can help me create this kafka connect cluster? Please help me with any references"
1899,2020-04-09 13:18:23,1586427503.0,dataengineering,How can I make the most of the AWS Free Tier to learn how to Data Engineer,fxpz42,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/fxpz42/how_can_i_make_the_most_of_the_aws_free_tier_to/,1.0,10.0,0.0,11441.0,"Wondering if anyone has any tips/recommendations on how to approach the AWS Free Tier account and make the most of the resources in order to get the biggest learning ROI?

What type of projects could I do? I've got some Python/Bash experience doing ETL stuff.

Thanks"
1900,2020-04-09 18:32:48,1586446368.0,dataengineering,AWS DATA ENGINEER SKILLS,fxuhoe,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fxuhoe/aws_data_engineer_skills/,2.0,22.0,0.0,11448.0,"As i proceed with my Data Engineering training, i will like you to advise me on the AWS skills i require to work. I am not planning for AWS certification yet but i want to first acquire the skills to be able to work first then i can think of certification. I will appreciate your advice"
1901,2020-04-09 19:00:09,1586448009.0,dataengineering,Why Architects Need an Event Portal; Designing a System that Disseminates Real-Time Data,fxuz2v,DigitalBackbone,,https://www.reddit.com/r/dataengineering/comments/fxuz2v/why_architects_need_an_event_portal_designing_a/,1.0,0.0,0.0,11447.0,
1902,2020-04-09 19:02:18,1586448138.0,dataengineering,Six + 1 Steps to Implement Event-Driven Architecture,fxv0n9,DigitalBackbone,,https://www.reddit.com/r/dataengineering/comments/fxv0n9/six_1_steps_to_implement_eventdriven_architecture/,1.0,0.0,0.0,11447.0,
1903,2020-04-09 19:46:43,1586450803.0,dataengineering,"Best way to ""host"" a SQL database locally?",fxvufn,geo_mapping,,https://www.reddit.com/r/dataengineering/comments/fxvufn/best_way_to_host_a_sql_database_locally/,1.0,12.0,0.0,11447.0,"Hey all

At work I made a Tableau dashboard that's live on Tableau Server and currently connected to an excel file. I would like to have this dashboard connected to a live database, but the IT team won't let me add tables/databases to the servers. 

The goal is to have the dashboard be as fresh as possible... if I just made a local SQL database that is only refreshed when my computer is on, then it's not really any different than the excel file, right? 

Any suggestions/workarounds would be appreciated!

Thank you"
1904,2020-04-09 20:26:12,1586453172.0,dataengineering,Ways to query CSV files with SQL locally,fxwl3q,andrewhann,,https://www.reddit.com/r/dataengineering/comments/fxwl3q/ways_to_query_csv_files_with_sql_locally/,1.0,10.0,0.0,11447.0,"I'm looking for suggestions on ways to query CSV files locally with standard SQL. Basically the functionality that Hive/Presto/Amazon Athena provide, but just something that's simpler, easy to run locally, not distributed, lightweight.  Something you might reach for instead of pandas read_csv() function when trying to quickly process a bunch of CSV files locally.  Any thoughts?"
1905,2020-04-10 00:25:04,1586467504.0,dataengineering,Launching a new Knowledge Graph tool called KgBase - Coding skills optional!,fy10dc,TheForager,,https://www.reddit.com/r/dataengineering/comments/fy10dc/launching_a_new_knowledge_graph_tool_called/,1.0,0.0,0.0,11451.0,
1906,2020-04-10 04:31:03,1586482263.0,dataengineering,Job Market in Data Engineering vs Data Science?,fy5ago,DJAlaskaAndrew,,https://www.reddit.com/r/dataengineering/comments/fy5ago/job_market_in_data_engineering_vs_data_science/,1.0,16.0,0.0,11454.0,"With the job market going down the drain, I'm set to finish a masters in Information and Data Science at the end of this year. There are enough fundamental and elective courses in my program that I could slide into a data engineering role after graduating, assuming I did some extra prep on the side as well. There's a lot of competition at the junior level for the Data Science job market, curious whether it may be easier to get an entry level data engineering job? Also, are companies more flexible with letting data engineers work remote compared to data science?"
1907,2020-04-10 04:34:39,1586482479.0,dataengineering,Is data engineering a career where if you find yourself in the right industry / company you can be making a lot of money doing the same exact thing with the same experience as another in a different company / industry,fy5cta,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/fy5cta/is_data_engineering_a_career_where_if_you_find/,1.0,0.0,0.0,11454.0,
1908,2020-04-10 06:02:17,1586487737.0,dataengineering,CTE vs Subquery,fy6wos,Archbishop_Mo,,https://www.reddit.com/r/dataengineering/comments/fy6wos/cte_vs_subquery/,1.0,24.0,0.0,11454.0,"Someone just offered me a keychain with the phrase ""CTEs over subqueries"" and I had a viscerally negative reaction. 

Then I did some introspection and realized the only reason I prefer subqueries is that the very smart person who first taught me SQL preferred subqueries in Postgres. 

Data Engineers of reddit, what in your experience are the relative merits of CTE and subqueries?"
1909,2020-04-10 06:23:21,1586489001.0,dataengineering,Hive Standalone Metastore,fy79m9,SynbiosVyse,,https://www.reddit.com/r/dataengineering/comments/fy79m9/hive_standalone_metastore/,1.0,8.0,0.0,11454.0,I can't seem to get Hive standalone to work without Hadoop. Will it run without it?
1910,2020-04-10 09:21:41,1586499701.0,dataengineering,Airflow: ERROR - Can't compile non template nodes,fy9znu,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/fy9znu/airflow_error_cant_compile_non_template_nodes/,1.0,0.0,0.0,11457.0,
1911,2020-04-10 11:56:27,1586508987.0,dataengineering,Building a Generic Enterprise Application Integration Pipeline,fyc687,Anurag870,,https://www.reddit.com/r/dataengineering/comments/fyc687/building_a_generic_enterprise_application/,1.0,0.0,0.0,11460.0,[https://medium.com/@AS870/building-a-generic-enterprise-application-integration-pipeline-74f2e9a2aa80](https://medium.com/@AS870/building-a-generic-enterprise-application-integration-pipeline-74f2e9a2aa80)
1912,2020-04-10 16:43:43,1586526223.0,dataengineering,Free Webinar on The Essentials of Data Science and Machine Learning,fyg22u,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/fyg22u/free_webinar_on_the_essentials_of_data_science/,1.0,0.0,0.0,11461.0,
1913,2020-04-10 20:17:12,1586539032.0,dataengineering,Looking for Data Modeling Books and Resources,fyk215,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/fyk215/looking_for_data_modeling_books_and_resources/,1.0,9.0,0.0,11463.0,About to accept a role where there is a lot of data modeling is expected. I was wondering if anyone has any recommendations for books or resources to solidify data modeling and data architecture expertise.
1914,2020-04-10 21:50:36,1586544636.0,dataengineering,How we launched a data product in 60 days with AWS,fym0xe,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/fym0xe/how_we_launched_a_data_product_in_60_days_with_aws/,1.0,4.0,0.0,11464.0,
1915,2020-04-11 00:04:50,1586552690.0,dataengineering,Best place to practice SQL for Amazon Interview?,fyorkj,motta_bull,,https://www.reddit.com/r/dataengineering/comments/fyorkj/best_place_to_practice_sql_for_amazon_interview/,1.0,23.0,0.0,11465.0,"Hello Folks,

I have interview on Tuesday for DataEngineering - BI position with amazon. I believe for first round will be in SQL and some Database/BI basic design questions. 

At this point in time, I want to make best use of my available time. Any recommendations to practice on SQL? 

Currently, I am working on Hacker rank. With LeetCode am not comfortable writing query in Leetcode. 

Any valuable comments or past experience would be really helpful. 

Thanks!"
1916,2020-04-11 00:50:57,1586555457.0,dataengineering,Any plans to get best out of master’s final semester?,fypp4q,motta_bull,,https://www.reddit.com/r/dataengineering/comments/fypp4q/any_plans_to_get_best_out_of_masters_final/,1.0,0.0,0.0,11466.0,
1917,2020-04-11 05:12:18,1586571138.0,dataengineering,Insight DE is no longer tuition free and now costs 7% of salary for 2 years. Curious about experiences from people who recently did the program.,fyucsa,fluffycatsinabox,,https://www.reddit.com/r/dataengineering/comments/fyucsa/insight_de_is_no_longer_tuition_free_and_now/,1.0,37.0,0.0,11472.0,"I had a call recently with someone from Insight, having recently done a virtual interview and the coding challenge. The guy explained to me that, starting with the Summer 2020 cohort, Insight will now require an income share agreement where, if a graduate finds a job within 6 months of completing the program and makes more than $100K, they'll pay Insight 7% over the next 2 years. He said that, this is so that (a) they can provide more alumni resources like career coaching and mock interviews, and (b) they had students who were graduating and not taking jobs with partner organizations, therefore Insight was not getting referral fees. I assume that the income share is for all of their programs, and not just DE. 

As early as 2 weeks ago, and well after I applied, Insight had a FAQ page where they stated explicitly that the program has no cost (unfortunately, they updated it within the past few days and I can't find the older version on the Wayback machine). So, this obviously makes a huge difference in my interest. I'm definitely not considering this and feel like I totally wasted my time applying. Come on, they need the money to provide mock interviews and a referral network? We all know that's bullshit. 

Just out of curiosity, has anyone done DE or any of their other programs lately? What did you think? The latest Reddit thread I've found about Insight is from a few years ago. The biggest thing I'm wondering about is: if they need students to pay because they aren't getting referral fees, is that because students aren't interested in their partner companies? I figure if the partner companies were desirable, students would have no problem working for them."
1918,2020-04-11 08:24:15,1586582655.0,dataengineering,Is data engineering considered boring?,fyx6f3,___24601,,https://www.reddit.com/r/dataengineering/comments/fyx6f3/is_data_engineering_considered_boring/,1.0,0.0,0.0,11474.0,
1919,2020-04-11 19:33:22,1586622802.0,dataengineering,Examples bigdata architecture,fz8x37,paulohscj,,https://www.reddit.com/r/dataengineering/comments/fz8x37/examples_bigdata_architecture/,4.0,2.0,0.0,11484.0,"Guys, do you know examples of big data architecture? Example: https://eng.uber.com/uber-big-data-platform/"
1920,2020-04-11 19:53:14,1586623994.0,dataengineering,Masters degree for senior roles?,fz9c80,codie-fz,,https://www.reddit.com/r/dataengineering/comments/fz9c80/masters_degree_for_senior_roles/,4.0,6.0,0.0,11485.0,"Hey guys,

I worked in Data Analytics/ Business Intelligence for more than three years and shifted to a Data Engineer role one year ago. Going forward I was wondering what weight a masters degree might have if I go for a senior role in a year or two. The other Data Engineers in my company all hold a masters, even though everyone one of them told me that this is absolutely not required. Might still be a relevant topic for me if I decide to switch companies.

Whats your experience on this? Do you feel like companies value experience more or will they likely not consider you for senior DE roles if you don't hold a masters degree?"
1921,2020-04-11 20:01:54,1586624514.0,dataengineering,BEST DATA ENGINEERING COURSE,fz9ow1,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fz9ow1/best_data_engineering_course/,1.0,0.0,0.0,11487.0,
1922,2020-04-11 22:17:33,1586632653.0,dataengineering,Data Engineer and Python (Numpy),fzf8j6,sdqafo,,https://www.reddit.com/r/dataengineering/comments/fzf8j6/data_engineer_and_python_numpy/,2.0,16.0,0.0,11503.0,"As a newbie, is Numpy part of the python skills i need to know? Do you as a Data Engineer make you of it?"
1923,2020-04-12 03:08:44,1586650124.0,dataengineering,Advice for transitioning from Senior Manager of a Product Analytics function to Data Engineering manager,fzm5in,RealEstateStepper26,,https://www.reddit.com/r/dataengineering/comments/fzm5in/advice_for_transitioning_from_senior_manager_of_a/,1.0,1.0,0.0,11507.0,"I’m a senior manager at a 200+ start up. Currently, I oversee a team that leverages SQL, Python, and Looker. I’ve come to the realization that this is kind of niche field and that I would be better off positioning myself to manage a Data Engineering team. I was wonder if anyone else has made a similar transition."
1924,2020-04-12 09:26:14,1586672774.0,dataengineering,DE interviews for Big N companies,fzrkig,ratzz505,,https://www.reddit.com/r/dataengineering/comments/fzrkig/de_interviews_for_big_n_companies/,12.0,5.0,0.0,11511.0,"I am currently working as DE at a big consulting company my stack is PySpark and SQL, I am most proficient in those. I do have experience in Python in general and currently learning Cloud. My background  is in CS, so I can code, but I wouldn’t say I have amazing programming skill, one of the reason I shifted for SWE to DE. My ultimate goal is to work at a Big N companies in some data centric role could be (I don’t care about title, my skills sometimes cuts through DE, DS positions). Recently I interviewed with a big fin tech company for a data engineering role, and I was kinda disappointed that I was asked Data structure questions such as heap, BST etc, and I ended up not getting it. I am sharpening my skills learning more big data (spark, Hadoop etc) and I am already pretty good in sql, and general python coding. but I am wondering if I am focusing on the wrong thing to accomplish my goal ? I know big N companies interview heavily using Data structure/algorithm for SWE, but is that case for DE/other data related position too ?
PS: Few years back I reached on site for one of the big N companies for “privacy analytic engineer” (I know,  mouthful title) position  and I was interviewed on sql coding, big data concept, domain knowledge, my skills wasn’t as sharp as now so ended up not getting it."
1925,2020-04-12 11:20:22,1586679622.0,dataengineering,Anyone know a good tutorial or repo that details how to build a backend for google maps?,fzsyny,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/fzsyny/anyone_know_a_good_tutorial_or_repo_that_details/,3.0,3.0,0.0,11513.0,Or something similar
1926,2020-04-12 16:51:38,1586699498.0,dataengineering,Master Thesis Survey,fzx3at,StephanieSchol,,https://www.reddit.com/r/dataengineering/comments/fzx3at/master_thesis_survey/,1.0,0.0,0.0,11520.0,
1927,2020-04-12 21:38:21,1586716701.0,dataengineering,Learn Apache Airflow through 12 hours of hands-on focused videos,g024t9,marclamberti,,https://www.reddit.com/r/dataengineering/comments/g024t9/learn_apache_airflow_through_12_hours_of_handson/,1.0,0.0,0.0,11524.0,
1928,2020-04-12 22:21:58,1586719318.0,dataengineering,What do ya'll think of SQLAlchemy?,g02xgv,iamiamwhoami,,https://www.reddit.com/r/dataengineering/comments/g02xgv/what_do_yall_think_of_sqlalchemy/,17.0,20.0,0.0,11525.0,I've heard lots of conflicting opinions on ORMs. It seems like these opinions are mostly driven by big ORM frameworks like NHibernate and Entity Framework. The one I'm most familiar with is SQLAlchemy. I've found it to be a reasonably good way to get my relational data into Python classes. I'm curious if there are any strong opinions or criticism of it from knowledgeable people.
1929,2020-04-13 04:41:31,1586742091.0,dataengineering,How do you save current files in entire Airflow directory to AWS S3?,g099wr,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/g099wr/how_do_you_save_current_files_in_entire_airflow/,3.0,5.0,0.0,11553.0,"I'm running tasks on an EC2 that produce files like csv, ipynb, and pdf. I want to save all of these to an S3 bucket. Is the best way to do this via an S3 hook? https://airflow.readthedocs.io/en/stable/_modules/airflow/hooks/S3_hook.html"
1930,2020-04-13 05:29:02,1586744942.0,dataengineering,What do data engineers do?,g0a0n7,letsgoisles37,,https://www.reddit.com/r/dataengineering/comments/g0a0n7/what_do_data_engineers_do/,21.0,21.0,0.0,11553.0,Give me examples of what projects and assignments you do all day. Give me an explanation of what you do in simple terms. Is your job stressful or fun?
1931,2020-04-13 07:04:33,1586750673.0,dataengineering,SQL to DE Pipeline transition,g0bjq7,pknpkn21,,https://www.reddit.com/r/dataengineering/comments/g0bjq7/sql_to_de_pipeline_transition/,4.0,7.0,2.0,11555.0,"Hi,

We are currently using AWS RDS Postgres DB to capture the data from our application and the major portion of data is in a very dynamic state and is stored in JSONB format  in postgres. We are currently using SQL queries on RDS read replicas to fetch the data for our analytical needs.

Some of the queries are very complex with 1000+ lines and this keeps on rising when new scenarios are added in the application.

We would like to transition this to a data engineering pipeline. Our current data volume is low(100k rows) and is not expected to rise exponentially.

The objective of moving out of SQL based approach is to reduce complexity, better maintainability &amp; to achieve error handling. What will be a good approach to handle such scenarios ? We dont have any real-time data requirements and the current thought is to establish a data pipeline and export data to S3 data lake with athena to query the data.

Can we handle this effectively by creating python/panda based data processing modules with a cron based scheduling in a EC2 box or should we be looking at pyspark based solution(Glue/EMR+Airflow etc.).

Please share your thoughts."
1932,2020-04-13 14:25:03,1586777103.0,dataengineering,"Kicking the Tires on Airflow, Apache’s workflow management platform – Architecture Overview, Installation and sample Azure Cloud Deployment Pipeline in Python",g0h4wf,dingopole,,https://www.reddit.com/r/dataengineering/comments/g0h4wf/kicking_the_tires_on_airflow_apaches_workflow/,6.0,2.0,0.0,11561.0,"Part 1 - [http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-1/](http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/)

Part 2 - [http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/](http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/)"
1933,2020-04-13 17:29:51,1586788191.0,dataengineering,Free Webinar on The Essentials of Data Science and Machine Learning,g0jzyv,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g0jzyv/free_webinar_on_the_essentials_of_data_science/,0.0,0.0,0.0,11562.0,
1934,2020-04-13 18:59:44,1586793584.0,dataengineering,Data processing with Akka Actors: Part II,g0lq2f,skrbic_a,,https://www.reddit.com/r/dataengineering/comments/g0lq2f/data_processing_with_akka_actors_part_ii/,7.0,10.0,0.0,11563.0,"Just finished Data processing with Akka Actors blog series, check it out. Source code available!

[https://aleksandarskrbic.github.io/akka-actors-2/](https://aleksandarskrbic.github.io/akka-actors-2/)

\#scala #akka #jvm #bigdata"
1935,2020-04-13 23:49:07,1586810947.0,dataengineering,Webinar: How are top data teams making the move to remote?,g0rbee,ReasonablyHank,,https://www.reddit.com/r/dataengineering/comments/g0rbee/webinar_how_are_top_data_teams_making_the_move_to/,1.0,0.0,0.0,11565.0,
1936,2020-04-14 10:25:52,1586849152.0,dataengineering,Switch to Data Engineering Role,g112ky,TheDuke4488,,https://www.reddit.com/r/dataengineering/comments/g112ky/switch_to_data_engineering_role/,0.0,2.0,0.0,11573.0,"Hi all, I'm trying to make a switch to Data engineering and would love any advice on this. Currently I'm a Data Analyst, working in reporting. I know SQL quite well and python at a functional proficiency. I build reports in ssrs, dashboards on tableau, write python programs to run and automate data exports , and build python ETL code for some of the smaller transactional processes in my company. I'm no expert in programming,but I know what tools to use to get a job done. I have a master's in engineering and have been working for a year now. I feel I'm being underpaid for my skillset and also not very satisfied with my job (internal politics and company culture in general).
What can I do to get into Data Engineering?. I've heard bi engineer role is kind of a gateway into Data engineering. Is this true, or am I qualified enough to get into Data engineering? I see that distributed computing (spark,Hadoop etc) and DAG(airflow etc) tools are in high demand these days. I am still learning these online, but is it necessary to know these ? 
What salary can I expect for a data engineering role with my level of experience? (Los Angeles Area).
Any advice is appreciated."
1937,2020-04-14 11:41:18,1586853678.0,dataengineering,Projects to work on,g11x8l,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g11x8l/projects_to_work_on/,19.0,32.0,0.0,11574.0,"Hi All,

I have a lot of exp in data engineering building real time platforms + cloud + DW + data lakes. I get some time ( \~ 1 hour ) everyday after completing my day to day task in my company. I am open on working on open source projects/ help/ blog, which ever way I can help utilising my 1 hour. Let use penny-university workspace in slack. I am there as Nipun agarwal
https://www.pennyuniversity.org/"
1938,2020-04-14 11:47:57,1586854077.0,dataengineering,Can someone help me use an Airflow hook?,g11zu5,[deleted],,https://www.reddit.com/r/dataengineering/comments/g11zu5/can_someone_help_me_use_an_airflow_hook/,2.0,1.0,0.0,11574.0,
1939,2020-04-14 12:32:09,1586856729.0,dataengineering,Tools to visualize database schemas from a sheet of tables and columns,g12hfn,MAFiA303,,https://www.reddit.com/r/dataengineering/comments/g12hfn/tools_to_visualize_database_schemas_from_a_sheet/,5.0,7.0,0.0,11574.0,"i have a sheet that has all tables and columns in database, and primary key columns.

is there an easy way to visualize them? tools i found online don't import and eat from a sheet"
1940,2020-04-14 14:06:13,1586862373.0,dataengineering,Data architecture design guidelines,g13k1g,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g13k1g/data_architecture_design_guidelines/,6.0,7.0,0.0,11574.0,"Hi all,

I am part of a big organisation which has just started to take baby steps towards a data driven company. I would like inputs from data architectures who can guide me or point towards relevant references on

\- design guidelines on data architecture which includes data modelling, data governance, technological data architecture and data warehousing"
1941,2020-04-14 14:42:51,1586864571.0,dataengineering,Pros cons OS,g140qh,frodaufire,,https://www.reddit.com/r/dataengineering/comments/g140qh/pros_cons_os/,3.0,8.0,0.0,11574.0,"I start a new job and have to decide between mac and Windows. I have used windows all my life but I've started realising that mac is a better option looking at the number of excess steps you have to take to do anything in windows. 
Thoughts ?"
1942,2020-04-14 16:15:02,1586870102.0,dataengineering,Free Webinar on The Essentials of Data Science and Machine Learning,g15bgu,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g15bgu/free_webinar_on_the_essentials_of_data_science/,1.0,0.0,0.0,11577.0,
1943,2020-04-14 23:55:12,1586897712.0,dataengineering,Data Engineering Syllabus,g1dody,dominictarro,,https://www.reddit.com/r/dataengineering/comments/g1dody/data_engineering_syllabus/,28.0,22.0,0.0,11584.0,"So I'm someone with little to no technical background (economics and psychology majors) outside of what I've picked up messing around with various libraries (python) on my personal projects. I've become evermore interested in data engineering and want to learn more. Problem: I have no idea where to start if I want to develop a first principles understanding of databases, querying, and optimal systems. I've taken it upon myself to build something close to 'a course', but it's mostly Medium articles, YouTube videos, and 'how to' guidelines sporadically arranged in a bookmarks folder. My one stipulation is that I don't want to just learn how to use a tool like AirFlow or Hadoop. I'm often finding websites that provide 'how to' links but not 'why' links. I want to have the knowledge and understanding that the engineers behind AirFlow and Hadoop had that allowed them to build those tools.

What I'm looking for is something like a syllabus. If you already know a place where this exists, please share! If not, taking a few minutes to write up your learning scaffold would be awesome for those of us just learning! The start can go as far back as you would like."
1944,2020-04-15 00:18:58,1586899138.0,dataengineering,Programming style,g1e43b,Luukv93,,https://www.reddit.com/r/dataengineering/comments/g1e43b/programming_style/,5.0,9.0,0.0,11583.0,"Hello,

I have learned Python for data cleaning and analysis. Now that I want to develop myself to build a data warehouse I am curious on what programming style you would advise me to use on building one?

On GitHub most of the code I find start with the following structure:

1. Importing modules
2. Defining functions
3. Defining a Main function that executes the functions one after another.

I have the following questions:
1. For building a datawarehouse would you advise OOP or Functional programming and why?
2. Could you provide me a structure that I can follow to build a datawarehouse? E.g. write a file with all functions, write a main file that executes the functions.


Would appreciate any help"
1945,2020-04-15 18:07:27,1586963247.0,dataengineering,Ten cheatsheets for data science and machine learning,g1tozk,TheTesseractAcademy,,https://www.reddit.com/r/dataengineering/comments/g1tozk/ten_cheatsheets_for_data_science_and_machine/,3.0,0.0,0.0,11598.0,
1946,2020-04-15 19:55:05,1586969705.0,dataengineering,An interview with Rookout's CTO about the importance of including non-technical roles in the data collection process,g1vtco,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/g1vtco/an_interview_with_rookouts_cto_about_the/,5.0,0.0,0.0,11600.0,
1947,2020-04-16 00:24:40,1586985880.0,dataengineering,"Newbie Here! can someone explain the difference (or put them into context of one another) the following words? *Hive, Hadoop, Spark, MapReduce*",g213lq,fiery_moon-liar,,https://www.reddit.com/r/dataengineering/comments/g213lq/newbie_here_can_someone_explain_the_difference_or/,25.0,12.0,0.0,11605.0,"Much Appreciated! I am hoping to get more involved, would like to use Airflow and EMR (which I think is an AWS instance of some sort?)"
1948,2020-04-16 01:03:00,1586988180.0,dataengineering,Do you have a DevOps team to assist in your Data engineer workflow ?,g21the,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/g21the/do_you_have_a_devops_team_to_assist_in_your_data/,6.0,6.0,0.0,11607.0,"So out of curiosity, what has DevOps engineer done for you ? Or is it expected for  Data engineer to knows the infrastructure side of things as well ? So things like building terraform module, setting kubernetes cluster and load balancing, setting up ci/cd aspect of it as well , making sure there is good security implementation in your infra."
1949,2020-04-16 15:24:32,1587039872.0,dataengineering,Apache Flink Serialization Tuning: Choosing your Serializer,g2dx5x,Marksfik,,https://www.reddit.com/r/dataengineering/comments/g2dx5x/apache_flink_serialization_tuning_choosing_your/,1.0,0.0,0.0,11625.0,
1950,2020-04-16 15:33:56,1587040436.0,dataengineering,How weird and ugly is writing queries like this?,g2e1za,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/g2e1za/how_weird_and_ugly_is_writing_queries_like_this/,0.0,12.0,0.0,11625.0,"    SELECT product3       AS product
        , seller18        AS seller
        , date(timestamp) AS date

I'm referring both to the commas and the spaces to align the names. I have landed into a company that uses this. I really dislike it, but I don't have any objective argument to fight against it.

Do you have any, beyond that this is ugly? Is there something like PEP8 for python for SQL where it is clear that this is not the way? Am I crazy and this is not that bad?

Thanks"
1951,2020-04-16 16:48:53,1587044933.0,dataengineering,Free Webinar on Chatbots: ML Model deployment in Production,g2f89c,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g2f89c/free_webinar_on_chatbots_ml_model_deployment_in/,7.0,0.0,0.0,11625.0,
1952,2020-04-16 23:31:12,1587069072.0,dataengineering,Do you store logs on any cloud storage?,g2mzhd,drarkow,,https://www.reddit.com/r/dataengineering/comments/g2mzhd/do_you_store_logs_on_any_cloud_storage/,1.0,0.0,0.0,11629.0,"Hey guys, I was wondering if you could take 1min to answer a simple survey.

We are checking how people analyze cloud storage objects, any insight can prove useful. 

[1-min survey](https://www.surveymonkey.com/r/9XTQZ8L)

Thanks in advance."
1953,2020-04-17 00:14:13,1587071653.0,dataengineering,A quick and dirty way to monitor data arriving on Kafka,g2ntd7,rmoff,,https://www.reddit.com/r/dataengineering/comments/g2ntd7/a_quick_and_dirty_way_to_monitor_data_arriving_on/,12.0,0.0,0.0,11632.0,
1954,2020-04-17 09:47:57,1587106077.0,dataengineering,Question for data teams working remotely,g2wr2g,mmanja,,https://www.reddit.com/r/dataengineering/comments/g2wr2g/question_for_data_teams_working_remotely/,6.0,3.0,0.0,11634.0,"Since I've been stuck at home for 5 weeks, I got into researching remote work for different teams, including data teams. A friend of mine sent me this article that I think summarizes it well:  [https://www.keboola.com/blog/how-to-empower-your-remote-data-team](https://www.keboola.com/blog/how-to-empower-your-remote-data-team)   
Just out of curiosity - is there anything super important that's missing on that list?"
1955,2020-04-17 15:51:39,1587127899.0,dataengineering,Scalable processing,g31eqd,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g31eqd/scalable_processing/,0.0,2.0,0.0,11639.0,"This is something I have written for those who would like to kickstart scalable processing with minimal cost and without entering hadoop spark world
https://medium.com/@ibnipun10/processing-at-scale-171a985c81c1"
1956,2020-04-17 16:15:45,1587129345.0,dataengineering,Free Webinar on Chatbots: ML Model deployment in Production,g31snj,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g31snj/free_webinar_on_chatbots_ml_model_deployment_in/,5.0,0.0,0.0,11640.0,
1957,2020-04-17 17:19:07,1587133147.0,dataengineering,Data Engineering MSc Dundee,g32v94,taukobongolsen,,https://www.reddit.com/r/dataengineering/comments/g32v94/data_engineering_msc_dundee/,1.0,3.0,0.0,11642.0,Hi.  Can anyone who has been on this course please provide feedback of their experience and also how useful it was for entry to Data Engineering roles? Thanks.
1958,2020-04-17 20:48:30,1587145710.0,dataengineering,A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node. (- from appendix of Ares Le Mandat 4th ed),g36tq2,thedowcast,,https://www.reddit.com/r/dataengineering/comments/g36tq2/a_hypothesis_that_the_federal_reserve_can_set/,2.0,9.0,0.0,11646.0," This is data going back to 1896 that shows how the Dow Jones performed  during times when Mars was within 30 degrees of the lunar node. The data  contains the daily percentage changes of the Dow Jones since 1896. This  information was extrapolated from sources believed to be reliable  regarding stock market data. [https://zenodo.org/record/3711110](https://zenodo.org/record/3711110)

#"
1959,2020-04-18 00:43:33,1587159813.0,dataengineering,"When should I use each of these formats? Do you prefer one over the other in certain situations? The 'long' format seems much easier to work with, but you rarely see data in this format.",g3b71p,azzipog,,https://www.reddit.com/r/dataengineering/comments/g3b71p/when_should_i_use_each_of_these_formats_do_you/,1.0,2.0,1.0,11652.0,
1960,2020-04-18 01:18:01,1587161881.0,dataengineering,"What do you enjoy about data engineering and what made you choose it instead of a ""sexier"" field such as ML Engineer or Data Scientist?",g3btwe,___24601,,https://www.reddit.com/r/dataengineering/comments/g3btwe/what_do_you_enjoy_about_data_engineering_and_what/,6.0,32.0,0.0,11653.0,
1961,2020-04-18 03:25:46,1587169546.0,dataengineering,DE is not for me - rant,g3e03e,penciltwirler,,https://www.reddit.com/r/dataengineering/comments/g3e03e/de_is_not_for_me_rant/,2.0,16.0,0.0,11652.0,"First, sorry about this rant, but I just wanted to know if there are other people who share the same sentiment.

Before I became a data engineer, I thought data engineering is super cool. I liked the idea of using systems to build data pipelines and solving ""big data"" challenges. But now that I am actually a DE, I think DE is rather boring. And it all boils down to this: ""software engineers build the tools, while data engineers just use them"".

Let's take Spark for example. Software engineers built Spark, while data engineers just need to learn how to use Spark. I feel like it is much more intellectually challenging to build the tools rather than use them. However, most data engineers do not play a ""builder"" role. I admit, not everyone can get into the companies that actually build these kinds of tools. Maybe I feel this way because I don't fully understand the tools I'm using. After talking to people working at Amazon and Google, it seems like they get to work on cool shit like Borg and tools that help millions of developers. Of course it doesn't have to be Google or Amazon, or even FANNG. Companies like Databricks and Hashicorp make cool shit for other developers too.

In the end,  I think I enjoy building lower-level things for other developers to use rather than building pipelines or automations. But yea, I don't think DE is for me, because I would much rather build the tools than use them."
1962,2020-04-18 04:59:35,1587175175.0,dataengineering,$60 incentive offered for feedback from data engineers,g3ffpp,MinecraftMountaineer,,https://www.reddit.com/r/dataengineering/comments/g3ffpp/60_incentive_offered_for_feedback_from_data/,2.0,0.0,0.0,11652.0," Hey everybody, I've been using a research site called Respondent for the past few weeks to make a few extra bucks during these tough times. Some of studies on Respondent do not apply to me so I have been paying it forward and sharing the opportunities with people who might actually use them. This research group is offering a $60 incentive to hear from data engineers. If this is you, and you could use a few extra bucks, follow [this](https://app.respondent.io/respondents/projects/view/5e9509c7d850fe00389a31f6/seeking-data-engineers-for-a-quick-30-minute-remote-activity?referralCode=joshuaallessio-ff8154cce629) link and sign up!"
1963,2020-04-18 07:42:42,1587184962.0,dataengineering,What skills are expected from a Data Engineer?,g3hpt1,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/g3hpt1/what_skills_are_expected_from_a_data_engineer/,2.0,12.0,0.0,11654.0,"I recently gave an interview for the same position, cleared rounds of sql, hadoop, spark comfortably but was rejected at the end because couldn't implement heap sort for a given problem, only gave partial solution.

So I'm confused, a data engineer should be as proficient as a SE in data structures? I'm average in DS, need to know for DE how good should be my DS?

Looks like a DE is more superior than SE if these are the new standards"
1964,2020-04-18 08:02:46,1587186166.0,dataengineering,9 Common Mistakes with Cloud Data Fusion,g3hyj6,TheSqlAdmin,,https://www.reddit.com/r/dataengineering/comments/g3hyj6/9_common_mistakes_with_cloud_data_fusion/,1.0,0.0,0.0,11654.0,
1965,2020-04-18 17:25:57,1587219957.0,dataengineering,Experienced DB Dev here looking to break into Big Data,g3obqw,hsvjsm,,https://www.reddit.com/r/dataengineering/comments/g3obqw/experienced_db_dev_here_looking_to_break_into_big/,1.0,1.0,0.0,11658.0,"Hello DEs,

would like some tips on someone who was originally from DB Dev and made it into Big data projects/jobs. Cheers"
1966,2020-04-18 17:49:24,1587221364.0,dataengineering,Data Engineer opportunity with Global Media company in NYC,g3ooks,philcor123,,https://www.reddit.com/r/dataengineering/comments/g3ooks/data_engineer_opportunity_with_global_media/,1.0,0.0,0.0,11658.0,"A global [**hashtag#media**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23media&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D) company is hiring multiple Mid-Senior Level Data Engineers to work across their digital platforms reaching millions of users daily!  

I'm Looking to speak with level Data Engineers who specialize in [**hashtag#ETL**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23ETL&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D) and have at least 3 years experience building end to end pipelines using technologies such as [**#AWS**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23AWS&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D)**,**[ **#Reshift**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Reshift&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D), [**#BigQuery**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23BigQuery&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D), [**#Spark**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Spark&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D), [**#Snowflake**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Snowflake&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D) and [**#Airflow**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Airflow&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D).    

They are currently onboarding fully remotely - 

 Contact: [**philip@alldus.com**](mailto:philip@alldus.com) for more details!"
1967,2020-04-18 18:54:06,1587225246.0,dataengineering,Has anyone successfully dockerized airflow?,g3pq24,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/g3pq24/has_anyone_successfully_dockerized_airflow/,1.0,32.0,0.0,11658.0,"Hi,
I am trying to dockerize airflow, mainly to have not to install it on local machine while development.
My python version is `3.5.2` and airflow version is `1.9.0`.

PS: I have tried the medium articles but for some reason, they are not working for me. If any of you has done it that would be a great help thanks!"
1968,2020-04-18 20:13:58,1587230038.0,dataengineering,"Can't start zookeeper, please help",g3r3a8,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/g3r3a8/cant_start_zookeeper_please_help/,1.0,0.0,0.0,11664.0,
1969,2020-04-18 23:49:42,1587242982.0,dataengineering,ETL Options,g3usdj,acanepa,,https://www.reddit.com/r/dataengineering/comments/g3usdj/etl_options/,7.0,30.0,0.0,11667.0," Hi all! 

I'm curious to expand my knowledge into ETL + Python. I have seen that Apache Airflow is quite popular, do I need to use another tool on top of Apache Airflow or that is just enough for robust ETL? 

With robust I mean, I can handle errors and deltas without a problem.

I'm not worried about scalability at the moment. Also, I've seen some people use Singer to standardize the consumption and output, do you recommend this tool or there are better ones?"
1970,2020-04-19 00:58:03,1587247083.0,dataengineering,Python UDFs in Redshift,g3vx7w,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/g3vx7w/python_udfs_in_redshift/,1.0,3.0,0.0,11670.0,Has anyone here written/use any Python UDFs in Redshift? I’m finding limited documentation and examples on this subject and figured I would ask the community here their experience.
1971,2020-04-19 06:41:34,1587267694.0,dataengineering,Practice project for spark with scala,g40xc3,Dirtycat99,,https://www.reddit.com/r/dataengineering/comments/g40xc3/practice_project_for_spark_with_scala/,1.0,2.0,0.0,11672.0,Can you suggest me some practice projects for spark with scala. I am a beginner and want to practice more.
1972,2020-04-19 16:27:56,1587302876.0,dataengineering,Python Data Engineering Tools: The Next Generation,g47nj4,will_flwrs,,https://www.reddit.com/r/dataengineering/comments/g47nj4/python_data_engineering_tools_the_next_generation/,1.0,8.0,0.0,11687.0,
1973,2020-04-19 17:36:48,1587307008.0,dataengineering,Open source MlOps tool,g48ohg,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g48ohg/open_source_mlops_tool/,1.0,3.0,0.0,11687.0,"Hi,

I am looking for some open source mlops tool like dataiku, sas,datarobot. I would like to know what tools you use in that spacd, I am mainly looking at open source. Maimly looking at deployment of models, versioning, experimentation."
1974,2020-04-19 20:07:29,1587316049.0,dataengineering,When should you use a cloud data warehouse?,g4b84b,tpedar50,,https://www.reddit.com/r/dataengineering/comments/g4b84b/when_should_you_use_a_cloud_data_warehouse/,1.0,12.0,0.0,11692.0,I'm wondering what your opinion is on when it makes sense to migrate to a cloud data warehouse and when using a postgres instance is enough.
1975,2020-04-19 23:37:31,1587328651.0,dataengineering,Black Hawk Down,g4ey7v,rbergesen,,https://www.reddit.com/r/dataengineering/comments/g4ey7v/black_hawk_down/,1.0,5.0,0.0,11700.0,
1976,2020-04-20 05:59:02,1587351542.0,dataengineering,Any recommendations on what should I be learning during the quarantine period?,g4kz2w,enginerd298,,https://www.reddit.com/r/dataengineering/comments/g4kz2w/any_recommendations_on_what_should_i_be_learning/,1.0,2.0,0.0,11715.0,
1977,2020-04-20 08:22:56,1587360176.0,dataengineering,"How do you deal with ""small data""?",g4mupp,pavlik_enemy,,https://www.reddit.com/r/dataengineering/comments/g4mupp/how_do_you_deal_with_small_data/,1.0,6.0,0.0,11719.0,"We store our data in HDFS and have a robust pipeline to stream ""big"" (tens to hundreds GBs/day) data into it based on Kafka and custom consumer. But often this ""big"" data (e.g. clickstream) needs to be joined with ""small"" data (like promotions, or brands or whatever) that has like 1000 records a day which is kinda small to stream to HDFS (files are too small). How do you guys deal with it?"
1978,2020-04-20 10:49:54,1587368994.0,dataengineering,Flink &amp; Hive: Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration,g4ol1r,Marksfik,,https://www.reddit.com/r/dataengineering/comments/g4ol1r/flink_hive_flink_as_unified_engine_for_modern/,1.0,0.0,0.0,11722.0,
1979,2020-04-20 15:18:11,1587385091.0,dataengineering,An interview with Eventador CEO Kenny Gorman about the challenges of building a managed service for streaming data to simplify building real time applications,g4rqug,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/g4rqug/an_interview_with_eventador_ceo_kenny_gorman/,1.0,0.0,0.0,11729.0,
1980,2020-04-20 15:29:01,1587385741.0,dataengineering,25 Best Data Science Courses 2020,g4rw32,gamesbon,,https://www.reddit.com/r/dataengineering/comments/g4rw32/25_best_data_science_courses_2020/,1.0,1.0,0.0,11729.0,
1981,2020-04-20 15:30:45,1587385845.0,dataengineering,Using heroku with AWS RDS. Am I doing it correct ?,g4rwx0,abhipoo,,https://www.reddit.com/r/dataengineering/comments/g4rwx0/using_heroku_with_aws_rds_am_i_doing_it_correct/,1.0,2.0,0.0,11729.0,"Inspired by the top answer [here](https://www.reddit.com/r/dataengineering/comments/g1dody/data_engineering_syllabus/), I am creating a system to learn DE fundamentals.

This is what I aim to achieve - A web app that shows cryptocurrency prices scraped from [coinmarketcap.com](https://coinmarketcap.com). Data in my web app would be updated on a periodic basis, say every 10 minutes. There will also be a button to fetch latest data.

&amp;#x200B;

How I plan to achieve it - 

Python Scraper scheduled on AWS lambda.

Inserts data into AWS RDS Postgres server.

Django Web app hosted on Heroku. Reads from AWS RDS and calls AWS lambda as needed.

&amp;#x200B;

My doubts -

Before starting, I'd like your guidance to know if I am approaching the problem correctly.

I am avoiding the whole Docker / Kubernetes setup because I want to keep the solution as simple as possible."
1982,2020-04-20 15:47:06,1587386826.0,dataengineering,Free Webinar on Enterprise Data Science &amp; It's Project Lifecycle and Management,g4s51b,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g4s51b/free_webinar_on_enterprise_data_science_its/,1.0,0.0,0.0,11729.0,
1983,2020-04-20 17:11:18,1587391878.0,dataengineering,What is your QA strategy? How do you validate data state changes on a daily basis?,g4thap,pokeDitty,,https://www.reddit.com/r/dataengineering/comments/g4thap/what_is_your_qa_strategy_how_do_you_validate_data/,1.0,4.0,0.0,11732.0,"I've been having a hard time wrapping my head around QA'ing big data pipelines. At my organisation, we build pipelines in Scala using Spark SQL. It is the dev's responsibility to ensure unit testing is adequate, although we don't use coverage tool like SonarQube (we probably should). Unit testing isn't really an issue, but I struggle with testing a full end-to-end data pipeline. What tests do you perform that gives you confidence that your pipeline returns the proper data state?

For example, suppose you implement an SCD2 type transformation on a dimension. As fresh updates come in raw, you stage it using SCD2 (ie: row's endDate are closed, new records with fresh start/end dates are created with new surrogate keys, etc etc). The way I would test it (other than unit tests) would be to deploy a snapshot jar to a live QA environment which uses production data and runs the batch job on the same daily schedule as production. For a few days, reconcile the newly staged dimension with the raw data's delta changes. 

This is somewhat manageable with a dimension table, but once you get to a fact table spanning millions of records, joined to many dimensions in time, how do you ensure its state is the truth? Do you test it on a prod like environment? If so, yesterday's tests may no longer be valid as your batch jobs frequently update your data. How can you confidently tell yourself that your pipeline is good to be deployed in production?

I haven't used it, but Amazon's [deequ](https://github.com/awslabs/deequ) seems helpful in adding validation checks on dataframes. However, it doesn't help in answering the question ""is this record's state correct? Am I warehousing the truth?""

Thanks for reading, I look forward to hearing your input."
1984,2020-04-20 18:16:52,1587395812.0,dataengineering,🛎️ Webinar | How are top data teams making the move to remote?,g4un20,ReasonablyHank,,https://www.reddit.com/r/dataengineering/comments/g4un20/webinar_how_are_top_data_teams_making_the_move_to/,1.0,0.0,0.0,11731.0,
1985,2020-04-20 18:35:57,1587396957.0,dataengineering,Data Engineering and Freelancing,g4uzjf,InvisibleLioness,,https://www.reddit.com/r/dataengineering/comments/g4uzjf/data_engineering_and_freelancing/,1.0,7.0,0.0,11732.0,"To me, it seems as if Data Engineering would be the perfect freelance gig. Yet, when I look for projects, I don't find as many as I would expect. 

What is your experience with this? Have you worked as a /with DE freelancers? And what kind of companies should I look for if I am looking for freelance opportunities?"
1986,2020-04-20 20:12:01,1587402721.0,dataengineering,Cleaning spreadsheets with merged cells and missing labels: what's the best approach?,g4wti0,021fluff5,,https://www.reddit.com/r/dataengineering/comments/g4wti0/cleaning_spreadsheets_with_merged_cells_and/,1.0,3.0,0.0,11733.0,"I have a project that involves analyzing AP Exam test scores. To start, I need to download a few dozen .xls files containing historical data from CollegeBoard's website, but I've hit a pretty early roadblock. 

Each file contains performance data on every AP exam taken within a year, disaggregated by race. Their data files were clearly designed for a human reader, not a computer. There are a bunch of merged cells, a lot of columns contain numbers, letters, and symbols, only the first row of each group contains the race/ethnicity label, and the headers/row labels aren't consistent across each file. (I'm including a partial screenshot of one of the data files. These are also [available for download on CollegeBoard's website](https://research.collegeboard.org/programs/ap/data/archived/ap-2018), in case you want to download an .xls file for a US state and join my misery.)

My goal is to get all of these files into a tidy format like this:

&amp;#x200B;

|Year|Race/Ethnicity|Subject|Score|Students Earning Score|
|:-|:-|:-|:-|:-|
|2018|American Indian / Alaska Native|Biology|4|2|
|2018|American Indian / Alaska Native|Biology|3|5|

&amp;#x200B;

I'm struggling with the race/ethnicity labels. In the raw data, only the first row is labeled for each race, but I need each row to include the race/ethnicity. Considering that I have to repeat this process for \~24 files (and the race/ethnicity labels can differ from one file to the next), what's a good approach to getting the race/ethnicity label in every row?

I'm comfortable with R, SQL, and Power Query, but I'm open to any solution that doesn't involve sobbing while copy-pasting labels in Excel. 

Happy to provide more information/context if needed. Thanks, everyone!

&amp;#x200B;

https://preview.redd.it/cru8no8s70u41.png?width=1860&amp;format=png&amp;auto=webp&amp;s=ce974e3be78979f82694d68d667c8f0c2d3b1f25"
1987,2020-04-20 22:16:36,1587410196.0,dataengineering,"First DE project, plz criticize/give feedback.",g4z7mq,kvotheTHEinquisitor,,https://www.reddit.com/r/dataengineering/comments/g4z7mq/first_de_project_plz_criticizegive_feedback/,1.0,15.0,0.0,11732.0,"Been following this sub for awhile whilst learning DE.  Any feedback from this community would be awesome. I've include a data lake architecture diagram and the database schema I will be using for the structured data and the data warehouse.

&amp;#x200B;

https://preview.redd.it/iwcdawcuv0u41.png?width=1100&amp;format=png&amp;auto=webp&amp;s=9d784d036f8e4cb767b23559f912b60d8cdcfef5

https://preview.redd.it/6871yctvv0u41.png?width=760&amp;format=png&amp;auto=webp&amp;s=96b63cf0714b0aceeb27c4e81aeb3b9697a81c35

This project is a capstone project (for the udacity de-nano course) that I have come up with for the company I'm currently working for.  It's basically combining some data sources to ultimately make a data warehouse that can be analyzed.  I will be using Airflow (on my local machine) to orchestrate ingesting and storing the data, processing the data, data validation, and staging the data on redshift.  I will use pySpark on an AWS EMR cluster to transform some of the unstructured/semi-structered data in to structured data (in tabular format).

Some questions I have are:

* Is it fair to call this a data lake? Albeit a simple one.
* Do you think having this as a primary portfolio piece will be enough to get a job as an entry DE or at least an interview?
* What could be done better? I'm trying specifically to gain experience in AWS S3, Redshift, pySpark / EMR, and Airflow."
1988,2020-04-21 01:34:08,1587422048.0,dataengineering,What opensource cassandra modelling tool do you use,g52y10,datak12_,,https://www.reddit.com/r/dataengineering/comments/g52y10/what_opensource_cassandra_modelling_tool_do_you/,1.0,0.0,0.0,11735.0,"I am looking for an open source modelling tool for cassandra. Could you please share which one you use and its pros/cons.

Thanks,"
1989,2020-04-21 03:21:02,1587428462.0,dataengineering,Apache Airflow: The ExternalTaskSensor demystified,g54qu5,marclamberti,,https://www.reddit.com/r/dataengineering/comments/g54qu5/apache_airflow_the_externaltasksensor_demystified/,1.0,1.0,0.0,11738.0,
1990,2020-04-21 03:58:21,1587430701.0,dataengineering,Self-Service/Data Discovery for Internal Users,g55d9o,Soft-Degree,,https://www.reddit.com/r/dataengineering/comments/g55d9o/selfservicedata_discovery_for_internal_users/,1.0,6.0,0.0,11738.0,"Hi All,

I'm wondering how you handle exposing data to internal users?

Basically I would like a system where they can traverse the database tables in a frontend with search (and permissions) and then do data extracts for themselves rather than have the data team action them. These users are operations people so it can't be too technical.

What are people using as a self-serve data option? We could use PowerBI but it's a little clunky for discovery and extraction. Amundsen would seem to be ideal from a 10,000ft view (haven't tried it) but if it doesn't have direct download to csv it's a non-starter.

Could always do a custom internal website but really trying to not have to do a full build for this.

Thanks for your thoughts"
1991,2020-04-21 09:49:10,1587451750.0,dataengineering,"How to orchestrate a batch, monthly Machine Learning pipeline",g5abew,gary_wanderer,,https://www.reddit.com/r/dataengineering/comments/g5abew/how_to_orchestrate_a_batch_monthly_machine/,1.0,1.0,0.0,11749.0,"Hi, we have a machine learning pipeline made of several steps:

1. data collection + preprocessing to build the training dataset
2. model building based on data of step 1
3. data collection + preprocessing to build the dataset to be scored (predicted)
4. scoring of the new data

This pipeline would run on a monthly basis and is linear. Currently, each step has its corresponding docker file (something we decided to do because we want to separability of concerns).

We need to run this on Azure and use some tool to orchestrate the different docker containers without too much overhead, infra management, costs, etc. Basically something simple but robust,  able to scale if needed, and inactive when not running the pipeline. 

Some context:

* first pipeline in production
* data storage: azure data lake storage gen 2
* ideally, the orchestration tool would use some of the outputs of step n for step n + 1
* monthly run
* other pipelines will be added in the future (nothing real-time though, everything is batch)
* very small team

We are currently looking into KubeFlow, which we like a lot but we are wondering whether it's too complex for what we want to achieve here.

Do you have any recommendations? Similar flows that you set up?

Thank you!"
1992,2020-04-21 16:35:34,1587476134.0,dataengineering,Webinar on Enterprise Data Science &amp; It's Project Lifecycle and Management,g5fbt2,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g5fbt2/webinar_on_enterprise_data_science_its_project/,1.0,0.0,0.0,11755.0,
1993,2020-04-21 17:45:23,1587480323.0,dataengineering,Singer for ETL,g5gi1a,tpedar50,,https://www.reddit.com/r/dataengineering/comments/g5gi1a/singer_for_etl/,1.0,8.0,0.0,11760.0,Is anyone using Singer for their ETL? What are scheduling it with?
1994,2020-04-21 20:06:38,1587488798.0,dataengineering,Get into DE with side projects,g5j4ob,L4kks,,https://www.reddit.com/r/dataengineering/comments/g5j4ob/get_into_de_with_side_projects/,1.0,0.0,0.0,11763.0,"hi@all

I would like to start a side project related to DE soon to get some experience and enhance my Coding / Software Engineering skills (in Scala). Do you have some advice on how to get an idea for an appropiate project? My main issue is that I do not have an idea, which I can use as a baseline for a side project. All I can think of are things, I already solved (at least partially) in past proejcts. 

Tbh: I never started any side projcets. The projects I did for my studies or side jobs were already very time consuming. Now I have some spare time after my job and on weekends, which I'd like to use to improve my skill set.   

I am especially interested in stuff like ETL processes using REST endpoints and Hadoop / Spark. 

I apprecicate your help 

A bit about myself:

\- CS graduate, working as a research associate in the area of ""retrieval systems / semantic web / linked data"" since January 2020.

\-  During my studies, my main focus was mainly related to data modelling and management with insights in the fields of: supervised machine learning models, Information retrieval, knowledge engineering, search engines and NLP

\- Very familiar with Python &amp; and RDBMS, also had some experience in C, Kotlon and Web Frontend Technologies. Recently started learning Scala.

\- Thanks to my parttime jobs, I gathered some experience with many of the concepts and topics of the cookbook (linux, docker, shell scripting, JSON web tokens, cronjobs). I would not call myself an expert,

\- Using this cookbook(https://github.com/andkret/Cookbook) as some sort of guide to get into DE"
1995,2020-04-21 20:56:53,1587491813.0,dataengineering,Data Modeling ERD Programs for Data Vault,g5k2oo,cjjohn213,,https://www.reddit.com/r/dataengineering/comments/g5k2oo/data_modeling_erd_programs_for_data_vault/,1.0,4.0,0.0,11764.0,"Looking for recommendations on tools for creating and managing Entity Relationship Diagrams to support Data Vault 2.0 Data Warehouse.  

Would like ability to assign color for entity types (hub, link, sat).  I have been using [draw.io](https://draw.io) but I'm sure there is something more technical and can support exporting out sql DDL statements.

thanks for your help!"
1996,2020-04-22 00:09:58,1587503398.0,dataengineering,Conda vs pip and advise on setting up venv/git/directory,g5nrzt,Luukv93,,https://www.reddit.com/r/dataengineering/comments/g5nrzt/conda_vs_pip_and_advise_on_setting_up/,3.0,24.0,0.0,11772.0,"Hello,

I am currently working as a data analyst. The organization I work for has a rather small IT team, hence the need to perform ETL.

Last year I developed my SQL and Python skills and now I am following courses to build a data platform. I am currently using VSCode but have no experience with virtual environments. I simply import the packages that I need with ""pip install package"". 

Now that believe this is not a scalable approach I would like to hear your thoughts on whether to use conda or pip, and how I can keep track on the packages in a manageble way. Also I have dived into Git to store scripts in a repository but have no clue if my approach is right. 

Do you have 1 repository for managing a dataplatform, or multiple?"
1997,2020-04-22 00:54:58,1587506098.0,dataengineering,Data Lake in S3 Glacier,g5olws,dpasheek,,https://www.reddit.com/r/dataengineering/comments/g5olws/data_lake_in_s3_glacier/,1.0,13.0,0.0,11773.0,"Hi! My company is in the middle of a large scale data migration from on-prem servers into AWS. Personally, I have been tasked with moving datasets and tables that are older and no longer updated as one-time history loads into our S3 data lake. After taking some time to learn Glue and its features, I developed a relatively nice and parameterized pipeline that would be able to crawl the source data, migrate it to S3 in parquet format, and then crawl the target data so that it can be queried by Athena or Redshift Spectrum. However, I'm now being told to evaluate the option of migrating this data as archives into the Glacier storage class. From what I've read, data that is stored in Glacier must be in .csv format and can only be queried by Glacier Select as opposed to Athena or Redshift Spectrum. It seems unavoidable that the pipeline I've developed will need to be reconfigured, but for some of our larger datasets we had looked into partitioning the data as it gets migrated.

Does anyone know if Glacier Select will read partitions like Athena or how it interacts with them? And in a broader sense, has anyone specifically tried to build a data lake entirely within Glacier and can lay out some of the positives and negatives? I'm at a bit of a loss right now as there's kind of a lack of material online, so I'd be really appreciative of any guidance. Thanks!"
1998,2020-04-22 10:54:14,1587542054.0,dataengineering,"I wrote a blog post on Data Democratization strategies: they can have a significant impact on what a day-to-day life as a Data Engineer looks like, and completely change an Org's approach to consuming data, so I was excited to write about it!",g5x1av,dmateusp,,https://www.reddit.com/r/dataengineering/comments/g5x1av/i_wrote_a_blog_post_on_data_democratization/,1.0,11.0,0.0,11781.0,
1999,2020-04-22 13:30:15,1587551415.0,dataengineering,Here Is How You Can Apply Software Development Best Practices to Analytics Pipelines,g5yqgs,tanmaydeshpande,,https://www.reddit.com/r/dataengineering/comments/g5yqgs/here_is_how_you_can_apply_software_development/,1.0,0.0,0.0,11789.0,
2000,2020-04-22 13:35:59,1587551759.0,dataengineering,Difficulty to find and evaluate modern resources on data engineering,g5yszc,a-canadian-monkey,,https://www.reddit.com/r/dataengineering/comments/g5yszc/difficulty_to_find_and_evaluate_modern_resources/,1.0,15.0,0.0,11789.0,"Hello everyone,

&amp;#x200B;

I'm looking for resources (books, blogs, etc.) about modern data engineering and database design. The difficulty I encounter right now is that most resources may be outdated as the field is constantly evolving. Modern data warehouse and vendor like AWS can make advice that was valid 5 years ago isn't true anymore .This makes it extremely hard to know if a book will be worth my time and applicable to everyday work. My criteria are as follow:

\- the book (or other resource) is recent and well rated

\- the book is not recent but its principles are still valid and intemporal

So far I have these books on my list, but I'm not even sure they are still adapted to modern database design:

\- Designing data intensive applications (Kleppmann)

\- SQL antipattern (Karwin)

\- The datawarehouse toolkit (Kimball) (this one is a good example as it gets constantly recommended but not sure if still valid)

&amp;#x200B;

Many thanks!"
2001,2020-04-22 14:09:36,1587553776.0,dataengineering,Download Free Guide to Become a Big Data Engineer - 2020,g5z7l1,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/g5z7l1/download_free_guide_to_become_a_big_data_engineer/,1.0,0.0,0.0,11790.0,
2002,2020-04-22 16:23:00,1587561780.0,dataengineering,Webinar on Enterprise Data Science &amp; Its Project Lifecycle and Management,g61111,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g61111/webinar_on_enterprise_data_science_its_project/,1.0,0.0,0.0,11793.0,
2003,2020-04-23 00:00:39,1587589239.0,dataengineering,[META] We're Building a Wiki. Now what?,g69bpo,vogt4nick,,https://www.reddit.com/r/dataengineering/comments/g69bpo/meta_were_building_a_wiki_now_what/,3.0,21.0,0.0,11803.0,"Here's your chance to share your vision for the wiki. 

A few ~~weeks~~ months ago there was [some feedback](https://www.reddit.com/r/dataengineering/comments/ez9drx/data_engineering_wiki/) requesting a wiki for r/dataengineering. I offered to help bring it all together. 

Some unanswered questions on my mind:
- Does r/dataengineering even need a wiki? Other resources exist, e.g. [the GitLab Handbook](https://about.gitlab.com/handbook/business-ops/data-team/)
- What topics should be curated for the wiki? More broadly, what is the role of the wiki?
- Should there be limitations on who can/should contribute to or curate the wiki?

This isn't an exhaustive list. If something else is on your mind, please speak up."
2004,2020-04-23 01:47:07,1587595627.0,dataengineering,Airflow best practice for accessing/updating a file shared by multiple DAGs,g6b7qo,babyfacebrain666,,https://www.reddit.com/r/dataengineering/comments/g6b7qo/airflow_best_practice_for_accessingupdating_a/,1.0,1.0,0.0,11803.0,"I’m pretty new to airflow so sorry If this is something trivial. 

I have a set of dags that pull different data from the same 3rd party API. The api request requires login/token parameters that don’t chance but also start/end dates that do need to change on a daily basis. All of this is stored in a config JSON file (using Singer taps for ETL). I’ve been generating the start/end dates off the dag execution date and overwriting the config file at the start of each dag run. 

I’ve messed around with a bunch of different ways to handle this config file update and the simplest way so far seems to just have a config per dag and just make sure the task dependencies are ordered so the start/end dates are updated before the data pull task. I also made a sorta hacky sub-dag setup where the parent dag updates a single config file and sub-dags handle the data pulls. Both of theses work as expected but just wondering what would be the best way to handle this scenario. 

Thanks!"
2005,2020-04-23 07:25:49,1587615949.0,dataengineering,change data capture using hive,g6g82n,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g6g82n/change_data_capture_using_hive/,1.0,2.0,0.0,11810.0,"Hi guys,

Tried to write on how one can capture db changes in lake using hive. Hope you like it. help me with the feedback

[https://medium.com/@ibnipun10/change-data-capture-old-way-40f5beee000e](https://medium.com/@ibnipun10/change-data-capture-old-way-40f5beee000e)"
2006,2020-04-23 13:35:53,1587638153.0,dataengineering,Data Ingestion with Rust and AWS Lambda,g6kczj,nivenkos,,https://www.reddit.com/r/dataengineering/comments/g6kczj/data_ingestion_with_rust_and_aws_lambda/,1.0,0.0,0.0,11813.0,
2007,2020-04-23 14:01:31,1587639691.0,dataengineering,Multi Matrix Deep Learning with GPUs,g6kna1,saik2363,,https://www.reddit.com/r/dataengineering/comments/g6kna1/multi_matrix_deep_learning_with_gpus/,1.0,0.0,0.0,11814.0,
2008,2020-04-23 15:35:38,1587645338.0,dataengineering,Scala/Java/C++ in Big Tech Data Engineering?,g6luax,theranch6635,,https://www.reddit.com/r/dataengineering/comments/g6luax/scalajavac_in_big_tech_data_engineering/,1.0,6.0,0.0,11816.0,"I'm currently preparing to apply for data-engineering roles at FAANG and similar companies. One question I haven't found an answer to is how important Java/Scala/C++ programming skills are in these roles. If you have an experience with Scala in the context of Spark and Java in the context of Kafka/MapReduce is that enough? Or do you need a much more in depth knowledge of these languages? 

Really appreciate any thoughts.."
2009,2020-04-23 16:33:07,1587648787.0,dataengineering,Webinar on Enterprise Data Science &amp; Its Project Lifecycle and Management,g6mpdg,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g6mpdg/webinar_on_enterprise_data_science_its_project/,1.0,0.0,0.0,11818.0,
2010,2020-04-23 17:19:28,1587651568.0,dataengineering,Open Source Folks- What are your go to tools?,g6ngtf,-_--__--_-__-__--_-_,,https://www.reddit.com/r/dataengineering/comments/g6ngtf/open_source_folks_what_are_your_go_to_tools/,1.0,6.0,0.0,11818.0,"I've been only involved in Microsoft stack (Azure, Data Factory, SQL Server, SSIS, etc).

Obviously, if I were to do projects on the side for smaller companies, we wouldn't pay for sql server licenses and would want to watch costs closely in Azure. I could put it in Azure but the risk of inflating costs suddenly scares me.

I'm curious on what people use for applications/products for:

1.databases  
a. What language is used to write? SQL? Offshoot of SQL? Python, etc?  
2. server/hosting  
3. ETL or ELT tools  
4. Analytics/Visualizations

For a big company like the one I work for, it's like the following:

1. Sql Server  
a. T-SQL
2. Microsoft Azure (cloud) or on prem
3. Azure Data Factory (V2), SSIS, Stored Procs
4. Tableau"
2011,2020-04-23 18:59:46,1587657586.0,dataengineering,Dell Technologies and Ververica: Analyzing Continuous Data Streams Across Industries,g6p9c7,Marksfik,,https://www.reddit.com/r/dataengineering/comments/g6p9c7/dell_technologies_and_ververica_analyzing/,1.0,0.0,0.0,11823.0,
2012,2020-04-23 19:36:25,1587659785.0,dataengineering,Hiring a data engineer for the first time (no idea what I'm doing) — does this listing sound alright?,g6pxc3,unbundler,,https://www.reddit.com/r/dataengineering/comments/g6pxc3/hiring_a_data_engineer_for_the_first_time_no_idea/,1.0,53.0,0.0,11823.0,"I'm trying to fill a pretty neat role in Hamburg, Germany. It's a two-year gig in a new office within a beautiful old museum. The Data engineer will be working with sensitive **cultural data pertaining to colonial art theft and restitution**. It pays **€61k** which is just above Glassdoor's average rate for data engineers in Hamburg.

This will be translated into German but before that's done, are there any **red-flags** that stand out? Do the requirements sound appropriate? I'm not a data engineer and would love to hear if it looks attractive/feasible/etc.

**Requirements:**

* Degree in computer science, a related technical field, or equivalent practical experience.
* 2-4 years of industry experience with general programming languages (such as Python, Java, Scala) in the context of API interaction and data processing.
* Experience working with various kinds of APIs (especially RESTful ones) and data extraction.
* Experience with data querying, schema definition, and complex databases.
* Experience with ETL (Extract, Transform, Load) pipelines and programmatic automation.
* Experience working with tricky and unclean data sets.
* Experience with data exploration.
* Experience with version control on Github.
* Interest in working with sensitive cultural data!

**Bonus skills and experience:**

* Experience with robust data processing and pipeline tools such as Spark, Kafka, Dataflow, Airflow, Hadoop, etc.
* Data science experience in NLP (natural language processing) and machine learning.
* BSD Unix / Linux user.
* Knowledge of professional software engineering practices for the software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, and testing.

**Responsibilities and tasks include:**

* Translate data into research insights and questions and, on the other hand, work with researchers to answer them.
* Programmatically access differently-formatted APIs from various domestic and international institutions and digital collections.
* Collect and render accessible samples of this data for research and study by the team collectively.
* Co-development of more robust data pipelines for automated data retrieval.
* Attend weekly meetings to provide updates and give feedback.
* Encouraged to experiment with tools and methods relevant to the project and tasks at hand.

**Working environment and expectations:**

It is crucial for the data engineer to be open to experimenting with data as well as learning new development tools and frameworks. This is a green field project with limited supervision and high autonomy — as such, an interest in the cultural significance of the project is very important. The data engineer will be working with a research team in Hamburg alongside museum experts and collaborating with a remote design/development team lead and therefore will be expected to document and communicate work openly with team. The engineer will be afforded unique freedom in choosing what tools to use for data processing pipeline development."
2013,2020-04-24 03:07:20,1587686840.0,dataengineering,Data Warehousing Concepts Resources,g6y3j9,NotAClickBot,,https://www.reddit.com/r/dataengineering/comments/g6y3j9/data_warehousing_concepts_resources/,1.0,0.0,0.0,11831.0,Any of you guys can point me to any decent (paid or free) that has helped you gain a fundamental understanding of datebase construction/ dataware housing concepts?
2014,2020-04-24 04:24:04,1587691444.0,dataengineering,Aws emr deployment automation,g6zaoj,NakkiGN,,https://www.reddit.com/r/dataengineering/comments/g6zaoj/aws_emr_deployment_automation/,1.0,19.0,0.0,11834.0,"Hi all,

I was wondering how others are automating apache spark jobs on Emr.

Do u use datapipeline, airflow with levy, azure devops cicd etc

Thanks in advance, much appreciated :)

Cheers,
Nakul"
2015,2020-04-24 05:49:00,1587696540.0,dataengineering,Advice on first job?,g70j35,agdaman4life,,https://www.reddit.com/r/dataengineering/comments/g70j35/advice_on_first_job/,1.0,6.0,0.0,11839.0,"Got a job as an associate DE fresh out of a CS degree. I’m set to start in sept. The requirements for the job were basic proficiency with SQL/Python, Linux command line, and Hadoop familiarity a plus. Any advice on prepping this summer so I can maximize my work productivity in the beginning?"
2016,2020-04-24 08:27:00,1587706020.0,dataengineering,What are some ways you use bigQuery? Like some use-cases?,g72ms9,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/g72ms9/what_are_some_ways_you_use_bigquery_like_some/,1.0,5.0,0.0,11848.0,"I've recently just started playing around with bigquery. For someone like me, its just another sql engine but of course with great power.

I wanted to know how is it being used in the industry? Is it also another sql editor for you? What are some interesting ways it helps you solve some of your problems? Where does this fit in your product?"
2017,2020-04-24 13:02:48,1587722568.0,dataengineering,How to write Spark df to one excel file?,g75rji,csenaraths,,https://www.reddit.com/r/dataengineering/comments/g75rji/how_to_write_spark_df_to_one_excel_file/,1.0,6.0,0.0,11853.0,
2018,2020-04-24 13:16:58,1587723418.0,dataengineering,Decision Architecture,g75xfm,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g75xfm/decision_architecture/,1.0,3.0,0.0,11854.0,"Hi guys,

We are a big company and are planning to digitisation. Since a lot of people are not aware of the principles of a design and guidelines, there are projects that the team takes and start implementing it in a hap hazard manner.  We would like to introduce a process in place, where any project should pass the guidelines like technical arch, data arch, security specs, infrastructure specs, privacy specs, so that best practices are adopted before starting of any project. Is there a tool to help us out in this. Convert business requirements into proper designs and adherence to respective specs ? Any other suggestions are welcomed"
2019,2020-04-24 16:40:38,1587735638.0,dataengineering,Webinar on Enterprise Data Science &amp; Its Project Lifecycle and Management,g78k3m,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/g78k3m/webinar_on_enterprise_data_science_its_project/,1.0,0.0,0.0,11858.0,
2020,2020-04-24 18:03:55,1587740635.0,dataengineering,Advice on the correct tool for the job,g79xyw,adisunw,,https://www.reddit.com/r/dataengineering/comments/g79xyw/advice_on_the_correct_tool_for_the_job/,1.0,20.0,0.0,11862.0,"Hi everyone,

**Background**

I work at a startup fintech company where our tech stack primarily consist of Kubernetes, Kafka, S3, Postgres and some others. We have backend microservices written in Go, and we have a mixture of microservices and crons written in Python for generating reports that are then loaded into S3 (ETL like things). All services/crons run in a Kubernetes cluster and keep state (if they need to) via Postgres. This post is in regards to the data oriented services/crons that are written in Python.

The microservices written in Python boil down to scripts that are triggered off of some external Kafka event. We have a Go service that emits an event to a Kafka topic when a file is dropped to S3, and we have another Go service that emits an event to a Kafka topic that is an ""end of trading day"" event that basically just consists of a date field. The Python services are typically subscribed to one of these two Kafka topics.

There really is no need for the Python services to be services at all because they boil down to waiting for a Kafka event. They have endpoints, but they're just endpoints to trigger the logic to run via post request. 

&amp;#x200B;

**Question**

I've been researching schedulers that would fit well provided our tech stack. I've looked at Airflow, Flyte, and Prefect mostly. I gave most attention to Prefect due to this medium article [https://medium.com/the-prefect-blog/why-not-airflow-4cfa423299c4](https://medium.com/the-prefect-blog/why-not-airflow-4cfa423299c4). What i'm struggling with is to find a scheduler that fits well with Kafka. I want to trigger a DAG from Kafka events, and either the scheduler or the DAG itself would need to store Kafka offsets (preferably this would be delegated to the scheduler itself). There is a Prefect proposal which adds a Kafka event, which sounds promising.

Does anyone have any suggestions for other tools? Or any other advice to approaching a problem like this?

&amp;#x200B;

Thank you so much in advance!"
2021,2020-04-24 18:52:31,1587743551.0,dataengineering,Tools,g7asyr,rudeb0y_101,,https://www.reddit.com/r/dataengineering/comments/g7asyr/tools/,1.0,1.0,0.0,11862.0,"Currently looking at tools in an adolescent engineering team. I'm interested to know what tools people are using but more importantly the stories about how you came to choose your tools. I'd be interested to know what you use for orchestration, etl, etc.  


Anything you looked at in your choice of tools, or any notes/gotchas or something that swung the decision for you?"
2022,2020-04-24 20:47:25,1587750445.0,dataengineering,When to use Spark or Flink and when to use a database's built in functions,g7cxm1,gummnutt,,https://www.reddit.com/r/dataengineering/comments/g7cxm1/when_to_use_spark_or_flink_and_when_to_use_a/,1.0,9.0,0.0,11864.0,"Hi everyone,

I am somewhat new to data engineering and I got interested in it through using tools like Apache Spark for data science projects, so it wasn't until I got further into data engineering that I learned more about what could be done in relational databases.

Now the more that I use databases the fewer use cases I actually see for something like Spark. For instance, Spark can do some computations really quickly but loading the data back into the database takes an enormous amount of time and it just ends up being faster doing the computations in the database. 

So I am curious for those working with Spark or Flink in the real world, what types of projects will you always go to Spark for, or always go to a relational database for, and what types depend on quantitative factors?"
2023,2020-04-25 00:32:18,1587763938.0,dataengineering,A new role in data science: the data science architect,g7h3zu,TheTesseractAcademy,,https://www.reddit.com/r/dataengineering/comments/g7h3zu/a_new_role_in_data_science_the_data_science/,1.0,2.0,0.0,11872.0,
2024,2020-04-25 10:49:57,1587800997.0,dataengineering,"$1,084 worth of Artificial Intelligence &amp; Machine Learning by Morgan &amp; Claypool Books for $15 (-97% OFF)",g7pqu4,Audreii3234,,https://www.reddit.com/r/dataengineering/comments/g7pqu4/1084_worth_of_artificial_intelligence_machine/,1.0,0.0,0.0,11885.0,
2025,2020-04-25 16:06:56,1587820016.0,dataengineering,What approach should i take to land a Data Engineering job?,g7t84g,im_like_an_ak47,,https://www.reddit.com/r/dataengineering/comments/g7t84g/what_approach_should_i_take_to_land_a_data/,1.0,27.0,0.0,11889.0,"Firstly a little bg on me:

- Graduated CS this year.

- Familiar with Python(basics), and used to practice it a lot on hackerrank(4 stars on python) however havent been doing anything since the covid outbreak for past 2 months and lost touch a bit.

- Familiar with hadoop(hdfs, mapred, hive, pig and sqoop). Havent started with pyspark yet.

- Im currently studying numpy and will continue with pandas.

OFFTOPIC

This is unrelated but recently it has become very difficult for me to focus on studying, been wasting a lot of time on non essential things( anxiety issues ) so im very inconsistent. 
Any way you guys can share how u combat them?"
2026,2020-04-26 02:04:10,1587855850.0,dataengineering,Which unit testing framework for pyspark do you use ?,g83avx,NakkiGN,,https://www.reddit.com/r/dataengineering/comments/g83avx/which_unit_testing_framework_for_pyspark_do_you/,0.0,3.0,0.0,11900.0,"

[View Poll](https://www.reddit.com/poll/g83avx)"
2027,2020-04-26 02:45:29,1587858329.0,dataengineering,Request for comments,g83z77,saveitred,,https://www.reddit.com/r/dataengineering/comments/g83z77/request_for_comments/,1.0,2.0,0.0,11900.0,"Hello all,

I have to design and enterprise grade big data pipeline for a very large data sets. It's geographical location related data which will keep coming in periodically (no fixed interval). The source could static files or could also be a Kafka stream. Can someone please suggest what do I use for:

&amp;#x200B;

1. **Scheduling:** I have used autosys in the past for this purpose but this time I want to use something like Oozie or airflow. Something more advance. But I am curious what do others use.
2. **Processing:** I want to use Spark in batch &amp; stream processing modes. Are there any other options what you have experience with?
3. **ETL (in cloud):** Traditionally I have used SSIS &amp; SQL. But this time I want to use Azure data factory. Will that be a wise choice?
4. **Data lineage**: Basically I have never have kept any provision for data lineage in the past. What have you used for reliable data lineage?
5. **Data Quality**: I have used plain old python scripts for Data quality checks in the past. Does anyone have experience with better data quality tools?

Any other suggestions about building big Data ETL pipeline in general will be much appreciated."
2028,2020-04-26 04:46:09,1587865569.0,dataengineering,"How would you update 300,000,000 records out of 500,000,000?",g85t8o,__arc,,https://www.reddit.com/r/dataengineering/comments/g85t8o/how_would_you_update_300000000_records_out_of/,1.0,14.0,0.0,11901.0,"I was asked this question in an interview recently and was interested how others would approach the problem and explain their thoughts.

&gt;You have a `users` table with 500 million records. The primary key of this table is `user_id.` We are interested in the `is_active` column which serves as a flag for whether or not a user is active (e.g. 'Y' or 'N').  
&gt;  
&gt;There are 300,000,000 users that we need to update `is_active` from 'Y' to 'N'. How would you do this? What comes to mind?

My immediate thoughts were, chunk the data into different sets and process them in parallel (e.g. 100 separate three million record sets) 

1. What **signals** do you think the interviewer is looking for by asking this question?
2. **What would you think / do?**"
2029,2020-04-26 08:21:25,1587878485.0,dataengineering,Fetch Failed in Spark (Databricks),g88rcl,Chapstic1,,https://www.reddit.com/r/dataengineering/comments/g88rcl/fetch_failed_in_spark_databricks/,1.0,0.0,0.0,11905.0,"Has anyone encountered a fetch failed error when using Spark SQL on Databricks? I've tried increasing the number of partitions with spark.sql.shuffle.partitions.  


I have a feeling it's due to a count(distinct) shuffling too much data, but I'm not too sure how to troubleshoot it further. Would I get better performance from having multiple group by's instead of count(distinct)?"
2030,2020-04-26 12:35:53,1587893753.0,dataengineering,AWS Lake Formation - can you grant direct IAM access to S3?,g8bkac,WranglingData,,https://www.reddit.com/r/dataengineering/comments/g8bkac/aws_lake_formation_can_you_grant_direct_iam/,1.0,0.0,0.0,11909.0,"Hi Folks,

I am about to embark on a Lake Formation POC.  I have yet to get my teeth into it.

It has been solved as a ""solves all your problems"" by a consultant that we are using, but I am still thinking there are things outside of LF that we still have to administer.

As part of my project, I need to grant direct access to S3 for a specific role to get in and ingest the uploaded data files to S3 - basically they are going to copy them out into their own project/solution (yes, it seems strange, but this project needs to transfer these files out of AWS).

Can any LF users advise whether we can control this direct access to s3 via the LF console?  Or do we need to control this normally via bucket or IAM role policies?

Cheers"
2031,2020-04-26 15:36:21,1587904581.0,dataengineering,A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node. (- from appendix of Ares Le Mandat 4th ed),g8dma1,thedowcast,,https://www.reddit.com/r/dataengineering/comments/g8dma1/a_hypothesis_that_the_federal_reserve_can_set/,1.0,0.0,0.0,11913.0,
2032,2020-04-26 21:58:18,1587927498.0,dataengineering,Anyone went through netflix Analytics engineer OR DE interview?,g8k4h4,modqhx,,https://www.reddit.com/r/dataengineering/comments/g8k4h4/anyone_went_through_netflix_analytics_engineer_or/,1.0,18.0,0.0,11924.0,How was your interview experience interviewing in either of those positions? What to expect?
2033,2020-04-27 00:36:29,1587936989.0,dataengineering,ETL Hel(l)p,g8n0yo,jfftilton,,https://www.reddit.com/r/dataengineering/comments/g8n0yo/etl_hellp/,1.0,1.0,0.0,11927.0,"I work for a science and engineering department within the federal government.  The main job my group has is to keep hydrologic data coming in for real time operations from other agencies and data sources.  All of the data are time series that have pathnames associated with them such as location.flow.1hour.datasource-raw.

The group has been doing ETL work long before ETL had a name, we call it data acquisition.  The current setup is about 50 scripts, each with multiple configuration files that go and get data from various sources fired off by cron.  There are also another 50-100 scripts that perform calculations on the data.  The vast majority of this is done in python.  The actual process is much more complex and crazy and I don't want to get in to all of it, but it is highly confusing and really hard to get new people up to speed.  It includes lock files/lock directories making temp files moving them to directories, sftp, other deletion scripts...

This is all a huge maintenance suck as people complain about data not being available.  The vast majority of the time the data is simply not available to extract from the source and there isn't anything we can do about it.  There are rare instances where we will have to make an adjustment, however, so we always have to investigate, but the way the scripts are written there is a massive amount of work to pull out what is going on with one pathname when we are pulling 300 pathnames from a particular location.

I have an idea to improve the process that I want criticized, but it is very important to understand I am in the federal government and it is very difficult to use all the tools available to private industry, so cloud based solutions are generally not feasible.  Also the team of people that can actually write scripts is small, but we have a lot of people that want to be able to check on the data and see what is going on.

What I want to do is create an api that basically is a reverse proxy for all of our data sources.  It will handle the extract and transform part of the job.  I can then have another script that requests data from this single api and loads it into our database.  This will also allow me to rewrite some of these scripts in a way that offers more information and allow other people to go to a url so they themselves can see that the data isn't there.    

We are a real time operation so we are requesting data every 15 minutes-1 hour for most locations so the amount of data is not huge at each request.  I am just wondering if this is a good idea or I am missing something.  Is this a good solution or would this put me in trouble somewhere else?

I also want to get away from cron and use some type of scheduler.  I have looked at airflow, but think it would be too complicated for our team.  I saw [cronicle](https://github.com/jhuckaby/Cronicle) and think it looks like a good product, but don't see much information about it anywhere.  Does anyone have experience with this?

Any help/criticism/recommendations are appreciated.  Thanks!"
2034,2020-04-27 02:45:56,1587944756.0,dataengineering,Transform and Import JSON into Redshift,g8pb5k,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/g8pb5k/transform_and_import_json_into_redshift/,1.0,2.0,0.0,11930.0,"Hello D.E. community,

My company logs search data for one of our applications into a table in Postgres and I've been tasked with moving it into Redshift.  It's a single table with 4 fields:

* tracking\_token - varchar(64)
* event\_type - varchar (64)
* start\_time - timestamp
* data - jsonb

I'm looking for advice on how to approach this. I did some quick analysis and there's 50 distinct event types, each having a different JSON data structure. After some research this is what I was planning to do:

1. Dump the table to a .csv file and load to S3
2. Create a target table in Redshift
3. Create a JSONPaths file, mapping all of the JSON elements to a column in the target table
4. Use the COPY from JSON command, specifying a JSONPaths file

Is it that straightforward? I'm getting hung up on all of the different event types/JSON structures. Unless I can get the JSON schema from the SE's it would be a lot of analysis looking at each JSON schema and defining the DDL."
2035,2020-04-27 10:53:48,1587974028.0,dataengineering,Finished Udacity DE nanodegree. Should I take Data Analyist or Data Streaming next? Can anyone critique my plan to game plan to become a DE?,g8wb10,saamk,,https://www.reddit.com/r/dataengineering/comments/g8wb10/finished_udacity_de_nanodegree_should_i_take_data/,1.0,14.0,0.0,11940.0,"Hello, I graduated with BS in Networking earlier this year and started learning python, slq and about databases afterwards to improve my resume. I found I enjoy all three subjects more than networking and now I want to pursue a career working in data.

My studying has been disorganized until i started Udacity DE nanodegree and I like their format so I want to keep learning in their platform, especially since they have a free month promo going on. so I would love feedback on my plan:

**Plan 1**:

* ~~Udacity DE nanodegree~~
* Udacity Data Analyist or Data Streaming nanodegree
* Arizona State Data Processing at Scale class ([course outline](https://asuonline.asu.edu/docs/cse_511.pdf))
* ASU big data certificate and taking math classes
* Get into ASU MS in Computer Science

**Alternative plan:**

* Udacity Data Analyist
* WGU BS Computer Science
* Coursera data structures and algorithms Specialization

**Suplemental:** 

* Taking python and sql courses(halfway done with DE, DA with python and DA with SQL Server career tracks)
* Codecademy computer science track
* Working on own projects


Where I'm stuck are:

1.  Either taking the Data Analyst or Data Streaming nanodegree at udacity
1. weather to pursue a MS at ASU or BS at WGU. ASU has classes and projects related to data engineering but is the more expensive option. WGU will be the cheaper and faster route and help build CS foundations but it does not offer much hands on experience. Additionally I could still pursue a MS further down the road if I go with WGU.

Which plan is better? Which path can help me land a job quicker?
Cheers!"
2036,2020-04-27 11:28:57,1587976137.0,dataengineering,Learn how to improve the throughput of your data pipelines with Akka Streams,g8wro0,skrbic_a,,https://www.reddit.com/r/dataengineering/comments/g8wro0/learn_how_to_improve_the_throughput_of_your_data/,1.0,0.0,0.0,11940.0,[https://aleksandarskrbic.github.io/power-of-akka-streams/](https://aleksandarskrbic.github.io/power-of-akka-streams/)
2037,2020-04-27 18:54:51,1588002891.0,dataengineering,Critique/help with the data engineering plan for a small DS team [x-post r/datascience],g93bs5,datasnow,,https://www.reddit.com/r/dataengineering/comments/g93bs5/critiquehelp_with_the_data_engineering_plan_for_a/,4.0,4.0,0.0,11948.0,"I work for a small (~4 person) data science team within a much larger organization. The team is responsible for making two machine learning models, creating a single set of very important predicted values, and creating reporting and data validation tools relevant to those predicted values. I came on board about 4 months ago with experience in data science, systems administration, and devops. I have a strong linux background and plenty of experience with Docker and Kubernetes. 

I've been asked to improve the existing modeling pipeline. I've come up with a plan that I think is feasible given the organization's goals and (considerable) constraints, but I'm hoping to get feedback on potential pitfalls or things to add from people with more ML/devops/data engineering experience than myself. I also thought it might be fun for this sub to think through what the ideal toolchain might be given a pretty serious set of constraints.

## Goals

- Make our pipeline more robust. No more undetected data issues or breaking commits. Automatic unit and integration tests on all commits/merge requests.
- Improve pipeline transparency and reporting. Make summary and performance statistics about each model more easily available.
- Make testing and comparing new models significantly easier. More clearly tie new model results/objects to the code that produced them.
- Make the whole pipeline run continuously and automatically (given new data or other triggers).

## Constraints

- No cloud infrastructure. Everything has to be on-prem.
- Absolutely no additional money. Zero.
- Need to keep the developer toolchain as light as possible. It has to be usable by a team with limited devops/linux experience.
- Infrastructure can be (and is) linux + Docker based, but it has to be simple enough that if I die it's easy to understand and maintain for someone with a moderate devops background. For the same reason, all infrastructure setup has to be infrastructure as code.
- Any rebuild has to be done within 6 months of one person's full time work. This includes all infrastructure setup, code refactoring, CI/CD setup, and new code.
- The pipeline/modeling itself has to be written in R.

## Tools Available

- Hardware is limited to 2 beefy SQL servers, 2 beefy Ubuntu VMs, and ~6 beefy Windows workstations. 
- We recently upgraded to GitLab Silver for the whole organization and have all the features that go along with it.

## Current Setup

This is a relatively new team that had to get something up and running quickly, so they haven't yet had the time or resources to setup a mature ML pipeline or incorporate many devops best practices. However, they're committed to improving things and making the best system possible, hence why they asked for this plan. The current pipeline is:

1. **Data extraction/processing.** Data is stored entirely in SQL and feature engineering/data extraction is done via SQL views. The view definitions are stored in GitLab. There is one SQL server that is used for both reporting and modeling. Data extraction takes a *very* long time.

2. **Modeling.** The entire pipeline is written in R and is stored in a single, large GitLab repo. Scripts are manually triggered sequentially to run the actual pipeline and modeling. Data ingest/validation, modeling, model validation, and reporting are all roughly part of the same repo. This repo has no unit testing or integration testing.

3. **Reporting.** Reporting is done via R Markdown and a set of Shiny apps that exist separately from the main modeling repo. These reporting applications pull from the same SQL server as the main modeling scripts and report on the predicted values created in the modeling step. Model performance metrics are not available to the reporting apps.

Other notes:

- Intermediate data and model objects are not saved. The model specification and performance statistics of the best performing model are saved to an excel sheet. The predicted values produced by this model are saved back to SQL.

- Testing new models and/or functional forms is done manually by editing the main repo's R code. Model outputs are not tied to specific commits or branches.

## Planned Improvements

Given my constraints, I'd like to make the following improvements:

- Disaggregate the steps of the pipeline into discrete repositories/tasks that can be individually run, tested, and worked on. Add unit testing to each of these repos that runs automatically (via GitLab CI/CD). 
- Create an R package or packages that contains widely used functions and small datasets. Also add unit testing to these repos.
- Create a separate SQL server that mirrors the original server and is used exclusively for reporting.
- Use [DVC](https://dvc.org/) and [MinIO](https://min.io/) (running in Docker on a VM) to store the intermediate data produced by each step in the pipeline as well the final model objects. This is to prevent people from needing to constantly re-run the same data ingest scripts.
- Use DVC to define clear DAGs that automate the process of running the pipeline and collecting metrics on the results. Upload model metrics to a new table in SQL.
- Again use DVC to tie model output and data to specific commits and branches.
- Using the model summary metrics in both DVC and SQL, add some sort of reporting dashboard (Tableau, Shiny) that facilitates easy comparison of different models.

Those are my immediate thoughts for improvements, but I'm curious to get this sub's take as well. Additionally, I'd love to find a data engineering mentor if someone out there is willing to teach/talk.

**TL;DR:** You have 2 SQL servers, 2 VMs, a GitLab subscription, 0 money, and 1 person with linux experience. What's the most robust/transparent machine learning pipeline you can make?"
2038,2020-04-27 19:46:02,1588005962.0,dataengineering,Variables in Apache Airflow: The Guide,g94c9i,marclamberti,,https://www.reddit.com/r/dataengineering/comments/g94c9i/variables_in_apache_airflow_the_guide/,1.0,1.0,0.0,11950.0,
2039,2020-04-27 21:14:51,1588011291.0,dataengineering,Survey on team dynamics in the software industry.,g963dw,Blitz_2512,,https://www.reddit.com/r/dataengineering/comments/g963dw/survey_on_team_dynamics_in_the_software_industry/,1.0,0.0,0.0,11952.0,"Hi everyone,

At the Vrije Universiteit Amsterdam we are conducting a survey on team dynamics in the software industry. 

If you are working in the software industry, could you help us out and fill out our survey?It takes around 10 minutes to complete. 
Here the link: https://docs.google.com/forms/d/e/1FAIpQLSc3NQarMS3Vs07Czl965X_NadwFZjMrpW-rZXkG06hucoG5Kw/viewform?usp=sf_link

Thank you, stay safe!"
2040,2020-04-27 22:21:07,1588015267.0,dataengineering,Is NoSQL appropriate for data tables with dynamic user created columns?,g97e9x,nickkang1,,https://www.reddit.com/r/dataengineering/comments/g97e9x/is_nosql_appropriate_for_data_tables_with_dynamic/,2.0,5.0,0.0,11953.0,"I'm working on an application where the user can create a table with dynamic columns (similar to Microsoft Excel). **I'm not planning on running analysis on the data.** The requirements are: i) display on frontend, ii) edit on frontend, iii) import/export with Excel. 

Are there any considerations I should think about before jumping into NoSQL? To be honest, I've made EAV models for dynamic tables on SQL databases before and they've alway been a pain to deal with. It just feels very hacky for SQL.

Happy to provide any more info."
2041,2020-04-27 23:59:24,1588021164.0,dataengineering,"Creating my own ETL project, how should I start?",g99ax6,dontlookmeupplease,,https://www.reddit.com/r/dataengineering/comments/g99ax6/creating_my_own_etl_project_how_should_i_start/,7.0,15.0,0.0,11955.0,"I currently work as an Analytics Manager and would like to transition to something more technical and perhaps go into data engineering. I read somewhere that a good self project would be building a web scraper, cleaning the data, and loading it onto a database.

I'm familiar with BeautifulSoup and pandas and can probably build something out fairly quickly, but I don't know what I should be using as a database. Like I'm completely clueless on where to start with that. Do I use MySQL? Postgres? What should I be using?"
2042,2020-04-28 09:28:13,1588055293.0,dataengineering,Real time streaming video analytics,g9i02d,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/g9i02d/real_time_streaming_video_analytics/,1.0,3.0,0.0,11967.0,"Hi,

I would like to know how do people product-ionise their deep learning models.  I have a camera installed in company and would like to detect whether proper social distancing is maintained or not. If not, an sms should be raised and the person should be alarmed. I have the deep learning model. I would like to 

1) Capture the video stream from the camera in an ingestion service

2) Send streams to my model to detect 

3) If detected send an alarm

I would like to know how do you guys implement this use case ? What tools and tech you use ?"
2043,2020-04-28 14:37:56,1588073876.0,dataengineering,Enriching data with a fallback UI,g9lmqw,whelping_monster,,https://www.reddit.com/r/dataengineering/comments/g9lmqw/enriching_data_with_a_fallback_ui/,1.0,3.0,0.0,11971.0,"I have a problem at work where we have data streams coming in that we are suppose to enrich with our own data. We are mapping with our own data (mapping table) during ETL, but we want to have some kind of mapping tool for values that don't exist/are new/are wrong. In other words, if data comes in and our fuzzy matching doesn't return anything, it should place the data in a staging environment to be corrected manually by our analysts. 

What tool would you use to get this done in an efficient manner, if you wouldn't have the resources to build one?"
2044,2020-04-28 15:26:44,1588076804.0,dataengineering,Best Spark Certification,g9m912,randm95,,https://www.reddit.com/r/dataengineering/comments/g9m912/best_spark_certification/,1.0,12.0,0.0,11972.0,"Hello,

I want to prepare a Spark certification, and I doubt to go with Databricks ([https://academy.databricks.com/exam/databricks-certified-associate-developer](https://academy.databricks.com/exam/databricks-certified-associate-developer)) or Cloudera ([https://www.cloudera.com/about/training/certification/cca-spark.html](https://www.cloudera.com/about/training/certification/cca-spark.html))

What do you recommend?

Thank you very much :)"
2045,2020-04-28 15:29:06,1588076946.0,dataengineering,ATH Leaps - Hands-on platform to learn data science,g9ma1v,SanjanaSharma96,,https://www.reddit.com/r/dataengineering/comments/g9ma1v/ath_leaps_handson_platform_to_learn_data_science/,1.0,0.0,0.0,11972.0,"Crafted by industry practitioners, an innovative, hands-on platform to help you become a data scientist, all set to handle real business problems.

Register for free here — &gt;[ https://leaps.analyttica.com/](https://leaps.analyttica.com/)"
2046,2020-04-28 19:44:23,1588092263.0,dataengineering,Need some help...,g9qos8,saveitred,,https://www.reddit.com/r/dataengineering/comments/g9qos8/need_some_help/,1.0,9.0,0.0,11979.0,"I am looking for ETL ( big data &amp; otherwise )  related technical questions for an interview.

Can some of you chime in please? Thank you."
2047,2020-04-28 20:28:03,1588094883.0,dataengineering,Can you recommend a good FREE ETL validator or test automation tool for a data warehouse?,g9ri18,ddmmatias,,https://www.reddit.com/r/dataengineering/comments/g9ri18/can_you_recommend_a_good_free_etl_validator_or/,1.0,6.0,0.0,11980.0,"I'm a QA/Data engineer for a company and we have a big data warehouse. We get info from around 20 sources, download that into our tables, and transform that up to 5 times depending on the entity and the source.

We are experiencing many data issues and it has proven impossible to trust that information for both reporting and developing. Unfortunately I don't get extra budget so I'm looking into a good free ETL validator to conduct some pilot tests and maybe convince them on the value of it.

So, any recommendations of a good ETL FREE validator or a test automation tool?

BTW, the DB is mySQL, and for ETL we are using PHP jobs with yii framework, so it is custom.

Thank yoU!!

**Edit:** As a note, the full system is already implemented and working. I'm looking to create a testing layer over what we have, not changing the system (though we probably should at some point :P)"
2048,2020-04-28 20:59:43,1588096783.0,dataengineering,Kafka in Python: yay or nay?,g9s39u,powerforward1,,https://www.reddit.com/r/dataengineering/comments/g9s39u/kafka_in_python_yay_or_nay/,1.0,0.0,0.0,11982.0,
2049,2020-04-28 21:33:02,1588098782.0,dataengineering,Business Intelligence vs Data Engineer,g9spgs,bioinformaticsthrow1,,https://www.reddit.com/r/dataengineering/comments/g9spgs/business_intelligence_vs_data_engineer/,1.0,34.0,0.0,11985.0,"What exactly is the difference? After some googling, I'm still unclear on the differences."
2050,2020-04-28 22:49:53,1588103393.0,dataengineering,How do you guys relax your brains after a long day of coding?,g9u5u6,Rey661199,,https://www.reddit.com/r/dataengineering/comments/g9u5u6/how_do_you_guys_relax_your_brains_after_a_long/,1.0,8.0,0.0,11989.0,
2051,2020-04-29 00:02:04,1588107724.0,dataengineering,DE - The Best Path,g9vjv3,bmrtex,,https://www.reddit.com/r/dataengineering/comments/g9vjv3/de_the_best_path/,1.0,8.0,0.0,11991.0,"Hi everyone!

I studied engineering but it was not related to computers but due to the need for my job I started working as a data analyst and now more recently as a machine learning engineer.

I have studied and learned a lot on my own, whether machine learning algorithms or programming languages like SQL or Python.

I have been interested in learning Data Engineering because it is a topic that interests me and has been in high demand. But a lot of people tell me that I have to have computer training and that the time to learn all the bases to be a good DE is too long. I would like to know what you think? I don't want to be a bad professional. If possible, what is the best way to go in terms of training?"
2052,2020-04-29 01:18:07,1588112287.0,dataengineering,Anyone have experience with sqlanydb?,g9wz0a,fedeleeee,,https://www.reddit.com/r/dataengineering/comments/g9wz0a/anyone_have_experience_with_sqlanydb/,1.0,1.0,0.0,11995.0,"I’ve begun working from home and I’m trying to access my SQL Anywhere database through python on my personal laptop. 

At work I’m able to do so just fine using the python package ‘sqlanydb’ but when I copy and paste the same code and run on my laptop, I get 
“sqlanydb.OperationalError: (b’Database server not found’, -100)

Have the driver installed, ODBC saved and everything. I’m on a Mac."
2053,2020-04-29 07:20:07,1588134007.0,dataengineering,Data engineer vs development,ga2o3r,sunnyminnu,,https://www.reddit.com/r/dataengineering/comments/ga2o3r/data_engineer_vs_development/,1.0,11.0,0.0,12010.0,"I am a software developer with 10+ years of experience. I am planning to learn data engineering so trying to explore more about it. In some threads that I read online, they mention data engineer job is boring with monotonous work and also, the market is inflated. Can anyone throw some light."
2054,2020-04-29 08:39:41,1588138781.0,dataengineering,Does Flink enable communication across microservices?,ga3piz,DataEngineer121212,,https://www.reddit.com/r/dataengineering/comments/ga3piz/does_flink_enable_communication_across/,1.0,0.0,0.0,12028.0,
2055,2020-04-29 10:59:11,1588147151.0,dataengineering,Apache Flink: Memory Management Improvements with Apache Flink 1.10,ga5bkr,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ga5bkr/apache_flink_memory_management_improvements_with/,1.0,0.0,0.0,12062.0,
2056,2020-04-29 15:46:31,1588164391.0,dataengineering,Basic data pipeline with Airflow and Python for covid19,ga8sar,bia1999,,https://www.reddit.com/r/dataengineering/comments/ga8sar/basic_data_pipeline_with_airflow_and_python_for/,1.0,0.0,0.0,12099.0,
2057,2020-04-29 16:25:23,1588166723.0,dataengineering,Looking for a mentor to help a newbie who recently completed a data engineering bootcamp,ga9ees,codingisfun4me,,https://www.reddit.com/r/dataengineering/comments/ga9ees/looking_for_a_mentor_to_help_a_newbie_who/,1.0,8.0,0.0,12101.0,"I recently completed a Data Engineering bootcamp and built a data pipeline as my final project (Docker, Airflow, AWS, Spark, Lambda Functions, EMR clusters, SQL, relational and non-relational databases). I worked as an Analyst at a utility company and that's the most recent experience I have with 40k+ lines of data.

I figured I would give this a shot since it wouldn't hurt."
2058,2020-04-29 19:40:02,1588178402.0,dataengineering,Can DEs get jobs as Data Analysts?,gacsar,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/gacsar/can_des_get_jobs_as_data_analysts/,1.0,18.0,0.0,12132.0,"I'm enrolled in a 3-month Data Eng. course and I've come to the conclusion that there is only a few things that I've enjoyed so far: getting data from an API and cleaning the data, numpy and pandas. I have disliked everything else so far: Airflow, Kafka, etc. 

My background is in finance. I have a few years of gap on my resume due to health issues in my family (and with me). I began learning Python and SQL during this time. I am almost done with a Data Eng. bootcamp and I realized that I only like the aforementioned technologies. 

Sorry to be in such a bad mood, but focusing on the things that I do enjoy, would I better off applying to Data Analyst jobs? Would I even be considered having gone through a Data Eng. course?"
2059,2020-04-29 20:00:42,1588179642.0,dataengineering,Looking for expert opinions and experiences...,gad4mc,saveitred,,https://www.reddit.com/r/dataengineering/comments/gad4mc/looking_for_expert_opinions_and_experiences/,1.0,0.0,0.0,12134.0,"Hi all,

Looking  for comments from experts  and share their experiences. I have listed  the questions below. I could have posted them one by one but I think  they are related.

While loading large amount of data through a data pipeline:

1. What type of data clean-up you typically do or you have come across?
2. What are the data QC tools / techniques do you use in general?
3. What are the most commonly done or most essential data transformations ?
4. What are the typical steps / techniques to flag and fix the bad data?

Thank you in advance."
2060,2020-04-29 20:23:04,1588180984.0,dataengineering,Data Engineer (ETL Focus) opportunity @ Global Media Company,gadikh,philcor123,,https://www.reddit.com/r/dataengineering/comments/gadikh/data_engineer_etl_focus_opportunity_global_media/,1.0,2.0,0.0,12134.0,"A recent podcast guest (AI In Action Podcast) Is looking for a ETL Data Engineer who wants to work in a fast paced, innovative leading global media company!

The Company: A globally recognized Media Company in NYC are seeking a ETL Data Engineer to build pipelines for their digital platforms which reach millions of users daily!

The Tech: Exposure to fast paced, high impact projects, utilizing advanced Large volume, real time data sets within Cloud using Technologies like: **Snowflake, Airflow, AWS.**

What you’ll be doing: Play a crucial role in creating data systems that move data into and out of storage, transforming it to enable people and machines to do work in a streamlined way.

Reach out to [philip@alldus.com](mailto:philip@alldus.com) for more info!"
2061,2020-04-29 23:23:01,1588191781.0,dataengineering,Saving Money on Data Engineering in the Cloud,gagwbw,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/gagwbw/saving_money_on_data_engineering_in_the_cloud/,1.0,0.0,0.0,12149.0,
2062,2020-04-30 00:01:08,1588194068.0,dataengineering,Pattern of rocket fire from Gaza into Israel since 2007 correlating almost exactly with the position of the planet Mars in relation to the lunar node,gahmzl,thedowcast,,https://www.reddit.com/r/dataengineering/comments/gahmzl/pattern_of_rocket_fire_from_gaza_into_israel/,1.0,4.0,0.0,12154.0,
2063,2020-04-30 00:01:35,1588194095.0,dataengineering,"[hiring] Data Engineer in Brentwood, Los Angeles, with Python, Redshift, and AWS",gahnaw,CarolineEKehoe,,https://www.reddit.com/r/dataengineering/comments/gahnaw/hiring_data_engineer_in_brentwood_los_angeles/,1.0,6.0,0.0,12155.0," Interested in this role? DM me or email me at [caroline.kehoe@jobspringpartners.com](mailto:caroline.kehoe@jobspringpartners.com)

# Data Engineer / Python / Redshift

**Full Time | $140k - $165k | Los Angeles, CA** 

A growing healthcare start-up located in Brentwood is looking to add a Data Engineer to their growing Data team. Their Data Engineering team members are part of the Data Science team, working at the core of powering their healthcare platform proposing, prototyping, and deploying product features powered by machine learning and statistical analysis.

Members on this team are responsible for the design, development and implementation of custom data models to solve real world, healthcare related problems. This position is also going to be using ETL tools to integrate build production and post-production data pipelines that move data from various sources into data warehouses and into the cloud.

## Required Skills &amp; Experience

* 2-4 years Python data engineering experience
* Experience building out ETL data pipelines
* AWS experience, specifically with Redshift
* Experience with Postgres SQL
* Previous Healthcare experience
* Experience with Airflow and H2O
* CS or software engineering background

## What You Will Be Doing

Tech Breakdown

100% data engineering development. Building data models and developing ETL pipelines using Python, AWS, Redshift and PostgreSQL

## The Offer

* Competitive Salary: Up to $165,000/year, DOE

You will receive the following benefits:

* Comprehensive health benefits for you and family
* 401k with a company match
* Paid time off (up to 3 weeks)
* Growth opportunity and promotions from within

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) across 10 major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients."
2064,2020-04-30 00:09:07,1588194547.0,dataengineering,I'm looking for feedback on something I wrote (Functional Programming In Python),gahsmb,MenziesTheHeretic,,https://www.reddit.com/r/dataengineering/comments/gahsmb/im_looking_for_feedback_on_something_i_wrote/,1.0,4.0,0.0,12156.0,
2065,2020-04-30 12:45:41,1588239941.0,dataengineering,Bizprospex- Data mining- Data cleansing - Data verification- Data appending - Data scrubbing - Data scraping-Data verification-Data scraping-skip tracing-Email appending,gasomb,nehatiwari454545,,https://www.reddit.com/r/dataengineering/comments/gasomb/bizprospex_data_mining_data_cleansing_data/,1.0,0.0,0.0,12186.0,
2066,2020-04-30 17:42:47,1588257767.0,dataengineering,Data privacy and governance,gawxcl,ninja_coder,,https://www.reddit.com/r/dataengineering/comments/gawxcl/data_privacy_and_governance/,1.0,9.0,0.0,12195.0,What is the current landscape for big data privacy and governance ? I see tools like Atlas and Ranger. Is there anything else?
2067,2020-04-30 20:58:17,1588269497.0,dataengineering,Ideal Way to migrate data from their clients servers to our Premises,gb0p98,ovary_laid,,https://www.reddit.com/r/dataengineering/comments/gb0p98/ideal_way_to_migrate_data_from_their_clients/,1.0,0.0,0.0,12202.0,"Hi All,

For One of our client we will be making OneView of their Users and exposing it through API. The Logic and Processing will be on our servers. For this we will be taking their data dump at first and then  incremental. This will be exact snapshot of their data.

What Would be the Ideal Way to migrate the data from their servers to Our Premises?"
2068,2020-04-30 21:46:46,1588272406.0,dataengineering,Ideal way to replicate clients data in your servers.,gb1n1z,ovary_laid,,https://www.reddit.com/r/dataengineering/comments/gb1n1z/ideal_way_to_replicate_clients_data_in_your/,1.0,0.0,0.0,12202.0,"Hi All,

How do you all migrate/transfer the data from clients machine onto your servers.

What is odd in traditional way of transferring data i.e. through ETL tools. What  ETL tools you use in the process?  

What are the latest/upcoming tools for this purpose.

p.s. I generally use TalendOpen for this."
2069,2020-04-30 21:51:59,1588272719.0,dataengineering,What language do you use for data engineering at work?,gb1qlx,Carnegie118,,https://www.reddit.com/r/dataengineering/comments/gb1qlx/what_language_do_you_use_for_data_engineering_at/,1.0,19.0,0.0,12202.0,"A poll to gauge what language you guys use and potentially guide newbies.

[View Poll](https://www.reddit.com/poll/gb1qlx)"
2070,2020-04-30 22:07:55,1588273675.0,dataengineering,"Calculating speed, bearing and distance using Kafka Streams Processor API",gb21ft,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/gb21ft/calculating_speed_bearing_and_distance_using/,1.0,0.0,0.0,12203.0," I experiment with Kafka Streams using the public transport API in Warsaw, which results in the article below. Any comments are welcome :-)

[https://medium.com/@zorteran/calculating-speed-bearing-and-distance-using-kafka-streams-processor-api-9e95834b9e3d](https://medium.com/@zorteran/calculating-speed-bearing-and-distance-using-kafka-streams-processor-api-9e95834b9e3d)"
2071,2020-04-30 23:11:01,1588277461.0,dataengineering,Kafka &amp; Airflow for a data pipeline?,gb38pk,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/gb38pk/kafka_airflow_for_a_data_pipeline/,1.0,17.0,0.0,12203.0,"Hi everyone! I'm a SWE (9 years) making the switch into Data Engineering. Any help would be greatly appreciated!

I'm trying to architect a data/ELT pipeline, similar to Netflix's ([source](https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb)).

**Pipeline architecture**

- Kafka w/ ZooKeeper (messaging/stateful)
- AWS Redshift (warehouse)
- Apache Airflow (UI for workflow management),
- Spark (BI/data preprocessing)
- Zeppelin (web UI for data visualization)

**Airflow architecture**

- Celery for distributed task queue
- Redis as the in-memory cache
- Flower as the web UI for Celery clusters

I can't seem to across any articles which mention Kafka and Airflow being used in conjunction.

I plan on using Amazon MKS for Kafka, and Airflow / Zepplin will live in Fargate.

Questions:

1. Can Airflow be used to manage Kafka? I'd like to have Airflow provide some sort of web UI for what's going on in the Kafka clusters.
2. I understand Airflow is a workflow manager, task scheduler, etc .. whereas Kafka is a messaging/broker system to fan out events. Could someone further explain their differences and actual **production** uses?

Thank you!!"
2072,2020-04-30 23:18:24,1588277904.0,dataengineering,Verifying Apache Airflow Components,gb3dt6,yung_turd,,https://www.reddit.com/r/dataengineering/comments/gb3dt6/verifying_apache_airflow_components/,1.0,3.0,0.0,12205.0,Just a shot in the dark but has anyone had experience creating scripts/processes that verify if the Airflow Scheduler and Webserver is running? Is there any examples online I can look at?
2073,2020-05-01 02:49:43,1588290583.0,dataengineering,"(Irvine, CA) Data Innovator- $130k- 180k + Benefits",gb7akb,briiieananoel,,https://www.reddit.com/r/dataengineering/comments/gb7akb/irvine_ca_data_innovator_130k_180k_benefits/,1.0,0.0,0.0,12211.0,"-bitotech factory of the biopharma space
-they have their own resources to create their own drugs
-ton of drugs get researched but hardly any get approved because of a lack of data 
-they identify drugs that are working and they sponsor them and make sure they’re more spread out and used
-they end up developing the drugs that weren’t finished with clinical trials that they look to capitalize on
-they closed a deal for $3 BB 

Skillset: 
Senior Data Scientist, Architect, Engineer - SQL, Python, Spark, strong ed/degree, enterprise company, data, excellent comm. Must have healthcare experience"
2074,2020-05-01 02:53:15,1588290795.0,dataengineering,"(Newport Beach, CA) Senior Data Engineer- $75hr- $80hr CTH",gb7cwb,briiieananoel,,https://www.reddit.com/r/dataengineering/comments/gb7cwb/newport_beach_ca_senior_data_engineer_75hr_80hr/,1.0,2.0,0.0,12211.0,"World's leading asset and bonds trading company with a massive engineering department working with petabytes of data and develops features that are extremely impactful to the financial landscape. 

Seeking a Data Engineer with experience with Python, Data Engineering, ETL, Data Pipeline engineering, strong AWS."
2075,2020-05-01 04:57:27,1588298247.0,dataengineering,Is tying our numbers in to finances numbers apart of anyone’s job ? Does your job ever feel more like finance ?,gb9cn7,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/gb9cn7/is_tying_our_numbers_in_to_finances_numbers_apart/,1.0,4.0,0.0,12217.0,
2076,2020-05-01 04:57:30,1588298250.0,dataengineering,Error logging in Data pipelines,gb9co7,mdghouse1986,,https://www.reddit.com/r/dataengineering/comments/gb9co7/error_logging_in_data_pipelines/,1.0,2.0,0.0,12217.0,"Hi,

I am trying to build a data pipeline which pulls data from an API ( every 10 mins). This is first time I am pulling data from APIs at the latency.What kind of errors do I need to account for in this scenario, especially around API failures/latency.

Thanks"
2077,2020-05-01 06:24:18,1588303458.0,dataengineering,Saw this and had to share :),gbamgr,SeanLikesData,,https://www.reddit.com/r/dataengineering/comments/gbamgr/saw_this_and_had_to_share/,1.0,20.0,0.0,12222.0,
2078,2020-05-01 16:19:14,1588339154.0,dataengineering,Data Science Tools for Remote Collaboration in the Cloud - Data Unbound,gbhycn,DavidUnbound,,https://www.reddit.com/r/dataengineering/comments/gbhycn/data_science_tools_for_remote_collaboration_in/,1.0,0.0,0.0,12239.0,
2079,2020-05-01 16:22:19,1588339339.0,dataengineering,Prefect Dataflow Scheduler Podcast,gbhzz9,bobhaffner,,https://www.reddit.com/r/dataengineering/comments/gbhzz9/prefect_dataflow_scheduler_podcast/,1.0,0.0,0.0,12239.0,"I thought I would post this since workflow orchestration is always a hot topic on here.  Good interview with Jeremiah Lowin who was an Airflow committer before creating Prefect.

I think Jeremiah does a good job comparing the two and goes on to explain that Prefect is not a successor to Airflow

By the way, Software Engineering Daily covers a lot of DE related content and is definitely worth a subscribe.  

[https://softwareengineeringdaily.com/2020/04/29/prefect-dataflow-scheduler-with-jeremiah-lowin/](https://softwareengineeringdaily.com/2020/04/29/prefect-dataflow-scheduler-with-jeremiah-lowin/)"
2080,2020-05-01 17:43:38,1588344218.0,dataengineering,Free Webinar on How to Become a Data Scientist in 2020,gbjd1x,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gbjd1x/free_webinar_on_how_to_become_a_data_scientist_in/,1.0,0.0,0.0,12242.0,
2081,2020-05-01 18:41:09,1588347669.0,dataengineering,Airflow + Snowflake - How common is it to use these together?,gbkfg5,philcor123,,https://www.reddit.com/r/dataengineering/comments/gbkfg5/airflow_snowflake_how_common_is_it_to_use_these/,1.0,7.0,0.0,12245.0,"I am sourcing Data Engineers for a role we have on using tech stack: Snowflake, Airflow, Spark - But seems like theres not an abundance of Engineers using these technologies together - are there few companies using these tools together?"
2082,2020-05-01 20:46:21,1588355181.0,dataengineering,Importing data from SQL Server into an Excel spreadsheet,gbmqom,renzocrossi,,https://www.reddit.com/r/dataengineering/comments/gbmqom/importing_data_from_sql_server_into_an_excel/,1.0,0.0,0.0,12248.0,"Hey guys, the following video shows how to create a package on SSIS (SQL Server Integration Services) to import data from a SQL Server table right into an Excel template.
[From SQL Server right into Excel template](https://youtu.be/Yl8SRN_kzpA)"
2083,2020-05-01 21:02:36,1588356156.0,dataengineering,Help Me! I oversold my ability and need to set up a data warehouse fast!,gbn1es,FreeTooPlay,,https://www.reddit.com/r/dataengineering/comments/gbn1es/help_me_i_oversold_my_ability_and_need_to_set_up/,1.0,14.0,0.0,12249.0,"I recently joined a startup based in the US with a product similar to Shopify but for a specific niche industry.The CEO wants a data warehouse set up immediately to support data analysis and reporting requests from clients. I understand how a data warehouse works in general (I'm just a data analyst) so I volunteered to do it and thought I could just figure it out. Best way to learn right!

So now I turn to Reddit, the source of all knowledge. Are their any solutions for someone with limited technical knowledge to easily set up a data warehouse? We'll be connecting Stripe, PostgreSQL database, Google Analytics, Salesforce, and Tableau. I know a lot of solutions have pre-built connectors which should make things easier. Are there any obvious things I should know before getting started? Any recommendations? 

Also, I should mention this company is beyond frugal. That should be obvious though since they are having a guy with no experience set up their data warehouse instead of hiring a qualified data engineer."
2084,2020-05-02 04:32:58,1588383178.0,dataengineering,How much Object-Oriented Programming do you use as a Data Engineer?,gbwxrc,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/gbwxrc/how_much_objectoriented_programming_do_you_use_as/,1.0,27.0,0.0,12262.0,"In my short experience, I haven't encountered any.. but the job I'm applying for requires it and tested me for it on the interview process. I'm honestly a bit rusty.. haven't done much OOP since college.. but I guess I want to know where OOP might be used in DE."
2085,2020-05-02 05:51:37,1588387897.0,dataengineering,Enabling PM's &amp; Business folks with all views of data - ( Interview question),gbywrs,modqhx,,https://www.reddit.com/r/dataengineering/comments/gbywrs/enabling_pms_business_folks_with_all_views_of/,1.0,4.0,0.0,12266.0,"This is one of the questions I was asked at HBO for DE. Imagine HBO has a studio which goes out in different locations and films movies. There are tons of people like principal cast, hair &amp; makeup, photographer etc and 100 more categories of people involved &amp; costs associated with each of them + other supply chain dimensions.. 

How do you ENABLE Business folks(which may not know SQL) to be able to view this data? A BI dashboard is apparently too static, like what if they want to see or add some other kind of output THEMSELVES without coming to DE people again and again with slightly different requests? 

FOLLOW UP: What metric would you use for things like casts, film casts, and budgeting?"
2086,2020-05-02 08:09:32,1588396172.0,dataengineering,Distributed Systems newsletter issue #97,gc0vbg,cshekharpatil,,https://www.reddit.com/r/dataengineering/comments/gc0vbg/distributed_systems_newsletter_issue_97/,1.0,0.0,0.0,12269.0,
2087,2020-05-03 06:28:00,1588476480.0,dataengineering,Dynamodb bulk data write,gcjr96,NakkiGN,,https://www.reddit.com/r/dataengineering/comments/gcjr96/dynamodb_bulk_data_write/,1.0,2.0,0.0,12302.0,"I was planning on writting about 200 million items into a dynamodb table and looks like the writebatchitem can make 25 putitem requests per second. As you can imagine this would take a lot of time for the initial load. Any suggestions on improving performance folks ?

Thanks in advance."
2088,2020-05-03 22:59:21,1588535961.0,dataengineering,Does anyone here love geography and nature and have they found away to use data engineering to get as close to that passion as they can ? While still being a data engineer,gcx3ci,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/gcx3ci/does_anyone_here_love_geography_and_nature_and/,1.0,15.0,0.0,12321.0,So possibly you work on the backend of a mapping application or some type of work that gets you building different data pipelines and data warehousing data that is filled with environmental data. Maybe you work for a farming company or a satellite company or Planet Earth of Nat Geo ?? Not talking GIS only here. Just some other fusion of data engineering and nature. Would love to here someone who accomplished this
2089,2020-05-04 05:54:50,1588560890.0,dataengineering,Will spark and Hadoop remain important?,gd3lqy,Firm_Bit,,https://www.reddit.com/r/dataengineering/comments/gd3lqy/will_spark_and_hadoop_remain_important/,1.0,6.0,0.0,12331.0,"I’m fairly new to the field and haven’t used spark or Hadoop. I see it in job descriptions here and there though. 

Within my firm we’ve managed to build internal tools and applications to handle a lot of etl, and for POC we do a little scripting with a high level language like python. 

Do you think spark and Hadoop will remain relevant? Would it be enough to learn the managed cloud versions of Hadoop? General thoughts about spark and Hadoop if we’ve made it through a few years of DE without ever having touched those techs?"
2090,2020-05-04 05:58:45,1588561125.0,dataengineering,Databricks Spark Certification CERT020,gd3nq7,Gabehcoud2503,,https://www.reddit.com/r/dataengineering/comments/gd3nq7/databricks_spark_certification_cert020/,1.0,10.0,0.0,12332.0,"Hello folks,

Anyone here preparing the new Spark Certification by Databricks and interested by doing information exchange and peer to peer preparation ?"
2091,2020-05-04 13:48:45,1588589325.0,dataengineering,Free graphical CSV file editor for Windows 10,gd9ds1,jerha202,,https://www.reddit.com/r/dataengineering/comments/gd9ds1/free_graphical_csv_file_editor_for_windows_10/,1.0,3.0,0.0,12342.0,"I wrote a graphical CSV file editor for my own needs and then made it user friendly, robust and fast enough so I could sell it on Microsoft Store. Unfortunately my marketing skills are not up to my coding and engineering skills, so not very many people are buying it... so I thought I could just as well give it away here on Reddit for free now.

It's different from other CSV editors and Excel because it shows data graphically as line plots instead of in a grid. See if it seems useful for you here: [https://www.microsoft.com/sv-se/p/flow-csv-editor/9np4jt39w71d](https://www.microsoft.com/sv-se/p/flow-csv-editor/9np4jt39w71d) 

If it does, open Microsoft Store and in the menu select Redeem code. Here's the code: G427R-MK62P-4V4MC-J26FT-43CFZ . The code expires Sunday May 10th at 23:59 UTC.

Hope that's useful for someone!"
2092,2020-05-04 13:57:55,1588589875.0,dataengineering,Data virtualization platform vs data lake,gd9hxz,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/gd9hxz/data_virtualization_platform_vs_data_lake/,1.0,3.0,0.0,12342.0,What is your view on data virtualization platforms like denodo and building your own data lake. Why would one go for either one. Connecting to production systems directly can bottlenecks the dbs right? Again building data lake itself is a herculean effort
2093,2020-05-04 17:06:37,1588601197.0,dataengineering,Free Webinar on Introduction to Natural Language Processing,gdc85u,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gdc85u/free_webinar_on_introduction_to_natural_language/,1.0,0.0,0.0,12353.0,
2094,2020-05-04 17:11:49,1588601509.0,dataengineering,Free Webinar on How to Become a Data Scientist in 2020,gdcb6z,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gdcb6z/free_webinar_on_how_to_become_a_data_scientist_in/,1.0,0.0,0.0,12353.0,
2095,2020-05-04 17:50:42,1588603842.0,dataengineering,Best Practices on Collecting Rows (From File) and Bulk Insert,gdcyke,DataDev88,,https://www.reddit.com/r/dataengineering/comments/gdcyke/best_practices_on_collecting_rows_from_file_and/,1.0,3.0,0.0,12355.0,"Good morning!  
So, I'm looking to process records in an XML file, parse out node data, collect this data into records, and bulk insert into Oracle tables using Python.  In the past, I'd be using ETL GUI-based tools, but this is my first foray into using Python instead.

Volume-wise, probably looking at a 200k-500k records per file.    


I was curious if there were best practices around this - online I've seen several approaches.

1. Keep records in standard Python data structures,  iterate over the records and insert row by row with Oraclecx execute() - obvious, but slow solution
2. Keep records in standard Python data structures, use Oraclecx executemany() to put more records in at a time. 
3. Keep records in pandas dataframes, and write out using Oraclecx - not sure if putting them in a df gets me anything here.
4. Write out the processed rows to a CSV or other file, and then use SQL\*LOADER

Are there standard best practices with this?  Obviously, not the naive row by row solution...  But am I missing a technique?  Are certain methods better if you plan on executing multiple instances of the process at the same time (if you have multiple files to process)?  
Thanks!"
2096,2020-05-04 23:24:47,1588623887.0,dataengineering,An interview with Amar Arsikere about the complexities of data operations at enterprise scale and the approach that Infoworks taken to make it manageable.,gdjdiq,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/gdjdiq/an_interview_with_amar_arsikere_about_the/,1.0,0.0,0.0,12367.0,
2097,2020-05-04 23:40:46,1588624846.0,dataengineering,Dag in Airflow package imports wrong Python module,gdjoom,millions_of_spiders,,https://www.reddit.com/r/dataengineering/comments/gdjoom/dag_in_airflow_package_imports_wrong_python_module/,1.0,3.0,0.0,12368.0,
2098,2020-05-05 00:43:43,1588628623.0,dataengineering,🎥 Tutorial: How to stream data from Kafka to Elasticsearch with Kafka Connect,gdkukp,rmoff,,https://www.reddit.com/r/dataengineering/comments/gdkukp/tutorial_how_to_stream_data_from_kafka_to/,1.0,0.0,0.0,12369.0,
2099,2020-05-05 06:30:45,1588649445.0,dataengineering,Presto Question - Why is it fast? Is it actually fast?,gdqcav,skingkong,,https://www.reddit.com/r/dataengineering/comments/gdqcav/presto_question_why_is_it_fast_is_it_actually_fast/,1.0,11.0,0.0,12381.0,"OK my team is looking at Starburst and Presto truly has me scratching my head... Their marketing hinges on how fast they are, yet they are federating to slow source systems. Does anyone have any anecdotal data on performance across systems? Context: Using PostgreSQL and Cassandra on \~25 TBs of data as a first use case. These systems are already slow, so I'm curious how Presto helps?"
2100,2020-05-05 14:02:21,1588676541.0,dataengineering,Most Downloaded Artificial Intelligence Research Articles,gdvpu8,saik2363,,https://www.reddit.com/r/dataengineering/comments/gdvpu8/most_downloaded_artificial_intelligence_research/,1.0,0.0,0.0,12394.0,
2101,2020-05-05 16:25:34,1588685134.0,dataengineering,Free Webinar on How to Become a Data Scientist in 2020,gdxnyi,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gdxnyi/free_webinar_on_how_to_become_a_data_scientist_in/,1.0,0.0,0.0,12397.0,
2102,2020-05-05 17:15:11,1588688111.0,dataengineering,Data Integration Help,gdyhzi,datajen,,https://www.reddit.com/r/dataengineering/comments/gdyhzi/data_integration_help/,1.0,5.0,0.0,12399.0,"Hi!  I'm hoping someone can help me or provide some insight.  I work with a partner agency that is hoping to improve dataset integration. Briefly, they have one system that creates a user id and documents some information (this is related to covid testing and contact tracing). There is another system they use essentially like a service management tool. It takes data from system 1 (e.g. name, phone, tested positive) and uses this additional system to document case notes and provide service work updates (e.g. patient has not exhibited symptoms in three days).

Here's the issue:  the agency is manually exporting data from system one and uploading into system two nightly.  They use SAS for this, but i'm not sure how (I've only ever used SAS for analytics like MLR, cluster analysis, etc.).  

They need a faster and less manual way to integrate the data.  Ideally when one system is updated, the other is also updated much quicker than overnight and less manually (than export, import, matching, etc.).

I don't have a lot of specifics here; my agency was just brought on to help and i do not yet have information like metric names/types, job/etl schedule, etc.

Does anyone have any experience, insight, or suggestions?  I would appreciate it!
Thanks!"
2103,2020-05-05 17:44:56,1588689896.0,dataengineering,What IDE are you using ?,gdz0nl,Omar_88,,https://www.reddit.com/r/dataengineering/comments/gdz0nl/what_ide_are_you_using/,0.0,13.0,0.0,12402.0,"I'm curious to know what IDE's / Text Editors you use?

&amp;#x200B;

Basic SQL - Python - HTML and JavaScript i use VSCode

Data Warehouse Development I use Visual Studio

Edit sorry I made the poll single choice only (fail) please let me know why you use this IDE and how it improves your workflow.

&amp;#x200B;

for me I use VSCode mainly with python to test and write OOP code, I use a gist in Github to host my settings and extensions and move around different client environments. Also love the fact you can choose different environments to use for Python

[View Poll](https://www.reddit.com/poll/gdz0nl)"
2104,2020-05-05 18:47:16,1588693636.0,dataengineering,"Help - Looking for the platform used for this dashboard. No source code available, just this screen.",ge05vb,[deleted],,https://www.reddit.com/r/dataengineering/comments/ge05vb/help_looking_for_the_platform_used_for_this/,1.0,0.0,0.0,12404.0,
2105,2020-05-05 18:57:29,1588694249.0,dataengineering,Microservices in streaming data pipeline,ge0cs0,wizinet,,https://www.reddit.com/r/dataengineering/comments/ge0cs0/microservices_in_streaming_data_pipeline/,3.0,13.0,0.0,12403.0,"I have a complex streaming data pipeline that includes many microservices, each of them reads from a stream (AWS Kinesis in my case) and writes to another stream. In some cases, there are many services that read from the same stream.

I found out that the cost of the streams themselves is very high as I need to transfer my whole data over and over between all those services.

I want to keep the dependency of the microservices but avoid paying large sum of money on networking alone. Is there any architecture or pattern that handles this case?"
2106,2020-05-05 18:58:58,1588694338.0,dataengineering,Using airflow for super simple task — pointless?,ge0dsw,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/ge0dsw/using_airflow_for_super_simple_task_pointless/,1.0,18.0,0.0,12404.0,"Hi I’m a data science intern at a medium sized company that doesn’t really use the latest software tools. 

I’ve just been given a side project where they want me to make a dashboard using PowerBI. 

The data sits on a shared drive in a CSV, already pulled from the db by a different team. I have a python script that loads the data, pre-processes it, then stores the data on another shared drive where PowerBI looks for it. 

It needs to run every morning, so I was planning on making a cron job to schedule the script, but I have been considering moving more toward the DE side of things career-wise, so I thought maybe dabbling with Airflow might be a good idea. 

I know it’s overkill for what I’m doing but maybe it’ll be worth it to just to get somewhat familiar with Airflow? Or should I just wait until I have a “real” use case for it?

My end goal is to gain DE skills over the next few months before I graduate in the fall. Any advice is much appreciated!

Edit:
Getting access to the servers isn’t trivial — I initially wanted to add this data to a db on the server so that PowerBI could connect to it directly, but the IT team said I couldn’t — not because I’m an intern, but some policy they have about engineers adding their own tables to the db’s"
2107,2020-05-05 20:28:09,1588699689.0,dataengineering,Frameworks resources,ge2343,Sihal,,https://www.reddit.com/r/dataengineering/comments/ge2343/frameworks_resources/,1.0,0.0,0.0,12410.0,"In the vast ocean of possibilities, it is quite hard to understand which framework does what. Does the page/wiki, where most of the frameworks(with short description) are listed, even exist? What I have in mind is something like [scaladex](https://index.scala-lang.org/) for scala libraries."
2108,2020-05-05 23:35:08,1588710908.0,dataengineering,Working between oracle and sql server routinely,ge5l81,kudaros,,https://www.reddit.com/r/dataengineering/comments/ge5l81/working_between_oracle_and_sql_server_routinely/,2.0,14.0,0.0,12416.0,"I am a data scientist from a research background (e.g., a PhD with emphasis on physics, ML) and found myself in a different role immediately following graduation. So I’m familiar with python, R, enough SQL to write reasonably intermediate queries, etc. 

In my role I depend on a few people to gather data from various sources and place it in a sql server sandbox. This is a manual process with no automated pipelines. I’m no expert on data engineering best practices, but I doubt this system qualifies. 

So with that said, I’d like to get better at navigating these systems. 

I have data in sql server that I want to use a function in pl/sql on. This function has not been replicated in sql server and depends on tables in oracle to run. What are some more streamlined methods of moving this data to oracle, running this function, and reexporting to sql server? 

I know I can push flat files around, but is there any way to do all this from SSIS? Any recommended tutorials ?"
2109,2020-05-06 09:37:17,1588747037.0,dataengineering,Problem connecting to mysql with python,geeu9f,aalizadegan,,https://www.reddit.com/r/dataengineering/comments/geeu9f/problem_connecting_to_mysql_with_python/,1.0,0.0,0.0,12426.0," guys I'm trying to build my first pipeline. I have imported the pymysql and urllib3 libraries, but when I attempt to connect, it gives me an error that it cannot connect to that database host. I'm getting the host from mysql work bench as below  


https://preview.redd.it/vmx26gxxa3x41.png?width=405&amp;format=png&amp;auto=webp&amp;s=bfb08b58af8c2909b059cc6f0c868b5bfb1e02d3

 I wonder what should I put down as my host:   
\- 3306   
\- localhost:3306   
\- Mysql@localhost:3306"
2110,2020-05-06 15:51:10,1588769470.0,dataengineering,Datavault using pyspark,gejemk,mrcool444,,https://www.reddit.com/r/dataengineering/comments/gejemk/datavault_using_pyspark/,1.0,5.0,0.0,12433.0,"Hi All,

I am learning Apache Spark and as part of learning, I am planning to build a pyspark applications to dynamically load Datavault hubs, links and satellites in Redshift. The idea is to use the each application to load multiple hubs, links and satellites without hardcoding the columns in my applications.

If you have built an application similar to what I am thinking, can you tell me how you implemented it in Spark?

&amp;#x200B;

Thanks,

Mc"
2111,2020-05-06 17:12:07,1588774327.0,dataengineering,Free Webinar on Introduction to Natural Language Processing,gekqq8,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gekqq8/free_webinar_on_introduction_to_natural_language/,1.0,0.0,0.0,12436.0,
2112,2020-05-07 09:09:00,1588831740.0,dataengineering,Interesting Architecture Problem,gf12c0,SK4nda1,,https://www.reddit.com/r/dataengineering/comments/gf12c0/interesting_architecture_problem/,1.0,25.0,0.0,12472.0,"Hey all, 

I have an interesting problem. I've been reading through some posts but can't find the right tool for the job. 

Imagine 10  TB of data in the form csv files in Azure cold storage. Each filename has a timestamp and the data in the files contains a latitude and longitude. 

The stakeholder asks for data specific to several locations for a specific time period. I select the files based on the timestamp and I am left with 5 TB of files. 

This data has to go from Azure to an SQL server for disclosure to whatever stakeholder is waiting for it. Chucking everything in a 5 TB SQL table is obviously not the way to go. 

What platform or method could be used to filter the data in a pipeline from Azure to the SQL server? I was thinking to use Elastic Search but that would mean replacing the SQL server with Elastic. 

Does anyone have a solution or suggestion to this problem?"
2113,2020-05-07 09:29:52,1588832992.0,dataengineering,Impact in Data warehouse die to upgrade in source OLTP,gf1beu,rajneesh4u,,https://www.reddit.com/r/dataengineering/comments/gf1beu/impact_in_data_warehouse_die_to_upgrade_in_source/,1.0,0.0,0.0,12474.0," 

Hello Experts,

My  Source OLTP system is being upgraded from Oracle 12C DB to 19C, hence I  need to have impact analysis on OLAP system in downstream due to this  change.

Data warehouse is also using 12C currently.

I would like to connect with you if there is any suggestions and guidance to have key consideration based focus for this ask.

I am not aware about major changes between these 2 versions and hence having snapshot of key changes will be useful.

My ETL job which is based on ODI 12C tool should not break due to those changes during transformation and data processing.

Thanks,

Rajneesh"
2114,2020-05-07 16:51:14,1588859474.0,dataengineering,Free Webinar on Introduction to Natural Language Processing,gf6wup,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gf6wup/free_webinar_on_introduction_to_natural_language/,1.0,0.0,0.0,12486.0,
2115,2020-05-07 16:53:08,1588859588.0,dataengineering,Opinions on upGrad PGDM in Data Engineering,gf6y1v,rahulsahu910,,https://www.reddit.com/r/dataengineering/comments/gf6y1v/opinions_on_upgrad_pgdm_in_data_engineering/,1.0,2.0,0.0,12486.0,Hi. I am looking to do the PGDM course in Data Engineering from upGrad. Can anyone tell me whether that will be a wise decision?
2116,2020-05-07 22:56:52,1588881412.0,dataengineering,Kafka Certification?,gfdw77,welschii,,https://www.reddit.com/r/dataengineering/comments/gfdw77/kafka_certification/,3.0,3.0,0.0,12494.0,"I'm currently studying for the (Data Bricks) Apache Spark certification, and I've started to look for my next project. Is it worth getting certification for Kafka?"
2117,2020-05-08 00:00:48,1588885248.0,dataengineering,DBA -&gt; Data Engineer,gff4dv,HansSlinger,,https://www.reddit.com/r/dataengineering/comments/gff4dv/dba_data_engineer/,1.0,4.0,0.0,12497.0,"Hi all, I'm interested in any resources/case studies/stories for DBAs who are interested in becoming data engineers. Especially but not limited to Azure. Many thanks"
2118,2020-05-08 00:30:05,1588887005.0,dataengineering,Insight Data Engineering: thought/experience?,gffo2d,oatsativa,,https://www.reddit.com/r/dataengineering/comments/gffo2d/insight_data_engineering_thoughtexperience/,2.0,4.0,0.0,12498.0,"I'm a recent grad from UC Berkeley (graduating in 2 weeks yay) and I accepted an offer for Insight's Data Engineering program. I've had to turn down a couple interviews after accepting, but I am really worried about the success rate given my background. I talked to a few alumni and noticed most of them have graduate degrees. Can anyone share their experience with the job hunt/getting a job through Insight (or if not through Insight, through other ways)?"
2119,2020-05-08 09:21:46,1588918906.0,dataengineering,Onprem Oracle to Snowflake Data migration,gfnqng,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/gfnqng/onprem_oracle_to_snowflake_data_migration/,1.0,0.0,0.0,12511.0,"Hello everyone , 

&amp;#x200B;

We are planning to migrate the data from On premise Oracle to Snowflake environment . We are exploring various options to do this . Three things is our priority,

&amp;#x200B;

1. Transfer the data securely from Oracle without storing anywhere ( Directly load ) is preferred or Encrypt the data and transfer to Snowflake
2. We would need to do the History sync up and Daily incremental update as well. The problem is we do not have Key primary key to filter out the updated values/deleted values(Capturing Changed Data Only) for the particular day
3. One among the option is Bryteflow, We have spun up a new EC2 instance in the marketplace using Bryteflow . My question is Do we need to explicit make any connection setup in the Onpremise data setup. The networking part is very much confused between oracle to Amazon EC2.

[Bryteflow Arch diagram](https://preview.redd.it/5aebbmwqhhx41.png?width=4535&amp;format=png&amp;auto=webp&amp;s=ac789ac270f53f47ffb44fcdf6055c4b48b61cb0)

4. Please suggest , is there any alternatives and efficient and secure approach to do this."
2120,2020-05-08 15:28:58,1588940938.0,dataengineering,How to find top N similar rows in Spark/Spark SQL given id/row?,gfsax8,sam_butt,,https://www.reddit.com/r/dataengineering/comments/gfsax8/how_to_find_top_n_similar_rows_in_sparkspark_sql/,1.0,11.0,0.0,12524.0,"I want to find similar rows, I have researched a bit but couldn't find much.I thought of groupby but doesn't seem efficient.I have some sample data like this

`{""customer"":""customer-13"",""attributes"":{""att-a"":""att-a-6"",""att-b"":""att-b-9"",""att-c"":""att-c-15"",""att-d"":""att-d-12"",""att-e"":""att-e-10"",""att-f"":""att-f-8"",""att-g"":""att-g-1"",""att-h"":""att-h-11"",""att-i"":""att-i-14"",""att-j"":""att-j-2""}}`

`{""customer"":""customer-14"",""attributes"":{""att-a"":""att-a-4"",""att-b"":""att-b-1"",""att-c"":""att-c-2"",""att-d"":""att-d-13"",""att-e"":""att-e-4"",""att-f"":""att-f-9"",""att-g"":""att-g-10"",""att-h"":""att-h-4"",""att-i"":""att-i-15"",""att-j"":""att-j-3""}}`

`{""customer"":""customer-15"",""attributes"":{""att-a"":""att-a-9"",""att-b"":""att-b-9"",""att-c"":""att-c-15"",""att-d"":""att-d-7"",""att-e"":""att-e-10"",""att-f"":""att-f-12"",""att-g"":""att-g-5"",""att-h"":""att-h-3"",""att-i"":""att-i-5"",""att-j"":""att-j-4""}}`

`{""customer"":""customer-16"",""attributes"":{""att-a"":""att-a-15"",""att-b"":""att-b-11"",""att-c"":""att-c-13"",""att-d"":""att-d-14"",""att-e"":""att-e-7"",""att-f"":""att-f-8"",""att-g"":""att-g-7"",""att-h"":""att-h-8"",""att-i"":""att-i-3"",""att-j"":""att-j-6""}}`

I give the customer Id and number N to find top similar customers. Any idea of how to go about it?

For example, given customer-20 find the top 5 similar customers.

I'm relatively new. Thanks"
2121,2020-05-08 18:38:47,1588952327.0,dataengineering,First Job on Data Engineering,gfvew5,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gfvew5/first_job_on_data_engineering/,3.0,24.0,0.0,12540.0,"Hello, i'm a recently graduated from my university,

and landed on Data Engineering job.

I heard most likely my job is getting data from various source and place it into Data Warehouse.

The truth is really have no idea, because all i ever do (in the past) is write some SQL query from another table/database and then insert it to another database.

May i know what Data Engineer usually do? and what should i learn regarding Data Engineering?

( P.S i'm scared that i will not do well on my first job )"
2122,2020-05-09 17:41:39,1589035299.0,dataengineering,Spark | Performance Tuning with Ganglia and Sparklens,gggjrb,mjfnd,,https://www.reddit.com/r/dataengineering/comments/gggjrb/spark_performance_tuning_with_ganglia_and/,1.0,1.0,0.0,12589.0,
2123,2020-05-09 22:01:38,1589050898.0,dataengineering,Data architecture for transactional queries,ggldxh,nariver1,,https://www.reddit.com/r/dataengineering/comments/ggldxh/data_architecture_for_transactional_queries/,1.0,15.0,0.0,12596.0,"Hi everyone,

I work on a 2-years old startup and we are currently shifting to a data lake since we need to have a flexible schema for our platform. We are fully hosted in AWS.

We have drafted an architecture for analytical queries that involve aggregated information, but our team is also in charge of the transactional validations that the operations team needs to perform (eg. cross check that one credit card transactions is present both in our app, in the file that we receive from our payment processor, in the file that we receive from other systems).

We are seeing that our aurora postgresql database won't follow the scale that our business does and it is becoming harder to maintain and evolve. Most of those transactions holds jsons fields that their key-attributes changes per kind of transaction and also throughout time.

Is there any architecture suggested for transactional queries? 

I was thinking on a file storage such as s3 with a query engine as Athena/Presto to handle multiple source. Avro files sounds great for row access also. But I didn't find anything more developed or tested so I've wanted to open this post looking for advices."
2124,2020-05-10 01:08:06,1589062086.0,dataengineering,Need tooling/architecture guidance for medium-ish data size deployed in cloud,ggotnx,just1lettertooshor,,https://www.reddit.com/r/dataengineering/comments/ggotnx/need_toolingarchitecture_guidance_for_mediumish/,1.0,13.0,0.0,12601.0,"Hey guys really would appreciate a hand on this:  
This week I had a physical server go through a hardware failure and it's pretty much a paper weight now. I need to build a system to replace it, and it's my first time having to deal with a situation of implementation at this scale.   


Dataset is very structured, it's literally like 1-2TB of flat csv files. The original server ran a program called Redpoint Data Management which is pretty much high performance SSIS. We were using it to do large scale joins, like 300M rows looking for matches on another 100M rows of names/addresses/emails etc, which on Redpoint would run in 30mins give or take. The source set is 300M rows with over 100 cols, so I think I would really benefit from storing these in columnar format like Parquet since only a few of those cols are ever needed at once. The system is more or less stateless -- I can run a join and produce an output, then put the machine to sleep. It only needs to run when I have something to feed it, and the source data pretty much doesn't change.   


I need guidance on: the best way to store the files themselves since they are large, keeping compatibility in mind with the following point --   
more importantly: the tech I use to actually run the join operations (like should I just try and set up a high indexed regular sql db, would I benefit from a Spark or Hadoop ecosystem, etc), and how much compute I would need to get on the server from AWS.   


Normally I know the answer is 'hire an experienced data engineer' but my startup is pretty much broke after this COVID stuff (I haven't gotten a check in like 3 months) and without this getting back up we're really in the gutter, so cost effective solutions are preferred. Once implemented I'd really like to put up a post on here on how it works out so maybe it helps the next chum in my position down the line."
2125,2020-05-10 01:32:59,1589063579.0,dataengineering,"What's the best datastore of 30 million rows of an array of 512 int32s (the bert input ids)? Each array has an index, and we need to retrieve a couple thousand of these at a time.",ggp9dy,DisastrousProgrammer,,https://www.reddit.com/r/dataengineering/comments/ggp9dy/whats_the_best_datastore_of_30_million_rows_of_an/,1.0,1.0,0.0,12602.0,"I Tried pd.HDFStore, but it seems you can't store arrays with this, and retrieval seems to crash ram. Details: https://www.reddit.com/r/learnpython/comments/ggoyfa/appending_pandas_data_to_hdf_store_getting/"
2126,2020-05-10 17:11:31,1589119891.0,dataengineering,Ideas for ETL pipeline architecture,gh2674,thereal_artvandelay,,https://www.reddit.com/r/dataengineering/comments/gh2674/ideas_for_etl_pipeline_architecture/,1.0,23.0,0.0,12625.0,"Hello friends!

I would love your input on ideas for an ETL pipeline that I am setting up. The background is that we have a fragile and slow pipeline at the moment, and are in need of a remake. At the moment we use a cron job that triggers an export/import of production data to our data warehouse every hour. After that we run MySQL stored procedures to transform the data. It is not huge amounts of data that we are processing, \~10GB that is imported hourly. However, this will continue to grow rapidly, and I would like to build something that is able to last a while. The transform part of the pipeline (i.e. the stored procedures) are doing filtering, joining and group by:s.

 The problems I see with our current setup are the following:

* slow
* does not allow tasks to fail/retry
* processes the data in the same database where it is used for analytics (making other queries slow)
* does not allow processing in parallel
* hard to test (because of SQL)

This is my first experience of managing ETL pipelines, but my idea is to use Airflow to schedule and manage dependencies, and to do the actual processing in python instead of SQL, using e.g. pandas (I'm most comfortable working in python). I read that you could run Airflow on a Kubernetes cluster and have the tasks run as pods. Would that be worth the effort? Would it be better to use spark for the processing? What do the good people of reddit think of this setup? 

Any help is greatly appreciated!"
2127,2020-05-10 19:24:12,1589127852.0,dataengineering,Need feedback on my FAANG interview prep outline,gh4geo,Party_Farm,,https://www.reddit.com/r/dataengineering/comments/gh4geo/need_feedback_on_my_faang_interview_prep_outline/,1.0,13.0,0.0,12634.0,"*tl;dr need to figure out how to prioritize what to study for my upcoming FAANG interview given my weaknesses in SQL/data modeling*

I've got an interview at a FAANG this week which I was informed about on Friday, so it seems like they may be on a quick timeline. My specialty is building data pipelines from one external source to another (all in Python + Pandas) and building data pipelines within Snowflake, so I'm not super strong in SQL, not very experienced with data modeling, and I'm a bit far removed from my undergraduate years to remember fancy algorithms. I'm hoping to get some feedback on my proposed interview prep prioritization since I clearly won't be able to get around to everything.

1. SQL interview questions on Leetcode, starting with easy and working my way up to medium
   1. I looked at some of the SQL questions on this spreadsheet ([https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0](https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0)), and I was able to solve some of the hard questions (with some troubleshooting), so I'm unsure why the easy questions seem hard for me on Leetcode.
2. Python data structure questions and algorithms questions on Leetcode, starting with easy and working my way up to medium (just to make sure I can back up my experience)
3. Watch all of the data warehouse design videos on this spreadsheet ([https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0](https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0))
4. Watch system design videos on this spreadsheet ([https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0](https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0))
5. Continue my online course on Apache Spark (it's my only experience with distributed data processing)

And then anything else I get around to is bonus. Thoughts?"
2128,2020-05-11 00:19:29,1589145569.0,dataengineering,Standing up Puckels Airflow Docker image -- how to set up CI for shared /dags repo on github?,gh9uhv,tylerjaywood,,https://www.reddit.com/r/dataengineering/comments/gh9uhv/standing_up_puckels_airflow_docker_image_how_to/,1.0,4.0,0.0,12640.0,"Hey, previous Airflow user and now trying to set it up for my new org. I have a modified image that is running (finally!) and now I'm trying to figure out how to onboard the other 2 data folks. 

####background
The service runs with the `docker-compose-LocalExecutor.yml` command and in the .yml I mount a local /dags volume to the container. As expected, I can add a dag to /dags and it gets picked up by the scheduler/web UI. 

####goal
What I'd ideally like is to have the containers /dags dir sync to an external git repo. But if I'm running `git pull` in the dir as a RUN in the `Dockerfile` or `.yml` then the git repo will only be synced _when the container starts up_. 

####thoughts
It seems like one way I could accomplish this is by setting up a cron on the machine running the container that runs `git pull` against the mounted dir every minute or something, but this feels counter to the whole Docker idea because I'd need to go into the host machine and edit the crontab. 

And realistically, I'd like to have some sort of check before pulling dags in to the production instance. 

####questions
is there anyone who can speak to how they've set up a CI pipeline into a Docker container running Airflow? Any ideas of fruitful searches or tutorials I could explore? I've done a ton of searching on my own, but haven't yet found a solution but happy to be pointed in a new direction.

The more I think about it the more I realize I'll need more syncing to the repo to bring in any `.py` or .`sql` files that are needed for added dags."
2129,2020-05-11 03:07:05,1589155625.0,dataengineering,How does one prepare for a Data Engineering Intern position task,ghcrso,Unchart3disOP,,https://www.reddit.com/r/dataengineering/comments/ghcrso/how_does_one_prepare_for_a_data_engineering/,1.0,1.0,0.0,12645.0,"as the title says, I am fairly new to this field in particular, I have done a fair amount of ML, and DL in the past, aswell as Python and R, and touched on cloud abit, what should be the things I focus on now? since the things I learnt were heavily baised towards the machine learning side

Thanks"
2130,2020-05-11 08:55:24,1589176524.0,dataengineering,Great deal on Data Engineering books at Humble Bundle.,ghhvvw,dubven,,https://www.reddit.com/r/dataengineering/comments/ghhvvw/great_deal_on_data_engineering_books_at_humble/,1.0,10.0,0.0,12660.0,"Just bought it (15$USD), mostly for these: 

\- Spark: The Definitive Guide 

\- Kafka: The Definitive Guide 

\- Google BigQuery: The Definitive Guide 

 [https://www.humblebundle.com/books/definitive-guides-to-all-things-programming-oreilly-books](https://www.humblebundle.com/books/definitive-guides-to-all-things-programming-oreilly-books)"
2131,2020-05-11 09:39:53,1589179193.0,dataengineering,Building skills for careers in Data Engineering,ghifir,aditya166,,https://www.reddit.com/r/dataengineering/comments/ghifir/building_skills_for_careers_in_data_engineering/,1.0,2.0,0.0,12663.0,"Hello Folks,

I am trying to break into data engineering roles. I am competent in SQL, Python, Pandas and Machine learning Models in general. I have self taught my self Map Reduce and Spark and completed a couple of side projects. ( Nothing at enterprise level though).

What are some of the other skills that you feel I should pick up ? If you could list the resources that would be awesome.

&amp;#x200B;

Thanks!"
2132,2020-05-11 11:25:36,1589185536.0,dataengineering,Please help me come up with some examples of personal Objectives and Key Results (OKRs)?,ghjnwt,ManBearHybrid,,https://www.reddit.com/r/dataengineering/comments/ghjnwt/please_help_me_come_up_with_some_examples_of/,1.0,1.0,0.0,12665.0,"Hi all. 

I started the role of data engineer at a med-tech company in November. They knew I had only minimal experience in data engineering, and they agreed that I would be learning on the job (I have other transferrable skills). This is my first job at a large-ish company and I'm struggling with some of the formalized procedures and requirements. They've asked me to come up with some personal objectives and key results for the year, which is not something I've had to think about before.

I've thought about it and I've decided that my medium-term career aspirations are to become a technical lead in this role. I've thought about proposing that I attempt to write the AWS Big Data Engineering certification exam by the end of the year, and the company has said they would pay for training and the exam, and allow me to spend a portion of my work hours on online courses, etc.

Does anyone else have any other good ideas for things I could propose as personal development goals, and measurable results towards achieving them?"
2133,2020-05-11 12:43:46,1589190226.0,dataengineering,Best GCP offering to get data against user id during a web visit session,ghkk5o,popopopopopopopopoop,,https://www.reddit.com/r/dataengineering/comments/ghkk5o/best_gcp_offering_to_get_data_against_user_id/,1.0,2.0,0.0,12668.0,"We have data that includes content recommendation for users with a user id as the key.

This is currently in a BigQuery table clustered against this user id.

&amp;#x200B;

We want to query this table live every time there's a new session on our website.

We can easily do this with BigQuery but each query for a single id processes around 300mb as that seems to roughly be the size of a cluster.

Therefore the costs for this would add up quickly, e.g. at 50,000 daily sessions we are looking at 15TB processed or around $75/day. 

This feels like it's not a good use for BQ with its pricing model.

&amp;#x200B;

Can somebody suggest what's the best product within GCP to use for this?

Our lookup table would be about 2GB and growing with about 5MM rows and growing.

The schema consists of a hashed user id and a few (3-4) mainly string columns.

The data will be used for live website personalisation."
2134,2020-05-11 16:05:46,1589202346.0,dataengineering,Free Webinar on Introduction to Natural Language Processing,ghn6ds,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/ghn6ds/free_webinar_on_introduction_to_natural_language/,1.0,0.0,0.0,12675.0,
2135,2020-05-11 16:13:03,1589202783.0,dataengineering,"Difference between ""Other"" and ""Unknown""",ghnafn,markwusinich,,https://www.reddit.com/r/dataengineering/comments/ghnafn/difference_between_other_and_unknown/,1.0,4.0,0.0,12675.0,"Looking at a classification of our products at work, (we have a lot of them) and two of the categories are ""Other"", and ""Unknown"". Am I supposed to treat them differently?

Trying to find someone from the team that creating the classifications, but I suspect the answer is going to be complicated and not useful."
2136,2020-05-11 17:47:22,1589208442.0,dataengineering,Creating a Sybase hook/connection in Airflow,ghowlv,ovary_laid,,https://www.reddit.com/r/dataengineering/comments/ghowlv/creating_a_sybase_hookconnection_in_airflow/,1.0,5.0,0.0,12682.0,"Hi All,

A newb here, I am trying to get hands on Airflow.

I can see there is no default way to connect to sybase in web console(admin-connections).

How else can i create a hook for sybase in airflow.

Help appreciated.

Thanks"
2137,2020-05-11 18:46:05,1589211965.0,dataengineering,College Projects,ghq0fl,VintageBoujee,,https://www.reddit.com/r/dataengineering/comments/ghq0fl/college_projects/,1.0,2.0,0.0,12684.0,"I need advice on starting a project over the summer. I’ve looked around and haven’t really found a definite answer on the type of project to work on. I’m just finishing up my sophomore year in college. I know python, c++, and some sql, I’m also open to learning new things. I have used Flask recently even though my web dev skills are not great. I’m looking for a good data engineering project that would help me learn a lot and look good on a resume for an internship. Thanks in advance!"
2138,2020-05-11 19:24:53,1589214293.0,dataengineering,Apache airflow - Use cases and learn materials,ghqs6b,BuggyLoki,,https://www.reddit.com/r/dataengineering/comments/ghqs6b/apache_airflow_use_cases_and_learn_materials/,1.0,2.0,0.0,12685.0,"Hi folks, I am working as a data engineer mainly focused on Azure platform. I came to know about Apache Airflow and just curious to know more about it. Couldn't find any detailed resources online. Here are my Q's about it
1. Is this an ETL tool to build and orchestrate pipelines, similar to Azure data factory or AWS data pipeline
2. What are the key advantages compared to other similar tools/softwares
3. Is it really worth to spend time upon
4. Can you refer some YouTube videos/blogs/online materials where I can learn more about it.
Thanks!"
2139,2020-05-11 20:25:30,1589217930.0,dataengineering,Big Query - Learning Path,ghs0gg,bmrtex,,https://www.reddit.com/r/dataengineering/comments/ghs0gg/big_query_learning_path/,1.0,3.0,0.0,12688.0,"Hello everyone,

I'm starting to learn about Big Query, you know where I can find learning resources? 

Which courses do you advise? Are there any free ones?

Thanks!"
2140,2020-05-12 01:49:30,1589237370.0,dataengineering,MS in DS vs Diploma in CS,ghydv9,mango_sorbet13,,https://www.reddit.com/r/dataengineering/comments/ghydv9/ms_in_ds_vs_diploma_in_cs/,1.0,2.0,0.0,12696.0,"I am in between pursuing a master of science in data science and a diploma in computer science.

I am currently a data engineer, switched to the role from data scientist at the same company as there was a large need for the role. In my role as a data scientist I was mostly doing analysis &amp; reporting &amp; business intelligence and not much predictive modelling as the company is new and there is a lot of foundational data work that needs to get done. It’s my understanding that this ends up being the case for a lot of people hired into data scientist roles.

My initial motivation in applying for the master of data science was to get better in machine learning &amp; statistics. Now, i have a good foundational basis since I did a data science bootcamp at General Assembly - however there is still a lot that I would learn in the masters program. It would take me from an intermediate level to advanced.

That said, in the past year, working as a data scientist and not needing a lot predictive modelling beyond occasional side projects, I am now wondering if I would be better off in my career improving my skills as a software engineer through the diploma in CS, rather than acquiring a much deeper knowledge in ML algorithms/statistics through a masters - given that i gave a good basis already.

I’ve been enjoying my role as a data engineer though I would say it is too early for me to say with full confidence that I’d rather do data engineering than data science, for the rest of my career. I think I wanna steer away from business intelligence type analytics work, and enjoy being somewhere in the crux of data engineering / predictive modelling &amp; experimentation / ML.

I am pretty torn and must make a decision by June 1st so any advice/guidance/thoughts would be really appreciated!

\----------

Here are some of the pros/cons I came up with regarding the programs:

DS Masters:

Pros:

* Masters program, was harder to get into and can therefore be considered more prestigious 
* More “data” relevant

Cons:

* In terms of programming languages, teaches things that arent widely used in industry like SAS and Matlab. Since i already know python really well, i dont need R and the rest sound like technologies I would never need IRL 
* Since i did a data science bootcamp i already know some of the things in the curriculum so there will be some overlap. That said, I feel that my knowledge is at an intermediate level and this would be an opportunity to become really advanced in these topics
* Since i started working at my current company i haven’t seen a huge need for predictive modelling. Would i be better off in my career getting really good as a software engineer through a CS diploma than learning all these predictive modelling techniques on a deeper level?

CS Diploma:

Pros:

* Learning more relevant technical skills (Java/C++)

Cons:

* Less “data” relevant. Some courses sound like I would have 0 need for them IRL.. like computer organization and assembly language. 
* The attractive part of this program for me is the software engineering and programming related courses and I am thinking that I may be able to learn these things through experience on the job / online courses. - I find it easier to learn programming on my own online than learning statistics/ML. I also know many great software engineers who never studied CS so it seems like it is a field that one can learn solo more than other fields"
2141,2020-05-12 05:24:53,1589250293.0,dataengineering,Data Engineering - API,gi251v,mdghouse1986,,https://www.reddit.com/r/dataengineering/comments/gi251v/data_engineering_api/,1.0,13.0,0.0,12699.0,"Hi,

As part of Data Engineering do you guys build API end points? If yes what are the most common technologies and platforms out there that are used to build API for data scientists to consume?


Thanks"
2142,2020-05-12 07:03:19,1589256199.0,dataengineering,Masters Degrees Required?,gi3oct,qazwsx123_1_2,,https://www.reddit.com/r/dataengineering/comments/gi3oct/masters_degrees_required/,1.0,6.0,0.0,12705.0,"In your experience, is a Masters degree required/recommended for a career in Data Engineering?
Since data engineers are generally just software engineers that specialize  in the big data field, would a Masters in CS/Stats be necessary?
Would a DE be asked to fine tune machine learning algorithms?"
2143,2020-05-12 08:20:34,1589260834.0,dataengineering,Which companies have commercialized which Apache projects?,gi4rc5,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/gi4rc5/which_companies_have_commercialized_which_apache/,1.0,9.0,0.0,12709.0,"Hi DE'rs

I'm trying to make a list of Apache projects that have been commercialized or are the underlying tech of a company that is using it to sell their products

So far I'm able to see only two such projects, need your help in contributing to the list. Thanks

* Databricks: Apache Spark
* Elasticsearch: Apache Lucene"
2144,2020-05-12 08:31:23,1589261483.0,dataengineering,"Prod-ready Data Pipeline &amp; ML Pipeline Stack! (along w/ microservices, devops &amp; all the needed tooling!)",gi4wi0,string_engineer_2020,,https://www.reddit.com/r/dataengineering/comments/gi4wi0/prodready_data_pipeline_ml_pipeline_stack_along_w/,1.0,0.0,0.0,12709.0,
2145,2020-05-12 08:51:22,1589262682.0,dataengineering,Dockerizing a Kafka Streams app,gi56jy,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/gi56jy/dockerizing_a_kafka_streams_app/,1.0,0.0,0.0,12709.0,
2146,2020-05-12 09:22:58,1589264578.0,dataengineering,"Should I learn Data Engineering? If so, how do I begin?",gi5lha,erikumali,,https://www.reddit.com/r/dataengineering/comments/gi5lha/should_i_learn_data_engineering_if_so_how_do_i/,1.0,12.0,0.0,12710.0,"Hi everyone!

I've been mulling over if I should start learning data engineering

Let me give you the context:

1. I've been a business analyst for about 3-4 years now, and I'm currently leading a team of business analysts, creating reports, presentations, and dashboards that help our leaders make better decisions about the company. I primarily help Sales
2. Our data is f\*\*\*ed up and so disjointed. True sales data lives in a self-managed server. Opportunity data lives in Salesforce. Account data also lives in Salesforce, but is somehow disjointed from the opportunity data that should have spawned it. It's really, really messed up
3. We've been working with excel for a long, long time, and we haven't really developed new skills. We take hundred thousands rows of data, and work with it. And it has worked so far until now. Now, the business demands ever greater things, in a shorter and shorter span of time.
4. I only know of one guy, who has an idea of how to do some ETL thing (get data from Salesforce, merge it with some data from another source, upload it into his AWS). And he's in a very high leadership position, and can only work on ETL stuff in his spare time. He shouldn't even be doing that kind of work at his position, but he's really good at it.
5. Tableau is our company's analytics future. So having databases living in a cloud server, and letting my analysts create dashboards and generate insights from what they create in Tableau would be more efficient than running an excel file for 30 mins to 1 hour to process.

So yeah, that's the context.

We're getting more work. we have less time to do it. We can't hire more bodies. We need to scale.

&amp;#x200B;

What are your thoughts?

&amp;#x200B;

Thanks

Erik Sy Umali"
2147,2020-05-12 11:16:10,1589271370.0,dataengineering,Working with Geospatial data,gi71qy,Sihal,,https://www.reddit.com/r/dataengineering/comments/gi71qy/working_with_geospatial_data/,1.0,1.0,0.0,12713.0,"I'm making a side project, where I collect geospatial data by web scrapping and from OSM API. I've started with simple Java application, however, I would like to make it as a data flow, purely for learning purposes. 

Unfortunately, my knowledge about tools, and mostly connecting them, is, well, low. 

What is my goal?  
As a final result I want to visualize scrapped geospatial points on the map with the roads connecting them(from OSM).

Current flow:  
In standalone Java application I'm scrapping the data for geospatial points. There is a client consuming the OSM API for needed data. 

What I think it might be useful:  
Use Apache Spark for collecting and transforming the data. Then use somehow GeoSpark and Zeppelin to visualize the data. I was also thinking about using ES + Kibana for geodata, but it looks like the Zeppelin is enough. 

What do you think? Are there any better tools I can use? Did I miss anything?"
2148,2020-05-12 14:52:56,1589284376.0,dataengineering,Elasticsearch Design for news search engine on AWS. Need your help,gi9rm9,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/gi9rm9/elasticsearch_design_for_news_search_engine_on/,1.0,0.0,0.0,12715.0,"Hi everyone,

I am going to set up an Elasticsearch Service (7.4) by AWS for our News API product - [newscatcherapi.com](http://newscatcherapi.com/)

We are in beta now. We used some overkilling instances without much configuration and/or optimization. This time, we want to do it well, as we are planning to release our first stable version by the 1st of June.

*For a little back story, Newscatcher is a Data-as-a-Service company that builds an API to search through online news articles.*

*Just like Google searches the most relevant web pages, we return you the data on the most relevant news articles.*

*We aggregate the data from thousands of sources, and collect up to 20 data points on each article, such as published time, title, author, URL, language, country, topic, media links, etc.We help financial institutions, market analysts, media platforms, and PR agencies to analyse what the worldwide news are talking about.*

The data itself is the news articles with such basic information as:

* date published
* title
* article body
* authors
* URL
* etc.

Endpoints that we have:

* full-text search of different kind
* aggregation per hour or day

On the Index mapping, I think we are good. Anyway, I do not want to mix it all as it is already quite of a long-read. However, it gets much more difficult when it comes to choosing the right instance type and index design.

To the point

The data is loaded through a bulk insert via a Python code deployed on AWS Lambda. The *\_id* is already created by that time. Also, no duplicates. Right now, I can only assume how many records can be inserted at a time. I would say it should not be more than a 1,000 in a minute.

The index is constantly growing.

It is usually recommended to keep the shard size around 50 GB. I estimate to have 50GB within 2 weeks - a month of data.

# Cluster design

2 nodes. No dedicated master nodes.

Basically, I want my indexes to be 1 primary shard with 1 replica. Therefore, I need 1 node to hold primary shards and 1 node to hold replicas.

I do understand that:

&gt;If you lose a single node from a two-data-node cluster, that cluster stops accepting writes because there isn’t quorum (two) to elect a master.

With AWS, if cluster is down it should be rebooted from the latest snapshot automatically. Our ""golden"" source of data is stored in DynamoDB, and we prepared the script to reload the last day data to keep the ES up to date. Though, I still did not figure out if we are notified when the cluster is down?

Instance type

Initially, I thought about 2 nodes of **r5.large.elasticsearch** \- 2vCPU + 16GiB. This type of instance supports EBS. With no dedicated IOPS, EBS seem to have about 100 IOPS per sec that can boost sometimes. Provisioned IOPS is too expensive for our budget.

Another option that seems like a killer to the previous one is to choose **i3.large.elasticsearch** which has 2vCPU + 15.25GiB and **1 x 475 NVMe SSD**. NVMe SSD should be 10,000 IOPS per sec and above, right?

The price difference between those two is not that big - $0.186 vs $0.25. Plus, you have to pay for EBS in case of **r5** instance.

1. Am I missing something here?
2. Is 2 shards a better option? Assuming that each node has 2 CPUs?

# Index design

Assume that 40-50 GB is one month of news data.

**Option 1. Template &amp; monthly indexes**

We will send data from each month to its monthly index which is defined by a template.

Drawback - from month to month, the size of indexes will be different. It also can exceed the ""optimal"" size.

Advantage - we can write the queries for our API in such a way that they will hit only 1 or 2 indexes. For example, I have an index which holds only the news data from May. If our API receives a request to find all the articles mentioning ""Apple"" from 5th to 10th of May, we can navigate them to search only at newscatcher-may  
index.

**Option 2. Rollover index**

Rollover will guarantee that 1 index does not have more than 50GB of data (or, whatever we say). However, it means that every read operation will hit all of the indexes.

If I understand it right, one year of data should be 12 indexes. Assume each one is 1 primary shard and 1 replica shard. Even if my read request requires only the data from 1 index, it will have to hit all of those 12. To do this operation in parallel, 12 vCPUs are required, correct?

1. Which of those 2 options do you prefer?
2. Anything else you could advise?

&amp;#x200B;

Given that we want to be able to serve 100+ calls a minute, should it be enough? Or, the only way to know is by testing it?"
2149,2020-05-12 16:05:45,1589288745.0,dataengineering,How to get data from Facebook,giaugl,csenaraths,,https://www.reddit.com/r/dataengineering/comments/giaugl/how_to_get_data_from_facebook/,1.0,3.0,0.0,12717.0,"What is the best method for get data like comments, reacts from facebook page"
2150,2020-05-12 16:53:07,1589291587.0,dataengineering,[GUIDE] Sagemaker: SSH to notebook instances,gibmp6,derivablefunc,,https://www.reddit.com/r/dataengineering/comments/gibmp6/guide_sagemaker_ssh_to_notebook_instances/,1.0,0.0,0.0,12719.0,
2151,2020-05-12 17:18:07,1589293087.0,dataengineering,Free Webinar on Introduction to Natural Language Processing,gic27q,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gic27q/free_webinar_on_introduction_to_natural_language/,1.0,0.0,0.0,12720.0,
2152,2020-05-12 17:28:16,1589293696.0,dataengineering,An interview with StreamNative co-founder Sijie Guo about his experience contributing to the Pulsar framework for streaming data and its community.,gic8mt,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/gic8mt/an_interview_with_streamnative_cofounder_sijie/,1.0,0.0,0.0,12722.0,
2153,2020-05-12 18:41:22,1589298082.0,dataengineering,"Professional Data Engineer, still lacking DE skills",gidlxc,floydhead11,,https://www.reddit.com/r/dataengineering/comments/gidlxc/professional_data_engineer_still_lacking_de_skills/,1.0,52.0,0.0,12724.0,"I have been working as a Data Engineer for a big firm where we primarily use ETL and Orchestration Tools provided by other companies. 
We are predominantly A SQL shop where almost 90% of my work is writing procedures or functions. The little scheduling that has to be done is done using IICS and the likes. There is very little Python or Powershell or Java that I use (although I know it). Certainly no docker or big data tools.

When I read this thread or see the job market, it looks like almost everyone wants a DE who knows Spark, Hadoop, Kubernetes, Python, Shell, etc.

I know I am good at SQL and have added value to our processes over time and I am capable of achieving a lot more. I can make a difference.

But somehow, I just have this FOMO that I might even be doing DE anymore or that none of my skills are relevant anymore.

Given the current pandemic, it just increases my anxiety and I'm unsure where do I go from here.

So I had the following questions for the group -
1. Are my skills still relevant? Is it normal?
2. If I do not have professional access to all the meta DE skills (PySpark, Hadoop, etc), how do I gitgood at them?
3. For your DE roles, how did your interviews go? My DS+Algo game is average at best
4. How do I differentiate myself from others who have all these meta skills? Again, I do think I am a difference maker but unable to portray it online/on resume

I hope I was able to get my points across.

Past experience says that some might shame me about this, but, I look forward to the constructive stuff!

Thank you!"
2154,2020-05-12 20:44:57,1589305497.0,dataengineering,How should I prepare for a system design interview for a Junior position?,gig6gn,yzhivago,,https://www.reddit.com/r/dataengineering/comments/gig6gn/how_should_i_prepare_for_a_system_design/,1.0,1.0,0.0,12728.0,I have an interview at a big mobile game company for a Junior position. I have been working as a data engineer for almost a year now and I do mainly maintenance and take care of our ETL process. I don't have much architecture knowledge and I have no idea what kind of questions I can expect to be asked. They said the interview is going to be a system design case. I have already passed the first whiteboard interview about coding and SQL. What should I study to prepare for this one?
2155,2020-05-12 20:46:51,1589305611.0,dataengineering,Thesis choice related to data engineering,gig825,mihai_zanfir,,https://www.reddit.com/r/dataengineering/comments/gig825/thesis_choice_related_to_data_engineering/,1.0,1.0,0.0,12728.0,"Hi!

I'll be finishing university next year and I want to learn more data engineering related tools and concepts.

Do you have any recommandations for any topics, technologies I should push for as a beginner? What would be a good choice for a thesis? Also, do you have any ideas on possible subjects?"
2156,2020-05-12 20:49:07,1589305747.0,dataengineering,How Do I Become A Kafka Expert,gig9v4,artya4,,https://www.reddit.com/r/dataengineering/comments/gig9v4/how_do_i_become_a_kafka_expert/,1.0,3.0,0.0,12729.0,"My company purchased Kafka and Confluent a couple months ago and are in the process of implementing it. Minus some random dabbling and some trainings I have very little experience with Kafka. My background is as a Data Engineer in a SQL Server environment. I have experience with C++, Python, and SQL (obviously). With the current quarantine situation and my being blocked with a couple things at work I have a decent amount of time of my hands. My question for all of you is if I was trying to build myself a plan to become a Kafka Expert how should I spend my time? I’m sorry for how general this request is but the more I pick at Kafka the more I feel like I could fall down many rabbit holes in my learning and mostly I just want some guidance on where to focus."
2157,2020-05-12 20:56:16,1589306176.0,dataengineering,"View from across the data lake: Developing the mileage indicator using our self-service data platform (Spark ML, Databricks, culture)",gigfsj,markcrossfield,,https://www.reddit.com/r/dataengineering/comments/gigfsj/view_from_across_the_data_lake_developing_the/,1.0,0.0,0.0,12729.0,
2158,2020-05-12 21:24:01,1589307841.0,dataengineering,Data Engineers: How much machine learning do you get to work on in your jobs?,gih2pc,___24601,,https://www.reddit.com/r/dataengineering/comments/gih2pc/data_engineers_how_much_machine_learning_do_you/,1.0,2.0,0.0,12731.0,
2159,2020-05-12 23:59:01,1589317141.0,dataengineering,Help to choose the data engineering problem.,gik9t8,BratCondrat,,https://www.reddit.com/r/dataengineering/comments/gik9t8/help_to_choose_the_data_engineering_problem/,1.0,0.0,0.0,12737.0,"Hi everyone,    
I am finishing DE training program. I need help to brainstorm the project that I could present in 8 weeks.   
Thank you in advance"
2160,2020-05-13 01:49:57,1589323797.0,dataengineering,Data Mapping or ETL tool that's business user friendly?,gimcq4,iSawAMoose,,https://www.reddit.com/r/dataengineering/comments/gimcq4/data_mapping_or_etl_tool_thats_business_user/,1.0,2.0,0.0,12742.0,"I'm looking to explore tools in which moderately technical business users can do some data mapping. They get files from clients that change from time to time and it is the analysts' job to change the mapping (similar firms have data mappers doing this).

Currently, Alterx is being used but the perception is that it is overpriced for this function.

Any recommendations for where to start looking for a replacement? (Changing their job description or bringing in more technical data mappers isn't an option.)"
2161,2020-05-13 02:28:19,1589326099.0,dataengineering,How to create a udf for Pyspark dataframe with conditions?,gin1f5,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/gin1f5/how_to_create_a_udf_for_pyspark_dataframe_with/,1.0,0.0,0.0,12743.0,"&amp;#x200B;

I have the following dataframe in Pyspark. It has a primary id and there are four possible values for codes (code\_1,code\_2 and code\_3): 01, 07 and 06 and null, and 4 respective amounts for each of the code buckets.

&amp;#x200B;

https://preview.redd.it/aa8rxwby4fy41.png?width=634&amp;format=png&amp;auto=webp&amp;s=e56dd26edcee4bc709b0b15e66e3ecde99b535e6

&amp;#x200B;

I need to create a udf which takes the greatest amount for each code category (01,07 and 06) for each row. This should accept each code as a parameter and loop through all 3 code buckets to find matches. It should then create 3 new fields which represent the maximum amount for each target code in each row. (max\_01, max\_07 and max\_06)

&amp;#x200B;

max\_01, max\_07 and max\_06 are the outputs I am looking for..

&amp;#x200B;

Here is my attempt at the udf:

&amp;#x200B;

&amp;#x200B;

        def get_max_amt(code_col, amt_col, code): # code_[1-3], amt_[1-3], codes: 01,06,07
            col_indices = []
            maxval = 0
            for i in range(len(code_col)):
                if code_col[i]==code:
                    col_indices.append(i)
            
            for index in col_indices:
                maxval = max(maxval, amt_col[index])
            
            return maxval
        	
        
        func_udf = f.udf(get_max_amt, FloatType())
        
        
        output = df.withColumn(""max_amt_01"", func_udf(struct('code_1'), struct('amt_1'), f.lit('01')))
        		   .withColumn(""max_amt_07"", func_udf(struct('code_2'), struct('amt_2'), f.lit('07')))
        		   .withColumn(""max_amt_06"", func_udf(struct('code_3'), struct('amt_3'), f.lit('06')))"
2162,2020-05-13 13:05:47,1589364347.0,dataengineering,Apache Airflow: Store and read logs of your DAGs in AWS S3,giw699,marclamberti,,https://www.reddit.com/r/dataengineering/comments/giw699/apache_airflow_store_and_read_logs_of_your_dags/,1.0,0.0,0.0,12770.0,
2163,2020-05-13 13:41:10,1589366470.0,dataengineering,Cleaning all data points between 0th and 1st second,giwm15,R0b0tg,,https://www.reddit.com/r/dataengineering/comments/giwm15/cleaning_all_data_points_between_0th_and_1st/,1.0,4.0,0.0,12773.0,"I am working on some gas sensor. I have collected a lot of data, but the data points are separated milliseconds apart. I need to clean all the data points so that the final data points will be 1 second apart. I have a csv file for the data points that I have collected. Is there any way of doing this in Google Sheets or Libre Calc?"
2164,2020-05-13 14:14:54,1589368494.0,dataengineering,Udacity DE Nano Degree in a Month?,gix1d6,welschii,,https://www.reddit.com/r/dataengineering/comments/gix1d6/udacity_de_nano_degree_in_a_month/,1.0,27.0,0.0,12775.0,"How realistic do you think it is to complete this within the offer of one free month. I finished my contract so I am not working from home. If anyone has done this, how many hours did they put in a day?"
2165,2020-05-13 14:36:38,1589369798.0,dataengineering,Data Engineering: What is it?,gixbrb,DavidUnbound,,https://www.reddit.com/r/dataengineering/comments/gixbrb/data_engineering_what_is_it/,1.0,8.0,0.0,12777.0,
2166,2020-05-13 15:04:22,1589371462.0,dataengineering,Suggestions on how to handle a problem from an End to End DataScience Project.,gixpf7,esp_py,,https://www.reddit.com/r/dataengineering/comments/gixpf7/suggestions_on_how_to_handle_a_problem_from_an/,1.0,6.0,0.0,12777.0,"For Fun and To Build my portfolio, I am working on an end to end data science project.

What I am doing is this :

I am scrapping tweets from twitter, saving them in a Mongo Database, load them from the database to clean them and apply some machine learning algorithm on them (Topic modelling) to find which topics people are talking about on twitter. All this is done in a Jupyter Notebook.

What I want now, Is to put everything in production.

\- I want to load my data every hour,  apply the same process.

\- Then I want to build a front end that will display the results of my analysis.

\- I also don't want to save my data in my database for more than 24 hours it can grow very large, I want to be able to clean everything after 1 day.

How should I handle this? any ideas on the tech I should use, any link to a similar problem?

What is the appropriate workflow to do this?   
I am doing everything in python."
2167,2020-05-13 16:48:06,1589377686.0,dataengineering,Python script automation,gizd67,iffexibility,,https://www.reddit.com/r/dataengineering/comments/gizd67/python_script_automation/,1.0,6.0,0.0,12789.0,"I have a python script that gets update from a site, how can I automate the running process on github or wherever? The main purpose of the automation is to get the difference between the data daily, any idea please?"
2168,2020-05-13 17:46:27,1589381187.0,dataengineering,Principles of lazy data documentation — and how to get your team onboard,gj0f20,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/gj0f20/principles_of_lazy_data_documentation_and_how_to/,1.0,0.0,0.0,12793.0,
2169,2020-05-13 18:14:24,1589382864.0,dataengineering,Setting up Role-based Access Control (RBAC) with UAA &amp; LDAP in Ververica Platform,gj0yap,Marksfik,,https://www.reddit.com/r/dataengineering/comments/gj0yap/setting_up_rolebased_access_control_rbac_with_uaa/,1.0,0.0,0.0,12797.0,
2170,2020-05-13 19:30:15,1589387415.0,dataengineering,Blogged: Building a Telegram Bot Powered by Apache Kafka and ksqlDB,gj2ef3,rmoff,,https://www.reddit.com/r/dataengineering/comments/gj2ef3/blogged_building_a_telegram_bot_powered_by_apache/,1.0,0.0,0.0,12802.0,
2171,2020-05-13 20:21:35,1589390495.0,dataengineering,[Product Research] An ETL Tool to capture and stream data in real-time to anywhere,gj3cj8,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/gj3cj8/product_research_an_etl_tool_to_capture_and/,1.0,3.0,0.0,12807.0,"Hey DErs

I was thinking of working on a side-project kind of thing to come up with a tool that can help capture web events, stream them to any where you want like crm, warehouse, zapier etc plus there'd be an option to track custom events and do transformations on the fly.

It'd could be usefull for example, where when people do specific action on your website, you'd want to target your google ads to them only or maybe trigger a mail from mailchimp at the same time.

What do you think? What can I improve on? What could be my challenges? What seems positive here?

Looking for feedback"
2172,2020-05-13 21:30:30,1589394630.0,dataengineering,getting maximum value for first element in list of lists in PySpark column,gj4o59,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/gj4o59/getting_maximum_value_for_first_element_in_list/,1.0,0.0,0.0,12805.0,"I have a PySpark column with values of list of lists like this

&amp;#x200B;

        row 1: [['01', '100.0'], ['01', '400.0'], [None, '0.0'], ['06', '0.0'], ['01', '300'], [None, '0.0'], ['06', '200.0']]
        row 2: [[None, '200.0'], ['06', '300.0'], ['01', '500'], ['06', '100.0'], ['01', '200'], ['07', '50.0']]

&amp;#x200B;

I need to compare elements with same first element in the list of lists and filter out the arrays with the maximum second element for each pair. While the array may have different codes for the first element, I want to filter out array elements containing '01' , '06' or '07' and add two columns to my dataframe.

&amp;#x200B;

So the result for a sample row above would look like this:

&amp;#x200B;

        [['01', '400.0'], ['06', '200.0'], ['07':'0']
        [['01', '500.0'], ['06', '300.0'], ['07':'50']

&amp;#x200B;

what's the most efficient way to do this?

&amp;#x200B;

https://preview.redd.it/1socvp6qsky41.png?width=438&amp;format=png&amp;auto=webp&amp;s=8c8f4f296fa1cb98b45d1a1460211eb5a4c4fe79"
2173,2020-05-13 22:45:53,1589399153.0,dataengineering,What foundations are needed in this role?,gj66y8,MassW0rks,,https://www.reddit.com/r/dataengineering/comments/gj66y8/what_foundations_are_needed_in_this_role/,1.0,10.0,0.0,12811.0,"Most posts I see mention that a data engineer should know SQL, Python, Spark, Airflow, AWS,  etc. Aside from that, what foundational knowledge should a data engineer know? For example, data modeling is something along the lines of what I am looking for. 

I just accepted a more junior role and want to come in as prepared as possible. Learning tools is one thing, but I feel like knowing the baseline theory is arguably more important."
2174,2020-05-13 23:29:17,1589401757.0,dataengineering,"Why GitLab is building Meltano, an open source platform for ELT pipelines",gj722d,MeltanoDouwe,,https://www.reddit.com/r/dataengineering/comments/gj722d/why_gitlab_is_building_meltano_an_open_source/,1.0,29.0,0.0,12815.0,
2175,2020-05-14 01:29:19,1589408959.0,dataengineering,Best way to design dynamic changes in big data ETL jobs (spark)?,gj9dzo,TKTheJew,,https://www.reddit.com/r/dataengineering/comments/gj9dzo/best_way_to_design_dynamic_changes_in_big_data/,1.0,0.0,0.0,12823.0,
2176,2020-05-14 04:42:52,1589420572.0,dataengineering,Can I hold company data in a local database if I'm using it for work purposes?,gjcrpm,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/gjcrpm/can_i_hold_company_data_in_a_local_database_if_im/,1.0,10.0,0.0,12837.0,"I feel I may have one hand tied behind my back where I'm not given write-access to company's Redshift database.

I'm creating reports in Tableau where the data sources are at times coming from multiple data sources e.g. MySQL, Redshift, MS SQL Server, flat files etc.

Since IT aren't giving me a database of my own, or an ability to create/insert data into the data warehouse, I feel to create a local postgres instance, load data to it in order to store and transform the data and then export a csv that is then used in Tableau (automating the process in Python).

Question is:

* Am I allowed to hold company data on my machine in a database? Even though the database is used for work purposes?
* How safe/unsafe is this? Are there ways to make this process more secure?"
2177,2020-05-14 05:12:43,1589422363.0,dataengineering,Google BigQuery for side projects?,gjd8o9,BeerMang,,https://www.reddit.com/r/dataengineering/comments/gjd8o9/google_bigquery_for_side_projects/,1.0,4.0,0.0,12840.0,"I'm checking out cloud database storage for some side projects. AWS pricing seems pretty confusing to me, and Azure is too expensive. I came across google BigQuery. A few things I like about it are that you can use ANSI SQL (I am most comfortable in TSQL). It also seems pretty generous with storage (Anything under 1 TB is free). 

If I wanted to take a few free APIs, build a pipeline to ingest that into a cloud DB and then have SQL queries to analyze, it seems like it's a good way to go.

Anyone with experience in one of the other tools? I'm hoping to invest the time into the best fit for a personal project and don't want to waste time on the wrong tool."
2178,2020-05-14 06:02:01,1589425321.0,dataengineering,Any young data engineers want a freelance project?,gje0bo,rednirgskizzif,,https://www.reddit.com/r/dataengineering/comments/gje0bo/any_young_data_engineers_want_a_freelance_project/,1.0,12.0,0.0,12865.0,"I’ve got a side project I’ve worked on for nearly 5 years now and it’s becoming too large for one man to work on. I can’t afford to hire any real software engineers, but if there are any fresh grads or students, or ideally maybe a good science grad student who wants practice producing code as a product, DM me. 

Possible Task 1:
Helpful knowledge:
Python
Pandas
RESTful APIs

Possible Task 2:
Helpful knowledge:
Python
RESTful APIs
MongoDB (pymongo)
git 

Possible Task 3:
Helpful knowledge:
Python
Webscraping libraries
RESTful APIs
SQL
AWS
git 

Like I said, I can’t afford much, but if the work is good I will provide some compensation, something less than $1000. I can teach some one willing to learn via Skype meetings, but only if you are willing to put out effort; I’m not going to be knocking down your door to schedule help sessions."
2179,2020-05-14 08:43:41,1589435021.0,dataengineering,The BEST bottom-up overview of Apache Kafka (2019)!,gjg9sd,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/gjg9sd/the_best_bottomup_overview_of_apache_kafka_2019/,1.0,0.0,0.0,12965.0,
2180,2020-05-14 10:43:53,1589442233.0,dataengineering,The BEST bottom-up overview of Apache Kafka (2019)!,gjhrmq,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/gjhrmq/the_best_bottomup_overview_of_apache_kafka_2019/,1.0,3.0,0.0,12997.0,
2181,2020-05-14 16:44:57,1589463897.0,dataengineering,Webinar on How To Choose the Right Data Science Program For You!,gjmmgv,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gjmmgv/webinar_on_how_to_choose_the_right_data_science/,1.0,0.0,0.0,13055.0,
2182,2020-05-14 18:27:47,1589470067.0,dataengineering,Where do you store your unit test and results?,gjoia6,markwusinich,,https://www.reddit.com/r/dataengineering/comments/gjoia6/where_do_you_store_your_unit_test_and_results/,1.0,2.0,0.0,13081.0,"I am migrating from a one man show to a team, I will not be managing, we will both report to the same (non-etl, but still technical) manager.

As we formalize a lot of the things I used to do on my own, I need a central shared place for our team to store things, that used to go in my local folders.

I am just curious, where do you store your one off unit tests and results? We have a place for tests that run every execution (validation reports) and that run every new implementation (regression tests), but where should we store all the one off tests we create to validate the tiny little change we make that either are too expensive to run every day, or maybe even changed our archive data, and never needs to be run again?

We have rally to track our work (we have a template test for regression test that gets added to each implementation), git to track our code (which houses our validation tests). Other than creating a shared folder somewhere, we are trying to figure out where we should store our one off tests.

Rather than try and solve our problem (because I don't want to spell out all the nooks and crannies of our development process) I am just looking to gather existing solutions. HOW DO YOU SOLVE THIS PROBLEM?"
2183,2020-05-14 19:24:51,1589473491.0,dataengineering,Automating the process of writing incomming CSV’s to a SQL database,gjpl6a,Luukv93,,https://www.reddit.com/r/dataengineering/comments/gjpl6a/automating_the_process_of_writing_incomming_csvs/,3.0,54.0,0.0,13088.0,"Hello,

We receive multiple CSV’s every week that follow the same structure of the vendor of one of our systems. Unfortunately the vendor does not provide a REST API so we have to deal with the vendor sending us CSV’s.

What are best practices of automating the process of writing CSV’s to a database? How would you deal with this situation?

Thanks!"
2184,2020-05-14 21:39:01,1589481541.0,dataengineering,EBook - The Ultimate Guide on Implementing Agile for Data Teams,gjs7vp,ReasonablyHank,,https://www.reddit.com/r/dataengineering/comments/gjs7vp/ebook_the_ultimate_guide_on_implementing_agile/,1.0,0.0,0.0,13097.0,
2185,2020-05-14 22:31:28,1589484688.0,dataengineering,Data Product Demo and Release,gjt8qp,jrich8573,,https://www.reddit.com/r/dataengineering/comments/gjt8qp/data_product_demo_and_release/,1.0,2.0,0.0,13107.0,what are your sprint demos and release cycles?
2186,2020-05-15 08:30:02,1589520602.0,dataengineering,CSUDH is releasing a new master’s degree in systems engineering this fall. Would acquiring this help in getting into data engineering? (Link to the course below),gk36r1,throwawaystickies,,https://www.reddit.com/r/dataengineering/comments/gk36r1/csudh_is_releasing_a_new_masters_degree_in/,1.0,0.0,0.0,13135.0,https://www.csudh.edu/systems-engineering-ms/
2187,2020-05-15 10:07:00,1589526420.0,dataengineering,Data Quality on different source systems,gk4ca5,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/gk4ca5/data_quality_on_different_source_systems/,1.0,9.0,0.0,13139.0,"Hi,

We have various source systems like sql server, mysql etc.. and use powerbi for dashboarding. We would like to check the quality of the data i.e profile the data like if a column is email, how many nulls are there, how many values do not qualify for email etc... same for other columns, how many distinct values etc... Now there are more than 1000 tables that we have and would like to do it for all the tables. 

Would like to know if you use any tools to check the quality of the data in your source systems. How do you do it ?"
2188,2020-05-15 14:25:15,1589541915.0,dataengineering,Which Database for Huge Census Data,gk7c81,poincarebendixson,,https://www.reddit.com/r/dataengineering/comments/gk7c81/which_database_for_huge_census_data/,1.0,4.0,0.0,13151.0,"Hi all,

I'm wanting to query census data and wondering if what non-relational DB solution would be worth looking into. 

&amp;#x200B;

**In relational terms the data would look like**

\- 500k rows (people in census)

\- 50k columns (every answer from every question)

\- Values 0, 1, or Null (person answered, no, yes, or didn't answer)

&amp;#x200B;

**Queries**

\- Aggregated counts (e.g. number of people who ""born in state A"", ""are female"" and ""drive a car"") . In pseudo code it would be something like COUNT(born\_in\_A==1 AND female == 1 and drives\_car==1)

\- Performance &lt; 1 second for the above

&amp;#x200B;

**Scale consideration - in production** the solution must scale up to hosting around 50-100 of these census'. Only querying 1 census per query is required at this time.

&amp;#x200B;

Any and all thoughts are appreciated!

 Wouldn't be against **paying for a proof of concept** to get the ball rolling - just needs to demonstrate &lt;1 second performance with data at that scale. (if the aforementioned is against the rules then admins please remove the pay for bit - and sorry about that)"
2189,2020-05-15 15:06:01,1589544361.0,dataengineering,how to convince employers/higher management to pay for Data Engineering certifications,gk7vrw,traveling_wilburys,,https://www.reddit.com/r/dataengineering/comments/gk7vrw/how_to_convince_employershigher_management_to_pay/,1.0,2.0,0.0,13153.0,"while I agree that certifications are not a one stop substitute for actual projects, my employer may be willing to pay for our certifications, specifically the Databricks associate spark developer certification if I (my team) can convince them to pay for them. It'll be nice to flash it on linkedin, but I need to make a business case for how this will help engineers add value to everyday work. What are some of the points I can make?"
2190,2020-05-15 15:20:50,1589545250.0,dataengineering,does anyone know any good course to take for sql and spark?,gk83hh,rishavbhurtel,,https://www.reddit.com/r/dataengineering/comments/gk83hh/does_anyone_know_any_good_course_to_take_for_sql/,1.0,11.0,0.0,13153.0,
2191,2020-05-15 17:42:07,1589553727.0,dataengineering,Webinar on How To Choose the Right Data Science Program For Your Career,gkacoh,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gkacoh/webinar_on_how_to_choose_the_right_data_science/,1.0,0.0,0.0,13163.0,
2192,2020-05-15 18:36:41,1589557001.0,dataengineering,Watermarks in Apache Flink Made Easy,gkbcsh,Marksfik,,https://www.reddit.com/r/dataengineering/comments/gkbcsh/watermarks_in_apache_flink_made_easy/,1.0,0.0,0.0,13168.0,
2193,2020-05-15 19:25:36,1589559936.0,dataengineering,"Interviewing for a Data Integration Analyst/Engineer , my background is primarily data Analytics.",gkcab9,FruityPebblePug,,https://www.reddit.com/r/dataengineering/comments/gkcab9/interviewing_for_a_data_integration/,1.0,1.0,0.0,13170.0,"I have a final technical interview for a vey good software development company where I can definitely learn a lot and grow for the next few years. 

I've gotten through 2 interviews and a coding test. 

But I honestly do not know too much about data engineering and I've done more statistical analysis in more current role.

I'm very good at R which is what I choose to use for my coding test and cleaned up data and ran some basic data structure analysis. 

My SQL okay, I can do complex joins and understand database schemas. 

What should I expect? A big portion of this join is batch pulling data from a old system , transform it, and then upload it into a new system. 

I really have no idea how to build a data pipeline but I plan on spending this weekend at least familiarizing myself with some of the technology and key terms. 

On thing that has helped me so far is that I actually work in the same industry as the new company and have seen similar data before. 

Any advice?"
2194,2020-05-15 19:59:12,1589561952.0,dataengineering,Automating an existing ML project,gkcx84,Lewba,,https://www.reddit.com/r/dataengineering/comments/gkcx84/automating_an_existing_ml_project/,2.0,6.0,0.0,13170.0,"This might be more of a MLE question than DE, so I'll take this elsewhere if the mods say so. 

I'm a junior who works in a VERY small ML team. The python codebase is a house of cards and essentially every step of deployment is manual (I'm talking literal drag and drop at some points). Unfortunately my lead DS knows less about engineering than I do, so automating this training/retraining procedure is entirely on me. What would you say would be a good place to start for someone who has to learn and implement on the fly?

For context, we find relevant news articles for customers. But their needs can change, which should trigger a retrain of their model, and redeployment to production. Human editors should also be able to flag a model for retraining/inspection. There are hundreds of models in production.

Is something like Airflow overkill? Would a simple CRON job do the trick? Should I be pinging a Flask API to check status on models in production or would you recommend something else?"
2195,2020-05-15 20:53:55,1589565235.0,dataengineering,Would you advise to learn Kubernetes/Docker before landing a junior DE job?,gkdyhs,Luukv93,,https://www.reddit.com/r/dataengineering/comments/gkdyhs/would_you_advise_to_learn_kubernetesdocker_before/,1.0,11.0,0.0,13172.0,"I have worked as a data analyst for 2 years. During that time I studied Python and SQL and used them a lot during work. Now that I want to develop myself towards a DE position, would you advise me to learn docker and kubernetes? What would be the best way of learning them?

&amp;#x200B;

Appreciate your reply!"
2196,2020-05-15 22:07:03,1589569623.0,dataengineering,Python Classes in pipelines,gkfcds,djkaffe123,,https://www.reddit.com/r/dataengineering/comments/gkfcds/python_classes_in_pipelines/,3.0,5.0,0.0,13176.0,"Hi,

I work as a data scientist, but I have often done data engineering tasks to support modelling in production.

I use python, and I often end up only using functions for the pipelines, creating a number of interrelated pipeline steps. These steps are often just one long data manipulation, and it doesn't have a nice OOP feel so to speak.

I could just cut the pipeline into parts, but since many of the steps are unique, there is not a lot of reuse.

So I am wondering how you guys work with encapsulation in pipelines?"
2197,2020-05-15 23:15:34,1589573734.0,dataengineering,Career outlook of Data Engineering in a post-Covid world?,gkgmd7,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/gkgmd7/career_outlook_of_data_engineering_in_a_postcovid/,2.0,35.0,0.0,13177.0,"As the title suggests, what do you think about the prospect of Data Engineering after the affect of coronavirus pandemic diminishes? Would it become better than now or worse?

I am a non-IT Engineering PhD holder, thinking about switching career to Data analytics field, specifically to DE with a flair of Cloud Computing."
2198,2020-05-16 00:11:31,1589577091.0,dataengineering,Project suggestions for learning Google Cloud Date Engineering Services,gkhmbh,hardwaresinger,,https://www.reddit.com/r/dataengineering/comments/gkhmbh/project_suggestions_for_learning_google_cloud/,2.0,2.0,0.0,13179.0,"Hi all,

I want to improve myself in Google Cloud. following the tutorials may not be helpful in most of the time.  I want to to implement some projects while learning Google Cloud services (data proc, dataflow, cloud composer, bigquery etc)  Do you have any project suggestions? Or can you suggest any tutorial which includes practical projects too?

&amp;#x200B;

Thanks"
2199,2020-05-16 07:24:08,1589603048.0,dataengineering,Which educational path should I take in order to make it as easy as possible to transition into data engineering?,gkohm2,throwawaystickies,,https://www.reddit.com/r/dataengineering/comments/gkohm2/which_educational_path_should_i_take_in_order_to/,1.0,19.0,0.0,13190.0,"First, about me — I have a bachelor’s degree in nursing and am currently working as a medical coder for a large hospital in CA.

I recently finished a variety of preparatory courses in Python and SQL through Udacity and Udemy. I also obtained a nanodegree in Udacity’s Data Analytics.

**OPTION #1**

Enroll in WGU’s Master’s in Data Analytics and finish in 2 years 

(*Pros*: I am already admitted and will only need to pay to start. It’s also about $6k+ cheaper. I can go fast-paced and pay even less or go slower and pay more. 
*Cons*: The school has a less stellar reputation than most brick &amp; mortar schools. It has no ranking on USNews aside from being a rank #12 in Most Innovative school)

**The MSDA degree has the following courses:** Fundamental os Data Analytics, SQL for Data Analysis, Advanced Data Visualization, Statistics for Data Analysis, Programming in Python, Data Mining and Analytics, R for Data Analysts, Data Mining and Analytics II, SAS Programming I: Fundamentals, SAS Programming II: Business Analysis Applications &amp; Data Analytics Graduate Capstone. 

Get a data analyst position in healthcare since I’m already in the field and the SAS programming may actually be handy in.

Obtain a nanodegree in Data Engineering from Udacity.

Get a data engineering job!

**OPTION #2**

Get a good GRE score.

Enroll in CSUF’s online MSIT-DS degree and finish in 26 months.

(*Pros*: A brick &amp; mortar school with actual USNews ranking. Just browsing their course competencies, it seems it is closer to data engineering than the MSDA. 
*Cons*: More expensive and will likely take me longer. I need to take the GRE before being able to apply, which may take a while due to the pandemic.)

**The MSIT-DS degree has the following courses:** Issues in Business Information Technology (global networks, information security &amp; privacy), Statistics for Data Science (uses R for quantitative analysis and data visualization), Info Resources and IT Project Management, Business Databases: Design $ Processing (databases, data warehouse, SQL, database programming, object-oriented databases), Data Warehousing and Foundations in Business Intelligence, Capstone

Obtain a Data Engineer job???

Let me know what you think! Thank you so much!"
2200,2020-05-16 13:38:20,1589625500.0,dataengineering,What assignments should I ask from my mentor to learn as much as possible?,gksls3,throwawaybusan,,https://www.reddit.com/r/dataengineering/comments/gksls3/what_assignments_should_i_ask_from_my_mentor_to/,1.0,0.0,0.0,13197.0,"Just started interning at a medium-sized hedge fund and for my first few tasks, I was handed data ingestion tasks - basically performing HTTP requests to an API or scraping a stock exchange site and putting it in postgres. It's okay for now, but I foresee it getting boring real quick. I've heard a lot about spark/hadoop/kafka, and thought I would interact with those. What should I ask my mentor for?"
2201,2020-05-16 15:08:26,1589630906.0,dataengineering,Quiz:What content you do read on a daily basis to keep up with the trends in Data Engineering?,gktm4a,the_dataguy,,https://www.reddit.com/r/dataengineering/comments/gktm4a/quizwhat_content_you_do_read_on_a_daily_basis_to/,1.0,17.0,0.0,13202.0,"What content you do read on a daily basis to keep up with the trends in Data Engineering?

It would be better if you can post the URL too. Let's make this better by adding your answers."
2202,2020-05-16 17:53:28,1589640808.0,dataengineering,Is this scenario a form of data engineering?,gkvw7c,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/gkvw7c/is_this_scenario_a_form_of_data_engineering/,1.0,6.0,0.0,13207.0,"At work, we have signed up with Fivetran as our -ELT- tool to our Snowflake data lake, and then used WhereScape to transform and manage the data from dev to prod. 

Fivetran also allows you to build python connectors (which is our next phase). 

Thoughts?"
2203,2020-05-16 19:09:22,1589645362.0,dataengineering,DA considering becoming a DE,gkx61m,Tyraniczar,,https://www.reddit.com/r/dataengineering/comments/gkx61m/da_considering_becoming_a_de/,3.0,8.0,0.0,13213.0,"TL;DR I’m a DA (1 year exp after grad) and have a clear path to become a DE. Is DE a safer (opportunity/compensation wise) career than DA?

I’m currently a new grad data analyst (about 1 year post grad) and was “taken in” by our data eng team about 5 months ago. I participate in their standups and sprint planning and get along with them pretty well. 2 of them have hinted in the past that I should try to join their team in full capacity, so transition from a DA to a DE. Im strongly considering it. I’m the youngest and least experienced though so I’d need to hit the books in order to catch up to the others.

I really enjoy analytics but have been bogged down a bit by ridiculous requests from stakeholders/managers who don’t seem to understand how their own KPIs are calculated. On the flip side, I want to develop my engineering prowess but also don’t want to build pipelines all day. What attracts me to DE is I assume that career growth and compensation will be higher than as a DA and from there I can trans into SWE down the line if I want.

All this considered, should I start making moves to transition into DE? I can probably make the jump within 3 months if I really pursue it aggressively."
2204,2020-05-16 20:45:41,1589651141.0,dataengineering,"can someone with a degree in economics become a de with the ""right"" Master?",gkyukq,lahoyV1,,https://www.reddit.com/r/dataengineering/comments/gkyukq/can_someone_with_a_degree_in_economics_become_a/,2.0,8.0,0.0,13215.0,"Hello everyone I am in my first year in my university and studying economics.so i have been thinking what iam gonna do after i get my degree  and i found de really fascinating, so my question is what should my master focus to become de? Computer science, statistics, math?"
2205,2020-05-16 23:16:27,1589660187.0,dataengineering,Are datalakes really dead? Should I go full MPP data warehouse?,gl1ju8,Alert_Dragonfly,,https://www.reddit.com/r/dataengineering/comments/gl1ju8/are_datalakes_really_dead_should_i_go_full_mpp/,1.0,24.0,0.0,13224.0,"Hi everyone,

  
I have read that data warehousing is becoming cheaper year after year to the point that now we can do ELT instead of ETL (i.e load everything in database and model the data in-database). Is the datalake concept dead? I'm asking this question as in my organization we would like to switch to an incremental data storage (daily snapshot of prod db for historical analysis). Conceptually and also because of cost optimizations in my organization, we are converging towards the following solution in AWS:  


* Store daily snapshot of prod db in compressed parquet (reduce storage costs and improve performances for adhoc analytics queries with Athena)
* Load in Postgres the last x days of prod db snapshots. Build reporting models connected to our BI tool.

The above architecture seems to offer the best tradeoff between costs,  for our use cases but I might be wrong. Any opinions? And more generally do you really load everything in your data warehouse?

I'm struggling right now because everyone seems to go full MPP DWH (Redshift, Snowflake...) and load everything in it. But our current data stack does not use a MPP and it seems that there is no need right now. The fact that we also want to move from a daily reporting to an incremental reporting will likely increase the storage costs that will be even more high if stored in Redshift. I would need a strong case to move from RDS to Redshift. Such migration would be time consuming and costly with no apparent benefit in the short term.

  
Thanks!"
2206,2020-05-17 01:47:48,1589669268.0,dataengineering,Rekcurd: Open Source for Serving and Managing Machine Learning Models,gl41k1,keigohtr,,https://www.reddit.com/r/dataengineering/comments/gl41k1/rekcurd_open_source_for_serving_and_managing/,1.0,0.0,0.0,13234.0,"Repo: [https://github.com/rekcurd/community](https://github.com/rekcurd/community)

Hi all,

I would like to share our open source (Rekcurd) for serving and managing machine learning models. I hope this will be a useful reference for productionising machine learning models.

By this open source, you can write an online inference server in a similar way of python flask and can deploy it to Kubernetes cluster via dashboard. Since an inference server is gRPC server, you can access it from any languages. In our repository, we have an example of python gRPC client. Thanks to Istio, you can change the traffic weight of inference servers via dashboard.

This project was developing it as a company work and was using it in a product. Unfortunately, this project is no longer active, but we hope to continue developing it in the future.

I hope it will help you somehow.

Thanks."
2207,2020-05-17 07:08:23,1589688503.0,dataengineering,Data Scientist wants to go Data Architect,gl8xj1,l2reg,,https://www.reddit.com/r/dataengineering/comments/gl8xj1/data_scientist_wants_to_go_data_architect/,2.0,8.0,0.0,13236.0,"So long story short: I'm a physicist with a PhD in theoretical ecology (lots of math). When I started on a DS role, knowing ML/python/numpy/pandas/sklearn was enough because of how underdeveloped the tools were at the moment. Year passed and I noe feel good in a Senior role because I've learned many things including business and software engineering stuffs. Regardless, I'm aware of how this area is rapidly changing, specially in terms of easy-to-use the API are. Probably in a few years, any developer will be able to solve most of the common problems (and with that, I mean there'll be no need to understand validation/regularization/hyperparameter tuning problems). There will still be problems suited for a DS role, specially the ones with lots of feature engineering and that requires business understanding, but much less DS on the field will be required for sure.   
That's why I've been thinking in going thru the DA path, I've always admire what a data architect can do and the ecosystem they deal with.  
So I'm taking [this](https://towardsdatascience.com/who-is-a-data-engineer-how-to-become-a-data-engineer-1167ddc12811) post as a guideline. I wanted to ask you two things: 1) Do you think if Java/Scala is a must? 2) Can you give me some comments about if you think this is a good decision? Maybe I should stick to the most business part of DS and try to make my way on it, I know I'm good at it (but is really dislike).

Edit: typos."
2208,2020-05-17 20:36:15,1589736975.0,dataengineering,Projects to build to showcase DE skills,gljki4,DataD23,,https://www.reddit.com/r/dataengineering/comments/gljki4/projects_to_build_to_showcase_de_skills/,1.0,17.0,0.0,13251.0,"I am trying to break into DE and am at a loss for projects that I can do which I can put on my resume. I have AWS certifications, I know Python, SQL, and Spark. I know some of you may say make a COVID-19 tracker but besides that what else would be a strong project that would grab an employer's attention? Any help would be greatly appreciated!"
2209,2020-05-17 21:40:43,1589740843.0,dataengineering,"Mentioned Airflow as an alternative for our cron based ETL. Manager wants full cloud deployment. No idea where to start. Docker, k8s, Helm, Terraform, CI/CD, Container Registries, could anyone help me structure my approach?",glkr1o,tylerjaywood,,https://www.reddit.com/r/dataengineering/comments/glkr1o/mentioned_airflow_as_an_alternative_for_our_cron/,1.0,59.0,0.0,13254.0,"Here is my understanding after feeling like I've wasted 2 weeks just trying to orient myself

###Docker
understanding: 3/5

Basically I fork Puckel's Airflow image and tweak some of the config and entrypoint.sh script to my liking. I also include a /dags repo _in the build_. This container gets built and can be run anywhere and includes our dags. 

### Kubernetes
understanding: 2/5

My company has a cluster in Digital Ocean. Kubernetes provides a management layer for nodes and pods running containers. But running Docker-Airflow with the Celery operator, as I intend, will require multiple pods running the webserver, workers, scheduler and redis at a minimum. In order to orchestrate all of these deployments and build the necessary Controllers that provide for uptime, monitoring, and access to the deployment and its Pods. This is not really to be done manually, which is where we get to 

### Helm
understanding: 2/5 

Helm is basically a template you provide to your k8s cluster to specify the details of your deployment all at once through yaml. The yaml file contains information about the deployment like the images it should reference, what images need to be run, how many ReplicationSets and all the other k8s configuration stuff. Basically, the k8s cluster exists, my primary interaction with it for deploying should be through Helm charts that specify all the stuff I want included. 

### CI/CD
understanding: 1/5

we use Codeship. Codeship can be set up to be aware of Github events, allowing us to recognize pushes to various branches of our forked docker-airflow repo that builds our docker-airflow container. Codeship can be configured to rebuild, and run tests against, our image upon code changes. 

#####Okay, so how does this all come together?

If we have a /dags repo in the docker-airflow container repo, we would push new dags to a branch in that repo. 

Codeship will then rebuild the image and put it *??somewhere??* with the updated image. 

*??somehow??* a Helm chart that defines the k8s deployment learns of a new image available and is run to (upgrade || rebuild) our existing deployment referencing the new container image by it's tag (*??templating??*)

Our k8s cluster runs the Helm chart and the update is pushed. 

#####is this right?
is Codeship's CI/CD process for deploying the new container to some container registry the missing piece? How does my deployment become aware of and ship the new container?

#####other questions
Where do our credentials for the Docker metadata db and other secrets live? ENV vars defined in the image, or in k8s via Helm? 

Do I have any large conceptual misunderstandings? Any **pieces** missing from this? 

Unfortunately what went from a multi day project of just putting Airflow on a big ass server and running it w the LocalExecutor and a script running git pull on our dags repo every minute has spiraled way out of my depth and just trying wrangle all the necessary concepts and general structure of this project is preventing me from being able to chunk it and accomplish pieces. 

I know this is a huge post but _any_ help to get my arms around all this would be very appreciated"
2210,2020-05-18 00:26:18,1589750778.0,dataengineering,Data Engineering Problem for Microservices,glnrlb,Nova-Lord,,https://www.reddit.com/r/dataengineering/comments/glnrlb/data_engineering_problem_for_microservices/,1.0,10.0,0.0,13264.0,"I am a data analyst that is trying to understand how to solve my company's current data engineering problem. I do not have a background in data engineering, but if I do not lead these efforts, then our engineering team will never get around to solving this problem. That being said, I do have experience creating ETL tasks.

My company follows a microservice architecture for our products. That means we have about 20 live production databases that all contain valuable data for analytics. I am trying to gather some general ideas on how to extract this data into an environment for analytics purposes. Ultimately I will pitch these ideas to our engineers and they will work on the solution. What would be this sub's recommendation?

Other relevant information:
- All of these production databases are using AWS RDS AuroraDB.
- It would be easier to sell the idea to my company if the solution utilized tools that work well with AWS in general.
- The more cost effective the better.
- Probably the most important thing to the engineering team is that the solution cannot affect the performance of the live databases.
- New databases spin up relatively frequently (2 or 3 a year) and the schemas are updated frequently to accommodate new features. We would want something that is adaptable and as easy to maintain as possible, but who doesn't want that...

From a data analyst perspective, the company's current tools for analytics are lackluster and the data availability is very poor. I am trying to improve this so the analytics team can provide more value. Since I do not (and will never) have the ability to access the live databases (for good reason), I really want a solution that just moves the data into a place where I can access it. After that I could do the work to transform it into structured data and possibly start work on a data warehouse.

I have done some research on data lakes and what I have read about seems promising. However, I am a bit concerned about the costs. I am still quite lost and I would appreciate any advice. I am willing to do as much research as needed, but I would appreciate if anyone can point me in the right direction. Thanks in advance."
2211,2020-05-18 00:39:18,1589751558.0,dataengineering,Can anyone working in bioinformatics / genetics / genomics give me a picture of what their set up and workloads are like?,glo059,darosati,,https://www.reddit.com/r/dataengineering/comments/glo059/can_anyone_working_in_bioinformatics_genetics/,1.0,2.0,0.0,13265.0,"Hey there,

My background in computer vision and natural language understanding workloads and data engineering. I am quite comfortable with standard ML pipeline and deployment as well as standard ETL/ELT and streaming worklows for analytics and training.   


I am wondering if anyone has any experience on what some of the key differences with bioformatics / genetics / genomics data engineering would entail - any tools or standard processes that I can get familiar with or what unique aspects of those workflows or the data there?  
Any info on what that looks like or how i can become more familiar with that would awesome.  


thanks,  
Dom"
2212,2020-05-18 01:33:58,1589754838.0,dataengineering,What cloud resources are best for a simple data pipeline / warehouse?,glozjy,John_Mason,,https://www.reddit.com/r/dataengineering/comments/glozjy/what_cloud_resources_are_best_for_a_simple_data/,1.0,13.0,0.0,13268.0,"I’m trying to create a simple scheduled / automated ETL process that pulls data from an API, loads it into a hosted Postgres database, and performs some transformations before loading into a dimensional model. While this seems very simple in theory, I’m running into a number of errors when trying to deploy this code to one of the cloud platforms.

Right now, I have a simple python script that retrieves the data using *requests* and loads it to a hosted DB using *psycopg2*. What do you think is the best place/method of deploying this script and automating it to run nightly? Similarly, what’s the best way to orchestrate Postgres DB functions to run after the completion of the python script? Should I be using a more formal tool (AWS Glue?) instead?

I’ve mostly encountered issues getting the python environment set up in the cloud (GCP provides unclear errors, and AWS just times out when I try to run a zip file deployed as a lambda). Is there a simpler method? Am I best off using *psycopg2* to call the DB functions, or do the cloud platforms have a method to do that? Should I be developing in a docker container to keep my local environment in sync with the cloud environments (and thus reduce these deployment headaches)?

Thanks in advance for any help!"
2213,2020-05-18 02:13:52,1589757232.0,dataengineering,Thinking of starting DE consulting firm to support open source tech,glpoop,NakkiGN,,https://www.reddit.com/r/dataengineering/comments/glpoop/thinking_of_starting_de_consulting_firm_to/,1.0,0.0,0.0,13269.0,"I'm thinking of starting a small DE consulting firm to support companies with open source data technologies such as kafka, cassandra, spark , elastic search and Hadoop.

Any advice or suggestions for someone who has gone through this process before ?"
2214,2020-05-18 03:27:53,1589761673.0,dataengineering,"Kafka in Simple Terms: Producers, Topics, Brokers, Partitions, Segments",glqx0i,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/glqx0i/kafka_in_simple_terms_producers_topics_brokers/,1.0,5.0,0.0,13272.0,
2215,2020-05-18 06:24:48,1589772288.0,dataengineering,"Opinions on a potentially strange ""Data Engineering"" interview I had? Would welcome an outside view",gltnq2,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/gltnq2/opinions_on_a_potentially_strange_data/,1.0,12.0,0.0,13280.0,"Hi,

A couple days ago I had a final round interview with a ""Director of Data Science Engineering"". I found it a bit strange and perhaps aggressive on her end, but I'm not sure due to my experience in DE.

**About me**: Staff Software Engineer, 9-10 years. Been doing Data Engineering for the past 1 year (kafka, airflow, spark, redshift) and 5+ years of day-to-day AWS development &amp; architecture experience. I feel very comfortable with the stack mentioned, so I was really surprised to hear her feedback.

**The interview**

* It started off normal, getting to know you sort of questions
* So in my prev position, I architected and built the workflow management, data pipeline and ML pipeline
* She seemed to doubt that I had done all of this work myself
* Workflow: Airflow w/ Celery, Data pipeline: Kafka (producers/connect/stream in py), Redshift, Spark, Zeppelin, ML: AWS SageMaker, with Airflow managing the steps w/ SageMaker python operators

Once I mentioned the above pipeline, she started talking about how her team is the ""A-team"", ""the best of the best"" in her company. She also seemed abrasive to the tech stack I described, and didn't seem particularly interest in it (despite it being quite cool).

She mentioned how they have this ""huge, massive data lake. 10,000 tables"".

She also mentioned the following:

* She comes from a data analyst background
* Her ""top engineer"" also comes from a data analyst background, and always looks at the data first whenever there's an issue rather than the code
* She really values ""complex, analytical SQL"" skills
* Thinks R (the language) is fundamental for a data engineer
* ""Amazon is trash, really slow with their offerings""
* ""My engineers can easily be software engineers""

Whenever I asked about the actual tech stack, she always responded by saying it was a ""very complex data lake with thousands of tables"".

She seemed to look down on software engineers for whatever reason. 

At the time, I respected what she said. However the more and more I think about it, the more I realize how strange her feedback is.

So does anyone have any idea what's going on? Is ""Data Engineering"" anything like she was describing? To me it sounds like she values a strong Data Analyst background .. and to me, everything she described sounded like a Data Analyst role, not a Data Engineer?

Thanks!"
2216,2020-05-18 15:28:32,1589804912.0,dataengineering,An interview with Swarm64 CEO Thomas Richter about optimizing PostgreSQL on high performance hardware and FPGAs for analytical workloads.,gm0j96,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/gm0j96/an_interview_with_swarm64_ceo_thomas_richter/,1.0,0.0,0.0,13296.0,
2217,2020-05-18 15:52:21,1589806341.0,dataengineering,A Guide for Unit Testing in Apache Flink,gm0vw6,Marksfik,,https://www.reddit.com/r/dataengineering/comments/gm0vw6/a_guide_for_unit_testing_in_apache_flink/,1.0,0.0,0.0,13297.0,
2218,2020-05-18 16:07:09,1589807229.0,dataengineering,Certificate program in Data Science - Demo Session | In collaboration with IBM,gm1446,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gm1446/certificate_program_in_data_science_demo_session/,1.0,0.0,0.0,13297.0,
2219,2020-05-18 16:30:02,1589808602.0,dataengineering,The Big Data conference Berlin Buzzwords takes place online in June 2020,gm1h85,plainschwarz,,https://www.reddit.com/r/dataengineering/comments/gm1h85/the_big_data_conference_berlin_buzzwords_takes/,1.0,0.0,0.0,13299.0,"[Berlin Buzzwords](https://berlinbuzzwords.de/) have announced that they are teaming up with [Haystack](https://haystackconf.com/) the search relevance conference, and [MICES](https://mices.co/)  the e-commerce search event to host a week of virtual talks, panel  discussions, workshops and training sessions covering open source big data projects under themes of search, scale, store and stream.

You can find out more about the event here ([https://berlinbuzzwords.de/news/berlin-buzzwords-goes-virtual](https://berlinbuzzwords.de/news/berlin-buzzwords-goes-virtual)) and see the schedule here ([https://berlinbuzzwords.de/schedule](https://berlinbuzzwords.de/schedule))"
2220,2020-05-18 17:46:13,1589813173.0,dataengineering,Airflow: how and when to use it,gm2rg7,schoolgurllou,,https://www.reddit.com/r/dataengineering/comments/gm2rg7/airflow_how_and_when_to_use_it/,1.0,26.0,0.0,13299.0,
2221,2020-05-18 17:46:49,1589813209.0,dataengineering,Airflow: how and when to use it (Advanced),gm2rtl,schoolgurllou,,https://www.reddit.com/r/dataengineering/comments/gm2rtl/airflow_how_and_when_to_use_it_advanced/,1.0,2.0,0.0,13299.0,[https://medium.com/@alexagriffith/airflow-how-and-when-to-use-it-advanced-238ea6b63f13](https://medium.com/@alexagriffith/airflow-how-and-when-to-use-it-advanced-238ea6b63f13)
2222,2020-05-18 18:11:17,1589814677.0,dataengineering,Looking for reading / terminology on a specific type of sql query that looks back to find current state at t0 in time filter,gm3846,terminal_bound,,https://www.reddit.com/r/dataengineering/comments/gm3846/looking_for_reading_terminology_on_a_specific/,1.0,0.0,0.0,13299.0,"I have some queries I've been doing a lot where timeseries rows contain a \`state\` column. A conceptual analog is an alert table with rows being the alert as is transitions to various other states (and states can go backwards, unfortunately). continuing the analog, if states are open/ack/closed, rows for an alert in chronological order may go from open -&gt; ack -&gt; open.

The root thing i'm trying to figure out is illustrated by the following example (\`t\` has been boiled down to timeticks for simplicity).

    time | state
    t=0  | open
    t=3  | ack
    t=7  | closed
    
    |  |  |  |  |  |  |  |
    o        a           c

If I have a query ""give me the total t where alarm is `open` from t=2 to t=5"", I need to know at t=2, the current state is in open. I've been solving this looking at latest states for each id/timetick using a sql query, but it's pretty serial (per id/timestamp serial...)

    -- snowflake SQL but seems pretty standard
    SELECT *
    FROM sensor_readings t1
    where ts = (
        SELECT MAX(ts)
        FROM sensor_readings t2
        WHERE t1.sensor_id = t2.sensor_id
        AND ts &lt;= ""the timestamp of the datapoint""
    );
    -- and there is an equivalent one that uses parition by that i use.

In any case, doing this for every unique timetick in a batch is not scalable since this is on data that has a decent velocity (few 100k records every hour). I'd rather it be a request/response cycle type of query if i can help it, but an hourly batch is completely OK. What i'm playing with currently is a join to a table of date stamps to fill in the empty ticks with what datapoint they are so that the ticks look more like:

    |  |  |  |  |  |  |  |
    o  o  o  a  a  a  a  c

... though i have not benchmarked anything here on large datasets so still getting there.

If there's no special nomenclature, then cool. But I was trying not to re-invent the wheel on this one. Is this a named thing that I can read more best practices about? I've actually ran into a few flavors of this situation so I figured i'd better ask before I go in to deep with the wrong solution."
2223,2020-05-18 18:44:57,1589816697.0,dataengineering,Product Management to DE,gm3veu,tonguewin,,https://www.reddit.com/r/dataengineering/comments/gm3veu/product_management_to_de/,1.0,2.0,0.0,13303.0,"I've seen alot of questions around getting into data engineering from data analysis and computer science but how about a closely related field but different skill set such as a PM. I've done a bit of data analysis and can cobble up together POCs with python and jupyter notebook. I have decent SQL experience but I am a product manager by day. I'm okay with taking a step back in my career to jump but I'm not sure of a good path to doing so. I was thinking of going the data analyst route and then switching to data engineering but if anyone has a different perspective, I'd greatly appreciate it."
2224,2020-05-19 05:58:43,1589857123.0,dataengineering,Using oauth and python requests to automate reporting - need a redirect uri,gmgc1z,pbj800100,,https://www.reddit.com/r/dataengineering/comments/gmgc1z/using_oauth_and_python_requests_to_automate/,1.0,2.0,0.0,13335.0,"So I'm trying to access an API that uses oauth2.0.  I just want to do some automated reporting and I plan to run this locally so I can push to a local db, don't want to set up any servers. From reading online I'm pretty confident that I NEED a redirect uri to use oauth, can anyone confirm? If there's any other way I'd love to hear as I have 0 experience with setting up a website and I just need to get a single access token one time so I'd rather not go through the trouble of doing it for this if I don't have to. If this is the only way, how would you recommend setting this up as a person with no experience?"
2225,2020-05-19 09:03:49,1589868229.0,dataengineering,Is there a defined learning pathway for data engineering?,gmiw1x,iamsourabhh,,https://www.reddit.com/r/dataengineering/comments/gmiw1x/is_there_a_defined_learning_pathway_for_data/,1.0,20.0,0.0,13337.0,"I have been wanting to transition to data engineering, but what all things I need to learn so I can myself call a data engineer?  
What are the projects that I can I build on my own to show my skills on data engineering?"
2226,2020-05-19 16:50:40,1589896240.0,dataengineering,Free Demo Session - Certificate Program in Data Science | In Collaboration With IBM,gmoxis,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gmoxis/free_demo_session_certificate_program_in_data/,1.0,0.0,0.0,13348.0,
2227,2020-05-19 21:03:18,1589911398.0,dataengineering,Data Engineering Interview Experience,gmtqcz,aditya166,,https://www.reddit.com/r/dataengineering/comments/gmtqcz/data_engineering_interview_experience/,2.0,10.0,0.0,13352.0,"Hi All,

About me - I am a recent graduate student trying to get into DE and aligned roles. I am proficient in SQL, Python, Spark, RDBMS, NOSQL, Cloud databases,Data Warehousing, ETL.  I have worked in the data related fields before coming to grad school but and not much in Big Data.

Today I had a 30 minutes technical phone interview for a Data Engineering role. While I was able to explain the big data architecture, its use cases and all the moving parts of the ecosystem and answer questions about map reduce, spark and its functions and use cases  the interviewer kept asking me if I had worked on any enterprise level Hadoop Cluster again and again.

The interviewer came to the interview unprepared and I am pretty sure he didn't even read my resume. He only spoke for 5 minutes and for the rest of 25 minutes I was trying to sell myself, it seemed like a one sided interview. While I am confident I answered all the technical questions correctly, I feel my lack of experience in enterprise Hadoop might be a reason why he would reject. I am technically sound but the fact that I do not have experience in ""enterprise level Hadoop""  bothers me. There is no way I can get this experience online or by doing side projects. Also I feel let down by the lack of conversation or no conversation by the person taking the interview. 

Do you feel he was unfair expecting me the know this for someone straight out of college ?"
2228,2020-05-19 21:43:58,1589913838.0,dataengineering,"DataCamp is completely free through 5/22, no credit card is required! There are 330+ courses on Python, SQL, Scala, and all the technologies you need mastery of as a data engineer. No risk, all reward!",gmujcf,Elvish__Presley,,https://www.reddit.com/r/dataengineering/comments/gmujcf/datacamp_is_completely_free_through_522_no_credit/,3.0,14.0,0.0,13356.0,
2229,2020-05-20 00:40:20,1589924420.0,dataengineering,Akka Stream Coexistence With Akka Typed,gmxxm3,borakaplan,,https://www.reddit.com/r/dataengineering/comments/gmxxm3/akka_stream_coexistence_with_akka_typed/,1.0,2.0,0.0,13359.0,
2230,2020-05-20 00:54:30,1589925270.0,dataengineering,Where can I find real time or live stream data?,gmy74d,oatsativa,,https://www.reddit.com/r/dataengineering/comments/gmy74d/where_can_i_find_real_time_or_live_stream_data/,1.0,5.0,0.0,13358.0,"I'm working on a project that requires using streaming data (must be at least 5000 messages per second). It's an open ended project, but I don't know what sources or APIs provide this for free. I'd like to work with messy data as well."
2231,2020-05-20 05:14:25,1589940865.0,dataengineering,Thoughts of pre calculated data,gn2oug,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gn2oug/thoughts_of_pre_calculated_data/,1.0,10.0,0.0,13367.0,"Recently, im tasked to ease up the load of our company server to do some reports  query, and i realized there are data that can be pre-calculated (ex: pre calculate yesterday item stock, and use it for today report, instead of calculate it during report load).

For now im thinking to do this

1. Create a table to store the pre calculated data
2. Create a Stored Procedure to calculate the data and insert into the table

Regarding this, i need some insight is there any better way to do this ?"
2232,2020-05-20 15:02:57,1589976177.0,dataengineering,Top 10 Used Emojis 2014 - 2020,gna3lu,Rankings_Mania,,https://www.reddit.com/r/dataengineering/comments/gna3lu/top_10_used_emojis_2014_2020/,1.0,1.0,0.0,13380.0,
2233,2020-05-20 15:54:07,1589979247.0,dataengineering,DBT vs Matillion vs Data Form - Snowflake Transformations,gnat7c,Oct04,,https://www.reddit.com/r/dataengineering/comments/gnat7c/dbt_vs_matillion_vs_data_form_snowflake/,1.0,0.0,0.0,13381.0,"Hey guys,

We're about to embark on Snowflake and are considering the tools to work with this ecosystem. In terms of E and L we're thinking FiveTran is a good fit. In terms of T, we're aware FiveTran do *some* transformations but that it's not their sweet spot.

We'll be ingesting data from SQL, KAfla, Salesforce etc and transforming ready for consumption by say Power BI or others.

We considered Matillion for everything but thought FiveTran was better for E and L, we're now looking at DBT and Data Form. Just wondered what peoples views were on this really.

Dataform = 150$ per month, up to five users. 

DBT = $50 per month per user

We're looking at 3-5 users, so Data Form would probably be cheaper, and is basically built on DBT.

Matillion is priced per usage so hard to call exactly but to be honest won't be mega bucks so price isn't a huge factor between these at present.

Modelling wise we're undecided whether to do big tables, dimensional, data vault or what at this stage..."
2234,2020-05-20 15:54:23,1589979263.0,dataengineering,DBT vs Matillion vs Data Form - Snowflake Transformations,gnatcx,Oct04,,https://www.reddit.com/r/dataengineering/comments/gnatcx/dbt_vs_matillion_vs_data_form_snowflake/,1.0,22.0,0.0,13381.0,"Hey guys,

We're about to embark on Snowflake and are considering the tools to work with this ecosystem. In terms of E and L we're thinking FiveTran is a good fit. In terms of T, we're aware FiveTran do *some* transformations but that it's not their sweet spot.

We'll be ingesting data from SQL, KAfla, Salesforce etc and transforming ready for consumption by say Power BI or others.

We considered Matillion for everything but thought FiveTran was better for E and L, we're now looking at DBT and Data Form. Just wondered what peoples views were on this really.

Dataform = 150$ per month, up to five users. 

DBT = $50 per month per user

We're looking at 3-5 users, so Data Form would probably be cheaper, and is basically built on DBT.

Matillion is priced per usage so hard to call exactly but to be honest won't be mega bucks so price isn't a huge factor between these at present.

Modelling wise we're undecided whether to do big tables, dimensional, data vault or what at this stage..."
2235,2020-05-21 02:23:58,1590017038.0,dataengineering,Automating pipeline execution with Ploomber,gnlhnt,ploomber-io,,https://www.reddit.com/r/dataengineering/comments/gnlhnt/automating_pipeline_execution_with_ploomber/,1.0,12.0,0.0,13400.0,"Hi everyone,

I  few months ago, we announced the release of Ploomber, a tool to  automate execution of multi-step pipelines (where each step can be  anywhere from a Jupyter notebook, Python function or even a SQL script).

Ploomber  allows you to automatically pass the output from one step to the next one and keeps track of source code changes to trigger execution of outdated steps.

I'm happy to  announce that we've made great progress in the past months to make  Ploomber a robust tool. We now have an interactive demo so you can see  it by yourself (no installation required, but might take a few minutes  to be ready for execution): [https://mybinder.org/v2/gh/ploomber/projects/master?filepath=basic-tutorial%2Fnotebook.ipynb](https://mybinder.org/v2/gh/ploomber/projects/master?filepath=basic-tutorial%2Fnotebook.ipynb)

Code: [https://github.com/ploomber/ploomber](https://github.com/ploomber/ploomber)

Please let me know any questions or feedback you might have."
2236,2020-05-21 03:59:42,1590022782.0,dataengineering,How to run a data quality metric review session?,gnn3hp,windyslide,,https://www.reddit.com/r/dataengineering/comments/gnn3hp/how_to_run_a_data_quality_metric_review_session/,1.0,5.0,0.0,13407.0,"Hello folks!  


I'm a tech lead on an infrastructure team and we recently started supporting the ML team's data needs. I want to roll out a data quality review session. My goal is to meet on a bi-weekly cadence and start figuring out where our data sucks and how we can prioritize bug fixes, improve contracts with other teams, etc.   


Tentatively, for the ""mvp"" of this meeting, I just want to take the top K metrics that seem historically out of wack, and correlate them with our model scores and business rule engine outcomes.   


How stupid is that? What would you do instead? Any guidance is super appreciated."
2237,2020-05-21 09:45:15,1590043515.0,dataengineering,CPU bound tasks vs I/O bound tasks,gnru9v,Luukv93,,https://www.reddit.com/r/dataengineering/comments/gnru9v/cpu_bound_tasks_vs_io_bound_tasks/,1.0,4.0,0.0,13415.0,"Hello,

I have a questions on a CPU vs I/O bound tasks.

CPU bound tasks:

\- Execute faster if you optimize the algorithm

\- Execute faster if your processor has a higher clock speed (can execute more operations)

I/O bound tasks:

\- Program is reading from an input (like a CSV file)

\- Program is writing to an output (like a text file)

\- Program is waiting for another program to execute something (like a SQL query)

\- Program is waiting for another server to execute something (like an API request)

&amp;#x200B;

Where are SQL queries executed? Probably on a server, but what if the SQL database isn't running on a server?"
2238,2020-05-21 10:54:30,1590047670.0,dataengineering,"For those who ask ""What should be my project to find a job in data engineering/science""?",gnsm39,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/gnsm39/for_those_who_ask_what_should_be_my_project_to/,1.0,11.0,0.0,13415.0,"Hey all,

&amp;#x200B;

From time to time, I see people in this sub asking for some ideas on what their project should be.

I'm going to give you 1 idea and describe why it is complex enough. However, one thing before we start.

Which is, **you have to be curious about what you are doing**. Plus, your work has to bring some value. **You are not coding for coding when you are employed.** You have to bring value to your employer.

[Newscatcher](https://github.com/kotartemiy/newscatcher) is a Python package that allows you to collect the latest news articles (already normalized). *Disclaimer: I did it.* 

It has thousands of news sources. You can filter out by topic (economics, business, science, etc), language, and country. 

You can use is as your data generator. Then, you can build a pipeline to collect and store this data. How and where is up to you. 

&amp;#x200B;

Why it is an interesting task for a data engineer:

1. Some fields will be missing from one source from another. Though it is normalized, sometimes \`published\` field will not be present, for example
2. Deduplication. Newscatcher returns the latest news only. So, most likely every time you check the source ([nytimes.com](https://nytimes.com), for example) the vast majority of news articles will be the same
3. Concurrent/parallel processing. Try not to do it with a loop
4. Time zone normalization. The different news feed has different time zones. If you store all the data in one place, you have to normalize the timezone 
5. Full-text search. If you have to search through the collected data, SQL will not be enough (SQL is horrible for the full text search)

And finally. If you go for it, write a concise and clear README. I have been hiring myself. You cannot even imagine how few people who search for their first dev job has a github with clean repositories. **Noone will dig into your code if you do not provide a basic README where you explain what you did.**

Moreover, having a good README will show that you can explain yourself well (that is a big part of real work, trust me). 

&amp;#x200B;

Good luck"
2239,2020-05-21 16:13:47,1590066827.0,dataengineering,Certificate Program in Data Science - Free Demo Session,gnwmh8,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gnwmh8/certificate_program_in_data_science_free_demo/,1.0,0.0,0.0,13427.0,
2240,2020-05-21 16:28:19,1590067699.0,dataengineering,Are there any front-to-back tutorials for a basic DE project?,gnwuiq,phenderbender,,https://www.reddit.com/r/dataengineering/comments/gnwuiq/are_there_any_fronttoback_tutorials_for_a_basic/,1.0,16.0,0.0,13428.0,"I’ve yet to find a comprehensive, not outdated, easy to follow tutorial for a basic DE project from start to finish. Something like scraping a site for data, cleaning it up, and loading to a database, for example. 

Has anyone come across something like this that they found helpful?"
2241,2020-05-21 17:14:46,1590070486.0,dataengineering,Automating ETL with AWS Lambda,gnxlvp,codie-fz,,https://www.reddit.com/r/dataengineering/comments/gnxlvp/automating_etl_with_aws_lambda/,1.0,2.0,0.0,13438.0,
2242,2020-05-21 22:54:22,1590090862.0,dataengineering,Transition Sucks,go3xof,DataFreakk,,https://www.reddit.com/r/dataengineering/comments/go3xof/transition_sucks/,1.0,8.0,0.0,13444.0,"HI guys I'm planing to shift to Data Engineer role from BI developer (Tableau) I know Tableau ,MSSQL and Power BI .I know basic duties of DE like distribution of data to hdfs and process with spark and streaming for Analytics  .Buttt Do I need to learn DBA and ETL tool (SSIS) to be Data Engineer ? I m just avg at queries and Haven't worked on DBA activities and ETL activities ? If so Which ETL best for aspiring DE ?"
2243,2020-05-22 01:28:16,1590100096.0,dataengineering,Is Java still a good language to learn?,go6sto,F0xHak3r,,https://www.reddit.com/r/dataengineering/comments/go6sto/is_java_still_a_good_language_to_learn/,1.0,10.0,0.0,13448.0,"I will be a freshman this fall and I have an interest in Data Engineering. I am already learning Python on my own and will SQL in the near future. I was wondering if I should also learn Java. I've seen people talk about it from time to time, but from what I see, it's better to learn python. What do you guys think?"
2244,2020-05-22 06:02:11,1590116531.0,dataengineering,Getting started with Airflow locally and remotely,gobaoe,tuankid,,https://www.reddit.com/r/dataengineering/comments/gobaoe/getting_started_with_airflow_locally_and_remotely/,1.0,7.0,0.0,13455.0,
2245,2020-05-22 15:07:23,1590149243.0,dataengineering,Apache atlas on hdinsight,goi5b9,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/goi5b9/apache_atlas_on_hdinsight/,1.0,0.0,0.0,13478.0,"Has anyone deployed apache atlas on hdinsight. We would like to explore this but think there are not much resources present for atlas on azure. We are a very small team with devops so would like the deployment and managebility as low as possible.

Any other option plz let me know. We would like to explore it on azure cloud"
2246,2020-05-22 16:01:23,1590152483.0,dataengineering,How much coding experience for entry data engineer job?,goixhk,Berodney,,https://www.reddit.com/r/dataengineering/comments/goixhk/how_much_coding_experience_for_entry_data/,1.0,39.0,0.0,13480.0,"Hello all,

First off I'd like to say if there's already a similar post please redirect me before adding a response. 

I went to school for management information science ( I like to think of it as business computer science) so I have a little bit of coding experience. My school's MIS program was a bit different though. We dipped our toes in a lot of different use cases, but didn't become experts on anything.

I had one class on access and SQL. One class on Java( very basics). Another class on networking (no coding). And another class on excel/python. I didn't really get much coding experience at all. At most I'd say I'm a beginner 1 or a beginner 2 at most.

I graduated last spring (2019) and am working for a technology company doing project management/ a little bit of data work. I use postgreSQL for data pulls every now and then. I also can create simple tables and functions, but nothing too crazy.

I'm looking to switch to a data engineer job in the next year as from what I've researched it seems like something I would enjoy doing. My concern is I'm not going to have enough coding experience and be completely lost. 

Do companies expect their entry level data engineers to have a lot of coding experience? I spoke to one data engineer at my company and she said she learned 90% of her coding on the job. Just looking to get some more feedback from others.

Thank you for your time,
Nick"
2247,2020-05-22 18:01:18,1590159678.0,dataengineering,LinkedIn Learning,gokxf9,duckiemama,,https://www.reddit.com/r/dataengineering/comments/gokxf9/linkedin_learning/,1.0,5.0,0.0,13489.0,"Yosh!

I just graduated with a BSc of Mechanical Engineering. Since the current job market for this field in my country is pretty bad, I am thinking of exploring the field of data engineering. Currently, I am studying [Master SQL for Data Science](https://www.linkedin.com/learning/paths/master-sql-for-data-science?u=43752620) from LinkedIn Learning since it's provided free from my school. Is it worth to continue this path of learning to strengthen my SQL coding? Or should I jump to study [Data Engineer Mastering the Concepts](https://www.linkedin.com/learning/paths/become-a-data-engineer-mastering-the-concepts?u=43752620)? I did data science and data visualisation course in Udemy last year, kinda rusty with my python code."
2248,2020-05-22 19:33:29,1590165209.0,dataengineering,Top programming languages from 1980 to 2020,gomm4s,professional3512,,https://www.reddit.com/r/dataengineering/comments/gomm4s/top_programming_languages_from_1980_to_2020/,1.0,0.0,0.0,13490.0,
2249,2020-05-22 20:14:25,1590167665.0,dataengineering,"Does ""SQL Server Big Data Clusters"" can replace HDP/CDW/CDP?",gone61,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/gone61/does_sql_server_big_data_clusters_can_replace/,1.0,1.0,2.0,13497.0," [https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sql-server-ver15) 

&amp;#x200B;

https://preview.redd.it/u1e4tmyanc051.png?width=250&amp;format=png&amp;auto=webp&amp;s=80ae494054010faa158b15aa700078282d8efe8a

SQL Server + HDFS + Spark on Kubernetes.  What do you think about it?

I'm wondering if anyone is already using that solution on-premise or in a cloud platform."
2250,2020-05-23 02:09:14,1590188954.0,dataengineering,Is Data Engineering with Google Cloud Professional Certificate from Coursera worth it?,gotzg9,Unchart3disOP,,https://www.reddit.com/r/dataengineering/comments/gotzg9/is_data_engineering_with_google_cloud/,1.0,4.0,0.0,13512.0,"I am curious guys, I have almost finished the first course in this specialization but I am curious what are your thoughts on this specialization, I am on my final year of my bachelor's degree in CS, and I aspire to have a career in data engineering/data science in the future would you recommend this for someone in my shoes? and what are your thoughts on Google's [Professional Data Engineer Certificate](https://cloud.google.com/certification/data-engineer)

Thanks!"
2251,2020-05-23 02:23:10,1590189790.0,dataengineering,suggestions for building a GIS online system,gou7j9,alecrimi,,https://www.reddit.com/r/dataengineering/comments/gou7j9/suggestions_for_building_a_gis_online_system/,1.0,5.0,0.0,13512.0,"Hi,

I would like to build an online system GIS based like [this](https://vac-lshtm.shinyapps.io/ncov_tracker/?_ga=2.13477298.488812408.1590163938-1612325661.1588942367) but with multiple users (e.g. different users see different points on the map) . I would be ok with Shiny as i have a lot of code in R, but what is the best way to store those things in database (starting from which database) assuming this will be on a webserver or AWS machine. Do you recomend to do this on a AWS machine?"
2252,2020-05-23 05:07:16,1590199636.0,dataengineering,Does anyone use C++ for data engineering?,gowqso,fbormann,,https://www.reddit.com/r/dataengineering/comments/gowqso/does_anyone_use_c_for_data_engineering/,1.0,25.0,0.0,13516.0,"Hi guys, I know this sounds like a really dumb question but I was wondering if anyone uses C++ for data engineering tasks at all in a daily/weekly basis? 

I am learning C++ just to go deeper into computing concepts but I find it difficult to get into the underlying libraries that some Python packages are built on top of (so that I can learn how they integrate/use C or C++ underneath).

I know there are a ton of uses in Deep Learning but I am focusing on the data engineering side in this question."
2253,2020-05-23 08:31:30,1590211890.0,dataengineering,Apache Spark Introduction for Beginners,goziuc,codie-fz,,https://www.reddit.com/r/dataengineering/comments/goziuc/apache_spark_introduction_for_beginners/,1.0,0.0,0.0,13522.0,
2254,2020-05-23 11:31:54,1590222714.0,dataengineering,Can someone help me with AWS use cases in Data Engineering space?,gp1hqw,v_radha,,https://www.reddit.com/r/dataengineering/comments/gp1hqw/can_someone_help_me_with_aws_use_cases_in_data/,1.0,4.0,0.0,13525.0,Can someone help me with where can AWS be leveraged better in Data engineering/ Data science space?  Any examples can be helpful..
2255,2020-05-23 12:44:23,1590227063.0,dataengineering,Where are SOLR/Elastic Search used in BitData Ecosystem,gp27uu,pawan_it17,,https://www.reddit.com/r/dataengineering/comments/gp27uu/where_are_solrelastic_search_used_in_bitdata/,1.0,9.0,0.0,13526.0,"Hello Everyone,

I am trying to get a high level overview of Big Data set up and data engineering. I understand that datalakes/datawarehouses are where the data is stored from the source via ETL and it is on top of this data wharehouse a Data Scientist builds his reporting for management using softwares like Tableau.

If this is the set up, where would one use search platforms like SOLR/ElasticSearch?."
2256,2020-05-23 13:53:56,1590231236.0,dataengineering,How is Materialize different from kafka streams + connect?,gp2y0o,hyperflow_,,https://www.reddit.com/r/dataengineering/comments/gp2y0o/how_is_materialize_different_from_kafka_streams/,1.0,0.0,0.0,13526.0,"It seems to me that you can do what Materialize (https://materialize.io) does pretty easily with some kafka streams and the JDBC sink connector for kafka connect.

I have a feeling I’m missing something here, what are the key differentiators or use case differences?"
2257,2020-05-23 16:09:29,1590239369.0,dataengineering,How to become a data engineer,gp4nj6,Bumpy-Booboo,,https://www.reddit.com/r/dataengineering/comments/gp4nj6/how_to_become_a_data_engineer/,1.0,7.0,0.0,13533.0,"I am an Operations Analyst for a UK fintech and would like a career change. I work with a lot of MI reporting and have grown an interest in statistics and analysis. I've been inspired by a lot of the people I work closely with to look into becoming a data engineer.

However I am an absolute beginner - I have no experience with coding, data bases, nothing! I know SQL, python would be good to learn.

My questions are:

- What qualifications do I need if any? I have A levels but do not have a degree. Am I able to self teach myself everything I need to know?

- What are the best courses to learn the skills I need? I've looked into Udacity but there seem to be some really bad reviews. I need something that I can work at my own pace as I will still be working monday to friday.

Any help would be really appreciated!"
2258,2020-05-23 17:16:45,1590243405.0,dataengineering,How much programming does a Data Warehouse Engineer do?,gp5n76,cappitak,,https://www.reddit.com/r/dataengineering/comments/gp5n76/how_much_programming_does_a_data_warehouse/,1.0,5.0,0.0,13536.0,"I'm currently a Java Developer. My friend put in a good word for me for a Database Warehouse Engineer position that recently opened at her company.  So I'm wondering what a day as a database engineer looks like. 


I have slight experience with Hadoop, Spark, and SQL and I really enjoyed working with them, so I'm thinking about applying.  The job posting says they're looking for C# and .NET experience so it sounds like a great opportunity to learn new languages and frameworks. But it doesnt sound like Database Engineers do a ton of programming and I don't want to box myself out of Software Developer jobs in the future."
2259,2020-05-23 17:51:11,1590245471.0,dataengineering,Help with data engineering internship.,gp66th,Bbdbz,,https://www.reddit.com/r/dataengineering/comments/gp66th/help_with_data_engineering_internship/,1.0,1.0,0.0,13539.0,"So I have a data engineering internship coming up and I am a bit lost. I am studying data science, so I am not that familiar with the techniques and skills required for data engineering. My internship starts in 1 month. Is there any sources you can recommend so I don’t look like a total goof when I start."
2260,2020-05-23 19:09:33,1590250173.0,dataengineering,Build your first data warehouse with Airflow on GCP,gp7icb,tuankid,,https://www.reddit.com/r/dataengineering/comments/gp7icb/build_your_first_data_warehouse_with_airflow_on/,1.0,7.0,0.0,13541.0,
2261,2020-05-23 19:37:54,1590251874.0,dataengineering,[Product Research] A tool that can convert large dataset to API?,gp7z90,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/gp7z90/product_research_a_tool_that_can_convert_large/,1.0,7.0,0.0,13539.0,"Occasionally, we come across datasets that are too large for our local machines to handle or even on the cloud we have budget constraints.

How about a tool that convert your data into an API service, requesting data based on different parameters.

Does something like this exist? Can this be helpful?"
2262,2020-05-23 21:05:38,1590257138.0,dataengineering,Learning Apache???,gp9itp,GiveWaveUWU,,https://www.reddit.com/r/dataengineering/comments/gp9itp/learning_apache/,1.0,1.0,0.0,13542.0,"Hello! I’m an undergraduate student studying computer science (focusing on data analytics). I’m interested in learning Apache Spark and Hadoop, but I’m not sure if I have the foundation to do so. I know Python, R, and some SQL.

The company I’m interning at, the ceo said that he could potentially offer me a job if I am familiar with hadoop/spark"
2263,2020-05-23 22:07:20,1590260840.0,dataengineering,"Most ""Data Engineers"" are really Database Administrators",gpalqw,char_pointer_string,,https://www.reddit.com/r/dataengineering/comments/gpalqw/most_data_engineers_are_really_database/,1.0,17.0,0.0,13542.0,"I would say the reason why DBAs were rebranded to ""Data Engineers"" is simply because the title ""Database Administrator"" is nowhere near as sexy as ""Data Engineer"".

However, the majority of Data Engineering jobs are really just the same as DBA except in a modern form.

Bash scripts, python scripting, tons of SQL, managing data sets, data cleansing, data analysis, Etc.

Not much actual engineering taking place imo."
2264,2020-05-23 23:08:12,1590264492.0,dataengineering,Data Vault (2.0) practical experience - Is it viable?,gpbnns,youderkB,,https://www.reddit.com/r/dataengineering/comments/gpbnns/data_vault_20_practical_experience_is_it_viable/,1.0,4.0,0.0,13545.0,"Hi, everybody,

I am a new employee in a company and the only one there with DWH/ETL experience.

At the moment there is the wish to rebuild the DWH (which was built up by a service provider and was not supervised by anybody afterwards) and I am supposed to make a proposal for this.

The company has completed a merger and has not merged the two ERP systems, but has brought them up to the same software level. The database structure is 1:1 identical. In the future these two ERP systems will be merged into one Cloud ERP system (in the next two to three years). There are also considerations to introduce a CRM and Product Information Management (PIM) system.

So for me the question is which architecture is the best to integrate the two ERP systems into the new DWH to be built and to be flexible enough to add CRM and PIM systems later.

At first I think of the Data Vault (2.0) methodology, but I have mixed feelings about it. It looks tempting because it promises agility, is business entity oriented, can be loaded in parallel, is audit-capable and brings along historization.

But on the other hand, you get a large number of tables (due to the HUB/SAT/LNK structure), thus more storage space is needed and the need for a DWH automation tool. 

It is not designed as an access layer for the frontend, so that you are back to dimensions and facts (Star Schema) again, which according to literature are primarily created as views. This I imagine as not very performant. The Data Vault methodology seems to be very complex and one tries to avoid the bad query performance by using additional constructs like BRIDGE and PIT table.

Therefore I would be interested in practical experience with Data Vault projects. Did they work? How does your experience with Data Vault compare to Kimball's approach? In the end, dimensions and facts are created for the Data Vault model as well.

Wouldn't it also be a good way to use a Persistent Staging Area and load dimensions and facts directly from there (after applying business rules)?

Btw. the DWH database would be in the cloud (Snowflake).

Thx and KR"
2265,2020-05-23 23:22:03,1590265323.0,dataengineering,Is it a best practice to use Scala with Flink?,gpbwbx,samhld,,https://www.reddit.com/r/dataengineering/comments/gpbwbx/is_it_a_best_practice_to_use_scala_with_flink/,1.0,2.0,0.0,13545.0,"Not sure if this is the right community for this question but it was my best guess.  This question stems from my interest in learning Scala to work with both Spark and Flink.  I don't really want to delve into learning Java just for this.  That said, the tutorials I've seen for Flink have been all Java-based...but even some of them discuss how much better of an experience it is to use Scala instead.  So why are these tutorials in Java? 

I understand that Flink is written in Java but it has Scala APIs and the main packages in the downloads page refer to Scala. 

I would much rather use Scala.  Can someone set me straight here?"
2266,2020-05-24 04:18:37,1590283117.0,dataengineering,New to Scala - what big data techniques should I know?,gpgrrh,AWSnQA,,https://www.reddit.com/r/dataengineering/comments/gpgrrh/new_to_scala_what_big_data_techniques_should_i/,1.0,2.0,0.0,13552.0,"Hello, I am interesting in learning Scala, so I am starting a project on an efficient implementation of k-nearest neighbors or decision trees classifier, with a view towards big data. Is there a technique should I consider when implementing this? Thanks!"
2267,2020-05-24 06:13:28,1590290008.0,dataengineering,How to become a Data Engineer?,gpidnx,EveryRace,,https://www.reddit.com/r/dataengineering/comments/gpidnx/how_to_become_a_data_engineer/,1.0,1.0,0.0,13555.0,"I am currently doing undergrad majoring in computer science with a concentration in data science. i have no intentions on going to grad school, and im interested on how to become a data engineer. My current course path would involve a lot of advanced math and coding. would a data science concentration be valuable towards a career in data science? what can i do on my own to prepare myself or give myself an edge over others?"
2268,2020-05-24 06:14:23,1590290063.0,dataengineering,"Couldn't find a good comprehensive article on setting up Airflow 6 months ago. I wrote one here: a setup using docker-compose, and included instructions on setting up PyCharm too! Hope you could get something out of it!",gpie3f,teddyhar,,https://www.reddit.com/r/dataengineering/comments/gpie3f/couldnt_find_a_good_comprehensive_article_on/,1.0,9.0,0.0,13555.0,
2269,2020-05-24 10:19:44,1590304784.0,dataengineering,ModuleNotFoundError encountered after airflow installation,gplf95,ploughthrough,,https://www.reddit.com/r/dataengineering/comments/gplf95/modulenotfounderror_encountered_after_airflow/,1.0,2.0,0.0,13566.0,"hi guys I've followed the airflow installation steps in the following link, but when I do `airflow initdb` it gives me this error:  
 [https://medium.com/@ryanroline/installing-apache-airflow-on-windows-10-5247aa1249ef](https://medium.com/@ryanroline/installing-apache-airflow-on-windows-10-5247aa1249ef) 

https://preview.redd.it/uvs8iy2lyn051.png?width=1043&amp;format=png&amp;auto=webp&amp;s=642d4536a70ca9536e96a2085547ef95823db5fc

i didn't find any useful resolution online. There is only one post about issue with python 3.8, but My python version is 3.7.4 and even with that, the post does not have a clear solution."
2270,2020-05-24 22:51:42,1590349902.0,dataengineering,need help with hive query,gpw4h6,awythrwawy,,https://www.reddit.com/r/dataengineering/comments/gpw4h6/need_help_with_hive_query/,1.0,9.0,0.0,13590.0,"I am trying to figure out if column 'personid' is the same for every unique combination of columns 'id' and 'number'.  


for ex:  


&amp;#x200B;

|id|number|personid|code|
|:-|:-|:-|:-|
|114|23|billyxyz|1d2d13|
|114|23|billyxyz|1ee21134|
|114|23|bob123|12d32333|
|116|222|george1|12r1rf12|
|116|222|george1|12312ff1|
|...|...|...|...|

  
How can I check if every record with a unique combination of id and number has the same personid value or a differnt value?  


i am basically trying to make sure that each combination of id and number always has the same personid value. In otherwords, records with id=114, number=23 should always have personid equal to the same value and so on and so forth for all the other unique id and number combination. How can I achieve this?"
2271,2020-05-25 00:40:44,1590356444.0,dataengineering,Need help with count(distinct col1) to get proper count of distinct values,gpy0gh,awythrwawy,,https://www.reddit.com/r/dataengineering/comments/gpy0gh/need_help_with_countdistinct_col1_to_get_proper/,1.0,5.0,0.0,13593.0,"how can i get the count of distinct column values for a col1 but ignoring NULL values AND also ignoring empty string values '' WITHOUT using a where conditional (the reason for this is in my previous post. I am doing a group by and not selecting the column i am doing a count distinct on. I do not want to include this column in the select because i dont want to group by it. More info in my last post if curious). Anyways

for ex:  
if i have 3 records:  


|col1|||
|:-|:-|:-|
|123|||
|126|||
|''|||
|NULL|||

  
Observed behavior:

select count(distinct col1) from table

returns 4

&amp;#x200B;

Expected behavior:

select count(distinct col1) from table

should return 2 (123, 126 are the distinct values ignoring empty string values and null values)"
2272,2020-05-25 02:47:50,1590364070.0,dataengineering,I made airflow-cdk to make it as simple as possible to deploy Airflow by leveraging the AWS cdk.,gq05rk,knowsuchagency,,https://www.reddit.com/r/dataengineering/comments/gq05rk/i_made_airflowcdk_to_make_it_as_simple_as/,1.0,2.0,0.0,13599.0,"[The github repo for the package is here](https://github.com/knowsuchagency/airflow-cdk) and [here is the link to pypi](https://pypi.org/project/airflow-cdk/).

There have been several articles written already about the benefits of airflow for data engineers and some great write-ups on how to develop for airflow and deploy it. That said, it's still a complex piece of infrastructure and there is certainly a learning-curve to using and deploying it effectively. I wanted something to help developers like myself hit the ground running without having to spend days or even weeks learning how to use it outside the context of something like Composer or Astronomer.

For those unfamiliar, the AWS cdk is an `infrastructure is code` tool developed by AWS that allows you to author cloudformation templates in one of several languages, such as Python. By leveraging the cdk, we can basically roll our own Airflow infrastructure using only Python! No more terraform or yaml engineering.

My hope is that this project gets more of us from zero-to-productive in Airflow. It's a very early project and contributions are more than welcome :)"
2273,2020-05-25 04:15:08,1590369308.0,dataengineering,DuckDB: SQLite for Analytics,gq1jkt,drecklia,,https://www.reddit.com/r/dataengineering/comments/gq1jkt/duckdb_sqlite_for_analytics/,1.0,5.0,0.0,13600.0,
2274,2020-05-25 05:05:55,1590372355.0,dataengineering,Data Engineering project for beginners,gq2bmf,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/gq2bmf/data_engineering_project_for_beginners/,1.0,34.0,0.0,13599.0,"Hi all,

Recently I saw a post on this sub reddit asking for beginner DE projects using common cloud services and DE tools. I have been asked this same question by my friends and colleagues who are trying to move into the data engineering field. So I decided to write a blog post explaining how to setup and build a simple batch based data processing pipeline using Airflow and AWS.

Initially I wanted to do it with both batch and streaming pipelines, but it soon got out of hand so decided to only do batch based first and depending on interest will do stream processing.

Blog: [https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition)

Repo: [https://github.com/josephmachado/beginner\_de\_project](https://github.com/josephmachado/beginner_de_project)

Appreciate any questions, feedback, comments. Hope this helps someone."
2275,2020-05-25 05:42:06,1590374526.0,dataengineering,Best setup for data problem,gq2viz,keg5038,,https://www.reddit.com/r/dataengineering/comments/gq2viz/best_setup_for_data_problem/,1.0,3.0,0.0,13599.0,"I have an interesting data problem that I’m looking to develop a solution for. Basically I’m looking for advice on how to set up my project. 

I work at a very small food manufacturing company that mills commodities, specifically wheat. 

We constantly purchase wheat at market price and quote potential customers a finished flour price. Besides the listed price on the CBOT, there’s a basis figure that’s basically the cost about the CBOT plus the freight cost, and also the price of feed. The details don’t matter for this exercise, but there’s basically three figures that affect the price of wheat. There’s some basic arithmetic once you have those three numbers, but really easy otherwise. Unfortunately right now, the process is in a single excel spreadsheet that is well designed for the process but does not tie into the different inputs automatically, rather it is a time consuming updating process. 

Along with being able to quote prices for customers a big part of the job is knowing our wheat position exactly, both what the average price of in house wheat is and how many bushels we have purchased. Obviously this is essential when quoting prices for customers. 

We currently manage these different processes with a number of different Excel spreadsheets. The spreadsheets were designed long ago and sort of work, but are very tedious. The updating process (farmer delivers, customer takes flour, etc.) is very manual and error prone. It also takes a long time to update. We should know our wheat position immediately, but sometimes it takes someone an entire day to update the multiple spreadsheets. 

I’ve taught myself enough Python to replace what work I used to do in Excel with Python. I’ve automated a lot of reporting using Python and am now looking to take the next step to improve the processes above. 

My basic question - is there a place I should get started on for best practices when setting up this project? I do not have much experience with SQL, but would assume that the best way to handle all of different tables that’ll need to be manipulated and queried? Right now, data is in a combination of Excel sheets and CSV files, but I assume SQL is safer and preferred? For example, I’m currently scraping wheat prices and writing them to a CSV once per day. To analyze the data, I’m reading in the CSVs into Pandas to report / manipulate. 

Would really appreciate any insight or links to help me wrap my head around the process? From my review, it sounds like creating the necessary tables in SQL and then using Python to automate the reporting sounds like the best way forward. As the project grows, I’d love to add more to it so want to make sure I’m setting things up with the long view in mind. 

Thanks in advance for any help!"
2276,2020-05-25 11:57:10,1590397030.0,dataengineering,How to run an Airflow job as backfilling from oldest to most recent DAG?,gq7m9i,damnko,,https://www.reddit.com/r/dataengineering/comments/gq7m9i/how_to_run_an_airflow_job_as_backfilling_from/,1.0,5.0,0.0,13608.0,"Hi, I've seen a PR in the Airflow repo adding the \`run\_backwards\` flag which gives the possibility to run a DAG as backfilling job, from the most recent DAG to the oldest: [https://github.com/apache/airflow/pull/4676](https://github.com/apache/airflow/pull/4676)

By reading the code and the documentation it seems to be a functionality of the Airflow cli, do you know a way to use that functionality when programmatically defining a DAG like so?

`from airflow import DAG`  
`dag = DAG(`  
`dag_id = ""..."",`  
`scheduled_interval = ""@daily"",`  
`run_backwards = True,`  
`...`  
`)`

Thanks"
2277,2020-05-25 14:22:27,1590405747.0,dataengineering,Airflow vs AWS?,gq9bax,stratguitar577,,https://www.reddit.com/r/dataengineering/comments/gq9bax/airflow_vs_aws/,1.0,17.0,0.0,13611.0,"Hi all, I just joined a new company and am leading an effort to diversify their ETL processes away from just using SSIS. I have good experience with Python and using tools like Kafka, Celery, AWS Lambda and AWS Batch. There are a couple of existing jobs right now that use Glue. 

As I am working on building a reusable Python architecture for these jobs and keep hearing about Airflow everywhere, I’m trying to understand what it really gives you other than easy scheduling, monitoring, and distributed processing. 

The goal of my role is to leverage serverless tools on AWS as much as possible to keep cost and maintenance low. Right now I feel like I can accomplish the same thing as Airflow and have a more diverse toolset on AWS. Cloudwatch gives me scheduling and centralized logs (while not the best UI for logs). I can run quick jobs using Lambda, larger ones using AWS Batch or AWS Glue. If I need to chain together multiple tasks kind of like a DAG there is AWS Step Functions. Not to mention the plethora of other tools at my disposal. I am comfortable configuring all of these with an IAC framework like Terraform. 

Some hesitation for Airflow includes needing a server to run the API on (vs pure serverless) and relying on Celery for task processing (had a lot of issues with Celery/RabbitMQ that makes me never want to have to use it again). 

So what am I missing with Airflow that AWS can’t provide?"
2278,2020-05-25 17:18:47,1590416327.0,dataengineering,Customer 360,gqbpss,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/gqbpss/customer_360/,1.0,1.0,0.0,13616.0,"I need to implement customer 360 in my organisation. I would like to know your thoughts. Here are some of them in my mind

1) Bring in all customer related information in the data lake from various data sources. Get all the customer information at runtime by joining with respective tables on data lake using spark ( delta lakes )

2) Use a document db to store one view of customer

3) Use knowledge graphs."
2279,2020-05-26 00:12:42,1590441162.0,dataengineering,Architecturing DAG that needs contextual throttling,gqjgrc,iamspoilt,,https://www.reddit.com/r/dataengineering/comments/gqjgrc/architecturing_dag_that_needs_contextual/,1.0,2.0,0.0,13633.0,"Explanation:

- I have a group of job units (workers) that I want to run as a DAG
- Group1 has 10 workers and each worker does multiple table extracts from a DB. Note that each worker maps to a single DB instance and each worker should deal with 100 tables in total for each DAG run
- Group1 has a limitation that says no more than 5 tables across all those 10  workers should be consumed at a time. For example:
  - Worker1 is extracting 2 tables
  - Worker2 is extracting 2 tables
  - Worker3 is extracting 2 table
  - Worker4 .. Worker10 need to wait until Worker1...Worker3 relinquishes the threads
- I should be able to create a single node Group1 that caters to the throttling and also have
  - 10 independent nodes of workers so I can restart them in case if anyone of it fails

I have tried explaining this in the diagram: https://imgur.com/a/Me797pR 

- If any of the worker fails, I can restart it without affecting other workers It still uses the same thread pool from Group1
- Group1 would complete once All elements of step1 and step2 are complete
- Step2 doesn't have any concurrency measures.

How do I implement such a hierarchy in Airflow for a Spring Boot Java application?"
2280,2020-05-26 00:18:08,1590441488.0,dataengineering,Simply moving data from on-prem MySQL to AWS MSSQL,gqjk57,AudioManDude,,https://www.reddit.com/r/dataengineering/comments/gqjk57/simply_moving_data_from_onprem_mysql_to_aws_mssql/,1.0,11.0,0.0,13633.0,"I need to query data from an on-prem MySQL server (behind SSH), and upload it to MSSQL on AWS. In concept this seems *really* simple:

1. Open an SSH tunnel
2. Query the MySQL database
3. Insert to MSSQL

Simple AF, right?

Right now I've got python scripts that do this for \~40 datasets. Crons kick off those scripts. It works but is clunky--I really want it to be simpler, have a UI, and have better logging/notification features.

I looked into Apache Airflow. I love the UI and it looks like it can do everything, but I'm really struggling with the learning curve. And it seems like overkill for my use case. Is there something simpler?

&amp;#x200B;

My ideal workflow would be

1. Get emailed if a script fails for some reason
2. Access a web-based UI that tells me when/why it failed, and what the script's history looks like
3. Make code changes
4. SSH to EC2, pull the changes, and kick off the script

I **really** want to avoid scheduling crons or reading logs in the terminal. I really suck at this and I want to focus on other things"
2281,2020-05-26 01:51:40,1590447100.0,dataengineering,How to build a SQL endpoint over a REST API,gql57x,ene-ce,,https://www.reddit.com/r/dataengineering/comments/gql57x/how_to_build_a_sql_endpoint_over_a_rest_api/,1.0,3.0,0.0,13639.0,"I have some web data that I process on a schedule. The API response returns the data in HTML tables, it gets read into pandas dataframes, processed, and gets put into S3. Ultimately the data will either get copied into Redshift or cataloged in Glue.

What I'd like to explore is building a SQL endpoint that can be queried on an ad hoc basis, such that a BI tool like Tableau could connect to it directly. The objective is to allow users to refresh the data source in Tableau and get the data from the API as opposed to relying on the Redshift/Glue table, which could contain stale data.

Ideally, I'd use only open source or AWS solutions. Any guidance? 

If I'm not mistaken, Presto has a feature that would support this. Would that be overkill for a use case like this? I've also come across a product called DataDirect by Progress, which appears to do exactly what I need but I have a strong preference for open source/AWS.

FWIW, I am proficient only with Python, but willing to put in the time to learn Java. This isn't an urgent project. More of a long term one.

Thanks!"
2282,2020-05-26 01:53:14,1590447194.0,dataengineering,Kafka Log = n+1 sorted array?,gql66g,char_pointer_string,,https://www.reddit.com/r/dataengineering/comments/gql66g/kafka_log_n1_sorted_array/,1.0,2.0,0.0,13639.0,
2283,2020-05-26 06:40:02,1590464402.0,dataengineering,Good first project for cloud dataops?,gqpkcl,WannaBeDataOps,,https://www.reddit.com/r/dataengineering/comments/gqpkcl/good_first_project_for_cloud_dataops/,1.0,2.0,0.0,13646.0,"I'm targeting AWS and SageMaker and would like a 'toy project' I can build to demonstrate my capabilities on the platform. What would be a good problem space/data set/domain in cloud big data/machine learning to 1) understand how cloud data ops works 2) familiarize myself with the technologies, techniques, applications, etc in the field?  What data set would I need?  What algorithms would I be using?  Thanks in advance."
2284,2020-05-26 10:40:26,1590478826.0,dataengineering,Best way to move/copy tables data between RDBMS?,gqsld7,irulchazi,,https://www.reddit.com/r/dataengineering/comments/gqsld7/best_way_to_movecopy_tables_data_between_rdbms/,1.0,0.0,0.0,13659.0,
2285,2020-05-26 10:50:19,1590479419.0,dataengineering,Caching python objects in AwsLambda,gqspfb,yemeraname,,https://www.reddit.com/r/dataengineering/comments/gqspfb/caching_python_objects_in_awslambda/,1.0,18.0,0.0,13659.0,"I'm looking for a way to cache python objects in Aws lambda environment. 
Basically i have a job that runs a piece of code on EC2 and updates some values in a dynamodb table.
The Lambda reads the dynamodb and outputs the results through Gateway api. 

My problem is that i think on every request lambda is reading from dynamodb and that is slowing the whole process. 

Is there any way to cache the dynamodb content in lambda env and use that local object instead of reading the table again and again, and also update those local objects when dynamodb is updated ?"
2286,2020-05-26 14:45:32,1590493532.0,dataengineering,An interview about the challenges of tracking the customer journey for B2B companies and how Dreamdata is addressing the problem with data integration,gqvi1h,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/gqvi1h/an_interview_about_the_challenges_of_tracking_the/,1.0,0.0,0.0,13666.0,
2287,2020-05-26 15:48:34,1590497314.0,dataengineering,Certificate Program in Data Science - Free Demo Session,gqwega,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gqwega/certificate_program_in_data_science_free_demo/,1.0,0.0,0.0,13668.0,
2288,2020-05-26 20:45:56,1590515156.0,dataengineering,Getting started with Spark and batch processing frameworks,gr1s4h,hszafarek,,https://www.reddit.com/r/dataengineering/comments/gr1s4h/getting_started_with_spark_and_batch_processing/,1.0,8.0,0.0,13677.0,
2289,2020-05-26 21:32:55,1590517975.0,dataengineering,Optimizing whole schema Extract Load procress,gr2oro,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gr2oro/optimizing_whole_schema_extract_load_procress/,1.0,9.0,0.0,13677.0,"Hi guys,

im new in Data Engineering and currently tasked to do some Extract and Load for entire schema, for now my solution is

1. fetch entire table data
2. insert the entire data into another table

im wondering if there's a better way to do this? even doing on local database, i feel it quiet slow, i kinda worried it wont work on production database, and for the records there's only 18 records each table (200-ish tables)

below is my solution

    def insert_data_for_table_with_primary_key():
        connection_source = psycopg2.connect(user=""postgres"", password=""admin"", host=""localhost"", port=""5432"", database=""postgres"")
    
        cursor_source_table = connection_source.cursor()
        cursor_source_column = connection_source.cursor()
    
        cursor_source_table.execute(query_table_with_pk)
    
        records_table_name = cursor_source_table.fetchall()
    
        connection_destination = psycopg2.connect(user=""postgres"", password = ""admin"", host=""localhost"", port=""5432"", database=""testing-2"")
        cursor_destination = connection_destination.cursor()
    
        for record in records_table_name:
            #get data for insert
            ten_minutes_earlier = get_ten_minutes_before()
            cursor_source_data = connection_source.cursor(name=""fetch_big_data"")
            query_select = ""select * from public.{table_name} where DATE_TRUNC('minute', updated_at::timestamp)='{ten_minutes_earlier}'"".format(table_name=record[0], ten_minutes_earlier=ten_minutes_earlier)
            cursor_source_data.execute(query_select)
    
            #get table column
            query_column = ""SELECT column_name FROM information_schema.columns WHERE table_schema = 'public' AND table_name = '{table_name}';"".format(table_name=record[0])
            cursor_source_column.execute(query_column)
            table_columns = cursor_source_column.fetchall()
    
            #construct column
            column = column_name_to_string(table_columns)
    
            while True:
                #get data
                source_records = cursor_source_data.fetchmany(size=100)
    
                if not source_records:
                    break
                #insert data
                query_insert = ""insert into {table_name}({columns}) values %s"".format(table_name=record[0], columns=column)
                psycopg2.extras.execute_values(
                    cursor_destination,query_insert,source_records,page_size=100
                )       
                connection_destination.commit()
            cursor_source_data.close()
    
        #close cursor
        cursor_destination.close()
        cursor_source_column.close()
        cursor_source_table.close()
    
        #close connection
        connection_source.close()
        connection_destination.close()

Thankyou in advance guys!"
2290,2020-05-26 23:27:31,1590524851.0,dataengineering,Share Udemy courses related to Data Engineer,gr4xwh,labobina,,https://www.reddit.com/r/dataengineering/comments/gr4xwh/share_udemy_courses_related_to_data_engineer/,1.0,1.0,0.0,13678.0,"Hello,

Can you share Udemy courses that are good resource to become a Data Engineer?"
2291,2020-05-27 13:44:57,1590576297.0,dataengineering,Building near realtime E2E pipelines for batch processing,grhftz,wannade007,,https://www.reddit.com/r/dataengineering/comments/grhftz/building_near_realtime_e2e_pipelines_for_batch/,1.0,8.0,0.0,13708.0,"I would like to solicit some design suggestions for building a near realtime ETL pipeline with below requirements. 

1. Need to call an API that returns a JSON object with hierarchical data (at least 3 levels). Need to parse the data and flatten it out. The each object in the response would have a timestamp in it (say x objects).  
2. Once flattened, need to write the data over to SQL  
3. The next time the pipeline runs (in a couple of mins), the same API would be called again and the response would now have few newer objects (x + n objects). Need to get these 'n' newer objects based on timestamp and push that over to SQL again  

All of this needs to be done in Azure. I am leaning towards ADF for scheduling and orchestration but not sure about compute. Given that it is near realtime, I would like to know if there are other viable options. Technically, this is not streaming and we are working only with batch data. I have also heard that ADF has something called Dataflow that can flatten out json objects, but I havent explored much.   

Any suggestions from the experts here?"
2292,2020-05-27 15:02:41,1590580961.0,dataengineering,Reach Out To Get Best Data Engineering Services,grifzc,Poojakhana52314,,https://www.reddit.com/r/dataengineering/comments/grifzc/reach_out_to_get_best_data_engineering_services/,1.0,0.0,0.0,13711.0,
2293,2020-05-27 15:46:52,1590583612.0,dataengineering,[Airflow] Best practice for polling SQL table for condition,grj2py,Shmoogy,,https://www.reddit.com/r/dataengineering/comments/grj2py/airflow_best_practice_for_polling_sql_table_for/,1.0,2.0,0.0,13713.0,"I am new to Airflow and trying to migrate over my cron (windows scheduler batch scripts) for higher resiliency, and a better UI for fails/success -- I am having trouble on finding best practices for something I need to do.

A lot of my jobs are based on running every 1 or 5 minutes, and checking a table for a condition, and then executing a script based on that requirement -- two adjusted examples are below:

    query = """"""SELECT TOP 1 Scheduled FROM Search_PushBids""""""
    curr.execute(query)
    Scheduled = curr.fetchone()
    if Scheduled[0] == False:
        print 'Quit'
    
    else:
        BusinessLogic code

or

    query = """"""Select ID, NewBid from BidTable""""""
    df = pd.read_sql(query, conn)
    if len(df)&gt;0:
        os.system('BidPush,py')

&amp;#x200B;

Is the Shortcircuit operator what I amlooking for here ?  I would like to be able to isolate 'successful' runs of the dags where there was actually a successful run of the entire workflow, versus a 'success' because it skipped to the end as there was no job scheduled.  Is there a proper/better way to run this type of flow?  I've got probably... thirty of these jobs that monitor our internal tables and process things based on records being created, or updated."
2294,2020-05-27 16:47:19,1590587239.0,dataengineering,25 Hot New Data Tools and What They DON’T Do,grk18w,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/grk18w/25_hot_new_data_tools_and_what_they_dont_do/,1.0,0.0,0.0,13714.0,
2295,2020-05-27 18:17:04,1590592624.0,dataengineering,Do data engineering interviews for faang companies (or faang tier) ask leetcode/algo questions?,grlkyf,jaspar1,,https://www.reddit.com/r/dataengineering/comments/grlkyf/do_data_engineering_interviews_for_faang/,1.0,29.0,0.0,13716.0,I don’t want to mindlessly study leetcode questions if data engineering interviews for these top companies don’t ask algorithm questions and is more based on sql/hive queries and Hadoop architecture based. Any insight appreciated.
2296,2020-05-27 18:48:42,1590594522.0,dataengineering,Apache Flink DataStream API Programming Guide,grm5j2,Marksfik,,https://www.reddit.com/r/dataengineering/comments/grm5j2/apache_flink_datastream_api_programming_guide/,1.0,0.0,0.0,13716.0,
2297,2020-05-27 20:57:10,1590602230.0,dataengineering,"How to read files in Elasticsearch? (doc, docx, pdf)",grop1w,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/grop1w/how_to_read_files_in_elasticsearch_doc_docx_pdf/,1.0,0.0,0.0,13721.0,
2298,2020-05-27 21:40:24,1590604824.0,dataengineering,Create a cluster of instances on AWS,grpjp5,hszafarek,,https://www.reddit.com/r/dataengineering/comments/grpjp5/create_a_cluster_of_instances_on_aws/,1.0,0.0,0.0,13722.0,
2299,2020-05-27 22:09:26,1590606566.0,dataengineering,Cloud sql tool,grq3zy,seanote1865,,https://www.reddit.com/r/dataengineering/comments/grq3zy/cloud_sql_tool/,1.0,8.0,0.0,13724.0,"Hi,

Wondering if a tool like this exists, and/or whether I'm the only person that would benefit:

I frequently work with large data files from disparate systems, which are slow to work with in Excel, Google Sheets, etc.  

I am very comfortable using SQL and would prefer it over spreadsheet functions.

Problem:

1) larger files bring my computer to a halt, what should take 3 mins takes 2 hours when you factor in all the waiting (excel on mac especially). 

2) not having ability to write SQL makes everything harder for me.  Google sheets attempt at SQL is a joke, requires too much setup and custom syntax

Solution (a proposed SaaS tool) - very fast and simple

\- upload xls, csv, whatever.  SQL table automatically provisioned in the cloud.  Table name and column defaults are extracted but can be renamed

\- nice fast and smart SQL interface (like a virtual IDE) that includes query builders for novices but supports basic SQL syntax for intermediates

\- data viz tool to easily make charts and graphs (like Mode Analytics if you've used that)

&amp;#x200B;

Does such a thing exist already? 

Would you use it if I built it?"
2300,2020-05-27 22:22:36,1590607356.0,dataengineering,Apache Airflow and Azure Container Registry,grqdan,OkieDaddy,,https://www.reddit.com/r/dataengineering/comments/grqdan/apache_airflow_and_azure_container_registry/,1.0,1.0,0.0,13724.0,"Have any of you airflow guru's successfully linked apache airflow to Azure Container Registry, for use by the DockerOperator? Everything I do ends in the same error.

    unauthorized: Application not registered with AAD.

I've created an Service Principal, tried using my own azure credentials, all to no avail. I'm not sure if I'm missing something, or if there is some extra configuration needed to get the azure registry to jive."
2301,2020-05-28 00:20:15,1590614415.0,dataengineering,Using pyspark code with Apache Beam?,grsm9r,romanX7,,https://www.reddit.com/r/dataengineering/comments/grsm9r/using_pyspark_code_with_apache_beam/,1.0,3.0,0.0,13731.0,"I started experimenting with pyspark a few months ago and recently discovered Apache beam and cloud dataflow. Is it possible to run pyspark code on Apache beam's dataflow runner?

The reason is that I have a project where I use pyspark to do some giospacial joins and I'm unsure if I can just reuse this pyspark code on Beam."
2302,2020-05-28 07:33:57,1590640437.0,dataengineering,Best Introductory Dev Guide to Airflow Concepts,grzi1j,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/grzi1j/best_introductory_dev_guide_to_airflow_concepts/,1.0,3.0,0.0,13749.0,
2303,2020-05-28 14:06:38,1590663998.0,dataengineering,"MPP data modelling, big tables",gs499a,stuartb113,,https://www.reddit.com/r/dataengineering/comments/gs499a/mpp_data_modelling_big_tables/,1.0,5.0,0.0,13758.0,"Hey guys,

We are considering how to model our data within Snowflake. Our key criteria is to have a model which is rapid to produce, flexible and appropriate for an MPP columnar DB like Snowflake. 

We are considering 'big tables' as it seems to fit the bill here, however, coming from a strict Kimble background we're struggling to get our heads around how this would work.

In particular we're thinking about our requirement to be able to do point in time reporting (not to be confused with time series) which normally would be handled by a combination of table versioning (SCD2) and the snapshotting of keys.

What experience do you guys have of this approach which you can share with us?

For those of you using an MPP DB what modelling techniques are you using? Dimensional, big tables, data vault, anchor or something else?"
2304,2020-05-28 16:15:17,1590671717.0,dataengineering,Free Demo Session - Certificate Program in Data Science,gs62gv,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gs62gv/free_demo_session_certificate_program_in_data/,1.0,0.0,0.0,13760.0,
2305,2020-05-28 17:14:28,1590675268.0,dataengineering,Is it feasible to acquire data engineering skills to support my own analytics work?,gs70v2,Kgs4vum,,https://www.reddit.com/r/dataengineering/comments/gs70v2/is_it_feasible_to_acquire_data_engineering_skills/,1.0,7.0,0.0,13762.0,"**TL;DR**: I’m a process engineer that wants to learn basic ETL and data warehousing. Is it realistic to learn these data engineering skills for my use case or is this too much material to cover alongside my main job?

I’m currently working full-time as a process engineer in the chemical industry. A part of my job consists of data analytics. By analytics I mean statistics and visualization, not machine learning.

My company and I suppose many other medium-sized companies in our industry have no in-house data science department and therefore often have poor or no data infrastructure. I want to work on projects for which the data lies outside of our production database. These projects will therefore have an important data engineering component, more specifically:

1. Building a basic ETL pipeline
2. Building a basic data warehouse

Here is an example project I worked on recently to give you a better idea of my use case:

A machine generates time series data for 60 variables. Each day this data is written in text files in a folder for each variable. The goal was to (1) join the time series from all folders by datetime (2) backfill missing data (3) move it to another place for storage.

I solved this by writing R scripts to process the data and saving it as CSV files in a target folder. I automated this daily process with a bash script scheduled with cron. This works fine but it’s not robust and would become messy for a more complex process.

In the future, I would like to use a tool like Apache Airflow instead of cron and an SQL database as a data warehouse instead of a folder with CSV files. The goal is to make the whole ETL process more robust and to allow us to integrate the processed data into our production database.

Here are some skills I have that I think are relevant to data engineering:

* Industry domain knowledge
* Experienced writing complex SQL-queries
* Proficiency with Python (not sure if R is relevant)
* Building a Docker image and deploying a Docker container
* Proficiency with Vim, the Linux command line and very basic bash scripting

I had two questions for people more experienced in data engineering:

1. How much ETL and data warehousing skills would I need to support my analytics work?
2. Can someone with more experience estimate how big the leap is from my current skills to becoming proficient with ETL and data warehousing for my needs?

Frankly, from my brief exploration I think ETL and data warehousing are fascinating subjects that would add massive value to our industry so I would enjoy having these skills, but I want to keep my feet on the ground as well.

Resources I found interesting:

* [A beginners guide to Data Engineering](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7)
* [Data Engineering Project for Beginners](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition)

edit: fixed link"
2306,2020-05-28 17:50:03,1590677403.0,dataengineering,Will Apache Beam take the crown from Spark?,gs7mcl,romanX7,,https://www.reddit.com/r/dataengineering/comments/gs7mcl/will_apache_beam_take_the_crown_from_spark/,1.0,11.0,0.0,13764.0,"Iv just recently discovered Apache Beam and have been using pyspark for about a month now.

At first glance, it seems like the flexibility to execute Beam pipelines on several different compute frame works (ie Spark, Dataflow, etc) make it a superior choice vs locking yourself into a Hadoop cluster with Spark.

Iv been exploring the idea of utilizing Spark to execute some tasks in my data pipeline. However, now that iv read up a little on Apache Beam, it seems like it's portability makes it a much better choice, especially since I can choose to execute jobs on Google Dataflow and not worry about managing a cluster.

From my admittedly short research, it seems like Beam and Dataflow would be the better choice for the majority of small teams who don't have the experience/resources to set up and maintenance Hadoop/Spark clusters.

tldr: I'd like to hear the opinions of those who have experience with either Spark or Beam/Dataflow. Why do you use one vs the other? And do you see Beam overtaking Spark?

Secondary question: When job postings list Spark as a qualification, are they looking for someone with experience setting up and mantaining an actual Hadoop cluster or someone who just knows how to write and submit Spark jobs?"
2307,2020-05-28 17:59:51,1590677991.0,dataengineering,Normalising a relational database model to insert into BigQuery newbie,gs7sfo,Bucephalus01,,https://www.reddit.com/r/dataengineering/comments/gs7sfo/normalising_a_relational_database_model_to_insert/,1.0,1.0,0.0,13765.0,"Hi thereI'm learning about big query. I have learned from here:  [https://cloud.google.com/bigquery/docs/nested-repeated](https://cloud.google.com/bigquery/docs/nested-repeated) that BQ performs best when your data is denormalised. I can do this using repeated and nested columns. However, I would like to know some fundamental rules and concepts around this say with a trival database. For instance I want to make an attempt at this with the database model from here:  [https://www.mysqltutorial.org/mysql-sample-database.aspx](https://www.mysqltutorial.org/mysql-sample-database.aspx)

So can anyone tell me some basic rules that I could apply to such a model below? It is hard to find good content on this on the net I believe.

&amp;#x200B;

https://preview.redd.it/k7meu5mqsi151.png?width=751&amp;format=png&amp;auto=webp&amp;s=72f36d0e19873e63d96876871efb0533d3f452b3

From what I'm reading in the Big Query documentation, one such basic practice is if entity A has multiple of another entity B, then entity B can be nested within entity A. So for example, Employees can be repeated fields nested within offices, and products can also be nested repeated fields within product lines. What about customers? do I nested them inside employees? then do I nest orders inside customers? I just don't know.

I hope this sounds like a reasonable question.

Thanks."
2308,2020-05-28 18:35:38,1590680138.0,dataengineering,Data engineering project for practice,gs8gof,leocharm,,https://www.reddit.com/r/dataengineering/comments/gs8gof/data_engineering_project_for_practice/,4.0,15.0,0.0,13766.0,"Hi There,

I recently failed an interview at a major tech company and it was primarily due to lack of experience in handling big data. In my day job, I don’t get an opportunity to handle big data so I’m debating taking up a project, on my volition, to better my skills and more importantly, get some real experience with handling big data. Can you suggest some ideas in how to go about this? Financial markets interest me so I was thinking of taking up a project in that domain.

Thanks for your feedback."
2309,2020-05-28 21:26:10,1590690370.0,dataengineering,Simply Install: Apache Hadoop,gsbp22,hszafarek,,https://www.reddit.com/r/dataengineering/comments/gsbp22/simply_install_apache_hadoop/,1.0,0.0,0.0,13773.0,
2310,2020-05-28 22:37:55,1590694675.0,dataengineering,Mulesoft vs Rhapsody,gsd4gd,datajen,,https://www.reddit.com/r/dataengineering/comments/gsd4gd/mulesoft_vs_rhapsody/,1.0,0.0,0.0,13775.0,"Hi!
I’m on a project team, aimed at determining best integration and API platforms. We’ve spoken a lot about DataStage (internally), but less about Rhapsody and mulesoft. 

Can anyone share a brief pro/con of the two? Or maybe when you would choose one over the other (other than cost and worker capacity)?"
2311,2020-05-28 23:48:53,1590698933.0,dataengineering,Apache Airlfow 2.0: What to expect?,gsehtc,marclamberti,,https://www.reddit.com/r/dataengineering/comments/gsehtc/apache_airlfow_20_what_to_expect/,1.0,0.0,0.0,13780.0,
2312,2020-05-29 09:25:04,1590733504.0,dataengineering,What could be the potential uses of a detailed water quality database?,gsngj3,theanswerisnt42,,https://www.reddit.com/r/dataengineering/comments/gsngj3/what_could_be_the_potential_uses_of_a_detailed/,1.0,1.0,0.0,13805.0,
2313,2020-05-29 15:25:37,1590755137.0,dataengineering,Need advice on using spark structured streaming,gss1dl,sloth_08,,https://www.reddit.com/r/dataengineering/comments/gss1dl/need_advice_on_using_spark_structured_streaming/,1.0,6.0,0.0,13819.0,"We have a data lake that was recently created on AWS S3 and new data is loaded using IBM CDC in the form of CSV files for the delta load. 
The task is to ETL the required data from this data lake into   Redshift. We are planning to use spark structured streaming for this as the new data can easily be loaded in our Redshift tables. There are about 250-300 tables that are going to be loaded.
Is this a good approach or should we be looking for some other alternative?"
2314,2020-05-29 15:54:10,1590756850.0,dataengineering,Moving from B2B Sales to Data Engineering,gsshcv,hyperplane_co,,https://www.reddit.com/r/dataengineering/comments/gsshcv/moving_from_b2b_sales_to_data_engineering/,1.0,7.0,0.0,13820.0,"Hey Everyone!

I've been doing weekly interviews with people in the data science and engineering space for work. I recently interviewed someone who transitioned from B2B sales to data engineering.

I thought this interview would be useful to share with this sub. I've posted the full interview transcript below.

\_\_\_

**From Sales to Data Engineering: Building Pipelines in the Aviation Industry**

Many people think you need a background in computer science or software engineering to become a data engineer.

However, data engineers are one of the most sought after roles by technology companies.

There is almost two-to-three times as much demand for data engineers than data scientists.

If you can build familiarity with the frameworks and how to architect pipelines, you can start interviewing.

I sat down with a Senior Data Engineer at GE Aviation who transitioned from a career in B2B sales.

&amp;#x200B;

**Can you give us a summary of your story?**

I did my undergrad in economics and history. I went to grad school for economics for my PhD and never finished.

I was an international student for the longest time, so it was really hard for me to get a job anywhere until I got married.

Then, I got a working permit, started working, and finally permanent citizenship.

It took a lot to start my career in a sense.

When I left grad school, I started working with a buddy of mine during the mortgage whirlwind. The housing market was going crazy from 2006-2009.

I rode that wave and it was profitable. When things crashed, I lost everything.

I couldn't find a job anywhere. There was no income anymore, so I moved to Cincinnati with a buddy of mine.

That's when I started working in the airline industry for Delta. I started from the ground level and moved on up. Eventually, they transferred me to Seattle, and that's where I was leading the whole safety department in Seattle.

I went to grad school for that as well. I started studying aviation safety. I love the aviation industry.

On the side, I was helping a buddy of mine. He owned a company and I was helping with the sales and business development side of the business. I was able to use my experience in the real estate market.

I ended up working for him full-time running sales for three years. Then, I moved back to Cincinnati because my wife wanted to raise our kids there. After getting settled, I found work doing sales.

I got bored. It was 2017 and that's when I decided I **needed to do something else**.

I had discovered the growing field of predictive analytics and wanted to learn more. I did my research. There weren’t many graduate programs at the time.

I started [learning R with Data Camp](https://www.datacamp.com/) and used other online learning platforms.

Then, I found a comprehensive online bootcamp. I signed up and quit my job. My wife decided to go back to work full-time.

After the program, I got a job as a data engineer with GE Aviation. And here I am.

&amp;#x200B;

**Wow, that’s an interesting story. You seem to be torn between Cincinnati and Seattle.**

**How did you get interested in data science?**

Before I went to grad school for economics, I was taking the prerequisites for the program.

I was doing a fair bit of econometrics. That was **always intriguing to me**. A lot of linear regressions and that sort of thing.

There weren’t as many career options back then. That was back in 2003 or 2004.

Econometrics has always been in the back of my mind. I enjoy that type of analysis.

I think a lot in terms of economic metrics. Rate of return, opportunity costs, that sort of thing.

Around 2016, data science started to explode along with the whole big data trend. It was all over the news.

I started learning more about data science. I found it put together the parts I enjoyed about economics with coding. That's something I always wanted to learn and that's why I got into it.

&amp;#x200B;

**A lot of people are torn between bootcamps and graduate degrees?**

**How was your bootcamp experience?**

The online bootcamp was great. The team was down to earth. It was personalized. My mentor was great. I was new to everything, but everyone was patient with me.

I'm still new to a lot of things. You learn as you go, it’s part of the career. I'm still learning a heck of a lot!

I wish I would have **done something more in-person**. Something with more structure, where I could connect easily with other students.

I did lose focus here and there. If I was in-person there would be more pressure and you would be able to learn from your peers.

You can ask questions of each other and learn from different perspectives .

Overall, I enjoyed the online bootcamp experience. I still reference the curriculum material from time to time, even after 2 years!

&amp;#x200B;

**Awesome.**

**So after the bootcamp, what was your job search like?**

It was tough. Especially around this area, a lot of the companies were just looking for people with STEM master's degrees or PhDs.

You have companies like [Kroger](https://en.wikipedia.org/wiki/Kroger) and [84.51](https://www.themuse.com/profiles/8451), which is a spin-off of [Dunnhumby](https://en.wikipedia.org/wiki/Dunnhumby). They were responsible for creating the Kroger Plus Card. Procter &amp; Gamble and GE both have a large presence as well.

I bombed my first couple of interviews. I didn't know what to expect. I was super nervous.

It took me about three to four months before I landed something.

I got in touch with a recruiter who worked at Tripoli, a company dedicated to placement at GE. He said he had a great position for me. I took a look at the job description and told him that it seemed like they wanted someone more advanced.

He said don’t worry about it, you’ll be a perfect fit. I decided to go ahead with the process.

At the same time, I was interviewing with another smaller company and they were pretty impressed. They liked the initiative I had shown learning data science and software development, plus my background in economics.

They said they’ll get back to me. Within a few days, I got an offer and accepted.

One week later I got an offer from GE. I was caught in the middle of the road.

I ended up rejecting the smaller company’s offer for two reasons. GE paid substantially more and I would have **more room for learning and growth**.

&amp;#x200B;

**What was the interview process like at these companies?**

There weren’t many coding challenges. That’s common out here.

Most of the questions will revolve around your background and work experience. Depending on who you interview with, you may have to **whiteboard out a hypothetical case study**.

I did get a question about query optimization, which I was only able to half answer since I didn’t have any practical work experience doing large SQL queries.

The process at GE was pleasant. I was there for two hours and interviewed with four different people.

The offer was a contract-to-hire, so in a sense, they can *interview me for six months* and then decide whether to keep me or not.

GE has a great culture around teaching and mentoring fellow employees.

&amp;#x200B;

**The heavy technical interviews don’t give people that are new to the field or get nervous in timed settings a fair chance.**

**Now that you are at GE, what’s your title, and what’s your day-to-day look like?**

I'm a senior data engineer.

The GE career ladder goes like an analytics engineer, data engineer, senior, staff, senior staff, and then principal.

As far as my day-to-day, I'm in a lot of meetings because I'm the lead developer on my team.

A lot of questions come to me. I **bridge the gap** between my team and the product owner\*\*.\*\*

The product owner is part of a different org and we're part of the data org.

I plan a lot of stuff. I make decisions in terms of what we should be doing, what we should not be doing, and how to troubleshoot.

I spend 50% of my time in meetings and the other 50% is in software development.

Right now, most of the work is maintenance and troubleshooting. Slow velocity, a new feature here and there, but nothing too crazy.

Within my team, I have the broadest tech stack knowledge. Most of the other team members have strong SQL / DBA backgrounds. They’ve been writing SQL for 15 years and are awesome at it.

However, I wanted to diversify. I’m interested in the architectural part of things.

If we want to implement any automation or anything outside of simply writing code, I’m the one who dictates what to do, how to do it, and what tech to use.

&amp;#x200B;

**What are some upcoming projects that you’re excited about?**

We were about to migrate from on-premises to AWS.

However, with the pandemic, the leap to the cloud has been **put on hold**.

I had started to rewrite all the functions we had in PySpark and SparkSQL. All of our code is mostly in SQL and we need to modify it to fit the Spark model.

We plan on using AWS services like Glue, S3, and Aurora for our data warehousing solution.

Then, anyone can pull data directly from that into a dashboard.

Everything got put on hold, but that’s what I'm excited about. Moving to a new development stack within a different environment.

&amp;#x200B;

**One thing I find intriguing is that you studied data science, but ended up working in data engineering?**

**Do you ever want to go back to the analytical side?**

I enjoy the engineering aspect more.

You get to play around with so many different tech stacks. There are **always new technologies to learn**.

I find it more fun.

&amp;#x200B;

**To me, it’s more about the mindset.**

**Do you want to know why people are clicking on an ad? Or, do you want to build a pipeline that combines and normalizes three different data sources?**

Exactly, I enjoy answering the second question.

I find system optimization more intriguing than analytical insights.

Data wrangling is more hands-on and more involved. It’s a lot of thinking about **how to structure and architect the pipeline**.

I eventually want to be an architect and focus on high-level design and implementation.

&amp;#x200B;

**That would be a good role based on your interests**

**Taking a step back, what career advice would you give your younger self?**

In terms of my younger self, it was hard for me to focus on a career. I was taking any job I could get because I was on a work permit.

I ended up getting my permanent residency by the time I was 28.

&amp;#x200B;

**So if you're a foreign national, you should get married as soon as possible?**

Yup, exactly. Find a partner.

All joking aside, I wish I applied myself more in school. That's one of my biggest regrets.

I had a blast, but I wasn’t thinking about the next step. Before I went to grad school, I had to take a bunch of prerequisite courses.

**Whatever you start, finish it**.

I had that problem and I kind of still do. I sometimes start a side project and don’t finish it.

If you're going to start something, finish it.

&amp;#x200B;

**That’s a good one.**

**How do you keep up with trends in the field?**

I go to conferences often. I enjoy them.

I went to Strata, at Microsoft one and we were planning for a PySpark one by Databricks. However, that has gone virtual now.

I'm strongly committed to continuing education. I want to keep learning.

&amp;#x200B;

**What do you like about conferences?**

The networking is great. Sometimes the talks can be boring, especially at big ones

I enjoyed the PyData conference I went to. It was focused on one specific subject, but at the same time instructional. They shared a lot of information.

The bigger conferences cater to the masses. The topic has to be broad.

That’s one of my takeaways. I’d much rather go to a small conference. You’re going to get to the point quicker.

The trip is fun as well, but I truly **enjoy the learning** that happens at the conference.

At the upcoming Databricks conference, our team signed up for the class about Spark Streaming. We’re all getting started on PySpark and SparkSQL, so we were excited about that.

I was looking to learn something new and apply it. That’s the main reason I wanted to go to the conference.

&amp;#x200B;

**What sort of hobbies do you do outside of work?**

I used to play soccer. Not anymore with everything going on.

Drinking beers. I enjoy a beer with friends and a good meal.

I’d say, regularly, cooking, playing soccer, and beers with friends.

&amp;#x200B;

**Those were all the questions I had.**

**Stay safe and best of luck as you grow at GE.**

Thanks, you take care as well."
2315,2020-05-29 17:09:04,1590761344.0,dataengineering,Moving from Salesforce Einstein Analytics to Data Eingineering,gsto0q,barbapapalone,,https://www.reddit.com/r/dataengineering/comments/gsto0q/moving_from_salesforce_einstein_analytics_to_data/,2.0,3.0,0.0,13825.0,"Hello, 

&amp;#x200B;

I am a 25F working as a business intelligence consultant on Einstein Analytics which is an analytics cloud provided by Salesforce. It is my first job, and I have been working on that platform for almost a year. I studied IT for decision making with machine learning, data mining and big data courses. My dream is to work in the data engineering area, but life made it that my first job was more Business intelligence oriented in the Salesforce environment. I now really want to convert to data engineering working with Spark, Hadoop, Scala, Python and everything that excites me just thinking about it.

&amp;#x200B;

I was wondering if I had a chance on that, since I don't have concrete real-life experience, and I am a junior who's first job wasn't data engineering related. (I gratuated just about a year ago and got my first job right after that).

&amp;#x200B;

Plus, what are your tips to compensate this lack of experience, do you know any good certifications I could pass to prove I have got the necessary background to my future employer? Do you think a solid github could help? Or an active stack overflow account? If the answer is yes, what do you think are good projects one can build by themselves to show they master the skills to be a data engineer? 

&amp;#x200B;

I am also contemplating data science, but I feel like data engineering is more suitable for my profile.

&amp;#x200B;

Thanks!"
2316,2020-05-29 18:43:43,1590767023.0,dataengineering,Rant: Hired as a Data Engineer No Data Engineer Responsibilities,gsvb1f,MassW0rks,,https://www.reddit.com/r/dataengineering/comments/gsvb1f/rant_hired_as_a_data_engineer_no_data_engineer/,1.0,12.0,0.0,13827.0,"I was offered a position at a large company as a Cloud Data Engineer. I was super excited and instantly began reading about Spark, Airflow, HDFS, and all of the data related services. Once I started, I learned that other teams do the actual engineering and my team supports the client facing infrastructure on AWS. That’s fine in itself, but even that is broken into teams. My team isn’t even allowed to make our own VPCs. Don’t get me wrong. The salary is great, but this isn’t what I signed up for.

It’s more like I’m 1/4th of a cloud engineer. I don’t know how that bodes for future jobs. I can’t imagine leaving and getting anywhere near my current salary because I won’t have the complete experience of a cloud or data engineer job title.

I’m less than 5 years into my career and I’ve changed positions 4 other times already before this one (2 different companies. 1 internal transfer per company. Laid off once), so I don’t think I’m in a position to leave. I’d look like a job hopper. I just don’t know what to do. I’m afraid of not having the skills to advance in my career without a large pay cut."
2317,2020-05-29 20:00:27,1590771627.0,dataengineering,How does StoryXpress video analytics help in gauging the performance of my videos?,gswq4y,icecold96,,https://www.reddit.com/r/dataengineering/comments/gswq4y/how_does_storyxpress_video_analytics_help_in/,1.0,0.0,0.0,13829.0,
2318,2020-05-29 20:49:40,1590774580.0,dataengineering,Outside Faang and maybe IBM any great places to start and build a career as a jr data engineer? I feel like startups blend of role responsibility and would love to iron out the skills first before the craziness of a startup,gsxnbr,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/gsxnbr/outside_faang_and_maybe_ibm_any_great_places_to/,1.0,8.0,0.0,13829.0,
2319,2020-05-29 21:21:41,1590776501.0,dataengineering,Need help weighing options for next DE opportunity,gsy92p,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/gsy92p/need_help_weighing_options_for_next_de_opportunity/,3.0,17.0,0.0,13829.0,"Currently a new-to-mid-level DE in the Bay Area..

I have two opportunities in front of me at the moment and I don't know what would be the right career move.

**About me:**

I'm 29 and I've been a DE for about 2 years in the Bay Area at only one company, but have had some pretty good transferrable experience there. Earlier in my career I had prior experience as a DBA, DB Engineer and a java programmer.

**What I'm looking for**:

I'm primarily looking for more growth and experience in my next role. At my current company, my growth has been stifled and I've been in need to reset at a new company.  I'm certainly looking to be challenged again, but I also don't want to be overwhelmed. I don't know which one of these jobs could be a more natural transition for someone with my experience.

I also have **imposter syndrome** when interviewing for new DE roles. I don't know if anyone else is the same way. 

# Opportunity 1:

* Series B startup with 50M+ raised
* Product is an NLP focused platform
* Technologies used: Databricks Delta, Spark, Streaming, Python, Airflow

**Why it's appealing to me:**

* Good experience - I'd be the first DE hire and would have a lot of impact on the team. Core pipelines are already built out, but I'd be basically managing the data warehouse and pipelines going forward.
* Product itself is recession proof.. (it has the capacity to replace people - saving companies money?)
* Culture seems awesome, but at times - kind of forced.

**Why it's not appealing to me:**

* I feel a bit of imposter syndrome. I really don't know if I can handle this type of work being the only DE.
* Also not used to being the only DE.. I feel like bouncing ideas off one another is important part of growth as a DE
* Series B companies are not always stable.
* Total Comp was kind of weak by Bay Area standards ($130k, no hiring bonuses, some equity) - could not negotiate either.

**Additional questions:**

* Is NLP a growing area of Data Science?
* What is the perspective of DE roles where they use Databricks?

# Opportunity 2:

* 12 month contract role at FB as a DE (w/ W2 benefits through a 3rd party)
* Technologies used: Hive, Presto, Python

**Why it's appealing to me:**

* I've never worked at a FAANG company and would love to at least once.
* Hourly rate is nuts: $85/hr and would
* Potentially good experience, and could make the transition to FB as an actual employee much easier - which is something that I have interest in doing.

**Why it's not appealing to me:**

* We're in the middle of a pandemic and contractors are usually the first to go if they need to lay people off.
* Contractors likely to be treated differently than actual employees - the work might not be the same.
* No benefits from Facebook, other than the food  (which cannot be reaped at the moment due to COVID-19 and likely WFH through 2020)

**Additional questions:**

* How would hiring managers from other companies objectively see your experience if you were a Facebook contractor.
* Not a lot of companies seem to be using Presto, how does this matter in comparison to companies that use Snowflake (for instance)

&amp;#x200B;

Any thoughts or insights would be really helpful!!"
2320,2020-05-29 22:58:31,1590782311.0,dataengineering,Corona Stats in The World at 2020,gt02xw,knzakaria,,https://www.reddit.com/r/dataengineering/comments/gt02xw/corona_stats_in_the_world_at_2020/,1.0,0.0,0.0,13834.0,
2321,2020-05-30 02:42:52,1590795772.0,dataengineering,How to market myself as a freelancer?,gt3xpa,enginerd298,,https://www.reddit.com/r/dataengineering/comments/gt3xpa/how_to_market_myself_as_a_freelancer/,1.0,10.0,0.0,13841.0,"Hi everyone, now with the remote working is indefinitely in place, I found myself having more free time to work on side projects for profit, I wanted to pick everyone's brain on how to do this and whether should I market it as a product (data visualization/storage/database) or by service? and say if I have a personal blog or website how should I put it out there for customers to see it?"
2322,2020-05-30 03:54:36,1590800076.0,dataengineering,How many of y’all have to do analytics when etl dev work runs low?,gt51ek,3000deltron,,https://www.reddit.com/r/dataengineering/comments/gt51ek/how_many_of_yall_have_to_do_analytics_when_etl/,1.0,4.0,0.0,13843.0,
2323,2020-05-30 16:12:33,1590844353.0,dataengineering,Ranking Of Countries by Population,gte2rd,knzakaria,,https://www.reddit.com/r/dataengineering/comments/gte2rd/ranking_of_countries_by_population/,1.0,0.0,0.0,13864.0,
2324,2020-05-31 07:15:57,1590898557.0,dataengineering,Best android version from 2012 to 2020,gtsero,professional3512,,https://www.reddit.com/r/dataengineering/comments/gtsero/best_android_version_from_2012_to_2020/,1.0,0.0,0.0,13899.0,
2325,2020-05-31 10:25:30,1590909930.0,dataengineering,Write data to SQL and archive in Excel,gtun3c,Luukv93,,https://www.reddit.com/r/dataengineering/comments/gtun3c/write_data_to_sql_and_archive_in_excel/,1.0,11.0,0.0,13903.0,"Hello,

I have a scenario where I need to write an Excel sheet to a SQL database. Our administration office adds new data to this file every day, thus I only want to read in the new data that they added.

Basically I want:

1. Write new data to SQL
2. Empty the sheet after data has ben written to SQL
3. Archive the data that was written to SQL in Excel

Would this be possible with Python, and could you provide me some sample code or references?"
2326,2020-05-31 19:46:15,1590943575.0,dataengineering,Apache Flink: Batch as a Special Case of Streaming - towards a unified data processing framework,gu2cpn,Marksfik,,https://www.reddit.com/r/dataengineering/comments/gu2cpn/apache_flink_batch_as_a_special_case_of_streaming/,5.0,9.0,0.0,13927.0,
2327,2020-05-31 21:29:04,1590949744.0,dataengineering,Anyone here work with SAP data?,gu465e,pokeDitty,,https://www.reddit.com/r/dataengineering/comments/gu465e/anyone_here_work_with_sap_data/,4.0,24.0,0.0,13929.0,"I used to be an SAP BW consultant and for the last few years, I've been working exclusively in big data platforms. Since reading up on big data platforms and warehouses, I have never seen SAP's BW on HANA mentioned anywhere. When I was working in SAP, it was being touted as the best datawarehouse and most performant. I imagine most major companies use SAP as their ERP systems. If SAP's warehousing solution is all that great, then why aren't I seeing it brought up more in presentations or data conferences? I feel like the only advantage with SAP's DW is its ease of integration with everything SAP.

My questions are, how do you handle SAP data? Are you using their DW or are you ingesting the raw data in another vendor's? Why / why not?

The more I think about it, the more I think SAP should be siloed in it's own proprietary product lines and extracted to a datalake only when it makes sense. But then, why not systematically extract the raw data into a datalake and save the licensing costs of runningh SAP's DW?

Thanks for reading, I hope I'm making sense!"
2328,2020-05-31 22:32:15,1590953535.0,dataengineering,What are data engineering/ML engineering interviews like?,gu5ao0,memcpy94,,https://www.reddit.com/r/dataengineering/comments/gu5ao0/what_are_data_engineeringml_engineering/,1.0,2.0,0.0,13930.0,"I have only worked at one company.  I got a return offer from my internship and worked at the same company as a data scientist after finishing my MS.



My role was originally building ML models and doing data analysis, but eventually I started taking on more responsibilities that are similar to ML engineering and data engineering.



As I move onto other companies, what are data engineering interviews like?  I'm currently more familiar with the interview process for software engineers, which is why I asked."
2329,2020-06-01 00:06:52,1590959212.0,dataengineering,Advice on change data capture?,gu70hw,mohanros,,https://www.reddit.com/r/dataengineering/comments/gu70hw/advice_on_change_data_capture/,1.0,4.0,0.0,13930.0,"Has anyone tried to understand product analytics based on database change data. I think it could be interesting to see how often specific database fields change, but not sure if there's a product off the shelf to do this.

Thanks!"
2330,2020-06-01 08:08:51,1590988131.0,dataengineering,"How To Visualize Public Transport Using Kibana, Elasticserach, Logstash (Elastic Stack) and Kafka Streams on top of Docker",gueem6,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/gueem6/how_to_visualize_public_transport_using_kibana/,1.0,2.0,2.0,13935.0,
2331,2020-06-01 12:50:20,1591005020.0,dataengineering,Next programming language after SQL and Python,guhqaw,youlikedags_,,https://www.reddit.com/r/dataengineering/comments/guhqaw/next_programming_language_after_sql_and_python/,1.0,40.0,0.0,13951.0,"Hi everyone, I'm a data analyst thinking about transitioning to data engineering. I have sql and python down but I can't decide what to learn next because the options are quite overwhelming. There's java, scala, go, kotlin, rust and some others. Should I go with something old or a newer language? I've read that java or scala should be the way to go, but I'm not sure if this advice is already outdated or not. And with the newer languages it's even more confusing as they seem to overlap quite a lot. Other technologies I know that I should probably learn docker and aws and probably some other stuff.

Thanks a lot!"
2332,2020-06-01 13:57:26,1591009046.0,dataengineering,"maintainability using a data-driven approach using some of Terraform's functions, variables, and count",guijaw,UnicornPrince4U,,https://www.reddit.com/r/dataengineering/comments/guijaw/maintainability_using_a_datadriven_approach_using/,1.0,0.0,0.0,13951.0,
2333,2020-06-01 17:05:31,1591020331.0,dataengineering,Advice - I need an additional server for python transformations. Currently I am a developer moving into the DE world.,gul82k,BaronJohnVonGreenbrg,,https://www.reddit.com/r/dataengineering/comments/gul82k/advice_i_need_an_additional_server_for_python/,1.0,6.0,0.0,13965.0,"Start up company is in weird situation where their data imports are currently managed by the parent company and is entirely done with SSIS.

Currently they are extracting individual tables from the third party billing system's live database onto a database in their server.  We have read access to those tables and have been given database space where I have created a DWH for the analysts to use and create MI reports.

We are using SSIS to update the DWH and send out reports but the agent scheduler in SSMS is not great, therefore I want to manage this process with Airflow and take advantage of setting dependencies in DAGs.  I am already using Python on my local machine to import data from third party APIs and SFTPs, this is taking hours to complete.

I asked to install Python on the server, which they refused. They are requesting we purchase another server from them (to keep python separate from the data) and we can only install Python and nothing else.  This would be an SSD 2TB server costing around £380 a month.  My other idea was to use something like Kafka to get our own data from the billing system in a better fashion than SSIS, in their eyes I would need another separate server for that as well.

Hardware is something I have never had to deal with, is this correct?

I have been given the green light to get an additional server .  We want to keep our copy of the live system and DWH on their server for now as we can use their DBAs but want a separate server just for python, Kafka and eventually a copy of the billing system. 

Bear in mind we cannot store data outside of the EU, what would be the best approach?"
2334,2020-06-01 17:59:17,1591023557.0,dataengineering,Visual docker-compose.yml file generator,gum45f,whiffersnout,,https://www.reddit.com/r/dataengineering/comments/gum45f/visual_dockercomposeyml_file_generator/,2.0,20.0,0.0,13967.0,"Hi all,

I made a little tool a while back to help me generate quick docker-compose files for a bunch of projects that come my way, and also to keep references around without having to dig through repo code.  I thought it would be useful to share it in the data engineering community.

The site is here [https://nuxx.io](https://nuxx.io/).  And I have a simple sample of a complete project here [https://nuxx.io/2kdWM69Tjk](https://nuxx.io/2kdWM69Tjk) which can be cloned and tweaked.

I have a few features that I am working on that I will push out soon. They are; configurations options for build, networks, deploy, and secrets. The project is a WIP, so generated code format errors, incorrect differences between compose versions should be expected. One of the reasons why I am sharing this project now is to field suggestions, generated yaml corrections, and other features that people would like to see.

Thanks."
2335,2020-06-01 18:40:19,1591026019.0,dataengineering,DockGrid - An easy to use Airflow alternative for job orchestration and distributed computing,gumt12,[deleted],,https://www.reddit.com/r/dataengineering/comments/gumt12/dockgrid_an_easy_to_use_airflow_alternative_for/,2.0,1.0,0.0,13970.0,
2336,2020-06-01 18:57:48,1591027068.0,dataengineering,Set up PostgreSQL and Airflow,gun4nn,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gun4nn/set_up_postgresql_and_airflow/,3.0,10.0,0.0,13971.0,"Hello folks,

so im thinking to rent a $5 Digital Ocean to learn on how to set  up PostgreSQL and Airflow on brand new server, here's my scenario

1. 2 Diffrent DB with 1 table each, 1 for main database and 1 for backup
2. Python script to insert every 5 minutes
3. Airflow to Extract data from main database and load it into backup database every 10 minutes

would $5 Droplets sufficient for this ? only for learning purpose tho"
2337,2020-06-01 19:02:17,1591027337.0,dataengineering,An easy to use Airflow alternative for job orchestration and distributed computing,gun7md,commonsnook,,https://www.reddit.com/r/dataengineering/comments/gun7md/an_easy_to_use_airflow_alternative_for_job/,2.0,10.0,0.0,13971.0,"Hi everyone,

For the last couple of months, we have been working in an Airflow ""alternative"", I'm using quotations here because we don't have 1/10th of airflow features, but still... for the cases where Airflow is too much expensive or just too much, we hope to make dockgrid a viable alternative.

Right now we have the essential features:

* cron based jobs
* pipelines from registry, git, tarball
* email notification on job failure
* docker parameters and environment variables

One of the biggest strengths is the ""near zero-configuration"" principle: it takes around 5 minutes to join a Linux computer into the grid.

Since we have Grid in the name, we don't expect homogeneity, nodes can be geographically dispersed and composed by desktops, servers, raspberry pi, AWS, Azure, etc.

If you wanna give dockgrid a try, we have a free plan that allows you to create a grid with up to 5 nodes. We don't have documentation yet, but if you have worked with Jenkins, Airflow, or similar tools before, you will probably find easy to operate dockgrid (and as a last resort you can always reach us at contact\_at\_dockgrid.com).

I would love to hear r/dataengineering feedback and understand what sort of features you would expect for the ideal tool in this field.

Thanks and happy processing!

[https://www.dockgrid.com](https://www.dockgrid.com)"
2338,2020-06-01 20:58:27,1591034307.0,dataengineering,How to access S3 data from Spark,gupe0g,hszafarek,,https://www.reddit.com/r/dataengineering/comments/gupe0g/how_to_access_s3_data_from_spark/,0.0,0.0,0.0,13975.0,
2339,2020-06-02 15:59:22,1591102762.0,dataengineering,Airflow Looping Cycle,gv6vj6,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gv6vj6/airflow_looping_cycle/,1.0,8.0,0.0,13996.0," So im creating a DAG for daily backup,restore and delete, with branching

1. do daily backup restore and delete (always runs)
2. if its saturday do weekly backup,restore and delete
3. if its first day of month do montly backup,restore and delete

when i test it for some reason it wont reach the end\_task\_op, instead when it reach last task before end\_task\_op it keep restarting on starting from daily\_backup\_op

here is full code (sorry had to post it on github, quite long code)

[Full DAG Code](https://github.com/ad17-2/dag-testing)

![img](yqju5ussvh251 ""full DAG cycle"")

where did i go wrong ? been debugging this for quite 2 hours :("
2340,2020-06-02 17:20:25,1591107625.0,dataengineering,Label Studio Release Notes 0.7.0 - Cloud Storage Enabled Label Studio,gv86hi,ramiyengar,,https://www.reddit.com/r/dataengineering/comments/gv86hi/label_studio_release_notes_070_cloud_storage/,2.0,0.0,0.0,13997.0,
2341,2020-06-02 18:20:18,1591111218.0,dataengineering,Gross domestic product per capita in USA,gv975m,knzakaria,,https://www.reddit.com/r/dataengineering/comments/gv975m/gross_domestic_product_per_capita_in_usa/,1.0,0.0,0.0,14000.0,
2342,2020-06-03 00:32:36,1591133556.0,dataengineering,Best AWS Certification for a Data Engineer,gvgcw4,DataD23,,https://www.reddit.com/r/dataengineering/comments/gvgcw4/best_aws_certification_for_a_data_engineer/,1.0,16.0,0.0,14010.0,"Just like the title suggests, I would like to know which AWS certification would be the most beneficial in having. I do have all three associate certifications and I am thinking about getting either the DevOps Engineer Professional, Database Speciality, or the Data Analytics Speciality. Eventually, I would like to have them all but at the moment I am having a hard time choosing which one would bring me the most benefit. Also, I suppose this is necessary to know as well (I am trying to get my foot in the door to data engineering). If anyone has any tips or suggestions I would really love to hear from you!"
2343,2020-06-03 01:47:34,1591138054.0,dataengineering,Software Engineering course recommendation?,gvhrky,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/gvhrky/software_engineering_course_recommendation/,0.0,1.0,0.0,14013.0,"After a thorough review of the Data Engineering field and my ultimate goal of becoming a Machine Learning Engineer, it seems that these roles are basically Software Engineering.

I have enrolled in Jose Portilla's Python course and also contemplating his SQL course. Do I need to learn software (development) engineering and is there any recommended udemy courses?"
2344,2020-06-03 06:21:40,1591154500.0,dataengineering,How to pre-process large datasets for machine learning with Spark,gvm2kw,coldmonk93,,https://www.reddit.com/r/dataengineering/comments/gvm2kw/how_to_preprocess_large_datasets_for_machine/,1.0,0.0,0.0,14022.0,First medium article. Mostly targeting beginners learning spark and data engineering. Please feel free to remove this post if it doesn't belong here.
2345,2020-06-03 07:21:48,1591158108.0,dataengineering,Large scale adoption of open source DB,gvmwje,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/gvmwje/large_scale_adoption_of_open_source_db/,1.0,8.0,0.0,14022.0,"Hi All,

I see a lot of databases like infulxdb, neo4j which are open source but a major feature that they do not provide in the community edition or the free one is clustering. This makes it a lot difficult to adopt them in production systems and well as large scale adoption. 

I believe to improve adoption of these databases, this must have feature should be provided in the open source as well. Your thoughts ?"
2346,2020-06-03 14:49:47,1591184987.0,dataengineering,An interview about Upsolver's mission to build a data lake that empowers the database administrator to step into the world of big data,gvsifu,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/gvsifu/an_interview_about_upsolvers_mission_to_build_a/,1.0,0.0,0.0,14035.0,
2347,2020-06-03 15:37:10,1591187830.0,dataengineering,Learning Data Engineering: Which database system to begin with?,gvt5pz,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/gvt5pz/learning_data_engineering_which_database_system/,1.0,39.0,0.0,14036.0,"I understand the importance of SQL, and I am looking forward to learning it soon. But which database system should I start with?

As per ranking: Oracle, MySQL, and MS SQL server is the top 3 database system. I come from Windows background so MS SQL server would be I kinda familiar with me. But MySQL is free and open source.

1. What are the differences among these 3?
2. Would it be a good idea to stay with the LAMP framework (MySQL in this case)?"
2348,2020-06-03 17:48:44,1591195724.0,dataengineering,Looking for free and current datasets for football(soccer),gvvap2,seeyainvalhalla,,https://www.reddit.com/r/dataengineering/comments/gvvap2/looking_for_free_and_current_datasets_for/,2.0,8.0,0.0,14039.0,"Hello all,

&amp;#x200B;

Trying to do something worthwhile with my COVID time and work on a project. Most of the current datasets come with a staggering paywall. Anyone know where I could get these for the 19/20 season either free or at a reasonable price?  Sorry if this is obvious I am pretty new at this."
2349,2020-06-03 18:59:56,1591199996.0,dataengineering,How do I prepare for my Data Engineering internship interview,gvwla7,Unchart3disOP,,https://www.reddit.com/r/dataengineering/comments/gvwla7/how_do_i_prepare_for_my_data_engineering/,1.0,2.0,0.0,14041.0,"Hi guys,  
So recently, I have finished a task and managed to get myself an interview tomorrow, but I am abit lost on what I should be studying today inorder to prepare for this interview?

To give you some background, my task was to web scrape a website using a language of my choice, and I have used python with the beautifulsoup package

Thanks!"
2350,2020-06-03 20:22:09,1591204929.0,dataengineering,Resources for learning Apache Calcite,gvy5ep,not-abhi,,https://www.reddit.com/r/dataengineering/comments/gvy5ep/resources_for_learning_apache_calcite/,1.0,2.0,0.0,14042.0,"So I went through the Apache Calcite documentation. I understand the code in the tutorial and what the documentation says but somehow I'm not able to glue everything together. Are there other resources (books, videos) which will take me step by step to how to build thinking in writing code in calcite for a beginner?

Thanks in advance.

P.S english is not my native language."
2351,2020-06-03 21:04:43,1591207483.0,dataengineering,DE for Quantitative Finance?,gvyzf6,beaverhair,,https://www.reddit.com/r/dataengineering/comments/gvyzf6/de_for_quantitative_finance/,1.0,2.0,0.0,14043.0,"Does anyone here work in a data engineering capacity in quantitive finance or related industry? I’m looking to get into this industry and just trying to get a feel for the day to day, what sort of pipelines/data you are building/work with, what tech you work with, pros/cons, etc.

Really don’t have specific questions which I know doesn’t help to provide an answer but just looking for some general insight from those in this role."
2352,2020-06-03 23:09:33,1591214973.0,dataengineering,What to concentrate on first to become employable as a data engineer? (pragmatic approach),gw1kf1,dondraper36,,https://www.reddit.com/r/dataengineering/comments/gw1kf1/what_to_concentrate_on_first_to_become_employable/,2.0,57.0,0.0,14047.0,"Hi,I have been working as a data scientist for around 1.5 years, but now I feel like I am really interested in data engineering. Unfortunately, the current tasks at the company I am currently working at are pretty simplistic where my tasks lately have been just to process datasets, build models, validate them, retrain as new data comes in, etc. Nothing even remotely close to real data engineering.

I have analyzed a few dozens of job positions and it seems that there are so many different technologies one is supposed to know even for junior positions.

That said, I realize that at the core of data engineering there are Python and SQL.  My current job is not very SQL-demandng, so I would say this is something I should start with for now.

How proficient should I get with SQL? There are a few books that are recommended: SQL Queries for Mere Mortals and Database Design for Mere Mortals. Both of them are huge. Question is, does it make any sense to read such bricks to pass an interview? Or would it be, say, enough to master everything in a good SQL tutorial (like the one by Mode Analytics)?

My knowledge of Python is pretty decent, but of course, I keep improving and learn new things every day.

I also know about the book ""Designing data-intensive applications"", which I am currently reading. Even though honestly, some concepts are not that obvious due to the lack of experience, but I guess what is really important is to understand the basic principles.

Apart from Python, SQL and reading Designing Data-intensive applications, what should the next steps be?"
2353,2020-06-04 01:03:56,1591221836.0,dataengineering,Seeking advice Bootcamp and move to DE career path,gw3t7t,noetic11,,https://www.reddit.com/r/dataengineering/comments/gw3t7t/seeking_advice_bootcamp_and_move_to_de_career_path/,1.0,1.0,0.0,14050.0,"The Linux Foundation has what looks like a pretty good deal ($600) on their [Linux Foundation Cloud Engineering Bootcamp](https://training.linuxfoundation.org/training/cloud-engineer-bootcamp/) . I'm on break from school, where I'm studying Analytics. After a year working as a Data Analyst, I want to slightly pivot from the Data Science path I'm pursuing to a Data Engineering path. 

This looks like a good technical foundation, since I don't have a CS background or proper programming experience. I'm getting better at python and trying to implement good practices. I have logged a good bit of time in Fedora but I'm mostly copy pasting Unix, Bash commands if I need to get something done. 

The course covers system administration, devops, containers, and Kubernetes with system admin and Kubernetes certifications. 

Seems like I'd be a good candidate. Would this bootcamp be a good move to kickstart a move toward Data Engineering?

If not, any advice on alternative steps would be much appreciated.

Thanks for your time."
2354,2020-06-04 02:05:56,1591225556.0,dataengineering,Non-CS STEM-degreed DE here. Is a masters a good move to deepen knowledge?,gw4zgh,Firm_Bit,,https://www.reddit.com/r/dataengineering/comments/gw4zgh/noncs_stemdegreed_de_here_is_a_masters_a_good/,1.0,4.0,0.0,14051.0,"Hi

I’m a DE in ad-tech with about 2 years of scripting python, heavy sql use, and plumbing under my belt. I’m doing pretty well, still fairly junior, and enjoy my work. The wall I seem to constantly hit is lack of basic CS. While I usually figure it out, I know what I’m building isn’t always great. 

os, large application architecture, infrastructure, etc would all be made easier with some fundamentals, no? I also feel that lack of knowledge in fundamentals would prevent me from attempting big projects or being a strong contributor on a really talented team. 

I don’t want to do another BS, but I’m thinking of a masters in CS. I’ve signed up for some pre reqs this summer as a first step. 

Would you say that strong CS and software engineering fundamentals are critically important to being the best DE you can be? 

Would you say that an MS in CS (plus some basics taken before hand) would get me those fundamentals? 

Thanks for any and all opinions."
2355,2020-06-04 09:28:27,1591252107.0,dataengineering,Data Science or Data Engineeeriy,gwbvqt,awesomeabhi5,,https://www.reddit.com/r/dataengineering/comments/gwbvqt/data_science_or_data_engineeeriy/,1.0,3.0,0.0,14064.0,"Hi Everyone,

I will try to keep it brief and to the point. Please pardon me for any mistakes.

I have been working for a big US bank through a consultancy as a data engineer for past 4 years. Since I now know most of the systems, the job seems a bit mundane. I recently gave an interview at another consultancy for similar position and got selected. 

Now, my consultancy is offering to move me to the  junior data analyst/ scientist role for the same client if I chose to stay back( This was something I had told my managers long back that I am interested in working for that team)

I have a bachelor's degree in Computer Science. 

My question is, should I stay back and build a career in data science or stick to my data engineering role and try to improve my skills in that area? 

Could you please let me know in terms of long term career  opportunities, which one would be best for me? I don't have a master's degree. Would that be an issue with going for Data Scientist roles even after I gain experience?

A lil bit about what we do as data engineering team, among other things, we built batch pipelines through ETL tools to feed data into Hadoop for our analytics and model development teams."
2356,2020-06-04 11:50:33,1591260633.0,dataengineering,"Spread of Coronavirus (COVID-19) in India, a visual timeline until 30-May-2020",gwdns9,Evening_Sale,,https://www.reddit.com/r/dataengineering/comments/gwdns9/spread_of_coronavirus_covid19_in_india_a_visual/,1.0,1.0,0.0,14068.0,
2357,2020-06-04 16:56:39,1591278999.0,dataengineering,Webinar on Introduction to Computer Vision,gwhwsm,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gwhwsm/webinar_on_introduction_to_computer_vision/,1.0,0.0,0.0,14076.0,
2358,2020-06-04 17:52:38,1591282358.0,dataengineering,Scheduling and error handling for python pipelines?,gwixn8,infazz,,https://www.reddit.com/r/dataengineering/comments/gwixn8/scheduling_and_error_handling_for_python_pipelines/,1.0,22.0,0.0,14079.0,"I'm stuck on exactly what to do for this problem!

I have multiple python projects that need to run on a certain schedule in order to run make api calls and automate moving data between disparate sources. They need to run at weird hours in order to make the data available during business hours.

I am currently using Apache NiFi to schedule these jobs. It does a good job of running the scripts, but it is not easy to see if anything fails and it's event more difficult to see WHY it failed.

What I am in search of is a tool that allows for scheduling python scripts to run, it needs to be something that can be hosted within my company's firewall, and it would be nice if it has some level of error handling.

Some tools I have looked at include Prefect, Airflow, and Luigi. They all seem to do these things in some capacity. Does anyone have an opinion on these tools? Thoughts on something else I should be looking at?"
2359,2020-06-04 21:00:14,1591293614.0,dataengineering,"AWS batch vs AWS Sagemaker, which one is better to run a 2-hour long job?",gwmjx8,data-david,,https://www.reddit.com/r/dataengineering/comments/gwmjx8/aws_batch_vs_aws_sagemaker_which_one_is_better_to/,1.0,13.0,0.0,14085.0,"**I need to run a daily ML job inside a docker container with python**. This job is doing the following for about 300 objects:

1. Pull data in. For each object it takes 10 second to pull data.
2. Some data cleaning/aggregation.
3. Fit a model and save it to s3. Fitting a model takes max 10 minutes to do this.

**What is the best tool to do deploy and run something like this easily in AWS? What is the best way to process so many things quickly?**

Keep in mind:

\- Having a job taking &gt; 4 hours is not an option, so either I need big machine or some way to distribute the job

\- Ideally, the deployment script should be ""easy""

&amp;#x200B;

Looking at AWS batch,  it seems that I need to configure quite a lot of things to get it running, while sagemaker feels slightly wrong as most examples I have seen seem to be more focused on 1  model.  


Bonus

I have done some research online but I cannot find practical example of either AWS batch or AWS sagemaker for a similar tasks..any tips? Code/repo examples are greatly appreciated!"
2360,2020-06-04 23:25:49,1591302349.0,dataengineering,Apache Airflow - Understanding Components,gwpg00,showIP,,https://www.reddit.com/r/dataengineering/comments/gwpg00/apache_airflow_understanding_components/,1.0,4.0,0.0,14096.0,"Hi, I'm being brought into a very simple Airflow project and had a couple questions:  


1. It appears we currently have a web node and a scheduler node...can the scheduler run dags? From the documentation it seems like we need a worker node
2. airflow.cfg is currently set to use SequentialExecutor, which seems like a bad idea for ""prod"". Before moving to fully distributed workers with something like Celery, will LocalExecutor give us the ability to run multiple dags at once on a single worker?
3. Assuming we need worker nodes, how does the scheduler become aware that there are worker nodes available?

Thanks so much!"
2361,2020-06-05 01:07:32,1591308452.0,dataengineering,Python dependencies/imports,gwrf84,sbwhitney,,https://www.reddit.com/r/dataengineering/comments/gwrf84/python_dependenciesimports/,1.0,2.0,0.0,14101.0,"We’re building out lambdas for an api using python. When the lambdas are deployed to the cloud the imports need to be relative (import module_a). However, when we run the unit tests via a Jenkinsfile, we need to pass absolute paths (from folder1.subfolder1.subfolder2.module_a). We need to pass the absolute paths because we would like to pass a single command ‘pytest’ from the subfolder1 level in order to collect all the tests in the path ‘folder1/subfolder1/*/tests/. I floated the idea of zipping up the lambdas with the folder structure intact. What would be a better solution?"
2362,2020-06-05 03:18:11,1591316291.0,dataengineering,"Vertica MPP Database Overview And TPC-DS Benchmark Performance Analysis (Part 1, 2, 3 &amp; 4)",gwtsau,dingopole,,https://www.reddit.com/r/dataengineering/comments/gwtsau/vertica_mpp_database_overview_and_tpcds_benchmark/,1.0,0.0,0.0,14104.0," Part 1 - [http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-1/](http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-1/)

Part 2 - [http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-2/](http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-2/)

Part 3 - [http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-3/](http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-3/)

Part 4 - [http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-4/](http://bicortex.com/vertica-mpp-database-overview-and-tpc-ds-benchmark-performance-analysis-part-4/)"
2363,2020-06-05 03:25:30,1591316730.0,dataengineering,Kicking The Tires On BigQuery – Google’s Serverless Enterprise Data Warehouse,gwtwti,dingopole,,https://www.reddit.com/r/dataengineering/comments/gwtwti/kicking_the_tires_on_bigquery_googles_serverless/,1.0,0.0,0.0,14104.0," Part 1 - [http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-1/](http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-1/)

Part 2 - [http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-2/](http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-2/)"
2364,2020-06-05 05:50:49,1591325449.0,dataengineering,"Real-time Stream Analytics and User Scoring Using Apache Druid, Flink &amp; Cassandra",gww96m,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/gww96m/realtime_stream_analytics_and_user_scoring_using/,1.0,8.0,0.0,14107.0,
2365,2020-06-05 08:13:16,1591333996.0,dataengineering,Data model for rideshare app,gwybxf,techblogp,,https://www.reddit.com/r/dataengineering/comments/gwybxf/data_model_for_rideshare_app/,1.0,10.0,0.0,14114.0,Looking for help to improve this data model for a rideshare app.  What potential questions can be asked about the data for this kind of app?
2366,2020-06-05 12:03:15,1591347795.0,dataengineering,Asking about Getdbt and possible alternative,gx1248,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gx1248/asking_about_getdbt_and_possible_alternative/,1.0,9.0,0.0,14123.0,"Hello folks, is there anyone here using getdbt ? currently im researching about it, planning to use it to create pre-calculate query (im sold by its abilities to create table based on select query)

I wonder, if it possible to have diffrentiate between target and data source ( host to host ideally),

for example Database X in server 1 as data source and Database Y in server 2. 

because as far as know the profile.yml in .dbt is used as target database.

and is there any alternatives for getdbt ?

thankyou in advanced folks!"
2367,2020-06-05 15:59:44,1591361984.0,dataengineering,Free Webinar on Introduction to Computer Vision,gx43xv,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gx43xv/free_webinar_on_introduction_to_computer_vision/,1.0,1.0,0.0,14130.0,
2368,2020-06-05 17:55:53,1591368953.0,dataengineering,Feedback on a new Airflow plugin for data status,gx637h,SpinalPap11,,https://www.reddit.com/r/dataengineering/comments/gx637h/feedback_on_a_new_airflow_plugin_for_data_status/,0.0,7.0,0.0,14136.0,"I work at a small consulting company and we do a lot of data engineering architecture work.  One of our biggest and most constant problems is assuring people when the data is good or notifying them when it's not.  I think we've all been in situations when bad data has gone out and you have to slink into someone's office and explain how the API key expired last night.

This is our first cut at solving that problem with an Airflow plugin.

Github: [https://github.com/Raybeam/rb\_status\_plugin](https://github.com/Raybeam/rb_status_plugin)

I also wrote an article about it to explain more about specific use cases: [https://medium.com/raybeam/use-airflow-to-project-confidence-in-your-data-abd160f3dc8c](https://medium.com/raybeam/use-airflow-to-project-confidence-in-your-data-abd160f3dc8c)

If you like it, great!  But what I'd really love to hear is if it actually fixes anything for you and where it falls short.

Thanks!"
2369,2020-06-05 19:23:13,1591374193.0,dataengineering,Data Tools for a small company,gx7p81,unclickablename,,https://www.reddit.com/r/dataengineering/comments/gx7p81/data_tools_for_a_small_company/,2.0,38.0,0.0,14144.0,"I am NOT a data engineer. I am a computer scientist within the biosphere. Suppose I join a very small biotech company and I want to pick the right data tool, what should I use?

The requirements: easy and safe storage, easy access for visualisation. Affordable. It does not need to be for crazy amounts of data or high number of queries. 

Basically I'm looking for a data management and business intelligence tool in one. Or multiple tools if necessary.

Any input appreciated!"
2370,2020-06-06 04:08:35,1591405715.0,dataengineering,Difference between a Database Administrator and Data Engineer?,gxharl,dbthrowaway28p,,https://www.reddit.com/r/dataengineering/comments/gxharl/difference_between_a_database_administrator_and/,1.0,8.0,0.0,14164.0,"Title. 

Thanks!"
2371,2020-06-06 19:18:26,1591460306.0,dataengineering,Udacity Dataengineering Nano degree won’t send me AWS credits,gxtvcb,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/gxtvcb/udacity_dataengineering_nano_degree_wont_send_me/,1.0,10.0,0.0,14184.0,"Has anyone else taken the nano degree and had the issue where they haven’t sent the credits for days? This has stalled my progress for a few days now and I’m considering just paying for the usage myself.

I’m frankly really disappointed by their customer service. 

Also, what they charge (ignoring their free month) vs what you get is so off base.

I have a Dataquest subscription as well and I feel overall it’s a way better experience even though just limited to mainly to python and sql. I signed up for udacity because ppl had said its more in depth than Dataquest and that has not been my experience. It might be broader but not deeper."
2372,2020-06-07 00:22:42,1591478562.0,dataengineering,"What's a ""typical"" DE toolset and workflow? What's yours?",gxzbfm,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/gxzbfm/whats_a_typical_de_toolset_and_workflow_whats/,1.0,11.0,0.0,14198.0,"Hi all, 

I'm a DS intern tasked with some light DE tasks and looking to find a DE role after this internship ends (tired of DS, but that's another story...) 

I'm currently taking the udacity DE nanodegree course, and while it's pretty good, they do most things via jupyter notebooks which isn't really realistic (although I understand why they do things this way).

I'm curious about how you professional engineers go about doing things... there doesn't seem to be much online about this (or I'm bad at searching). 

So, how do you do things at your job? What are some DE best practices? 

Here are some questions I have: 

1. When writing code, do you use an IDE or are most things done in the command line and/or with vim?
2. What do use to test SQL queries? (e.g. IDE plugin, Jupyter, dedicated SQL client, etc)
3. What tools do you use to monitor the health/status of pipelines?
4. How do you store/retrieve credentials to databases/web services?
4. For online services, do you manage clusters/instances via the web interface or all through code?

I'd love any additional input you all have! Thank you"
2373,2020-06-07 02:00:23,1591484423.0,dataengineering,Udacity nanodegree: alternative Udemy course?,gy0w3y,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/gy0w3y/udacity_nanodegree_alternative_udemy_course/,1.0,8.0,0.0,14202.0,"The Udacity nanodegree is often recommended here but I can't afford this (yes I know about the free one month offer). Udemy is having a sale in Australia at this moment and after purchasing two of Jose Portilla's Python and SQL courses, I'm hooked. So is there a good course on udemy that can be considered as alternative to Udacity nanodegree on Data Engineering?"
2374,2020-06-07 02:25:59,1591485959.0,dataengineering,Airflow config file corrupted. How do I regenerate it?,gy1c1x,ploughthrough,,https://www.reddit.com/r/dataengineering/comments/gy1c1x/airflow_config_file_corrupted_how_do_i_regenerate/,1.0,3.0,0.0,14203.0,"Hi guys, 

I was new to airflow and I somehow corrupted the config file while struggling to get it working on windows. I uninstalled the docker and reinstalled everything, but the config file seems to still be corrupted. I was wondering how could I reset/regenerate this file?

below is a screen shot of how my config file looks now when I do vi \~airflow/airflow.cfg

Thanks

![img](xk3xa7l0jd351)"
2375,2020-06-07 08:07:57,1591506477.0,dataengineering,Covid-19 Infected Ranking | TOP 10 Country since 31 Dec 2019 (Update to 06 Jun),gy6cp0,datavtworld,,https://www.reddit.com/r/dataengineering/comments/gy6cp0/covid19_infected_ranking_top_10_country_since_31/,1.0,0.0,0.0,14216.0,
2376,2020-06-07 10:03:15,1591513395.0,dataengineering,Best practices for automating Aws workflow using python/aws-cli ?,gy7oml,yemeraname,,https://www.reddit.com/r/dataengineering/comments/gy7oml/best_practices_for_automating_aws_workflow_using/,1.0,13.0,0.0,14218.0,"Hi all!   
I'm fairly new in the aws setup (data scientist with light DE work for productionization )and would like to learn how experienced folks manage their cloud stuff.   
Here is my stack: 

* Lambda: Fairly small code that i basically wrote on the aws console itself. It is connected to API GW and gets data from various sources. Now as the code base is getting bigger I'm feeling that I'll have to migrate to the zipping and uploading option of lambda. Also maybe I need a git repo for it. 
* Ec2 : 
   * Python pipelines, that take data from RDS and send various output files to dynamoDB and S3. Till now it was in testing phase so I was doing it manually but now I want to automate this part maybe with a cron.
   * Jupyter notebooks, that sometimes work parallel with the pipeline and generate some charts for business use case. 

My current workflow is that I spin up the instance then I ssh into it and use codeserver to get vscode environment for writing code for pipeline and Jupyter notebook for notebooks.  
I'm thinking of at least automating the starting-and-ssh-ing part, from some cli/boto3 based option   


Thanks, how do you guys do it? any tips"
2377,2020-06-07 12:51:33,1591523493.0,dataengineering,Apache airflow,gy9fg6,djkaffe123,,https://www.reddit.com/r/dataengineering/comments/gy9fg6/apache_airflow/,1.0,7.0,0.0,14225.0,"Hi,

I'm considering using airflow for my ML pipeline on AWS, and I have some questions about basic setup (perhaps noobish). 

I would prefer a serverless setup, but that seems out of the question in the cloud, since you have to host an airflow server?

The airflow server seems like a bit of an overkill, and I'm unsure how to control costs, scaling, python environments and memory/cpu/gpu requirements to each pipeline step?

Alternatively I thought about using it as 'command station', where the ML pipeline just delegates pipeline tasks to fargate and lambda functions - this seems nice, because I can get all the flexibility that I want using those other services, while keeping the server small.  


The reason I want to use airflow is of course because it seems awesome, and I would like to have that overview over the pipes etc. I could just setup a bunch of lambdas/containers, but I would never know if anything had failed.  


Much appreciated!"
2378,2020-06-07 13:44:51,1591526691.0,dataengineering,Struggling to understand ETL with Airflow,gya2pn,niix1,,https://www.reddit.com/r/dataengineering/comments/gya2pn/struggling_to_understand_etl_with_airflow/,1.0,21.0,0.0,14226.0,"Currently learning Airflow and ETL pipelines in general. Have read the airflow docs, watched a few pydata talks on it too and have decided to start an ETL project to properly cement the ideas.

I've decided to build a daily ETL pipeline that gets my sleep data from the fitbit api and some weather data and transform it and then store it in a data warehouse (maybe BigQuery??). So it's not a complicated pipeline.

I'm really trying to follow best practices like making my tasks idempotent but I'm struggling to see how thats possible whilst making separate tasks for extract and transform and load operators. 

I feel like it's best to separate the extraction and transformation tasks (both PythonOperators) but I'm not sure how to do that without having to use something like S3 to transfer data between them. Is using XCom a bad idea to send data between tasks? I feel like introducing an S3 bucket really overcomplicates this pipeline. 
Also I'm not sure if it's possible to test a transformation task that requires XCom because of the dependency of the extraction task.

Any help would be appreciated, thanks."
2379,2020-06-07 18:33:22,1591544002.0,dataengineering,Why don't more data engineers use SAP?,gyeh8l,kenglunkekl2,,https://www.reddit.com/r/dataengineering/comments/gyeh8l/why_dont_more_data_engineers_use_sap/,1.0,11.0,0.0,14237.0,"SAP Hana, SAP data services, etc.

I have a job where SAP is being used heavily on the data engineering / backend side of things, and it's quite common for companies to use SAP where I live. 

but SAP isn't something you hear of much on here. Why is that?"
2380,2020-06-07 19:28:01,1591547281.0,dataengineering,Best Practice for Data Dedup,gyfemn,helpmedev33,,https://www.reddit.com/r/dataengineering/comments/gyfemn/best_practice_for_data_dedup/,1.0,5.0,0.0,14240.0,"Hey guys not sure this is the right sub but wanted to get some answers on best practices for small scale data deduplication. My use case is I want to call an API using a lambda function that inserts new data into a MySQL database. There should already be data in the database that can be identified by a primary key.

When I request the data from the API, all the data that is in the database should also be in the response as well as the new data (think under 1000 rows). How would I go about de duping the data before insert? 

My thought is first create an array of primary keys by querying the DB table, then create an array of all the primary keys from the request, create a new list of only new primary keys and only insert the new values based off the new primary key list. 

Does this make sense? Or is there a better method?"
2381,2020-06-07 20:29:48,1591550988.0,dataengineering,How to extract key value pair from scanned PDF file ??,gygi6z,dsudhansu,,https://www.reddit.com/r/dataengineering/comments/gygi6z/how_to_extract_key_value_pair_from_scanned_pdf/,1.0,1.0,0.0,14244.0,
2382,2020-06-08 04:08:28,1591578508.0,dataengineering,Data engineering career path.,gyoyel,ultrajedii,,https://www.reddit.com/r/dataengineering/comments/gyoyel/data_engineering_career_path/,1.0,21.0,0.0,14271.0,"Hello everyone, I hope you’re all doing well. I am just wondering what career path (job experiences) everyone began with prior to become a data engineering. I’m interested in going into data engineering. I am studying computer science at a university with relatively no experience in the data engineering field. I do have a couple summer internships on my resume in software development, but nothing data related. How could I begin my journey to becoming a data engineer? Help desk job? More internships? AWS certificate? Any advice would be greatly appreciated!! Thanks in advance."
2383,2020-06-08 04:10:27,1591578627.0,dataengineering,Is data engineering going to die / be automated soon?,gyozi4,kdagawdfrommisc,,https://www.reddit.com/r/dataengineering/comments/gyozi4/is_data_engineering_going_to_die_be_automated_soon/,1.0,25.0,0.0,14271.0,"There are more and more ETL tools coming to market that even non-technical people can use with ease.

There's a bunch of automation now when moving data around, and several prebuilt data warehouse solutions / OLAP databases that can fit with any business data with extremely minimal effort."
2384,2020-06-08 08:12:59,1591593179.0,dataengineering,Covid-19 Death Ranking | TOP 10 Country since 11 Jan 2020 (Update to 06 Jun 2020),gyskh6,datavtworld,,https://www.reddit.com/r/dataengineering/comments/gyskh6/covid19_death_ranking_top_10_country_since_11_jan/,1.0,0.0,0.0,14279.0,
2385,2020-06-08 10:44:38,1591602278.0,dataengineering,Tools for data pipelines in company where R is widely used,gyutme,less83,,https://www.reddit.com/r/dataengineering/comments/gyutme/tools_for_data_pipelines_in_company_where_r_is/,1.0,3.0,0.0,14282.0,"Hi, 

I'm exploring alternatives to the current setup for running reoccurring data pipelines. Now we're having pipelines set up in TeamCity running bash scripts and R scripts in docker containers to do batch processing (not big data).

What tools/frameworks could be worth exploring when R is important to execute models?"
2386,2020-06-08 11:49:22,1591606162.0,dataengineering,Struggle running Kafka on WSL and make producer on windows,gyvplz,[deleted],,https://www.reddit.com/r/dataengineering/comments/gyvplz/struggle_running_kafka_on_wsl_and_make_producer/,1.0,0.0,0.0,14378.0,
2387,2020-06-08 14:08:12,1591614492.0,dataengineering,Need advice,gyxc23,red_abhi,,https://www.reddit.com/r/dataengineering/comments/gyxc23/need_advice/,2.0,2.0,0.0,14385.0,"To the great people of this sub, I need some advice. I am graduating with a computer science degree this year and got a job as a data engineer in a fast growing startup from my college placements. I honestly had no clue about what a data engineer does before the interview and I got picked up based on my decent analytical skills. I know the basics of SQL, can code basic numpy pandas on python and have average skills on data structures(C++). I start my job October and I wanna know what I can learn so that I can be a high performing employee. Can someone suggest some online courses, I am confused there are so many. Willing to spend a few bucks if that helps."
2388,2020-06-08 14:32:34,1591615954.0,dataengineering,#Updated. Top country by total confirmed covid19 cases from 31 December,gyxncn,professional3512,,https://www.reddit.com/r/dataengineering/comments/gyxncn/updated_top_country_by_total_confirmed_covid19/,1.0,0.0,0.0,14385.0,
2389,2020-06-08 15:17:50,1591618670.0,dataengineering,PostgreSQL - Kafka Architecture suggestions,gyy95t,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/gyy95t/postgresql_kafka_architecture_suggestions/,2.0,7.0,0.0,14385.0,"Hello folks,

Some of you maybe getting used to see me in this subreddit asking stupid questions :(

today im back, recently im researching about kafka, and i plans to use it to improve my company data stream. Here is my design.

[data stream design \(sorry for white background\)](https://preview.redd.it/kczmy0x3ho351.png?width=795&amp;format=png&amp;auto=webp&amp;s=19cba361261869832db171313dcf4e9727742a54)

My company is small-to-mid size, but extensive on reporting and currently are building some predictive modelling, im planning to use this architecture on my company data stream, the target can be analytical engine, etc.

what do you think guys ? any suggestions are welcome and im still learning on Data Engineering"
2390,2020-06-08 15:49:37,1591620577.0,dataengineering,An interview with Astasia Myers of Redpoint Ventures on the data management industry trends that she is paying attention to as an investor.,gyyppv,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/gyyppv/an_interview_with_astasia_myers_of_redpoint/,1.0,0.0,0.0,14385.0,
2391,2020-06-08 16:28:51,1591622931.0,dataengineering,Webinar on Introduction to Computer Vision,gyzc3l,[deleted],,https://www.reddit.com/r/dataengineering/comments/gyzc3l/webinar_on_introduction_to_computer_vision/,1.0,0.0,0.0,14386.0,
2392,2020-06-08 16:29:48,1591622988.0,dataengineering,WTF is data oriented programming??,gyzcmi,romanX7,,https://www.reddit.com/r/dataengineering/comments/gyzcmi/wtf_is_data_oriented_programming/,18.0,14.0,0.0,14386.0,"Iv come across the term ""Data Oriented Programming"" a few times now and still haven't found a solid article or video that gives a quick and simple overview of what the idea is.

The one video I came across that seemed promising mentioned that it was specific to languages like C and C++.

Can anyone give a quick overview of what data oriented programming is or point me to a good resource? Also, does it apply to python?"
2393,2020-06-08 18:52:25,1591631545.0,dataengineering,Any tips about this data ingestion process?,gz1vlk,aleebit,,https://www.reddit.com/r/dataengineering/comments/gz1vlk/any_tips_about_this_data_ingestion_process/,1.0,1.0,0.0,14393.0,"First of all, englesh is not my first language (any Brazilian here?)

I would like your opinion about the Hadoop ingestion process I am developing.
I need to ingest files from ftp server to HDFS.
In FTP, I have a zip file (20GB), inside this I have 10 .csv files (one with 90GB).
The ultimate goal is to make csv available on HDFS, without any transformation.

I'm currently doing:
     - python script to download the zip on Linux (ftplib);
     - python for unzip in a directory other than the OS.
     - bash put files inside HDFS (hdfs dfs put).

This is a short version, I have some log functions in python, inserting the log in a hive table, among other things.

Is there a better way to insert these .csv files into HDFS?  Any way to do the download directly on HDFS and not on the OS?
Any tip is welcome."
2394,2020-06-08 23:27:52,1591648072.0,dataengineering,How to handle a multitenant data warehouse?,gz7ani,iepsenn,,https://www.reddit.com/r/dataengineering/comments/gz7ani/how_to_handle_a_multitenant_data_warehouse/,1.0,3.0,0.0,14396.0,"Hi,

I'm a very noob in the topic, and I actually want to know how to handle a multitenant data warehouse (each customer has a unique schema or a global schema that abstract different type of information in data)? 

Which tools I would use? I was thinking in Delta Lake from DataBricks, but I don't know if it's ok for my needs...

Any suggestion?"
2395,2020-06-09 03:19:06,1591661946.0,dataengineering,Does anyone keep a portfolio page?,gzbuyg,cannablubber,,https://www.reddit.com/r/dataengineering/comments/gzbuyg/does_anyone_keep_a_portfolio_page/,20.0,6.0,0.0,14406.0,"I usually just link my GitHub for job apps, but as I learn more, I find that my GitHub is more about other things I'm interested like ML, datascience, or random pet-projects. I do so much DE work during the week, but my resume just represents it with a couple of dots on a page and it lacks representation on my GitHub as well. So, I'm wondering if any DEs keep portfolio web pages or similar to discuss, in depth, the projects they have worked on and any details that get lost in the simplicity of a resume."
2396,2020-06-09 09:57:55,1591685875.0,dataengineering,Data Reporting Analyst to Data Engineering,gzhwp1,palendrome298,,https://www.reddit.com/r/dataengineering/comments/gzhwp1/data_reporting_analyst_to_data_engineering/,7.0,13.0,0.0,14422.0,"I work as a data reporting analyst and I build reports in python while extracting that data from databases using SQL. 

Skills: MS Access, MS Excel, JIRA, SQL, Python, R, VBA, Oracle DB, OBIEE, Teradata, Tableau and IT Risk Management. Not all my skills come from my current job.

I would like transition to Data Engineering but I’m not 100% sure where to start. Here’s my idea of basic data engineering from reading sources:

Find some data that you can get on a recurring basis (daily/hourly/etc). write a python script to get that data. Clean that data. Store the data in a database. Write a shell script to automate it so that its getting this data on a daily/hourly/etc basis.

Here are my questions:
-	There’s no data that’s not already cleaned that you can get on a recurring basis. So is it really ETL or just EL?
-	Is everyone using a programming language to clean the data or are there tools that does this and you just push a button? 
-	Would calling the python script in a bat file and putting it on a task scheduler work as shell scripting it? Or does everyone use Linux and their bash shell script?
-	Is my idea Data Engineering? or would I need to incorporate it with new technology(Spark, Cloud databses, Hadoop) for it to be? 
-	If I need to incorporate it with new technology what points in my idea will those new technologies need to be placed that’s if my idea is the basis of data engineering?"
2397,2020-06-09 15:17:54,1591705074.0,dataengineering,Linux Data Visualisation Tools.,gzm0d8,welschii,,https://www.reddit.com/r/dataengineering/comments/gzm0d8/linux_data_visualisation_tools/,1.0,3.0,0.0,14429.0,"Apologies if this is in the wrong forum, but I am looking for a way to present and visualise data using Ubuntu, in the absence of power BI or tableau. It's for a job interview as an analytics manager, previously I have worked as an analyst and engineer."
2398,2020-06-09 16:38:46,1591709926.0,dataengineering,Webinar on Introduction to Computer Vision,gznbe8,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/gznbe8/webinar_on_introduction_to_computer_vision/,1.0,0.0,0.0,14430.0,
2399,2020-06-09 17:59:01,1591714741.0,dataengineering,Struggling to understand Airflow infrastructure,gzoqxw,Sk1tterSlumber,,https://www.reddit.com/r/dataengineering/comments/gzoqxw/struggling_to_understand_airflow_infrastructure/,16.0,9.0,0.0,14434.0,"Help! I am trying to set up airflow in kubernetes and am having trouble finding the “best practice” 

So i have a repo full of python code with bash scripts used to trigger said python code. Before i was using cron to schedule my tasks, but am finding it becoming more complicated. After deciding to try out airflow and messing with dags for the past week i finally got a hang of using bash operators to run my pipeline locally. Now i want to try using k8s operators to run my dags on k8s. I have forked the airflow repo and tested the docker image out. What I am struggling with is how are all this and my dags deployed to k8s? Do i have a different repo for all my dags and have each dag pull my python code on every run? If so do I have to rebuild my airflow image every time i create a new dag or change my python code base?

Would appreciate any kind of help, thanks if you can point me to any good resources. Sorry if my English is bad Im not a native speaker."
2400,2020-06-09 18:51:49,1591717909.0,dataengineering,What is Data Engineering in 2020 ?,gzprhv,Raghu1982bakki,,https://www.reddit.com/r/dataengineering/comments/gzprhv/what_is_data_engineering_in_2020/,7.0,9.0,0.0,14435.0,"Data Engineering ?  


NewVanage Partners has surveyed Fortune 1000 business &amp; technology and found that an immense amount of increase in the pace of investment being spent between 50 million to 500 million and above on BigData/AI initiatives. Despite the investment in Bigdata/AI has increased by over 66% in 2019, but the confidence among organization in measuring themselves in business results has decreased to 19% ....  


In this [article](https://devopsdummies.com/modern-data-engineering-in-cloud-platform/) you can get an idea on What is Data Engineering today and high level stages of a data pipelines."
2401,2020-06-09 21:51:11,1591728671.0,dataengineering,Free COVID-19 News API,gztdc9,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/gztdc9/free_covid19_news_api/,0.0,0.0,0.0,14438.0,
2402,2020-06-09 21:57:28,1591729048.0,dataengineering,"Hey guys, my name is Brian Zheng and I am the inventor of the Pop-Up Sunshades. There are so many things I wish I could tell my past self when I first started off as an inventor and entrepreneur! For all of you who are just starting off, what are some things you’d like to learn about the industry?",gzthvs,brianzhengitm,,https://www.reddit.com/r/dataengineering/comments/gzthvs/hey_guys_my_name_is_brian_zheng_and_i_am_the/,0.0,3.0,0.0,14438.0,"And feel free to join my FREE LIVE OPEN ZOOMINAR tonight!

[https://www.ideatomillion.com/live](https://www.ideatomillion.com/live)"
2403,2020-06-10 00:38:01,1591738681.0,dataengineering,Does anyone know of a NoSQL DB Difference cheat sheet for System Design interviews?,gzwv7x,flamez_callahoon,,https://www.reddit.com/r/dataengineering/comments/gzwv7x/does_anyone_know_of_a_nosql_db_difference_cheat/,10.0,5.0,0.0,14439.0,"I’m looking for some high level overview on when I’d want to choose between, say, Redis or DynamoDB, or key-value vs wide-column, and why exactly. Just looking for enough that would get me through a system design interview."
2404,2020-06-10 01:56:34,1591743394.0,dataengineering,Sftp-lib on pyspark,gzyg0o,aleebit,,https://www.reddit.com/r/dataengineering/comments/gzyg0o/sftplib_on_pyspark/,2.0,0.0,0.0,14443.0,"Hi guys,

Anyone can make this work on jupyter notebook?
https://www.jitsejan.com/sftp-with-spark.html

I just don't know how use this jar as dependency..

Any help?"
2405,2020-06-10 06:22:23,1591759343.0,dataengineering,Looking to Help,h02s90,smccaffrey,,https://www.reddit.com/r/dataengineering/comments/h02s90/looking_to_help/,32.0,47.0,0.0,14454.0,"I’ve been looking to get out of my comfort zone lately, and wanted to start with helping people on this subreddit in anyway that I can.

For context, I’ve been working in data for a few years, I have a BS in Physics and a MS in Business Analytics, and for the last year and a half I’ve been working at a data engineer at an as tech startup. We ingest and process about 2 TBs of new data each day, and we analyze about 30 PBs of data/year.

Professionally I have intermediate to advanced experience with with Python, Java, Airflow, SQL, BigQuery, redshift, etc. I’ve worked with all major cloud providers, and have worked a lot on the devops side of data engineering.

For non technical experience, I’ve spent a lot time designing data products, managing projects, working with executives, designing dashboards, interviewing data engineering candidates, and just talking with business folks with the intent to create trust in data.

Ask me whatever you want and I’ll give my honest opinion/answer, and if I don’t know the answer I’m hoping to learn from you."
2406,2020-06-10 11:01:57,1591776117.0,dataengineering,How is Data Engineering in your company?,h06ep3,[deleted],,https://www.reddit.com/r/dataengineering/comments/h06ep3/how_is_data_engineering_in_your_company/,39.0,29.0,1.0,14457.0,"To preface this question I work for one of the FAANG big tech companies as a Data Engineer and have had prior experience in a few smaller companies.

My current experience in big tech is that it has been over simplified and we are re-branded BI Engineers pumping out SQL queries day in and day out.

The pay is very good but the stocks I get are almost half of what a Software Engineer earns and the yearly pay is slightly lower than a Software Engineer as well. This is moderately fair considering the smaller scope of work I do compared to them.

In my previous smaller companies they did not have a clear definition of the role and I would find myself doing a breadth of very many different things - one week I would be a Platform Architect drawing out plans to migrate off of Microsoft SQL to Cassandra for certain work load, other week I would be a DB Admin sharding MYSQL databases and another I would be a Systems Engineer containerising services to interact with data sources. I was also paid more than the average Software Engineers due to the very large scope I worked on contrary to their traditional front-end/back-end role definition. 

This could be a side effect of big tech being more organised than the smaller ones. The type of work I described in the smaller tech is still present in big tech but they are just Software Engineers that have more precisely carved roles rather than a Jack of all trades (Master of none) in my previous experiences. 

After almost 2 years of working in big tech, I've become very unsatisfied with my role and it doesn't feel worth the money I'm earning and trying to test the job market for roles that would be a better fit. It is very likely I take a massive pay cut as I've also gotten very good raises year on year. That is something I would have to evaluate for a potential career change as well for my continued expenses (rent, food, leisures, etc.).

I have the option to switch to Software Engineer in the big tech but I would have to pass a few rounds of interviews and my pay would not change significantly to be equal to my peer Software Engineers, so I would pretty much take on bigger scope but for similar pay which seems very unfair and would be better off to quit and re-apply to the big tech firm again as a Software Engineer. 

I get the feeling we are at a crossroads similar to Data Scientists where every company has a different definition of the role (Data Analyst, Machine Learning Engineer, Statistician, etc.) and lead to similar disappointments in role expectation and the actual work involved.

Anyway, enough ranting, what do you folks think the market is like out there for Data Engineers and how is it in your company? 

Should I just give up pursuing Data Engineer roles and rebrand myself as Software Engineer with Data Infra expertise and begin looking for similar roles?"
2407,2020-06-10 14:27:56,1591788476.0,dataengineering,Papaya Production Ranking | TOP 10 Country from 1961 to 2018,h090jq,datavtworld,,https://www.reddit.com/r/dataengineering/comments/h090jq/papaya_production_ranking_top_10_country_from/,1.0,0.0,0.0,14461.0,
2408,2020-06-10 15:44:32,1591793072.0,dataengineering,Seeking Data Analyst -&gt; Data Engineer Resources,h0a57d,bohemian03,,https://www.reddit.com/r/dataengineering/comments/h0a57d/seeking_data_analyst_data_engineer_resources/,1.0,1.0,0.0,14463.0,"Hello,

I currently work as a data analyst with the Business Intelligence team. We are about 4 people and we have just recently step up different development paths. One being the analytics path and the other being the engineering path. I've decided that I would like to take the engineering path, but I was wondering if there are any online courses that could point me in that direction.

Our team is focused on building our data infrastructure right now. We use AWS/Redshift and a lot of our ETL process is built using JavaScript. I've been working as a data analyst for about 2 years so I have knowledge of SQL, Python and databases.

Do you have any tips and recommendations for resources to learn the following?

* design patterns
* orchestration and pipelining methods
* tracking implementation
* data pipelines
* ETL processes 
* JavaScript
* how to write production-quality code

If anyone has recommendation for an overall data engineering course, that would be even better. Thank you so much.

EDIT: I think something like the Udacity's DE nanodegree would be great, but it is unfortunately too expensive for me."
2409,2020-06-10 16:03:21,1591794201.0,dataengineering,Advice after completing Udacity Nano Degree,h0ahkc,IrCuriouss,,https://www.reddit.com/r/dataengineering/comments/h0ahkc/advice_after_completing_udacity_nano_degree/,2.0,4.0,0.0,14464.0,"Hello,

I recently completed the Udacity Data Engineering Nano Degree and I think it was a good introduction course about data engineering tools and concepts. I would like to try implement it at my work and also proceed with my self-learning, therefore, I would like to ask for some advice :

1) At my work, there is an ETL pipeline that load measurement data to multiple tables based on experiment year: *measurement\_2008, meausurement\_2009, measurement\_2010,...,measurement\_2020.* The database designer has since left and I volunteered to maintain (and upgrade, if possible) this database. With my beginnner SQL knowledge, querying all measurement data from these tables at once will require a lot of joining, and the number of tables will keep increasing as the years change.

If I understand correctly from the distributed database concept, it is possible to only have 1 table *measurement* and partitioned efficiently across the clusters, but I am not sure how to transition from the yearly-split tables to a distributed database. How should I migrate the spllit-tables to one table and use Spark (or Redshift) to handle the partitioning? 

2) I am still a bit confused in about database design /architecture for big data application. I want to learn more about how to choose an efficient ""sort"" or ""dist"" in a database design for faster access/query of big data. Is there a book or reading materials that can help me to learn about this database design concept?

3) For experienced data engineers, may I ask your advice if I should first deepened my understanding in Python and SQL for database design or should I first explore the new tools such as Docker and Kubernetes ?

Thank you in advance for your advice."
2410,2020-06-10 18:38:04,1591803484.0,dataengineering,How to get a job in data engineer,h0ddmk,cglez1280,,https://www.reddit.com/r/dataengineering/comments/h0ddmk/how_to_get_a_job_in_data_engineer/,0.0,0.0,0.0,14465.0,Hey guys this is my first time posting on this subreddit. I’m a rising junior pursuing a math and computer science degree with a minor in data science. I also have done data science research and plan on continue doing it for the rest of my time at college. So far I have one project done but I realized data science doesn’t have a lot of programming. I’ve thought about data engineer and after reading seem to like it but I’m not sure how to go about getting an internship for summer 2021 in data engineer or a job after college. Any advice would be greatly appreciated.
2411,2020-06-10 22:05:01,1591815901.0,dataengineering,What is a Unified Data Model?,h0hoc7,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/h0hoc7/what_is_a_unified_data_model/,1.0,3.0,0.0,14469.0,Not sure if this is standard DE definition or jargon specific to this company I'm working with.
2412,2020-06-10 22:19:09,1591816749.0,dataengineering,Guidance to become a good data engineer,h0hz0j,farooqkaziz,,https://www.reddit.com/r/dataengineering/comments/h0hz0j/guidance_to_become_a_good_data_engineer/,3.0,5.0,0.0,14469.0,"Hi, I want to learn the technical tools, languages, and concepts to become a good data engineer. Can you tell me any one resource just like one-stop-shop for all the ingredients required for becoming a good data engineer, and start a self project to help me get entry level job as a data engineer. My current background is data reporting and analytics on datawarehouse side where I build on premises data marts on top of datawarehouse through SSAS Tabular and SAP BusinessObjects Universe, and build reports and data visualization by utilizing Crystal reports and power BI. 

Regards, 
FA"
2413,2020-06-10 23:28:53,1591820933.0,dataengineering,Can Luigi be used for this kind of a use-case(details insidee)?,h0jg7r,ThiccShadyy,,https://www.reddit.com/r/dataengineering/comments/h0jg7r/can_luigi_be_used_for_this_kind_of_a/,3.0,3.0,0.0,14471.0,"I have a particular dataset which is over 3 million rows and corresponding to this dataset, there is a collection of hundreds of csvs which are semantically related to the dataset but cannot directly be joined with it. Furthermore, the csvs each can have dozens of fields themselves.

My current workflow involves doing some basic cleaning and pre-processing of the 3 million row dataset, choose specific fields from some specific(of the 100+) csv files and put this together in json format before reading it as a dataframe and joining with the 3 million row dataset in pandas. Can this kind of workflow be automated with Luigi? Also, since the workflow is a precursor to an ML model, can it store generated visualizations (via Matplotlib or other) for me?

I have no previous experience with Luigi or data engineering in general, so I'd appreciate any inputs here on whether this can be done, or if another tool would be better suited for it"
2414,2020-06-11 00:02:22,1591822942.0,dataengineering,Am I a data engineer?,h0k5o0,bis_fury_war,,https://www.reddit.com/r/dataengineering/comments/h0k5o0/am_i_a_data_engineer/,16.0,8.0,0.0,14473.0,"I'm currently quite satisfied with my job but always looking at how to position myself for future work. My issue is my title is Application Developer. 

I don't develop any applications really. 

I'm kind of like a end of pipeline data engineer? 

I work for a place that ingests a TON of data and we have people who scoop up all of the data and maintain the pipeline for the entire company. 

I work for a specific department offering them MUCH more from the pipeline than anyone else is getting. 

To get at the data you basically need to know SQL and then to learn something from it you must program. 

So I focus on pulling the data out of the data warehouse by maintaining a library we wrote in python. This way someone who works in my dept is able to ask me to find something very specific that requires 40 steps and hundreds of lines of code. I am then able to do all of that after understanding what they want and deliver the data to them in a clean easy to ingest format. 

I also do some genetics sometimes, I just needed to learn how to do technical things like run a GWAS so I did it. 



Sometimes I do statistics stuff for some data. 

I also help maintain and administer a server with end users.

I also wrote a very custom pipeline using bash/python for a specific group at my company to take their very dirty dirty and clean it and put it into a simple postgres db. 

How do I move this job forward over the next 5 years and if they dropped me today what the hell would I apply for?

I also should mention that I have a computer science degree. 

Oh and at times I'm also asked to machine learning and visualization stuff. So I implement a random forest algorithm or any of the other reasonably simple machine learning approaches.

I also maintain an internal tool/website so I guess that's my app but it's about 0.1% of my job

I'm definitely learning a lot and growing but I just don't really know... I feel like I'm a Data Engineer/Data Scientist/Developer/Sys Admin

GWAS: https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies"
2415,2020-06-11 01:35:25,1591828525.0,dataengineering,How much do Data Engineers in Toronto make,h0m1zd,chmod0644,,https://www.reddit.com/r/dataengineering/comments/h0m1zd/how_much_do_data_engineers_in_toronto_make/,7.0,1.0,0.0,14479.0,"Hello everyone, I live in Toronto and have been working in the Data ecosystem for about 4 years now , previously as a Data analyst and now as a Data Engineer. My recent data engineering experience was with SSIS. No python, no fancy stuff in cloud just plain SSIS. A position I was looking at has its tech stack in AWS ecosystem and I was offered 85K/yr which sounded rather low for a Data engineer. Just wondering what the going rate is for Data engineers in the greater Toronto area in general."
2416,2020-06-11 07:20:15,1591849215.0,dataengineering,Amazon SQL interview,h0rwuf,manofsteele414,,https://www.reddit.com/r/dataengineering/comments/h0rwuf/amazon_sql_interview/,38.0,10.0,0.0,14482.0,"I have my Amazon first SQL Interview for BA in a week and I wanted to know what are the tips for this interview journey from the people who have already aced it or even failed it?

Also what are some of the effective study sources to practice SQL for interview, I am pretty intermediate with SQL data analysis and has been practicing LeetCode for sometime."
2417,2020-06-11 11:36:25,1591864585.0,dataengineering,"Starting a small business to provide data insights to small businesses in a small town area, based on open source and raspberry pi",h0vayh,MacFarl8ne,,https://www.reddit.com/r/dataengineering/comments/h0vayh/starting_a_small_business_to_provide_data/,3.0,2.0,0.0,14482.0,"I have 13 years of Sys Admin expierence, im addicted to data but cant quite satisfy this need. In my actual job (less sysadmin) im fiddling with python, pandas, Holoviews, Matplotlib, jupyther notebook, data visualisation, data cleaning and transforming different inputs (rest, xls, csv,xml) into cleaned output data for the last 4 years. Additionally i know my way around linux, raspberry,docker, webscrapping, scrapy, selenium, git, sql

Reading alot about big data and etl tools ( no hands on expierence) i was thinking about putting all this skills together and starting a business, which focusses especially at small businesses to setup open source tools, to provide them insight on their data. Living in the middle of germany in a small town area, there are plenty of small town businesses. The ""vibe"" is different then big businesses, so things move slow and budgets are small

The focus would be on businesses, which have maximum 2,3 key applications which are holding their data. The normal approach would be to use something like microsoft BI to access the databases, transform data and produce charts all in one application. But i like to set an open source  foundation, based on big data Principles. This means ETL, Data Warehouse and Business Analytic all based on Open Source and running on cheap local raspberry pi hardware. Im very good in self learning, so the open source tools should have a good documentation. 

There wont be anything cpu heavy like realtime data, etc. so i guess one or multiple pis would be sufficient. Most basic tasks would be small stuff to start like dashboards to sum up all orders by week, top 10 customer sales, show Website visits, etc. If then the need for more and more data insights rise, i could move the tools i use to stronger hardware, etc and grow step by step.

So in short as a first step i want to start with cheap and easy replacable hardware, configure it after "" big data principles"" with ETL , Datawarehouse and Business Analytic/Visualisation

So this is my idea.
1 choose good open source etl, database and business analytics tool (elastic stack?)
2 install them on raspberry pi (one or multiple)
3 go to customer, analyze and access and join different data
4 create dashboards and reports

If someone could push me in the right direction and offer some advice, that would be great."
2418,2020-06-11 11:41:52,1591864912.0,dataengineering,Apache Flink: Stateful Functions 2.1.0 Release Announcement,h0vdd0,Marksfik,,https://www.reddit.com/r/dataengineering/comments/h0vdd0/apache_flink_stateful_functions_210_release/,1.0,0.0,0.0,14482.0,
2419,2020-06-11 11:47:14,1591865234.0,dataengineering,Apache Spark + Holistics,h0vfpa,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/h0vfpa/apache_spark_holistics/,2.0,0.0,0.0,14482.0,"For those who use Apache Spark with Holistics, how do you guys integrate the result from pyspark into holistics ?"
2420,2020-06-11 14:05:41,1591873541.0,dataengineering,Strawberry Production Ranking | TOP 10 Country from 1961 to 2018,h0x7yy,datavtworld,,https://www.reddit.com/r/dataengineering/comments/h0x7yy/strawberry_production_ranking_top_10_country_from/,1.0,0.0,0.0,14484.0,
2421,2020-06-11 14:07:17,1591873637.0,dataengineering,Programming skills of a Data Engineer,h0x8sv,noobingaround,,https://www.reddit.com/r/dataengineering/comments/h0x8sv/programming_skills_of_a_data_engineer/,1.0,2.0,0.0,14484.0,"Hello pro Data Engineers,   


In your experience what level or what concepts of  programming are important for a Data engineer. Is covering leetcode a right path to crack interviews or you think its' too advanced. It's better to focus on broad technologies  of data infrastructure with elementary programming."
2422,2020-06-11 16:40:33,1591882833.0,dataengineering,Data Governance Tooling,h0zi83,thenabzter,,https://www.reddit.com/r/dataengineering/comments/h0zi83/data_governance_tooling/,1.0,4.0,0.0,14494.0,"Hi all,

I currently work within an analytics dept in a insurance company. The quality of the data ingested is quite poor: no data profiling, metadata is lacking etc.

Would like some recommendations on how to tackle this issue. (i.e. installing Data Quality tools, suggesting certain frameworks)"
2423,2020-06-11 17:00:35,1591884035.0,dataengineering,Reproducible Distributed Random Number Generation in Spark,h0zu97,real_trizzaye,,https://www.reddit.com/r/dataengineering/comments/h0zu97/reproducible_distributed_random_number_generation/,2.0,0.0,0.0,14495.0,
2424,2020-06-11 17:19:23,1591885163.0,dataengineering,Setting up Airflow on a local Kubernetes cluster using Helm,h106fy,iamspoilt,,https://www.reddit.com/r/dataengineering/comments/h106fy/setting_up_airflow_on_a_local_kubernetes_cluster/,15.0,0.0,0.0,14495.0,"I wrote this guide on quickly setting up Airflow on a local Kubernetes cluster using Helm package manager. It also enables you to mount a local dag directory so that you can write and update them on your local machine and run them in the K8s cluster.

This is part of my series on [Building an end-to-end data pipeline from scratch](https://medium.com/uncanny-recursions/building-an-end-to-end-data-pipeline-from-scratch-a9abc74734b2).

I will be covering the KupernetesPodOperator next and probably even do comparison with the CeleryExecutor so watch out that space.

Post link: https://medium.com/uncanny-recursions/setting-up-airflow-on-a-local-kubernetes-cluster-using-helm-57eb0b73dc02"
2425,2020-06-11 19:21:28,1591892488.0,dataengineering,Do Databricks products (i.e. delta lake) water down the experience for the Data Engineer?,h12gqe,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/h12gqe/do_databricks_products_ie_delta_lake_water_down/,1.0,7.0,0.0,14501.0,
2426,2020-06-11 22:22:26,1591903346.0,dataengineering,Data Engineering Roadmap,h16142,Demon_Slayer151,,https://www.reddit.com/r/dataengineering/comments/h16142/data_engineering_roadmap/,17.0,21.0,0.0,14507.0,"Hi so I am considering entering the fields of data engineering and thought of posting my potential roadmap and seeking advice and guidance to ensure I learn whats best during this quarantine time so I can truly make use of my time. 

Some background about me is I am going into my 4th year of studies majoring in Statistics. I had a business analyst internship that fell through due to the virus so its very likely that I will be graduating with no experience. I want to really learn the skills and have myself ready so that I can have a job when I graduate assuming this virus situation is over by then (June 2021ish is when I will graduate).

**Roadmap**

So I have read around on this sub and seen the book ""Designing Data-Intensive Applications"" and I have a got a PDF and plan on going through it. I have seen the buzzwords ""Python"", ""SQL"", ""Kafka"", ""Spark"" and etc. So I know python pretty well up to like object oriented programming. I intend to learn data scraping and like numpy, and pandas using this udemy course:[https://www.udemy.com/course/complete-python-developer-zero-to-mastery/](https://www.udemy.com/course/complete-python-developer-zero-to-mastery/)

For SQL I know very very basic queries so I intend to learn more of it using SQLBolt.

I am hoping someone can point me to what other important fundamental things I need to learn like kafka and spark and etc with some good resources

So far this is what I have and want to go through and I just wanted peoples opinions about it and like if theres anything better.  I have seen in multiple places that say ""learn fundamentals and make a good project and you can easily land a entry level data engineering role"". I just dont want people to confuse that I am under the impression it is easy to land a job hence why I am doing it. I have taken computer science courses and realized I defintely want to go into the software/technology industry and I have been trying to take a crack at webdev but really dont like it and really find data engineering very interesting. 

I would appreciate any input :)"
2427,2020-06-12 01:23:39,1591914219.0,dataengineering,What are some common evolution of a data process/infrastructure in a company?,h7811r,drecklia,,https://www.reddit.com/r/dataengineering/comments/h7811r/what_are_some_common_evolution_of_a_data/,3.0,2.0,0.0,14520.0,"For those who have had the chance to either be the first DE and work their way till a mature data team, or those who have worked multiple DE jobs at different company stages, what are some common evolutions in the process/infrastructure?"
2428,2020-06-12 02:49:27,1591919367.0,dataengineering,OKRs for a Data Engineering Team,h79kuf,VerdantScrimmage,,https://www.reddit.com/r/dataengineering/comments/h79kuf/okrs_for_a_data_engineering_team/,5.0,2.0,0.0,14522.0,"I work on a data engineering team that operates/manages our company's data platform. At the moment, I'm struggling to contribute to our team's conversation around quarterly Objectives and Key Results (OKRs). For those who may be unfamiliar, OKRs provide a goal-setting framework in which a team sets objectives that are in line with what your org or company at-large wants to accomplish, and key results are specific, measurable benchmarks that support the larger objectives.

We use AWS and do everything in the cloud. We draw in data and events from our DynamoDB/RDS backends plus product telemetry to form our data lake, and from there we have pipelines that transform that data into tables and views that are used by various stakeholders in the company (an analytics team, plus a number of product dev/engineering teams). 

I'm just having a hard time translating what we do into measurable outcomes. My mindset has always been ""get that data from over there, to over here, make sure it's right, and get it in the right shape for someone to use it."" How can we put meaningful, measurable objectives and key results on that without simply re-wording ""do your job, a-hole"" or making up numeric goals just for the sake of having numbers?"
2429,2020-06-12 04:12:38,1591924358.0,dataengineering,As a DE how often to you use hyperscaler services vs on-prem,h7b026,s4hc,,https://www.reddit.com/r/dataengineering/comments/h7b026/as_a_de_how_often_to_you_use_hyperscaler_services/,2.0,8.0,0.0,14523.0,"How often do you use one of the hyper-scalers for DE work like Azure, AWS or GCP vs using on-premise systems?
When you do use a cloud provider, which services are they and which provider? For example Azure Data Bricks etc.

I’m trying the gauge the importance/relevance/pervasiveness of cloud data skills for DEs."
2430,2020-06-12 04:43:40,1591926220.0,dataengineering,Feedback on first Airflow ETL pipeline (Fitbit sleep pipeline),h7bini,[deleted],,https://www.reddit.com/r/dataengineering/comments/h7bini/feedback_on_first_airflow_etl_pipeline_fitbit/,3.0,2.0,0.0,14523.0,
2431,2020-06-12 06:23:29,1591932209.0,dataengineering,Getting Started with Airflow,h7d21j,smccaffrey,,https://www.reddit.com/r/dataengineering/comments/h7d21j/getting_started_with_airflow/,32.0,10.0,0.0,14526.0,"I recently made a post [here](https://www.reddit.com/r/dataengineering/comments/h02s90/looking_to_help/?utm_source=share&amp;utm_medium=web2x) about wanting to help people, and something that was clear from the responses was that a lot of people are having trouble getting started. Specifically, Apache Airflow was mentioned several times. So I took a crack at writing a basic ""Getting Started"" guide for Apache Airflow

[https://medium.com/@smccaffrey70/getting-started-with-airflow-locally-bc8b84e8c016](https://medium.com/@smccaffrey70/getting-started-with-airflow-locally-bc8b84e8c016)

Please let me know if this is at all helpful. I will be trying to create similar guides for Apache Spark and Kafka, both of which are mentioned a lot on this subreddit."
2432,2020-06-12 07:00:01,1591934401.0,dataengineering,Useful interview questions for data engineer role,h7dkdr,morespacepls,,https://www.reddit.com/r/dataengineering/comments/h7dkdr/useful_interview_questions_for_data_engineer_role/,7.0,3.0,0.0,14526.0,"Heya - I’m hiring for a new data engineer role at my company, and as it’s the first time we’ve/I’ve hired hired anyone for this role I wanted to ask for some insights from you all!

If you’ve hired a data engineer, what sort of questions have you found useful in the initial stages to show skills/knowledge? Do you set practical tasks, and if so what have you found works well from a technical point of view?

If you are a data engineer, what’s the best interview you’ve had? How was it structured, or how do you wish interviews had been set up to showcase your skills best?

Sorry for the long post and very broad question, appreciate any feedback or insights you have to share!"
2433,2020-06-12 09:30:08,1591943408.0,dataengineering,"Will Data Engineer Take Over Machine Learning Work In Future, Surpassing Data Scientist?",h7fi32,analyticsindiam,,https://www.reddit.com/r/dataengineering/comments/h7fi32/will_data_engineer_take_over_machine_learning/,0.0,0.0,0.0,14527.0,
2434,2020-06-12 14:34:16,1591961656.0,dataengineering,Senior Big Data Engineer Certification - Because Big Data Doesn’t Build Itself,h7j29w,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/h7j29w/senior_big_data_engineer_certification_because/,1.0,0.0,0.0,14527.0,
2435,2020-06-12 14:45:11,1591962311.0,dataengineering,Social Media Ranking | TOP 10 Popularity from 2009 Q2 - 2020 Q1,h7j7bv,datavtworld,,https://www.reddit.com/r/dataengineering/comments/h7j7bv/social_media_ranking_top_10_popularity_from_2009/,1.0,0.0,0.0,14527.0,
2436,2020-06-12 16:13:43,1591967623.0,dataengineering,[Azure] Recommend me a tech stack for medium-size ETL jobs?,h7kgns,PublixBrandOrBust,,https://www.reddit.com/r/dataengineering/comments/h7kgns/azure_recommend_me_a_tech_stack_for_mediumsize/,9.0,10.0,0.0,14527.0,"Currently working on a small product development team as a cloud data engineer. The product is designed to handle medium-sized data (total data size ~ 1-2 TB, daily volume ~ 500 MB) to process, conform, and visualize customer retail data. I'm tasked with researching and coming up with a recommended re-architecture using Azure IaaS or PaaS products.

Current stack includes:

* Azure Data Factory - used to schedule and orchestrate nightly batch loads of ~500 MB to 1 GB from CSV files that the customers drop to our cloud blob storage. We use the ""Data Flows"" feature in ADF (Spark with a GUI) to do the actual transformations and read the files from blob storage.
* Azure SQL Database - where we land and store the data with ADF. We replicate the data here twice - once in staging via a CDC process, and again in a ""presentation"" layer which is a star schema/dimensional model that customers can query indirectly via views. The staging -&gt; presentation data flow is done using T-SQL stored procedures.
* Azure Analysis Services - we have a couple tabular models that query the DB tables and store the results in-memory for very quick access in Power BI, which is our primary viz tool. Our customers are primarily using AAS as their primary interface for self-service via PBI, although some power users have direct access to those customer-facing views in the DB for ad-hoc/custom queries.
* Logic Apps for miscellaneous administrative tasks (moving files from hot to cool storage, etc.)

Some of the pitfalls of this setup include:

* Very expensive - we have to scale our AAS server wayyyy up to get the data in the tab models in a timely manner.
* Slow - our nightly jobs can take 3-4 hours to process only a couple million rows, so customers aren't getting their data first thing in the morning like they expect. This seems to mostly be on the DB side, where our log IO blows up and data can't be inserted fast enough.
* CI/CD is damn near impossible in Azure DevOps. We can never get our code to deploy to the different internal/customer environments automatically. Something is always breaking.

I'm willing to go with a more programmatic approach using something like Python/PySpark, but my head is spinning thinking about all the different platforms with overlapping features such as HDInsight, Databricks, Spark, Azure Synapse, Hadoop, SQL Server Big Data Clusters, etc."
2437,2020-06-12 18:36:59,1591976219.0,dataengineering,Amazon Data Engineer Interview coming up,h7my8t,ppalety,,https://www.reddit.com/r/dataengineering/comments/h7my8t/amazon_data_engineer_interview_coming_up/,37.0,35.0,0.0,14530.0,"Hi all,

I have an Amazon Interview coming up for Data engineer position. As there is no much information about Amazon DE interviews online, I am posting here. It will be great if anybody can give me some tips on preparation and what areas do I need to focus on. Please help me."
2438,2020-06-12 21:03:55,1591985035.0,dataengineering,How can I move a data processing script into production?,h7ps0z,BrisklyBrusque,,https://www.reddit.com/r/dataengineering/comments/h7ps0z/how_can_i_move_a_data_processing_script_into/,4.0,4.0,0.0,14531.0,"Hello,

My background is in statistics, and I recently obtained my first data internship. My knowledge of how to productionalize things is limited.

My company does a lot of data processing in Excel. Since I work for a small nonprofit and we hire a lot of youth, we normally don't push programming languages on them. However, I have R experience. I wrote a wrapper function that can automate a common pre-processing step in one of our workflows. The script takes in a .csv and applies some data cleaning steps, revisions of certain column names, etc.

So my question is, **is there any way to move my R script into production without forcing my colleagues to learn and download R**?

I am (vaguely) aware of web apps like Shiny and container applications like Docker. How difficult would it be to host the script on the company website as an I/O web app?

BACKGROUND: My local computer is Linux. Our db backend is hosted by Azure. We have an agile software development team in charge of the website. I would need to work with them to integrate anything in the website. My main language is R but I am open to Python solutions."
2439,2020-06-12 21:42:36,1591987356.0,dataengineering,Interview format for DE,h7qig1,encodej,,https://www.reddit.com/r/dataengineering/comments/h7qig1/interview_format_for_de/,2.0,2.0,0.0,14531.0,"I was wondering what is the norm for interviewing DE specifically for SQL questions? I know it depends but I guess I am hoping for most common pattern to emerge.  
Do companies use hackerrank or similar services for SQL questions or is it usually whiteboard exercise like DS/ALGO?  
Please enlighten."
2440,2020-06-13 05:07:41,1592014061.0,dataengineering,CS Certificate + DE Intensive OR CS Bachelor's?,h7yek4,kintaloupe,,https://www.reddit.com/r/dataengineering/comments/h7yek4/cs_certificate_de_intensive_or_cs_bachelors/,5.0,14.0,0.0,14538.0,"Happy Friday everyone,

I'm interested in making a pivot (no pun intended) to Data Engineering, and am looking at two possible paths:

1. [Certificate in Computer Programming](https://csuglobal.edu/undergraduate/certificates/computer-programming) \+ [Jesse Anderson's Data Engineering intensive course](https://www.jesse-anderson.com/get-your-data-engineering-dream-job/)
2. [Bachelor's in Computer Science](https://www.online.colostate.edu/degrees/computer-science-bachelors/curriculum.dot)

Of course, if time and opportunity cost were not factors, option 2 would be best, but if I could reasonably make it work with option 1, that would be ideal.

With option 2, I could take the Distributed Systems and Big Data classes, but I'm not sure if they would be as practical as Jesse Anderson's Data Engineering course.

My goal is to get a Data Engineering job with more variety beyond just ETL / Data Warehousing work.

I'm a Data Analyst with 5 years of experience in BI (reporting, dashboarding, analysis) and database administration (certified in SQL Server). I have a bachelor's in Management Information Systems. I have academic and professional programming experience (python, SQL).

Please advise. Thank you."
2441,2020-06-13 07:16:07,1592021767.0,dataengineering,Data Engineering hiring process,h80cug,NakkiGN,,https://www.reddit.com/r/dataengineering/comments/h80cug/data_engineering_hiring_process/,2.0,4.0,0.0,14539.0,"Hi all,

I thought it would be interesting to hear the interview process at your current firm for data engineers.

Ex:
Stage 1: phone interview
Stage 2: coding exercise for spark
Stage 3: tech white board session
Stage 4: team fit and HR
Offer

Thanks"
2442,2020-06-13 10:28:38,1592033318.0,dataengineering,Apache Airflow and Kubernetes — Pain Points and Plugins to the Rescue,h82weq,tomekanco,,https://www.reddit.com/r/dataengineering/comments/h82weq/apache_airflow_and_kubernetes_pain_points_and/,13.0,6.0,0.0,14542.0,
2443,2020-06-13 15:58:04,1592053084.0,dataengineering,Covid-19 Infected per capita Ranking | TOP 10 Country (updated on 12 Jun 2020),h8727u,datavtworld,,https://www.reddit.com/r/dataengineering/comments/h8727u/covid19_infected_per_capita_ranking_top_10/,1.0,0.0,0.0,14548.0,
2444,2020-06-13 16:40:43,1592055643.0,dataengineering,Reading data from an append only data store,h87qju,ed_elliott_,,https://www.reddit.com/r/dataengineering/comments/h87qju/reading_data_from_an_append_only_data_store/,3.0,10.0,0.0,14551.0,"Hi,

If you have an append only data store and for each new row you include a load time, is the only way to read out the current state to use a window function over the key and date and say “give me all the keys and for each key only give me the latest”?

Also how do you mark an entry as deleted?

For updates all I can think is either a window function and for deletes using another table with date of delete and the key, but if an entry is deleted, then added back you need to join the first table on where delete is &gt;= insert time.

All thoughts great fully received!


Thanks,


Ed"
2445,2020-06-13 18:35:16,1592062516.0,dataengineering,Tips for preparing for live coding/technical screen?,h89pdm,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/h89pdm/tips_for_preparing_for_live_codingtechnical_screen/,7.0,3.0,0.0,14552.0,"Hi all,

I have a ~2hr technical screen on Monday:

* 50% Python programming -- they're going to give me a dataset to wrangle to see how I solve programming problems
* 50% SQL -- all they've said about this is that I have to connect to one of their DBs 

My current plan is to practice SQL all weekend as I am rusty on some of the intermediate-advanced stuff.

I don't really know how to prepare for the Python portion since it doesn't sound like a Leetcode-style problem. I was thinking of maybe reading through the Pandas [getting started tutorials] (https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html) and the [essential basic functionality](https://pandas.pydata.org/docs/getting_started/basics.html) docs. Any other suggestions? I'm no Pandas expert but I feel like I can handle most things that might come up and I don't know what to study specifically.

Any tips on how I should prepare is much appreciated!"
2446,2020-06-14 01:01:48,1592085708.0,dataengineering,Google Certified Data Engineer: learning resources or courses?,h8gwhv,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/h8gwhv/google_certified_data_engineer_learning_resources/,9.0,9.0,0.0,14561.0,"Currently, I'm going over AWS certifications, and I feel that GCP certification would be the next important thing to have in my CV. While there are plenty of resources for exam preparation for AWS, I was wondering what does the community recommend for Google Certified Data Engineer certification exam?"
2447,2020-06-14 05:18:24,1592101104.0,dataengineering,Critique my Learning plan for Data Engineering?,h8l8yj,qazwsx123_1_2,,https://www.reddit.com/r/dataengineering/comments/h8l8yj/critique_my_learning_plan_for_data_engineering/,1.0,2.0,0.0,14561.0,"I have completed a data analyst internship and am set to graduate in december 2020. Ideally, I would like to get an entry level Data Engineer job, if not that, then a Backend Engineer job or a Data Analyst.

I have decent experience with SQL and Python from my internship (though my python still needs some work), some rudimentary unix and git knowledge too.

I am currently doing the Data Engineer track on Datacamp. It seems pretty easy so far, thought its pretty good for learning different technologies (surface level at least).

After the Data Engineering track, I plan to do: https://www.udemy.com/course/the-python-mega-course/.
The python project course and some projects on my own to improve on my python skills.

I read that backend engineering has a lot of overlap with data engineering, so its good to know some.
Django:https://www.udemy.com/course/python-and-django-full-stack-web-developer-bootcamp/.

For Data Engineering theory and cloud practice, I plan on doing:
https://www.udemy.com/course/data-engineering-on-google-cloud-platform/ and https://www.udemy.com/course/sql-nosql-big-data-hadoop/.



My Questions are:
1)
Is this enough to land an entry level data engineering job?

2)The datacamp tracks delves into Scala and Spark a bit too.
Are there any others resources that I should look into? How important is knowing Scala, considering now Pyspark is almost as good as Scala Spark.

3)Should I spend more time learning Java or Scala? I'm worried I may be pigeonholing myself if I spent a lot of time on Scala, considering its not really used outside of DE and Java is among the most used language out there.

4) Is the web development course worth doing? I noticed an increasing amount of data engineers need to have a decent knowledge of this to present their final solutions to the business and for the business or the DS team to use.

5) Queuing and Streaming  systems such as Kafka and Spark Streaming. Not sure what the best way to learn this.
I guess:https://www.udemy.com/course/apache-kafka-for-beginners/
https://www.udemy.com/course/kafka-streams-real-time-stream-processing-master-class/

6) For SQL, can some recommend a more advanced course?
I know joins, aggregates, create and insert, subqueries. 
Not so good with window functions and DB theory

7) Am I missing any other resources . Any other tips/resources would be very appreciated.
Thanks for your time!"
2448,2020-06-14 05:48:04,1592102884.0,dataengineering,How has data engineering changed in the past 3-5 years? How do you see data engineering changing in the next few years?,h8lqk2,___24601,,https://www.reddit.com/r/dataengineering/comments/h8lqk2/how_has_data_engineering_changed_in_the_past_35/,3.0,13.0,0.0,14561.0,
2449,2020-06-14 13:41:44,1592131304.0,dataengineering,Car Production Ranking | TOP 10 Country from 1999 to 2018,h8rk8y,datavtworld,,https://www.reddit.com/r/dataengineering/comments/h8rk8y/car_production_ranking_top_10_country_from_1999/,1.0,0.0,0.0,14564.0,
2450,2020-06-14 18:03:21,1592147001.0,dataengineering,Should I take the time to learn PySpark?,h8vdxe,cannablubber,,https://www.reddit.com/r/dataengineering/comments/h8vdxe/should_i_take_the_time_to_learn_pyspark/,1.0,27.0,0.0,14573.0,"Some background:
I have nothing to gain from learning PySpark in my current position. Currently, writing a lot of Airflow pipelines and incremental load SQL for a Snowflake data warehouse.

However, I do know that Spark is very popular in the DE industry and wonder if it is worth picking up in my free time. From what I've seen it is mostly just learning some new syntax, the real challenge is setting up and managing your own cluster, which is probably what I would want to work on if I committed.

If you have experience with Spark or PySpark, I am open to your opinion and any resources that you found helpful for learning are greatly appreciated."
2451,2020-06-14 19:18:41,1592151521.0,dataengineering,I am fresh at Data Engineering having a role of DBA in past. Please judge my resume as I am applying to companies for Data Engineering role.,h8wp6t,youareafakenews,,https://www.reddit.com/r/dataengineering/comments/h8wp6t/i_am_fresh_at_data_engineering_having_a_role_of/,1.0,3.0,0.0,14579.0,"Download the PDF from here.
~~https://file.io/IsTXZ21o~~
New Link: https://we.tl/t-OVvpXnC8LQ
If file is not recognised, append .pdf to the name of file. Under Linux it works fine.

Edit: PreviousLink expired for unknown reasons.

Thank you!"
2452,2020-06-14 21:50:33,1592160633.0,dataengineering,Managers and Directors - What is your day-to-day like?,h8zc1q,criickyO,,https://www.reddit.com/r/dataengineering/comments/h8zc1q/managers_and_directors_what_is_your_daytoday_like/,3.0,7.0,0.0,14592.0,"I'll start:

I've been an Analytics Engineering Manager for a pretty lean Analytics services team (2 DEs, 8 Analysts) at a Digital Marketing company for a few years now.  Due to recent cutbacks/departures, I've recently started managing Analysts in addition to Data Engineers/Developers.  Before this, my day-to-day would vary wildly, particularly due to the embedded nature of our Data Engineers.

* Analysts would field inquiries/report requests from clients, which they would execute on Hive/Spark clusters managed by third-party software.  Data engineers would unblock work by troubleshooting/optimizing queries, engines, and standing up data pipelines for data integration.
* Data engineers maintain and develop a proprietary Python library full of DB connectors and API wrappers.
* Data engineers maintain a legacy reporting platform (SQL templatization engine built in Flask+Angular.
* Data engineers also maintain ETL pipelines built in Databricks, orchestrated with Airflow. 

As a manager, I oversee the above in addition to managing work (JIRA, Smartsheet, Salesforce, GitHub), coordinating special projects with other departments, fielding internal inquiries about systems/data/tribal knowledge, facilitating career development through coaching, and hiring new engineers/analysts.

I still have my hands in some code; I'll address some bugs, add incremental features, close out small tickets, and handle administrative requests (access/permissions for systems/tools) on behalf of engineers/analysts.

Since we're lean, we manage most of our own immediate ecosystem: AWS for metastore (RDS)/datastores (S3) and EC2 servers for Jupyter lab environments and web apps, simple automated reporting jobs automated with Rundeck, ETL pipelines orchestrated by Airflow, monitoring in Grafana, internal dashboarding with Superset.  I also serve as a first-layer for cost management and optimization."
2453,2020-06-14 21:52:28,1592160748.0,dataengineering,How to simulate real time processing with historical data?,h8zdcu,oatsativa,,https://www.reddit.com/r/dataengineering/comments/h8zdcu/how_to_simulate_real_time_processing_with/,1.0,1.0,0.0,14593.0,"I'm working on a project to create a live dashboard that updates every hour. I have historical monthly reddit data (2013-2017) and I've been able to run it through spark and into Cassandra. However, I'm not really sure how to make it ""real time"" as it seems more like just batch processing in 1 hour intervals. My idea was to take the months and partition it into hourly data, and then run each of those hourly data into my pipeline. However, I'm not quite sure how that would work, perhaps I could use Airflow?"
2454,2020-06-14 22:26:21,1592162781.0,dataengineering,Future Data Engineering,h8zzf7,soujoshi,,https://www.reddit.com/r/dataengineering/comments/h8zzf7/future_data_engineering/,6.0,5.0,0.0,14595.0,What are some of the upcoming tools/technologies in data engineering to learn or get certified?
2455,2020-06-15 01:18:25,1592173105.0,dataengineering,What are the college majors that most people pursue to get into this field?,h931qa,GeminiDavid,,https://www.reddit.com/r/dataengineering/comments/h931qa/what_are_the_college_majors_that_most_people/,1.0,12.0,0.0,14605.0,Are all of you guys CS grads? I'm a software engineering major but I feel like CS is the preferred degree
2456,2020-06-15 05:16:57,1592187417.0,dataengineering,Does anyone want to help a poor soul trying to learn using the Udacity Data Engineering Couse?,h970sx,freebird348,,https://www.reddit.com/r/dataengineering/comments/h970sx/does_anyone_want_to_help_a_poor_soul_trying_to/,0.0,7.0,0.0,14618.0,"First of all, this sub is awesome -- thanks everyone who contributes to keep it great. 

I am in the process of learning Data Engineering and am at a point where I'm struggling with the Udacity course (I know it sucks, but its the most structured learning I can get now). 

Essentially, I am attempting to read the contents inside an s3 bucket. Does anyone mind if I chat them to ask some specific questions about what I'm doing wrong and why I'm receiving the errors I'm getting?"
2457,2020-06-15 07:57:23,1592197043.0,dataengineering,Analysis of Warsaw Public Transport Data in Kibana and Elasticsearch,h99bjh,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/h99bjh/analysis_of_warsaw_public_transport_data_in/,1.0,0.0,0.0,14618.0,
2458,2020-06-15 09:47:04,1592203624.0,dataengineering,Helm chart with KubernetesExecutor for airflow,h9atcj,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/h9atcj/helm_chart_with_kubernetesexecutor_for_airflow/,1.0,3.0,0.0,14618.0,"Hi,

I would like to know the best helm 3 chart for using airflow with kubernetesexecutor for airflow. I would like to use them in production. Any production ready charts?"
2459,2020-06-15 11:57:00,1592211420.0,dataengineering,Apache Genie vs Apache Livy for interacting w/ EMR Spark clusters via Airflow?,h9cgna,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/h9cgna/apache_genie_vs_apache_livy_for_interacting_w_emr/,1.0,1.0,0.0,14620.0,"From my understanding, both Genie &amp; Livy provide a REST API over the EMR clusters.

However, what are the pros and cons of each? Which one is more appropriate to use for data transformations -- particularly Spark running on Amazon EMR?

Which one is *easier* to work with and has better documentation?


Thanks everyone!!"
2460,2020-06-15 13:47:22,1592218042.0,dataengineering,Watermelon Production Ranking | TOP 10 Country from 1961 to 2018,h9dwy3,datavtworld,,https://www.reddit.com/r/dataengineering/comments/h9dwy3/watermelon_production_ranking_top_10_country_from/,1.0,0.0,0.0,14622.0,
2461,2020-06-15 17:31:01,1592231461.0,dataengineering,KPIs for a feature generation team,h9hgso,windyslide,,https://www.reddit.com/r/dataengineering/comments/h9hgso/kpis_for_a_feature_generation_team/,3.0,12.0,0.0,14628.0,"Hello!

I am a first-time tech lead of a team that primarily focuses on online feature generation and scoring of various ML models and rule sets. I'm trying my hand at writing KPIs and OKRs for my team this quarter and finding it tough to figure out how to appropriately measure our team.

On one hand, it would be impossible for the ML team to make any use of their models without our team. They need us to actually generate point-in-time data with suitable latency and score the darn thing.

On the other hand, it feels odd to take credit for their P/R improvements quarter-over-quarter.

So how do other folks measure business value of a team like this? Operational excellence seems obvious but I'm hoping for something a bit meatier than just engineering success metrics."
2462,2020-06-16 01:44:19,1592261059.0,dataengineering,Python: Azure SDK vs Amazon SDK (Boto3),h9qozh,SuccessfulFarmer,,https://www.reddit.com/r/dataengineering/comments/h9qozh/python_azure_sdk_vs_amazon_sdk_boto3/,1.0,14.0,0.0,14643.0,"I am in charge of developing a cloud stack to automate data ingestion and processing at a small company and am deciding between Azure Storage containers and Amazon S3 for flat file staging for ELT processes.

I can't find a valid alternative to Azure Data Factory in AWS for simple data ingestion but noticed that Azure recently refactored and deployed their python SDK. This makes it confusing and hard to find answers for the current version on StackOverflow, which I am using to script ETL jobs that I am running on a EC2 server to read and write from Azure Blobs.

Two questions:

1. Are there any benefits to using S3 over Azure Storage containers?
2. Is Boto3 straight forward and easy to use? Microsoft seems to change and update their cloud offerings willy-nilly and I'm not too keen on refactoring my code once a year due to drastic sdk changes"
2463,2020-06-16 06:36:15,1592278575.0,dataengineering,Is my Resume good enough to land an internship !,h9vrw1,Mhayc_en,,https://www.reddit.com/r/dataengineering/comments/h9vrw1/is_my_resume_good_enough_to_land_an_internship/,1.0,7.0,0.0,14647.0,"Hi, i hope you guys are doing great in these tough times. I'm a CS student , graduating next year . Because of Covid-19 our Engineering school cancelled this summer internship obligations , this is a big down fall on cv , and i don't know how to compensate for it. 

I want to have a good enough resume to land a good Data Engineering internship next year , but i don't have a project idea in mind, since i don't know what would impress recruiters. 

What would you guys propose for me to change or add in my resume? And what would be a perfect project in my situation to compensate this summer internship ?

Thank you for your effort ! [resume](https://drive.google.com/file/d/1CVUQFN1SoV96iCnCcRBd7jVvzAXg2AhM/view?usp=drivesdk)"
2464,2020-06-16 08:38:06,1592285886.0,dataengineering,Is this worth doing?,h9xl6d,PMScoMo,,https://www.reddit.com/r/dataengineering/comments/h9xl6d/is_this_worth_doing/,1.0,6.0,0.0,14649.0,"Hi there

I am currently working in SaaS. An Solution Consultant role that is very technical. Spend good amount each day in JS/PHP/Git. Limited devops and DB involvement. 

I have a econ/commerce background so am self-taught. Data intrigues me. I saw [this course](https://programsandcourses.anu.edu.au/2020/program/gcde) as it has been subsidized by government  here. The DE role looks interesting as it involves the compsci element that I love along with the data focus. Moving from web to DE would be a jump in technical ability I am sure, but I have been thrown in the deep end before. 

Is a course like this worth doing to get the fundamentals before getting into projects?"
2465,2020-06-16 08:41:09,1592286069.0,dataengineering,Electricity Price or Electricity source datasets/APIs?,h9xmoj,The_mCherry_Man,,https://www.reddit.com/r/dataengineering/comments/h9xmoj/electricity_price_or_electricity_source/,1.0,3.0,0.0,14650.0,"Does anybody know of APIs or sources that could be used to find electricity prices (in cost/kilowatt-hour) or electricity sources (e.g., X% coal, Y% geothermal, Z% solar, etc.) for given towns, counties, or states (or even countries)? I reached the limits of WolframAlpha's capabilities in a little less than 30 minutes, and most of the utility company websites I look up don't have that kind of info readily available.

Are there ways of getting this data directly from the sources?"
2466,2020-06-16 11:26:41,1592296001.0,dataengineering,"News, blog posts, etc. to keep up to date with the trends",h9zun8,oliveira_vagyok,,https://www.reddit.com/r/dataengineering/comments/h9zun8/news_blog_posts_etc_to_keep_up_to_date_with_the/,1.0,4.0,0.0,14651.0,"Hello!

I'm trying to curate a list of resources where I can get information about new tools, processes, stories and so on, about anything related to data engineering. I used to subscribe to the Data Eng weekly newsletter but now it's off.

Would be great if you could share your ""bookmarks""!

Thanks!"
2467,2020-06-16 11:53:20,1592297600.0,dataengineering,Data Lake vs Data Warehouse in Modern Data Management,ha07i6,suemethen,,https://www.reddit.com/r/dataengineering/comments/ha07i6/data_lake_vs_data_warehouse_in_modern_data/,1.0,1.0,0.0,14652.0,
2468,2020-06-16 12:47:43,1592300863.0,dataengineering,Flink for online Machine Learning and real-time processing at Weibo,ha0vl1,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ha0vl1/flink_for_online_machine_learning_and_realtime/,1.0,0.0,0.0,14653.0,
2469,2020-06-16 15:34:55,1592310895.0,dataengineering,Covid-19 Death Rate Ranking | TOP 10 Country (Updated on 15 Jun 2020),ha34v6,datavtworld,,https://www.reddit.com/r/dataengineering/comments/ha34v6/covid19_death_rate_ranking_top_10_country_updated/,1.0,0.0,0.0,14658.0,
2470,2020-06-16 17:56:49,1592319409.0,dataengineering,[Slides &amp; Recording] Integrating Oracle and Kafka,ha5kcg,rmoff,,https://www.reddit.com/r/dataengineering/comments/ha5kcg/slides_recording_integrating_oracle_and_kafka/,1.0,0.0,0.0,14665.0,
2471,2020-06-16 21:05:59,1592330759.0,dataengineering,Can someone who know Google Dataflow (Apache beam) and Java help me with this?,ha9821,rdv100,,https://www.reddit.com/r/dataengineering/comments/ha9821/can_someone_who_know_google_dataflow_apache_beam/,1.0,4.0,0.0,14673.0,"Hi,

I'm trying to build an analytics project. I plaan. to use Google Bigquery and Google Dataflow. Please see more details here: [https://www.upwork.com/jobs/\~01c56efb53e7c737be](https://www.upwork.com/jobs/~01c56efb53e7c737be)

I'm happy to increase the payment if I find the right person with the right experience."
2472,2020-06-16 21:33:51,1592332431.0,dataengineering,Data model for content tags?,ha9rix,themikep82,,https://www.reddit.com/r/dataengineering/comments/ha9rix/data_model_for_content_tags/,2.0,2.0,0.0,14673.0,"How would a platform, like YouTube, for example, store and retrieve tags for content? Any video can have many tags that describe the content of the video (i.e. 'car care', 'automobile', 'DIY'), but how is this handled efficiently at scale? A relational model seems like it would bog down with heavy join operations across a very cumbersome many to many relationship, so there must be a better approach.

A document data model? Some sort of columnar store db/service? Something else?

Thanks!"
2473,2020-06-16 21:56:48,1592333808.0,dataengineering,Virtual Data Warehouse and necessary Middleware,haa7y5,BolotoF,,https://www.reddit.com/r/dataengineering/comments/haa7y5/virtual_data_warehouse_and_necessary_middleware/,2.0,3.0,0.0,14675.0,"Hello guys,

I'm doing a research about the differences between Traditional Data Warehouse (Physical DW) and Virtual DW.

So, my question is: I searched alot about what type of middleware a Virtual DW needs, in terms of architecture and components, and I can't find anything related to this.

Can anyone of you help me?

Thank you in advance, best regards."
2474,2020-06-17 00:32:23,1592343143.0,dataengineering,Data Quality Engineer?,hada9b,wanna_be_a_DE,,https://www.reddit.com/r/dataengineering/comments/hada9b/data_quality_engineer/,1.0,2.0,0.0,14683.0,"Hi all,

About me:

* Sociology PhD about to graduate this summer
* Current data science intern at a large company until end of August ( mostly ML and dashboarding, but I've built some modest pipelines in airflow, etc.)
* I want to move from DS -&gt; DE

I was recently approached to interview for a \`data quality engineer\` position. I haven't heard of the role, it sounds like SDET? But for data? From the job description:

&gt; will be responsible for validating solutions and processes to enable analytics, BI, MDM, etc and ensure data quality + integrity in new data pipelines to scale.

&gt; * Develop data quality, testing plans, scripts, etc
&gt; * End to end ownership of quality and implementation
&gt; * Execute automation testing, track issues
&gt; * Participate in tech discovery + architecture design discussions

How would my career plan/trajectory of trying to go from DS -&gt; DE be affected if I had this role? It's been hard trying to find an entry-level DE position so I'm looking for alternatives.

Thoughts and opinions much appreciated!"
2475,2020-06-17 00:40:11,1592343611.0,dataengineering,Building a complete data validation platform (https://orgstack.io) - would love your feedback,hadfpj,1cph,,https://www.reddit.com/r/dataengineering/comments/hadfpj/building_a_complete_data_validation_platform/,1.0,0.0,0.0,14683.0,
2476,2020-06-17 01:18:57,1592345937.0,dataengineering,Building a complete data validation platform - would love your feedback,hae6kn,1cph,,https://www.reddit.com/r/dataengineering/comments/hae6kn/building_a_complete_data_validation_platform/,1.0,3.0,0.0,14686.0,"Hi everyone!

My name's Christian, and I'd like to share what we've been building at [OrgStack](https://orgstack.io/).  I'm posting this here as a ""request for feedback"" from the community that we serve.

Poor data quality is a costly problem for many organizations.  Invalid data is responsible for major product/service outages, and it can be very difficult to pinpoint the root cause of data-related incidents.  This requires a large investment of engineering resources.

OrgStack aims to streamline the process of managing data sources, receiving critical alerts, and tracing incidents to their source.  While building a solution to this problem, we drew inspiration from some of our favorite tools, including NPM, Loggly, and Jenkins.

A few baseline questions:

* Is data validation / data quality currently a problem for you?
* How do you solve it now?
* Which differentiating features would get you to switch?
* What would your ideal solution look like?

Thanks in advance for your feedback.  We really appreciate it!

&amp;#x200B;

\- Christian ([christian@orgstack.io](mailto:christian@orgstack.io))

&amp;#x200B;

https://preview.redd.it/0wn01366kc551.png?width=1149&amp;format=png&amp;auto=webp&amp;s=b74585ada3c3308ab3cf941153188bc4e9d7a264"
2477,2020-06-17 01:42:31,1592347351.0,dataengineering,Taking a job you feel un(der) qualified for?,haem5x,Folasade_Adu,,https://www.reddit.com/r/dataengineering/comments/haem5x/taking_a_job_you_feel_under_qualified_for/,2.0,14.0,0.0,14686.0,"Hi all

I’m a data science intern with a strong interest in data engineering. I graduated this month and my internship is up in August. For a variety of reasons I really don’t want to do data science anymore. 

It’s looking like I might be able to join a small startup as a data engineer, but I’m worried I will crash and burn. 

The main project I’ll be working on is porting old legacy databases into the cloud (AWS) and setting things up. I have experience with AWS, but nothing massive like that. 

Is it crazy for me to *not* take this position if offered? I mean I guess I can figure things out on the fly? I am just afraid of them thinking/finding out I’m a fraud two weeks in and firing me. 

Have any of you joined a company or had a position that was way over your head? What did you do? How did you handle it? 

Thanks!"
2478,2020-06-17 02:15:53,1592349353.0,dataengineering,"1st year Data Engineer, which PC specs? Best build?",haf6mi,HikariNour,,https://www.reddit.com/r/dataengineering/comments/haf6mi/1st_year_data_engineer_which_pc_specs_best_build/,1.0,10.0,0.0,14689.0,"Hello there!

I am starting a Data Engineering study this year in the Netherlands. I was wondering, which specs my PC would need and which build would be best? I am still in the process of buying and building so ANY tip would be extremely helpful!! Mainly tips on brands etc.

Thanks a lot by forehand!!"
2479,2020-06-17 02:32:25,1592350345.0,dataengineering,Is there a recommended learning path/core skills required for Data Engineering?,hafgs6,s4hc,,https://www.reddit.com/r/dataengineering/comments/hafgs6/is_there_a_recommended_learning_pathcore_skills/,2.0,20.0,0.0,14689.0,"Over the next 6 months I'd like to focus on developing data engineering skills to improve my career prospects inside and possibly outside of my company. I'm from a technical background, ERP implementations and have some exposure to data related topics but many topics will be new to me, learning python, Apache products etc.

At the moment I'm following the DataCamp DE track and reading through the DE cookbook by Andreas Kretz. I'll probably get the Designing Data Intensive Applications book and I'm planning to follow the Azure Data Engineering learning path and Data Engineering with Azure Databricks learning path for a start with some cloud specific skills.

Is there a recommended learning path/set of courses for Data Engineering, there seem to tons for Data Science, ML Engineering etc but wondering what other topics I should be focusing on? Im sure the above content won't teach me a lot of the core skills/competencies required as a DE and won't give me exposure to more commonly used products.

Any info is greatly appreciated."
2480,2020-06-17 05:36:19,1592361379.0,dataengineering,Data structures &amp; Algorithms for DE?,haicpc,munkeyt,,https://www.reddit.com/r/dataengineering/comments/haicpc/data_structures_algorithms_for_de/,1.0,19.0,0.0,14699.0,"*#TLDR: How much DS&amp;A is being applied in your daily work as DE?*

Few days ago I attended my first ever technical interview for a junior DE position. It was being conducted by a full-time DE at an e-commerce company. During my preparation, I focused on reinforcing my knowledge of Pandas, SQL, Spark, data modelling etc. based on what I have been reading on this subreddit. I was quite confident with my research-based analytics and DS background, and preliminary understanding on DE tools with a project.

Moving onto the actual interview, I was taken aback by his questions on algorithms. My confidence was totally dumpstered by questions like *""Tell me how will you implement an INNER JOIN operation"", ""How do you think a B-Tree helps speed things up in an index?""*. Nonetheless I gave it my best shot. The implementations that I described were of undesirable complexity O(n^(2)), given my shallow understanding on DS&amp;A. The answers I gave for those questions were workable, but not *efficient*. At the end of the interview, I asked his reasoning for the above rigourous session. He explained that,

1. *DE, at its core, is still software engineering, therefore will be evaluated in similar manner*
2. *While other companies are simply looking for query engineers, our company would appreciate someone who can get into the low-level code and understand the fundamental working principles (to write efficient code).*

His perspective is totally reasonable, but this experience has only increased my perception of gap between my current ability and the minimum requirement of a junior DE. So, I would like to appeal to the experienced DE out there.....

1. Given my limited experience in ETL pipelines, I am only hoping for a junior role and try to learn as much as I can in my first job. In your opinion, will you be comfortable of hiring a junior who is good at SQL and reasonable Python scripting, but not hyper-efficient code? Or would you like him to at least demonstrate some level of algorithm optimization capability?
2. How rigorously do you apply DS&amp;A in your daily DE tasks? And if so, which are the key parts that require this skill?
3. If #2 is absolutely critical for DE, do you have any suggestions on how to improve on this skill? All I've been seeing are theoretical videos, but I don't know how to apply them (either in DE or non-DE context) in real-life."
2481,2020-06-17 08:23:03,1592371383.0,dataengineering,Understanding modern data engineering conceptually,haktv8,amalik87,,https://www.reddit.com/r/dataengineering/comments/haktv8/understanding_modern_data_engineering_conceptually/,1.0,3.0,0.0,14705.0,"Let's start off with some background. I'm very familiar with traditional ETL/DataWarehousing alongside the end-usage in a Reporting tool. Let's say for example T-SQL, SSIS, Informatica, etc.

Conceptually, I'm not confused by the tech involved mentioned continually on this sub i.e. airflow, spark, python, etc . What I'm confused about is what is the distinguishing factor between 'traditional' ETL/business intelligence/reporting (Let's call this ""vanilla"" BI) and the content I see on this sub where trained software engineers from the Scala or Java stack are taking over ETL pipelines and writing them in a classically object oriented language like Java or Scala (And most of the time utilizing Python on top of that)? 

How can I understand this ""new"" category of Data consumption in depth? Is it still end users in an organization that will eventually consume this alternate ETL pipeline flow, or is the end user a different ETL/data team that will route the result of the work to their vanilla BI etc? Is this all simply related to real-time analytics and stream processing (i.e noSQL/key-value/array storage) vs oldschool batch processing in OLTP/OLAP? That for example would make sense as an explanation as well.

Thanks ahead!"
2482,2020-06-17 13:19:52,1592389192.0,dataengineering,Apache Flink: Flink on Zeppelin Notebooks for Interactive Data Analysis,haoiju,Marksfik,,https://www.reddit.com/r/dataengineering/comments/haoiju/apache_flink_flink_on_zeppelin_notebooks_for/,1.0,0.0,0.0,14713.0,
2483,2020-06-17 13:54:15,1592391255.0,dataengineering,Pear Production Ranking | TOP 10 Country from 1961 to 2018,haoxbg,datavtworld,,https://www.reddit.com/r/dataengineering/comments/haoxbg/pear_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,14714.0,
2484,2020-06-17 15:47:42,1592398062.0,dataengineering,A personal question to data engineers,haqg41,Marco_Villani,,https://www.reddit.com/r/dataengineering/comments/haqg41/a_personal_question_to_data_engineers/,1.0,7.0,0.0,14717.0,"Hi everyone! I am currently trying to make a positive impact on the life of some professionals. I know from experience that working on some projects can be very challenging for engineers, leaving a very short time for personal and dating life. 

That is why I have a couple of questions on this topic, which I believe to be very important: 

1. As a male engineer, do you ever think you don’t have enough time to dedicate to your personal life?

2. Do you ever feel there is something you could improve when it comes to meeting women (or men)?

To be clear: I am just getting started with this, not trying to sell you anything with these questions! I am simply particularly interested in the topic. I would really like to get your valuable insights, so that maybe a tailored solution can be provided in the future!

Thank you in advance for your help and your understanding: I know this might be considered a sensitive topic, but I also believe it is very important.

Looking forward to reading your answers !!"
2485,2020-06-17 17:32:13,1592404333.0,dataengineering,How to query data straight from object stores?,has9r2,gabrielfigmeira,,https://www.reddit.com/r/dataengineering/comments/has9r2/how_to_query_data_straight_from_object_stores/,2.0,5.0,0.0,14733.0,"Hi,

I’m spending too much time having to download and move files around to get the most out of my data hosted in my object stores (Backblaze b2 and Wasabi). 

Is there a way to query files directly from object stores? How would you describe your Object Stores  file processing?

I’m not a tech person, so I don’t work around databases at all.

Thanks in Advance!"
2486,2020-06-17 18:22:29,1592407349.0,dataengineering,An interview with the creator of StreamSQL on the complexities of building a feature store and the benefits that they provide to the development and delivery of machine learning models.,hat8u6,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hat8u6/an_interview_with_the_creator_of_streamsql_on_the/,1.0,0.0,0.0,14737.0,
2487,2020-06-17 18:26:41,1592407601.0,dataengineering,Failed Facebook DE Interview Twice - How to prepare. Please help.,hatbz8,stevieoz,,https://www.reddit.com/r/dataengineering/comments/hatbz8/failed_facebook_de_interview_twice_how_to_prepare/,4.0,25.0,0.0,14737.0,"Hello,

I interviewed with FB twice for the data engg position. Both times I was not able to go past the tech screen.

**First:** I was only able to solve 2 sql and 2 python questions. So I understand, I had to work on my speed and accuracy.

**Second:**  Format was changed. Was asked 4 sql and 1 lcompound python question. Also had to write sql and python in coder pad - text file and not a python env or sql unlike prev interview.

The compound python question had dict, sorting, lists and string manipulation.

Was not able to solve the 4th question on sql ... was able to solve the compound python question.

Did not clearly answer the question on code complexity. I gave a better solution using heap but was not able to articulate its complexity.

I was under the impression that if clear sql and python.  I should be good but got a reject.

**Now I am a bit lost as to how I can prepare. Please advise**."
2488,2020-06-17 20:19:39,1592414379.0,dataengineering,DE Champagne Problems,havnaz,cards_fan88,,https://www.reddit.com/r/dataengineering/comments/havnaz/de_champagne_problems/,1.0,1.0,0.0,14744.0,"Hi all,

I’m coming down to zero-hour to make a decision between three job offers. My focus with this Reddit inquiry is determining which seems most ideal for my next career objective: building a solid foundation for becoming a DE Lead/Manager.

&amp;#x200B;

**About Me:**

I have just over two years of DE experience on a small engineering team with a Fintech. The team was responsible for migrating legacy warehouses and pipelines into AWS. Being on a smaller team afforded me the opportunity to wear multiple hats and gain experience with a large swath of DE services, tools, and best practices. I’m passionate about DE to the point that I spend portions of my free time on this sub or researching new technologies/architectures (usually until being scolded to ""disconnect"" by my girlfriend).

&amp;#x200B;

**The Opportunities:**

All opportunities fall within my acceptable comp range and are a great culture fit. All involve coming in at the ground floor of development on a new engineering product or standing up new infra.

* Company A – HR/Staffing company where I would be the first member of a DE team (position is considered mid-level), seemingly responsible for the architecting, development, and monitoring of their data engineering initiatives. Pro: Management-esque experience and ability to directly impact data strategy. Con: Concerns about work/life balance and availability of senior mentorship 
* Company B – 100% remote Healthcare company where I’d come in at mid-to-senior level on a team planned to be fully built out (solutions architect, product managers, scrum masters, full range of engineer experience etc). Pro: exposure to industry best practices and working with varying specialists. Con: concerns on opportunity for advancement 
* Company C – Current Fintech I’m employed which has countered to match the other offers. I have great rapport within the organization and am considered a data SME for the source systems. The team has been given funding to build out significantly, although not until mid-2021. Pros: Familiarity with the company and positioned to be a core DE member. Cons: Slower team build and no immediate technical management resulting in an unstructured team dynamic

&amp;#x200B;

I’m not necessarily looking for a decision to be made, but more experience anyone may have in these types of team environments and what did/didn’t help your professional development."
2489,2020-06-17 20:30:04,1592415004.0,dataengineering,Zoe: a new CLI tool for Apache Kafka,havv3m,wlezzar,,https://www.reddit.com/r/dataengineering/comments/havv3m/zoe_a_new_cli_tool_for_apache_kafka/,1.0,1.0,0.0,14744.0,"Hi!

Within Adevinta, we are heavy users of Apache Kafka. We built our own tooling around it. Recently, we open sourced one of these tools called Zoe. It's a high level command line tool to easily interact with Kafka. We use it heavily here and it significantly optimized our workflow.

* Checkout [the repository here](https://github.com/adevinta/zoe) where you will find a screen cast that demo the tool.
* And [the documentation](https://adevinta.github.io/zoe/)

We will soon write an article about it.

Any feedback is welcome : )"
2490,2020-06-17 21:08:02,1592417282.0,dataengineering,DATA ENGINEER CERTIFICATION -DIEALAM,hawnhy,DataFreakk,,https://www.reddit.com/r/dataengineering/comments/hawnhy/data_engineer_certification_diealam/,0.0,0.0,0.0,14745.0,"I'm tableau Bi developer  with 3 years of experience and planning to shift to Data Engineer roles  and figured out stack which I have to improve like Hadoop,spark,airflow,python programming and SQL ,etc which is best Certication to get recognized as DE and improve knowledge

1.AWS BIG DATA SPECIALIST

2.GOOGLE CLOUD -PROFESSIONAL DATA ENGINEER?

Note : Certication does not mean great thing but I think it's a good step and I know projects and W.E matters But I like to have one Good DE Certication under my belt."
2491,2020-06-18 08:01:31,1592456491.0,dataengineering,Any website where we can take coding challenge on Spark?,hb81qr,tataiermail,,https://www.reddit.com/r/dataengineering/comments/hb81qr/any_website_where_we_can_take_coding_challenge_on/,1.0,8.0,0.0,14762.0,"I am preparing for Spark Data engineer job interviews. Good news is that market is opening up.

Do you guys know of any coding challenge site like: Hackerrank/Leetcode for Spark?

Interview questions on Spark I am only getting theoretical or conceptual questions on the internet.

How do you guys suggest preparing for Spark Interviews?

Thanks in anticipation."
2492,2020-06-18 09:05:21,1592460321.0,dataengineering,How to run tasks (which are not dependent on each other) in parallel in Luigi?,hb8x0g,ThiccShadyy,,https://www.reddit.com/r/dataengineering/comments/hb8x0g/how_to_run_tasks_which_are_not_dependent_on_each/,1.0,0.0,0.0,14764.0,"I have a number of tasks which I want to run of which task A is the one which should go first. Task A essentially creates the dataset which the subsequent tasks will use. The dataset it creates is essetially a few directories each of which consist of 100+ csvs holding the same data. Each directory represents a different source for data, and the csvs in each of these directories will be same. So the output of task A would look like:

    Dir A/
    Dir B/
    Dir C/ 

where each of these dirs have:

    some_data.csv
    some_other_data.csv  and so on.

After task A, tasks B and C work on collecting data from the same named csv across the different directories and putting them together. So for example, task B would collect some_data.csv data from the different directories and put together and so on..

Clearly, B and C are not dependent on each other so they should be able to run concurrently. It seems Luigi doesnt automatically do this. I have currently written the tasks so that the dependency graph looks like: A-&gt; B-&gt; C. How do I ensure that B and C actually run in parallel?"
2493,2020-06-18 09:33:28,1592462008.0,dataengineering,Data Analytics Consulting vs Software Engineering Role,hb9ar0,omurice28,,https://www.reddit.com/r/dataengineering/comments/hb9ar0/data_analytics_consulting_vs_software_engineering/,1.0,1.0,0.0,14764.0,"I'm a soon to be new grad with a Bachelors in Computer Science. As much as I would love to jump straight into a DE role, I don't think I have the leverage to do do, especially in the current economic climate. Hypothetically, if I wanted to pursue a career in data engineering, would I be better off accepting an offer for Analytics and Data consulting at a Big 4 accounting firm, or a software development/engineering role (not Faang) that is not necessarily related to data?

With the consulting role, it is directly related to data but not all my time would be focused on data engineering as it may also involve duties such as analytics and putting together slide decks. On the other hand, a software engineering role would be more technically rigorous but I'm not sure how hard the transition would be, given that it may encompass many different areas. 

Appreciate any insights into this."
2494,2020-06-18 14:04:56,1592478296.0,dataengineering,Competitive Ranking | TOP 10 Country/ Region from 2004 to 2019,hbcjcg,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hbcjcg/competitive_ranking_top_10_country_region_from/,1.0,0.0,0.0,14776.0,
2495,2020-06-18 15:03:20,1592481800.0,dataengineering,"Online Training Program, Techfest, IIT Bombay",hbdb60,adarsh0raj,,https://www.reddit.com/r/dataengineering/comments/hbdb60/online_training_program_techfest_iit_bombay/,1.0,0.0,0.0,14777.0,
2496,2020-06-18 15:48:00,1592484480.0,dataengineering,Data Engineering Programs - Become Certified Big Data Engineer,hbdyf6,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/hbdyf6/data_engineering_programs_become_certified_big/,1.0,0.0,0.0,14776.0,
2497,2020-06-18 16:12:21,1592485941.0,dataengineering,For those who use drag and drop ETL tools...,hbebys,cazual_penguin,,https://www.reddit.com/r/dataengineering/comments/hbebys/for_those_who_use_drag_and_drop_etl_tools/,1.0,28.0,0.0,14778.0,"I work on a small analytics team (\~6 of us) and we're using a drag and drop ETL tool to move data from our application databases in RDS over to Redshift, and also building new jobs within Redshift. 

How do you manage or enforce development standards with drag and drop tools? We have some developers who just take raw SQL and dump it into a single component and output the results. Mind you they don't even bother to push it through a linter or formatter beforehand.

 We have others who do a really good job of abstracting the business logic and building functional concepts which makes it very easy to understand, test, and debug the workflow.  


I'm of the opinion that readability counts and that in 6 months or a year if somebody needs to work on a workflow I built they could logically step through each process in the job and test, debug, refactor etc. 

We've started meeting to do ""code review"" the day before we deploy new changes to our production database and these conversations have started to come up more frequently as we review each others ETL jobs. My manager is a big proponent of organizing ETL logic in logical abstractions and it has started to cause a riff between him and one of our senior developers, who thinks that it's a waste of time to do it a different way.  


What have ya'll experience or what are you doing now to manage?"
2498,2020-06-18 18:19:47,1592493587.0,dataengineering,Data Engineering Apprenticeship,hbglth,jakebuilds,,https://www.reddit.com/r/dataengineering/comments/hbglth/data_engineering_apprenticeship/,2.0,45.0,0.0,14787.0,"Professional lead DE here. I've been tossing an idea around for a while and wanted to get people's thoughts about it.  


I read or get asked quite frequently: ""What is data engineering?"" ""How do I start?"" ""What is important to learn?"" ""How would I transfer from a more traditional software engineering position into a DE role?""... and the list goes on.  


These questions are *really hard to answer* because the term ""data engineering"" can mean everything from database administration, to business intelligence, to dataops/devops, to data pipelining, to sysadmin, to just pure software engineering. It also means that ""data engineering"" is harder to teach, train for, or learn on a one-size-fits-all basis.  


My personal career learnings (software/data engineering for the past \~8yrs and a different career before that) have always been very hands-on, real-life, and immediately-applicable. I've learned ""why"" and ""what"" and ""how"" based on what companies need or want IRL.  


Which brings me to the ultimate question: ***Would people be interested in a*** [formal data engineering apprenticeship](https://www.bostata.com/training-data-professionals?utm_source=reddit&amp;utm_medium=apprenticeship)***?***  


Is it scalable? Nope, and that's the point.  
Is it a proven model? Absolutely. Aristotle was taught by Plato. Philip Johnson studied with Breuer and Gropius. Most blue-collar industries have a significant apprenticeship-like component to professional development."
2499,2020-06-18 20:35:42,1592501742.0,dataengineering,Starting New DE Team,hbj842,Archbishop_Mo,,https://www.reddit.com/r/dataengineering/comments/hbj842/starting_new_de_team/,1.0,15.0,0.0,14791.0,"Hi y'all, 

I'm starting a new job in a couple of weeks. I'll be the first DE at a startup. My mandate is to build a centralized analytics platform.

Right now, I'm considering the following:

* Segment for data ingestions
* Snowflake for running ELT, storage, and querying
* DBT to build and test ELT 
* Airflow to schedule
* Looker to visualize
* Amazon ECS to provision custom hardware (e.g. for data ingestions not supported by Segment)

Are there things you would do differently? Am I missing anything?"
2500,2020-06-18 22:32:13,1592508733.0,dataengineering,Boost Data Engineering Productivity by 30X - for Free,hbl966,hnococo,,https://www.reddit.com/r/dataengineering/comments/hbl966/boost_data_engineering_productivity_by_30x_for/,1.0,0.0,0.0,14792.0,"Our Chief Product Officer (CPO) [Tomer Shiran recently blogged about the COVID-19 paradox](https://www.dremio.com/part-1-the-covid-19-paradox-advancing-your-data-analytics-programs-in-the-midst-of-a-pandemic/),  discussing how data and analytics are strategic, “must-have”  priorities, yet at the same time COVID is forcing almost every  organization to ramp up their productivity and cut costs, sometimes very  aggressively.

What if you could meet both objectives at the same time?

[https://www.dremio.com/boost-data-engineering-productivity-by-30x-for-free/](https://www.dremio.com/boost-data-engineering-productivity-by-30x-for-free/)"
2501,2020-06-18 22:56:12,1592510172.0,dataengineering,The Modern Data Engineer Stack,hblptw,tpedar50,,https://www.reddit.com/r/dataengineering/comments/hblptw/the_modern_data_engineer_stack/,1.0,5.0,0.0,14792.0,"I work at a company using a traditional analytics stack (SQL Server, SSIS, Informatica, Kimball, on-premise, etc). I really want to get some experience with the modern analytics stack (Cloud Data Warehouse, Airflow/Prefect, Python, dbt\_).

There is an opportunity to discuss the future of analytics at my company and possibly moving to a modern stack. 

My question is what are the advantages of the modern stack vs the traditional stack?

Fingers crossed I can get some real world experience with the modern stack."
2502,2020-06-19 00:39:05,1592516345.0,dataengineering,What are the use cases for Redis in BI/Data Warehousing (if any)?,hbnmdb,Rey661199,,https://www.reddit.com/r/dataengineering/comments/hbnmdb/what_are_the_use_cases_for_redis_in_bidata/,1.0,11.0,0.0,14793.0,I’m wondering if there are use cases for Redis in BI/Data warehousing or in combination with them
2503,2020-06-19 00:55:01,1592517301.0,dataengineering,advice for beginner: data tracking website,hbnwk3,rgsstressed,,https://www.reddit.com/r/dataengineering/comments/hbnwk3/advice_for_beginner_data_tracking_website/,1.0,0.0,0.0,14793.0,"i want to try simulating https://ncov2019.live/ - i know im nowhere near to being able to replicate the whole thing, but i would like to take small steps at being better at data analytics. someone mentioned data aggregation before, what are some other useful topics i could start learning?"
2504,2020-06-19 06:31:40,1592537500.0,dataengineering,Help Me Pass the Data Engineering Personality Test,hbt9l2,onestupidquestion,,https://www.reddit.com/r/dataengineering/comments/hbt9l2/help_me_pass_the_data_engineering_personality_test/,1.0,5.0,0.0,14806.0,"There are lots of questions about whether analytics or engineering is the more lucrative/secure/desirable path in the data careers space, but I haven't seen much on how to decide between the two based on what you actually enjoy doing.

About two years ago, I got out of a stressful healthcare management career and started working as a ""data analyst."" Mostly, I was doing data entry, but I was also organizing and cleaning my team's inputs. This eventually led to my current role at a small manufacturing firm where I do a little bit of everything when it comes to data. I don't have access to our production databases (Salesforce and our ancient ERP), but I have access to SQL Server copies of this data; these pipelines are maintained by IT.

Most of the data sources to which I have access are OLTP systems that require knowledge of the table relationships (not well-documented, and no PK-FK relationships in our ERP tables at all), so I've had to get reasonably good at analytical SQL (i. e., all the SELECT work, including CTEs, window functions, subqueries, aggregates, and the various formatting and conversion functions). I use these either for ad-hoc reporting in Excel or for more standard reports in Power BI. I've developed a decent amount of PBI knowledge and can write some competent if not exciting DAX.

I'm at a point where I'm very comfortable with my day-to-day work of ad-hoc and standard reporting. I understand the data sources quite well, and I'm able to write queries that answer our business questions quickly and efficiently. I would like to start putting downtime and personal time into improving my skill set and career prospects, but it's a little hard to know which way to go given that I generally enjoy most aspects of my job. If I had to rank things, they go like this:

1. Requirements gathering is probably my favorite. A lot of data consumers try to frame questions in terms of what data they need when they don't even have a firm idea of the business question they're trying to answer. I really do think that a quality solution starts with clear definitions of business value.
2. I also get a lot of enjoyment out of puzzle-solving. When I finally get a query that does exactly what I need it to do, I get a huge sense of satisfaction. I don't tend to get frustrated, even when working on a solution for a long time, so as long as I'm making some sort of progress, I'm pretty happy when I have something challenging.
3. I worked on a data modeling project for a new data source. It was intense, and I got a little burnt out by the end, but I mostly enjoyed the intense interviewing, source review, data modeling, and source-target mapping process.

With these interests and skills, do you have any suggestions about where to focus? I go back and forth between wanting to dig into statistics and experimental design to become a better analyst and developing database skills and ETL technology knowledge to build higher-quality data sources; given where my organization is, the latter is probably more important, but that's only a distant consideration.

Do you find DE work creative and engaging, or do you find the job satisfying in other ways? What sort of personality fits best with the job, and what would be a better fit on the analytics or BI side?"
2505,2020-06-19 14:00:53,1592564453.0,dataengineering,Natural Gas Production Ranking | TOP 10 Country from 1970 to 2018,hbyoph,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hbyoph/natural_gas_production_ranking_top_10_country/,1.0,0.0,0.0,14824.0,
2506,2020-06-19 14:29:40,1592566180.0,dataengineering,Udacity data streaming Nanodegree,hbz1lr,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/hbz1lr/udacity_data_streaming_nanodegree/,1.0,10.0,0.0,14824.0,"Hey folks of DE, 

I was looking forward to apply for the data streaming nano degree on udacity because I heard about the free one month access. But to my suprise it is only valid for America's and Europe and not India. So wanted to ask of anyone has done it and is it worth the $400 . 

Any suggestion/ replies are welcomed."
2507,2020-06-19 16:34:26,1592573666.0,dataengineering,Interview Tips and Preparation,hc0rzq,nilbro,,https://www.reddit.com/r/dataengineering/comments/hc0rzq/interview_tips_and_preparation/,1.0,12.0,0.0,14828.0,"Hi,

I have been invited for an interview for an automotive company for the role of Big Data Engineer. This will be first real interview for a Big Data role.  Hence I am looking for some preparation tips and common questions. Glassdoor did not help me much.

A little background of my skills. I have been in a support role for an ETL project and recently completed an internship where I was required to work with MongoDB, Kafka and PySpark.

TIA!"
2508,2020-06-19 18:48:04,1592581684.0,dataengineering,Industrialization of a ML model using Airflow and Apache BEAM,hc31vm,Snoo_47594,,https://www.reddit.com/r/dataengineering/comments/hc31vm/industrialization_of_a_ml_model_using_airflow_and/,3.0,1.0,0.0,14834.0,
2509,2020-06-20 00:53:40,1592603620.0,dataengineering,Data Engineering Youtube Channel!,hc9ogc,olympuk,,https://www.reddit.com/r/dataengineering/comments/hc9ogc/data_engineering_youtube_channel/,1.0,13.0,0.0,14853.0,"Hey guys!

I recently decided to create a Youtube Channel dedicated to discussing data engineering topics and just released my first video. It was just a way to start, more interesting, and hands-on videos will be released soon but for the time being it would mean a lot if you could check it out and let me know what you think.

 [https://www.youtube.com/channel/UCBT9cPM6LbCAIrFgg9Mi9YQ](https://www.youtube.com/channel/UCBT9cPM6LbCAIrFgg9Mi9YQ) 

Show some love! Thanks"
2510,2020-06-20 01:10:42,1592604642.0,dataengineering,Looking for Data Virtualization Product Input and Reviews,hc9z8b,splashout2,,https://www.reddit.com/r/dataengineering/comments/hc9z8b/looking_for_data_virtualization_product_input_and/,2.0,0.0,0.0,14853.0,"I'm looking for input/reviews from those with practical experience with \[Data Virtualization\]\[1\] products.  Specifically, I would like to put up a data virtualization facade that can be used by Business Intelligence tools (e.g., Tableau, InfoView, PowerBI, etc.) and/or a Java application. It needs to be able to connect to Oracle, MS SQL Server, Snowflake and Sybase IQ and ideally would include smart caching, etc., for better performance, especially when joining tables across DB's.

&amp;#x200B;

I did find this [https://www.datamation.com/big-data/top-data-virtualization-tools.html](https://www.datamation.com/big-data/top-data-virtualization-tools.html) of ""top 10"" products, but I am looking for more practical details. If possible, please include details on:

&amp;#x200B;

\- Ease of configuration, implementation and maintenance

\- Whether the product requires access to your data (i.e., a service) or can be run in-house or on cloud service provider.

\- General performance

\- Any gotchas, limitations or difficulties in using the product?

&amp;#x200B;

Some products in the running (unfortunately, all of these require you to sign up for a demo to get a peek):

&amp;#x200B;

\- Denodo

\- AtScale

\- Tibco

\- Data Virtuality

&amp;#x200B;

Thanks in advance for your input.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;"
2511,2020-06-20 03:14:00,1592612040.0,dataengineering,Continuously call API and save data to Influxdb,hcc3n7,Highzenberg,,https://www.reddit.com/r/dataengineering/comments/hcc3n7/continuously_call_api_and_save_data_to_influxdb/,1.0,4.0,0.0,14858.0,"I'm interested in creating a pipeline between an API and Influxdb. I would continuously call the API every 15 seconds or so and save the data to influxdb (timeseries database) on my server. I know I can do it either through a cronjob or a scheduler in a python script. However, I'm wondering if there is a more professional way to do it. I've heard about tools like Airflow but I'm wondering if that is overkill."
2512,2020-06-20 04:08:50,1592615330.0,dataengineering,Landing your first data engineering job,hcczes,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/hcczes/landing_your_first_data_engineering_job/,1.0,2.0,0.0,14859.0,"Hello everyone,

A couple weeks back I wrote an article explaining an approach you can take to land your first ""Data Engineering"" job. Got positive feedback from my subscribers and colleagues, so decided to post it here, the article is at [https://www.startdataengineering.com/post/approach-to-land-a-de-job/](https://www.startdataengineering.com/post/approach-to-land-a-de-job/)

The article takes a ""do enough to get an interview"" approach, where you don't have to completely master all topics  (you will have to keep learning to improve) but provide business value, to land an interview or get promoted within your current company. Hope this provides some direction and help for those looking to move into a data engineering position. Appreciate any feedback. Thank you."
2513,2020-06-20 06:35:45,1592624145.0,dataengineering,"Facebook Interview, Streaming Question",hcf0x4,datadawgg,,https://www.reddit.com/r/dataengineering/comments/hcf0x4/facebook_interview_streaming_question/,1.0,7.0,0.0,14862.0,"I have an onsite interview with FB coming up and the recruiter told me they would be asking me questions on how to design and implement a ""streaming"" pipeline, or in other words, how to make a visualization which updates on a constant basis. 

He said that this would be implemented in Python, while the ""batch"" pipelines would be implemented in sql. I have never created a streaming pipeline before, so i'm not sure what do expect from  or how to approach a question like this.

Would anyone have any advice or resources that they could point to?"
2514,2020-06-20 13:32:41,1592649161.0,dataengineering,Commercial Vehicle Production Ranking | TOP 10 Country from 1999 to 2018,hck10e,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hck10e/commercial_vehicle_production_ranking_top_10/,1.0,0.0,0.0,14873.0,
2515,2020-06-20 14:30:24,1592652624.0,dataengineering,Top Data Engineering Services,hckqz8,Poojakhana52314,,https://www.reddit.com/r/dataengineering/comments/hckqz8/top_data_engineering_services/,1.0,0.0,0.0,14874.0,
2516,2020-06-20 15:07:11,1592654831.0,dataengineering,IBM Data Science and AI Programs on Coursera Free for 30 Days,hcl8ku,awsconsultant,,https://www.reddit.com/r/dataengineering/comments/hcl8ku/ibm_data_science_and_ai_programs_on_coursera_free/,1.0,0.0,0.0,14874.0,
2517,2020-06-20 16:12:57,1592658777.0,dataengineering,Why are data engineering projects ideas hard to come by?,hcm33j,owila,,https://www.reddit.com/r/dataengineering/comments/hcm33j/why_are_data_engineering_projects_ideas_hard_to/,1.0,16.0,0.0,14876.0,"Something has been bugging my mind as a data analyst trying to transition into data engineering. Why are data engineering projects ideas hard to come by?
It's so difficult that when you have an idea, the data will be difficult to find. 

I will like your opinions on this and also some projects you built while learning."
2518,2020-06-20 18:08:57,1592665737.0,dataengineering,airflow not picking up dags from Kubernetes persistent volume,hcnsuf,sam_butt,,https://www.reddit.com/r/dataengineering/comments/hcnsuf/airflow_not_picking_up_dags_from_kubernetes/,1.0,0.0,0.0,14878.0,"Hi, I'm trying to follow this simple tutorial. but I am stuck in the last part. The dags which I put in the persistent volume aren't being shown/picked up on/by airflow.  
I check the logs as well but I don't get it.  
Its just two-three commands.

/opt/airflow/dags directory is empty. I'm unable to understand how the volume is connected to this directory?

[https://gitlab.com/usama.kaleem88/kubernetes-airflow](https://gitlab.com/usama.kaleem88/kubernetes-airflow)"
2519,2020-06-20 18:32:21,1592667141.0,dataengineering,airflow not picking up dags from Kubernetes persistent volume,hco6uq,sam_butt,,https://www.reddit.com/r/dataengineering/comments/hco6uq/airflow_not_picking_up_dags_from_kubernetes/,1.0,1.0,0.0,14879.0,"Hi, I'm trying to follow this simple tutorial. but I am stuck in the last part. The dags which I put in the persistent volume aren't being shown/picked up on/by airflow.  
I check the logs as well but I don't get it.  
Its just two-three commands.

/opt/airflow/dags directory is empty. I'm unable to understand how the volume is connected to this directory?

[https://gitlab.com/usama.kaleem88/kubernetes-airflow](https://gitlab.com/usama.kaleem88/kubernetes-airflow)"
2520,2020-06-20 22:49:45,1592682585.0,dataengineering,Best practices when scoping a complex pipeline architecture?,hcsk29,drumkeys,,https://www.reddit.com/r/dataengineering/comments/hcsk29/best_practices_when_scoping_a_complex_pipeline/,1.0,2.0,0.0,14892.0,"So I’ve been doing this for a bit, but I always have trouble estimating the scope/time of transformation pipeline projects.  Does anyone have advice as to best practice for scoping out the involvement? It’s a lot different than designing a DB architecture or a new platform to me, since it’s a lot of nuanced. I use a less popular software called Pentahoe Data Integration for ETL because it’s open source and allows us to quickly build custom transformation steps."
2521,2020-06-20 23:28:00,1592684880.0,dataengineering,Apache Spark 3.0 Released,hct67f,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/hct67f/apache_spark_30_released/,22.0,10.0,1.0,14895.0,
2522,2020-06-20 23:30:41,1592685041.0,dataengineering,Apache Pinot (OLAP similar to Druid) 0.4.0 Released,hct7pl,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/hct7pl/apache_pinot_olap_similar_to_druid_040_released/,1.0,0.0,0.0,14895.0,
2523,2020-06-21 05:06:43,1592705203.0,dataengineering,Study material for data architecture design,hcyotp,ploughthrough,,https://www.reddit.com/r/dataengineering/comments/hcyotp/study_material_for_data_architecture_design/,2.0,2.0,0.0,14904.0, any recommendation on study material for a noob data engineer who is planned to build data architecture of a start-up tech company from scratch?
2524,2020-06-21 12:07:34,1592730454.0,dataengineering,The best SQL vs NoSQL mindset I've ever heard,hd3tu2,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/hd3tu2/the_best_sql_vs_nosql_mindset_ive_ever_heard/,1.0,0.0,0.0,14911.0,
2525,2020-06-21 13:42:23,1592736143.0,dataengineering,Wheat Production Ranking | TOP 10 Country from 1961 to 2018,hd4wzo,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hd4wzo/wheat_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,14913.0,
2526,2020-06-21 13:47:11,1592736431.0,dataengineering,Top 15 goal scorers in FIFA World Cup (1930 to 2018),hd4z02,Evening_Sale,,https://www.reddit.com/r/dataengineering/comments/hd4z02/top_15_goal_scorers_in_fifa_world_cup_1930_to_2018/,1.0,0.0,0.0,14913.0,
2527,2020-06-21 15:37:03,1592743023.0,dataengineering,Databricks to host online Spark Summit starting tomorrow - Free &amp; Paid packages available,hd697h,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/hd697h/databricks_to_host_online_spark_summit_starting/,1.0,1.0,0.0,14916.0,
2528,2020-06-21 20:30:04,1592760604.0,dataengineering,BI Major who wants a more technical job/career. Is Data Engineering a viable option for me?,hdatbw,realincognita,,https://www.reddit.com/r/dataengineering/comments/hdatbw/bi_major_who_wants_a_more_technical_jobcareer_is/,4.0,12.0,0.0,14929.0,"Hello, I'm currently in the 5th semester of my BS in Business Intelligence. Unfortunately, during my last semesters I've realized I enjoy more the technical aspect of what I learn, and not so much the business part of it. I know that it's important to understand business, but I don't think I'd enjoy a job that focuses too much on it. I tried to change my major to CS or DS, but, unfortunately, I'd lose my scholarship if I did so.

Some of the courses I've taken / will take include Data Analytics, Data Mining, Databases, Predictive Analytics, Prescriptive Analytics. I've learned BI tools, SQL, R and a bit of Python in my courses. I'll also take a Machine Learning track, but it's not like I'll learn that much in only one semester. I've focused myself on learning more Python and SQL during this last year.

My initial plan was to get an entry level Data Analyst job and then transition to a DS one, but considering my education is very lacking in the Math department (no Linear Algebra, Multivariable Calculus or Advanced Statistics), and the difficult entry barrier that will only keep getting worse, I no longer think I'd be able to do this move even if I try to learn everything by myself. I also feel like I wouldn't be able to transition into many IT jobs considering my major doesn't have a lot of CS/Math courses.

I've been reading about Data Engineering and I've been really interested. Nevertheless, I have two questions:

1. Would someone from my background be able to transition into Data Engineering using online courses and personal projects?
2. Is there anything from my education/background that could give me an advantage or be useful? I feel like I'm studying and paying for something I probably won't use that much in the future.

TL;DR: I'm in my 5th semester of my BI major, but I want a more technical job/career. I planned to get into DS, but because of a lack of math education I don't think I'd be able to. I'm not sure if someone from my background could get into Data Engineering."
2529,2020-06-21 21:27:40,1592764060.0,dataengineering,Homelabs?,hdbthq,infiniteAggression-,,https://www.reddit.com/r/dataengineering/comments/hdbthq/homelabs/,2.0,2.0,0.0,14929.0,"Hello! I was wondering if any of you have a homelab that you use to hone your skills and if so, what skills would that be? Would you recommend someone at a beginner/intermediate level to look into one with the aim of bettering aforementioned skills?

Thanks!"
2530,2020-06-21 22:34:06,1592768046.0,dataengineering,Feedback on mini project for self learning? What should I NOT be doing and what else should I be doing?,hdcz83,dontlookmeupplease,,https://www.reddit.com/r/dataengineering/comments/hdcz83/feedback_on_mini_project_for_self_learning_what/,1.0,2.0,0.0,14932.0,"Just for context, I'm a total noob at DE and am trying to do self projects for fun and just to learn.

I am using this Covid19 API to get data:  [https://covid19api.com/](https://covid19api.com/). 

1. Essentially, I am writing some functions that are essentially a bunch of GET requests that will extract and process some data from this API and store it as a .csv file. 
2. I will then run Task Scheduler to automate the script to run daily. 
3. I will store the .csv file locally on my computer and load it into Postgre (all local)
4. I will create some tables and views so I can query my data in Postgres
5. I will use Tableau public and connect to Postgres and build some pretty dashboard to visualize all the data I imported in

Thoughts? I've thought about using AWS/BigQuery, but apparently you need to enter your credit card and I'm afraid of getting billed $$$."
2531,2020-06-21 23:38:41,1592771921.0,dataengineering,Is C++ worth learning to be a data engineering?,hde2ju,beer_chuggerr,,https://www.reddit.com/r/dataengineering/comments/hde2ju/is_c_worth_learning_to_be_a_data_engineering/,1.0,10.0,0.0,14936.0,"I would love to be a data engineer, and started watching a 10 hour long video on C++. I’m not done watching it but is it worth it or am I wasting my time? 

What else should I learn?"
2532,2020-06-22 09:05:13,1592805913.0,dataengineering,Transitioning to Data Engineering,hdmkgn,babie_kale,,https://www.reddit.com/r/dataengineering/comments/hdmkgn/transitioning_to_data_engineering/,1.0,6.0,0.0,14951.0,"Hi, I am a Mechanical engineer currently working in Power and Energy field. I have basic programming knowledge and i am thinking of transitioning to be a data engineer. I am curious to where to start. Any suggestions will be much appreciated. Thanks"
2533,2020-06-22 09:17:09,1592806629.0,dataengineering,Importance of Functional Programming in data Engineering?,hdmq8n,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/hdmq8n/importance_of_functional_programming_in_data/,1.0,15.0,0.0,14953.0,I was advised by a Data Engineer to upskill in functional programming. I was wondering how functional programming is important in this aspect and what is the best way I can learn it?
2534,2020-06-22 09:59:54,1592809194.0,dataengineering,External MySQL to S3 periodic ingestion?,hdna0d,the_dataguy,,https://www.reddit.com/r/dataengineering/comments/hdna0d/external_mysql_to_s3_periodic_ingestion/,1.0,5.0,0.0,14955.0,"What will be the cheapest maintainable and scalable way to perform this ingestion on a periodical way.

One of them is Stich data where you only need to configure.

I know writing a spark code also easy but less maintainable for resource crunch. 

Any other advice or design you guys suggest?"
2535,2020-06-22 09:59:55,1592809195.0,dataengineering,External MySQL to S3 periodic ingestion?,hdna0e,the_dataguy,,https://www.reddit.com/r/dataengineering/comments/hdna0e/external_mysql_to_s3_periodic_ingestion/,1.0,0.0,0.0,14955.0,"What will be the cheapest maintainable and scalable way to perform this ingestion on a periodical way.

One of them is Stich data where you only need to configure.

I know writing a spark code also easy but less maintainable for resource crunch. 

Any other advice or design you guys suggest?"
2536,2020-06-22 10:30:48,1592811048.0,dataengineering,Does file format matter when storing and querying from PostgreSQL?,hdno9b,oatsativa,,https://www.reddit.com/r/dataengineering/comments/hdno9b/does_file_format_matter_when_storing_and_querying/,1.0,5.0,0.0,14955.0,"Let's say I have a file in JSON and a file in Parquet. I run both of them through Spark and save to PostgreSQL. Would one be faster than the other in terms of writing to the database and querying?

I have a shit ton of data and I'm noticing it takes a long time just to query ""where year=2017"". Is it because I wrote the file as JSON? Or does that even matter since it got converted into a DataFrame in Spark? How could I make this simple query faster?

Sorry if this is confusing, I'm very new to DE and file formats/databases are a mystery."
2537,2020-06-22 12:20:56,1592817656.0,dataengineering,SQL Optimization on Join,hdp084,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/hdp084/sql_optimization_on_join/,1.0,5.0,0.0,14957.0," 

i have these 3 tables

Table\_A : 26 Column | 500k rows

Table\_B : 18 Column | 13,5k rows

Table\_C : 24 Column | 329k rows

and 3 kinds of query with diffrent join order

Query 1:

    select Table_A.* from Table_A  
    join Table_B on Table_A.Table_B_id = Table_B.id 
    join Table_C on Table_A.Table_C_id = Table_C.id  

Query 2:

    select Table_A.* from Table_A 
    join Table_C on Table_A.Table_C_id = Table_C.id 
    join Table_B on Table_A.Table_B_id = Table_B.id 

Query 3:

    select Table_A.* from Table_B 
    join Table_A on Table_B.id = Table_A.Table_B_id 
    join Table_C on Table_A.Table_C_id = Table_C.id 

Execution Time :

    Query 1 : 2093 ms 
    Query 2 : 901 ms 
    Query 3 : 700 ms 

My question is, why the execution time differ from each query? How do i optimize it even further ?"
2538,2020-06-22 14:56:02,1592826962.0,dataengineering,6 Great Data Engineering Courses,hdr0jo,enuintor,,https://www.reddit.com/r/dataengineering/comments/hdr0jo/6_great_data_engineering_courses/,1.0,0.0,0.0,14959.0,
2539,2020-06-22 15:31:29,1592829089.0,dataengineering,Extracting data from applicative production databases,hdrhw8,Alert_Dragonfly,,https://www.reddit.com/r/dataengineering/comments/hdrhw8/extracting_data_from_applicative_production/,1.0,2.0,0.0,14959.0,"Hi all,

I'm crossposting from /devops subreddit.

What are the best practices for data extraction from production applicative DBs?

For  context, I work in a data team where we serve internal teams by dering  insights from data (analytics). The classic use case is to pull data  from different sources and centralize them in a single analytics  database where we can perform some analysis to answer business  questions.

The (Dev)Ops runs a  daily bash script that would dump the complete DB into a file and SCP it  to the data machine. We would then restore it into our analytics DB.

We  are currently transitioning to another setup. Data is now in charge to  get the data by integrating the same script into our scheduling tool  (Airflow) to streamline the data overall process (we would be able to  run analysis as soon as all required applicative DB are restored instead  of waiting everything to be restored before doing analysis). To avoid  direct access to prod DB, (Dev)Ops created a read-only replica in-sync  with master DB.

The approach seems  reasonable but right now we are having many issues. The replica would  kill our dumping process as soon it would detect VACUUM operations at  main DB to avoid dumping stale data. After some researches, we found  some workarounds that would either:

\- Pausing VACUUM operations during at main DB to avoid desync

\- Manually add a delay of synchronisation when a long-query is running at replica

We  chose the latter one but both of them seem very wrong in case of a  blocked query: no VACUUMing at main DB or no sync anymore at the  replica. Moreover, we now need to increase the sync delay as soon as the  dumping operation time increases.

Are we doing things right? What are the best practices for this?

Thanks"
2540,2020-06-22 15:47:07,1592830027.0,dataengineering,Is Data Mining an important part of Data Engineering or Machine Learning Engineering?,hdrq0w,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/hdrq0w/is_data_mining_an_important_part_of_data/,1.0,21.0,0.0,14959.0,"I'm going to start a short course on Data Engineering soon, the course has 4 subjects: Python, SQL, Data Mining, and Data Wrangling.

Since my ultimate goal is to become a Machine Learning Engineer, should I replace the Data Mining subject with an introductory ML course (teaches traditional ML, no deep learning)? How important is Data Mining for a Data Engineer or Machine Learning Engineer?"
2541,2020-06-22 16:04:08,1592831048.0,dataengineering,Government Transparency Ranking | TOP 10 Country from 1980 to 2010,hdryx0,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hdryx0/government_transparency_ranking_top_10_country/,1.0,0.0,0.0,14959.0,
2542,2020-06-22 17:15:54,1592835354.0,dataengineering,Apache Flink - Local Setup Tutorial,hdt4b1,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hdt4b1/apache_flink_local_setup_tutorial/,1.0,0.0,0.0,14960.0,
2543,2020-06-22 21:00:17,1592848817.0,dataengineering,Is this a good junior data engineer's resume?,hdx9kr,MinkyPanther,,https://www.reddit.com/r/dataengineering/comments/hdx9kr/is_this_a_good_junior_data_engineers_resume/,4.0,32.0,0.0,14973.0,"Its been a month and i haven't even received a single call. I know i don't have coding experience but i am not sure what else can i do here.

I am still learning AWS and Hadoop/Spark to expand my skillset. 

https://imgur.com/BKYYyFy"
2544,2020-06-22 22:49:00,1592855340.0,dataengineering,Who owns writing tests at your organization?,hdze1p,PelicanIO,,https://www.reddit.com/r/dataengineering/comments/hdze1p/who_owns_writing_tests_at_your_organization/,1.0,5.0,0.0,14978.0,I have been having a back and forth lately with our DS team. The DS team feels DE should own writing tests. The DE team feels that A.) they sometimes lack context to write tests themselves and B.) DS can easily do this themselves via DBT. How do you all do it? Is it the responsibility of one team to write tests or is the responsibility more shared?
2545,2020-06-22 23:42:26,1592858546.0,dataengineering,"Searching for data on e-commerce to help improve international trade agreements for times of crisis, but cannot find any. Do you have any pointers?",he0e6d,Joseangelmc,,https://www.reddit.com/r/dataengineering/comments/he0e6d/searching_for_data_on_ecommerce_to_help_improve/,1.0,0.0,0.0,14979.0,"Hello, everyone, I am currently doing an investigation on e-commerce as part of a project to improve international trade agreement negotiation for times of crisis. My team wants to understand how e-commerce has been behaving before and after COVID-19 (preferably, but not limited to the EU and Latin America) so we can compare it to existing trade agreements and look for improvement opportunities in these.

The problem is that after searching in data banks like the World Bank, I cannot find any tangible data about e-commerce. I am a junior data analyst and probably my lack of experience is showing; could you please point me at where I might find something?

Thank you."
2546,2020-06-22 23:51:36,1592859096.0,dataengineering,How to find Snowflake + FiveTran experts?,he0k5e,2meirl1,,https://www.reddit.com/r/dataengineering/comments/he0k5e/how_to_find_snowflake_fivetran_experts/,1.0,10.0,0.0,14979.0,"My gf is looking for a data engineer to build a data lake with Snowflake and FiveTran expertise. She's having a hell of a time. Anybody have suggestions on where she could find some candidates? She's an expert on LinkedIn, Meetups, but beginning to wonder if this is a unicorn? Or just super new so not a lot of people yet?   
Any help is appreciated in pointing her in the right direction is appreciated!!! Have a great week!"
2547,2020-06-23 02:47:03,1592869623.0,dataengineering,"Is a GitHub repo required for a Data Engineering resume, and what should be in it?",he3n0e,ONLY_COMMENTS_ON_GW,,https://www.reddit.com/r/dataengineering/comments/he3n0e/is_a_github_repo_required_for_a_data_engineering/,1.0,8.0,0.0,14984.0,"I've been working in data for 5 years now and I'm looking to make the transition to Data Engineering. My current role is as a Senior Data Analyst, but it's more end to end development, so I have Data Engineering experience although I've never written a Data Engineering resume. I've heard that a GitHub link is necessary, but I'm not sure what I'd showcase on it as all of my ETL/big data experience is with my career. I have a few Python/Java coding projects I've worked on in my spare time, but again nothing ETL related. 

Is a GitHub link required for a Data Engineering resume? Is it worth adding coding projects without much ETL?"
2548,2020-06-23 12:27:47,1592904467.0,dataengineering,Looking for simple practice projects to use Scala (not Spark),hebgov,Rey661199,,https://www.reddit.com/r/dataengineering/comments/hebgov/looking_for_simple_practice_projects_to_use_scala/,1.0,6.0,0.0,14993.0,"I have been doing courses on Scala but mostly to learn the syntax and the language. A lot of these courses are theoretical and hard to relate to production code (of course that also has benefits). 

I am looking for simple projects (not in Spark) that involve things like grabbing data from one place and processing it concurrently."
2549,2020-06-23 12:33:05,1592904785.0,dataengineering,DataEngBytes call for speakers - online Aus hosted DE conference in August 2020,hebir6,HansSlinger,,https://www.reddit.com/r/dataengineering/comments/hebir6/dataengbytes_call_for_speakers_online_aus_hosted/,1.0,0.0,0.0,14993.0," Hi reddit DE community, do you have an interesting  story to tell in the Data Engineering space? This is a CALL FOR SPEAKERS  for the DataEngBytes conference I am involved in online on the 20th and  21st of August.  It is Australian hosted from our Data Engineering Meetup community (which is nearing 4000 members now that we've been hosting fortnightly Meetups during COVID)

[https://sessionize.com/dataengbytes](https://sessionize.com/dataengbytes) 

and our Meetup links in case you are interested:

Melbourne: [https://www.meetup.com/Melbourne-Data-Engineering-Meetup/](https://www.meetup.com/Melbourne-Data-Engineering-Meetup/)

Many thanks and all the best"
2550,2020-06-23 13:31:38,1592908298.0,dataengineering,Potato Production Ranking | TOP 10 Country from 1961 to 2018,hec6th,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hec6th/potato_production_ranking_top_10_country_from/,1.0,2.0,0.0,14993.0,
2551,2020-06-23 15:45:37,1592916337.0,dataengineering,Apache Flink training by Ververica (Paid) announced with sessions conducted between now and August,hedyjb,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/hedyjb/apache_flink_training_by_ververica_paid_announced/,1.0,0.0,0.0,14996.0,
2552,2020-06-23 16:22:39,1592918559.0,dataengineering,Does Elasticsearch lie? How does Elasticsearch work?,heeinu,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/heeinu/does_elasticsearch_lie_how_does_elasticsearch_work/,1.0,0.0,0.0,14996.0,
2553,2020-06-23 18:35:25,1592926525.0,dataengineering,What we got wrong about data governance,hegs7j,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hegs7j/what_we_got_wrong_about_data_governance/,5.0,2.0,0.0,15005.0,"Title say it all, but brings up the incompleteness of data catalogs as an agent of governance. Thoughts? 

[https://medium.com/@barrmoses/what-we-got-wrong-about-data-governance-365555993048?source=friends\_link&amp;sk=5678d0eeabb7ebfa17212358fd801fc4](https://medium.com/@barrmoses/what-we-got-wrong-about-data-governance-365555993048?source=friends_link&amp;sk=5678d0eeabb7ebfa17212358fd801fc4)"
2554,2020-06-23 18:35:56,1592926556.0,dataengineering,Airflow Summit 2020 (fully online),hegsjg,thibaut_barrere,,https://www.reddit.com/r/dataengineering/comments/hegsjg/airflow_summit_2020_fully_online/,3.0,2.0,0.0,15005.0,
2555,2020-06-23 19:42:16,1592930536.0,dataengineering,We've built a data engineering tool to make writing Spark code much easier,hei1ea,several27,,https://www.reddit.com/r/dataengineering/comments/hei1ea/weve_built_a_data_engineering_tool_to_make/,2.0,0.0,0.0,15008.0,"Hi all! 

We're a small team of ex-spark/hadoop/hive engineers &amp; compilers experts, who's has been hard at work to develop a ‘Code-First Data Engineering’ product. We’re super excited to announce the Public beta of Prophecy Data Engineering today!

It includes a ‘Code=Visual’ editor, where you can toggle between a fully-featured coding IDE and a visual graph editor, to author Spark code that is standardized, performant and maintainable. 

Let us know what we can do better and what you’re loving! You can sign up for beta at: [https://www.prophecy.io/blogs/prophecy-public-beta](https://www.prophecy.io/blogs/prophecy-public-beta)"
2556,2020-06-23 22:01:33,1592938893.0,dataengineering,is airflow kinda cruddy?,hekoqn,adappergentlefolk,,https://www.reddit.com/r/dataengineering/comments/hekoqn/is_airflow_kinda_cruddy/,2.0,9.0,0.0,15014.0,"we’ve been working with airflow recently based on the raving reviews as an industry standard but we’re honestly finding that it seems to be an application from another era that extremely mixes up execution and scheduling. the entire thing has containers basically as an afterthought, is not very well integrated with kubernetes, and there’s not a lot of isolating of logical environments within an airflow environment.  the approach of pushing your code/data to special folder itself strikes us as a tad odd. wondering what folks think here and what other schedulers you use?

[View Poll](https://www.reddit.com/poll/hekoqn)"
2557,2020-06-24 00:17:11,1592947031.0,dataengineering,Extract Data using API in Python,hen8ly,iwillgetintofaang,,https://www.reddit.com/r/dataengineering/comments/hen8ly/extract_data_using_api_in_python/,1.0,1.0,0.0,15018.0,"I cleared the sql round for a DE interview. Next is with Python. I was told i will be asked questions to call specific data using an open API. 

Any thoughts on what kind of questions i should expect or any pointers to resources?"
2558,2020-06-24 00:42:46,1592948566.0,dataengineering,End-to-end stages in Machine Learning model lifecycle,henpst,mtrxdv,,https://www.reddit.com/r/dataengineering/comments/henpst/endtoend_stages_in_machine_learning_model/,3.0,14.0,0.0,15020.0,
2559,2020-06-24 01:53:57,1592952837.0,dataengineering,Vertica: Update a table without using UPDATE,hep1pt,levelworm,,https://www.reddit.com/r/dataengineering/comments/hep1pt/vertica_update_a_table_without_using_update/,1.0,3.0,0.0,15023.0,"Hi experts,

I read some posts on SO and it looks like that `INSERT/SELECT` is a lot more efficient then `UPDATE` in Vertica database. Please let me know if I am wrong.

I have two tables:

table_agg, ALIAS t1

Contains some historical aggregation data, let's say with column id, a, b and c.

table_agg_staging, ALIAS t2

Contains same aggregation data, but just for yesterday, again column id, a, b and c.

I want to update t1 (table_agg) with following criteria:

- If t2.id is not found in t1.id, insert into t1
- If t2.a &gt; t1.a AND t2.c = 0 then t1.a = t2.a
- If t2.b &lt; t1.b AND t2.c = 1 then t1.c = t2.c

(You can also see here I cannot use UPDATE as it uses two different criteria for different columns)

Let's ignore the `INSERT` part (bullet point 1) because it's easy. For bullet points 2 and 3 it's also easy to use two `CASE WHEN THEN ELSE END` to do the job, but I don't know how to dump the updated data back to t1. So my question is, how do I dump the data back to t1? Do I need a third table?"
2560,2020-06-24 02:12:01,1592953921.0,dataengineering,Call For Presentations - Flink Forward Global 2020 - Deadline Extended to June 28th:,hepdtf,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/hepdtf/call_for_presentations_flink_forward_global_2020/,1.0,0.0,0.0,15025.0,
2561,2020-06-24 05:54:41,1592967281.0,dataengineering,Good courses for DE,hesw3s,ash0550,,https://www.reddit.com/r/dataengineering/comments/hesw3s/good_courses_for_de/,1.0,9.0,0.0,15029.0,"Hi , can anyone recommend a good online course for DE track . I currently work on Tableau and other BI tools and would like to gain knowledge on the DE track to get into a better job . I checked datacamp on someone’s recommendation but would appreciate if someone has a better recommendation. I already have an account with Linux academy but I only see certification courses , so if I’m missing anything there please let know
Thanks in advance !"
2562,2020-06-24 08:19:44,1592975984.0,dataengineering,Is it just me?,heux4p,hyperandaman,,https://www.reddit.com/r/dataengineering/comments/heux4p/is_it_just_me/,1.0,1.0,0.0,15032.0,"Does anyone else have a super hard time finishing Udemy courses? Even though the sections are full of 5-10 minute videos, I find them boring as hell"
2563,2020-06-24 11:30:02,1592987402.0,dataengineering,[Tutorial &amp; Video] Loading CSV data into Kafka,hexcqf,rmoff,,https://www.reddit.com/r/dataengineering/comments/hexcqf/tutorial_video_loading_csv_data_into_kafka/,1.0,0.0,0.0,15037.0,
2564,2020-06-24 14:28:20,1592998100.0,dataengineering,Beer Consumption Ranking | TOP 10 Country from 1961 to 2011,hezd5b,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hezd5b/beer_consumption_ranking_top_10_country_from_1961/,1.0,1.0,0.0,15059.0,
2565,2020-06-24 14:41:39,1592998899.0,dataengineering,VS Code or Pycharm for your python development?,hezj4q,tallwithknees,,https://www.reddit.com/r/dataengineering/comments/hezj4q/vs_code_or_pycharm_for_your_python_development/,1.0,8.0,0.0,15064.0,"Hey guys, just curious, and it will help me decide. Do you use VS Code or Pycharm for your general python development? 

If VS Code what plugins do you have?

[View Poll](https://www.reddit.com/poll/hezj4q)"
2566,2020-06-24 14:52:29,1592999549.0,dataengineering,Data-driven Matchmaking at Azar with Apache Flink,hezo0q,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hezo0q/datadriven_matchmaking_at_azar_with_apache_flink/,1.0,0.0,0.0,15064.0,
2567,2020-06-24 17:04:07,1593007447.0,dataengineering,"Which is faster, A MPP or spark sql",hf1ker,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/hf1ker/which_is_faster_a_mpp_or_spark_sql/,1.0,6.0,0.0,15077.0,"What do you think which is faster
A count query in a MPP like redshift 

Or

Same query using spark sql"
2568,2020-06-24 17:32:36,1593009156.0,dataengineering,QuestDB 5.0 Released With In-Browser Demo Featured,hf21hb,[deleted],,https://www.reddit.com/r/dataengineering/comments/hf21hb/questdb_50_released_with_inbrowser_demo_featured/,1.0,0.0,0.0,15086.0,
2569,2020-06-24 17:33:51,1593009231.0,dataengineering,QuestDB Released With In-Browser Demo on a 1.6B Row Dataset,hf228o,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/hf228o/questdb_released_with_inbrowser_demo_on_a_16b_row/,1.0,0.0,0.0,15088.0,
2570,2020-06-24 17:34:39,1593009279.0,dataengineering,An interview about how the GoodData platform lets you bring business analytics to your customers and end users.,hf22qt,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hf22qt/an_interview_about_how_the_gooddata_platform_lets/,1.0,0.0,0.0,15088.0,
2571,2020-06-24 19:34:22,1593016462.0,dataengineering,"Advice for structuring transition away from cronjobs and scripts to Airflow for managing our ETL? I have Airflow set up and ready to go, just curious about best practices for managing the migration now.",hf48uw,tylerjaywood,,https://www.reddit.com/r/dataengineering/comments/hf48uw/advice_for_structuring_transition_away_from/,1.0,6.0,0.0,15093.0,"I have an Airflow instance set up and ready to work, now I'm staring at our library of ~75-100 cron tasks and have no idea what is the best way to start moving them over and validating that they're running correctly. 

Most of our cronjobs call .sh or .py scripts that do like 15 other things so initially I'm just going to have Airflow call those scripts directly using the BashOperator but would like to have some sort of built in checks for validation against the cron job executions for accuracy. Is there any good way to accomplish this?

#####TL;DR
moving our ETL from cron to airflow DAGs. What is best practice for ensuring the DAGs are running correctly and validating them against output from the cron tasks?

######Edit: 
my first (probably naive) thought is to run the jobs in parallel for sometime but that will mean creating duplicate tables for the jobs to insert into and then comparing the rows landed in each, but if the scripts aren't running at exactly the same time, then there will naturally be different data in them. 

I guess in this approach I could pick some window of time within the execution date to compare both executions against? But actually copying all the scripts and editing the DDL statements to direct the Airflow executions to new duplicated tables seems like a real pain in the butt"
2572,2020-06-25 00:11:15,1593033075.0,dataengineering,Advice on getting more experience with ‘enterprise scale’ data engineering techniques,hf9l23,sglem,,https://www.reddit.com/r/dataengineering/comments/hf9l23/advice_on_getting_more_experience_with_enterprise/,1.0,9.0,0.0,15104.0,"I am a current data engineer with experience building out pipelines to combine disparate relational database systems and flat files using Python scripts and Apache Airflow. Common feedback I receive in failed interviews is that I don’t have experience moving large datasets (potentially petabytes) in a distributed system. I’ve recently learned some AWS and Spark fundamentals and have started learning the basics of Spark streaming but am still having trouble getting practice developing and testing pipelines at this scale given the lack of resources / infrastructure I have access to. Any advice? Any good certifications or courses that you feel would help? And more specifically, any best practices for testing ETL pipelines for data leaks when working with high volume data?"
2573,2020-06-25 02:47:28,1593042448.0,dataengineering,Testcase examples for verifying a hadoop -&gt; azure migration?,hfc5l0,alaskanloops,,https://www.reddit.com/r/dataengineering/comments/hfc5l0/testcase_examples_for_verifying_a_hadoop_azure/,1.0,0.0,1.0,15106.0,"Our company is migrating our big data jobs from an on prem hadoop based architecture to Azure, and I've been tasked with developing a set of test cases so that we can verify the data pipelines work as expected.  I'm having a hard time finding resources on this topic, and wondering if anyone has any ideas?  Has anyone gone through an on prem hadoop -&gt; cloud (aws/azure) migration before?"
2574,2020-06-25 03:52:27,1593046347.0,dataengineering,Help request: Can't connect to Redshift using Python,hfcwso,noahpoah,,https://www.reddit.com/r/dataengineering/comments/hfcwso/help_request_cant_connect_to_redshift_using_python/,1.0,6.0,0.0,15109.0,"I'm working through the Udacity DE nanodegree, and I'm doing the AWS module project. The first time I created a Redshift cluster using boto3 in a Jupyter notebook, everything worked fine. I deleted the cluster, and I can create a cluster again, but I can't connect to the database. Any help would be appreciated.

I'm adapting boilerplate code from the lessons. I can access data in s3, I can create an IAM role and attach the policy that enables S3 read access, and I can create the redshift client and use it to create a cluster.

I am getting a security group related error here, but I don't know if this is important. I'll add details about it below. In any case, I can get around this error and verify in the notebook and via the web interface that the cluster is available.

Then I use an ec2 resource to get a vpc object to authorize ingress to the cluster. I get another error here, but I can work around it (details below).

Then I try to use psycopg2 to connect to the database, but I can't. I spent a fair amount of time trying to figure out what the host is, and I gather that it's the cluster endpoint, but using the endpoint as the host input argument doesn't work. I also cannot connect using a SQL command adapted from the lessons (i.e., `postgresql://dbusername-password-@-endpoint-port/dbname`).

I'm stumped, so any and all advice, help, pointers to relevant docs, etc..., would be appreciated. I've been googling and reading AWS, boto3, and psycopg2 docs, but I'm still stumped.

So, specific advice about the errors I'm getting would be very helpful, obviously, but so would more general information about AWS, I think, since my grasp on the system is still fairly tenuous. Thanks in advance.

Possibly important error details:

1) When I create the cluster (using `redshift.create_cluster()`), one of the input arguments from the lessons was `ClusterSecurityGroup`. I had created a security group, and when I initially specified this input argument with a list containing only the name of this security group, it worked fine. Now it doesn't, telling me:

    An error occurred (InvalidParameterValue) when calling the CreateCluster operation: VPC security group cannot be used for the property ""ClusterSecurityGroups""

If I comment out this input argument, the cluster is created just fine, I gather with the default security group, which I think might be a problem.

2) Somehow, the security group I created (and failed to specify, as described immediately above) ends up associated with the VPC that I can access via my ec2 resource. The code I'm adapting from the exercise gets the security group from the VPC, then uses the `authorize_ingress` method.

One of the input arguments here is `CidrIp`, and this is specified as `0.0.0.0/0` in the exercise solution. If I leave this (and the same port number) in, I get this error:

    An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule ""peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW"" already exists

If I comment out the `CidrIp` argument, there is no error."
2575,2020-06-25 06:44:05,1593056645.0,dataengineering,Advantages of ELT architecture over ETL,hffg6e,MasterEpictetus,,https://www.reddit.com/r/dataengineering/comments/hffg6e/advantages_of_elt_architecture_over_etl/,1.0,33.0,0.0,15119.0,"My company has fairly recently moved to the cloud on Snowflake and adopted an ELT architecture, moving away from the traditional ETL model. This is the first time I work with ELT and I'm seeing many advantages. No longer we need to wait for those long jobs that, when are delayed or break, slow down several reports. There is no backfill, which tends to be a pain in ETL architectures. Also, there is no scheduling and dependency issues. We don't even need use airflow to manage ETL dependencies and automate job runs. 

Curious as to what others think since this is still pretty new to me."
2576,2020-06-25 06:49:59,1593056999.0,dataengineering,Project Ideas on Data Engineering portfolio projects and seeking Collaboration to work on them,hffj67,crazy_roadster,,https://www.reddit.com/r/dataengineering/comments/hffj67/project_ideas_on_data_engineering_portfolio/,1.0,21.0,0.0,15119.0,"Hi everyone,  
I am planning to work on a couple of projects to add to my portfolio and simultaneously gain expertise in Airflow, Python, and one of the Cloud Storage Platforms through these projects.   
I would be glad if anyone with experience in the above stack could provide suggestions on a good project use case. Thanks.

Also, if anyone is in the same boat and interested in working on projects, please feel free to contact me."
2577,2020-06-25 13:54:50,1593082490.0,dataengineering,Economic Complexity Ranking | TOP 10 Country from 1964 to 2016,hfk2o5,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hfk2o5/economic_complexity_ranking_top_10_country_from/,1.0,0.0,0.0,15130.0,
2578,2020-06-25 17:05:40,1593093940.0,dataengineering,Flink on Zeppelin Notebooks for Interactive Data Analysis,hfmo4e,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hfmo4e/flink_on_zeppelin_notebooks_for_interactive_data/,1.0,0.0,0.0,15135.0,
2579,2020-06-25 17:25:23,1593095123.0,dataengineering,GitHub action sql formatting,hfmzif,jmwagg105,,https://www.reddit.com/r/dataengineering/comments/hfmzif/github_action_sql_formatting/,1.0,6.0,0.0,15135.0,Anyone using github actions to format sql files and can make some recommendations? Trying to standardize formatting and maintain some flexibility. Thanks for the references.
2580,2020-06-25 17:37:13,1593095833.0,dataengineering,What is Data Observability?,hfn6km,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hfn6km/what_is_data_observability/,1.0,1.0,0.0,15135.0,"Interested to see if other teams have similar ""data observability"" solutions or if your definitions differ.

[https://towardsdatascience.com/what-is-data-observability-40b337971e3e?source=friends\_link&amp;sk=db5d16e059c4c1c84f9d436f1e686c5b](https://towardsdatascience.com/what-is-data-observability-40b337971e3e?source=friends_link&amp;sk=db5d16e059c4c1c84f9d436f1e686c5b)"
2581,2020-06-25 19:22:27,1593102147.0,dataengineering,Difference Between ETL &amp; ELT,hfp0zi,dbs2001,,https://www.reddit.com/r/dataengineering/comments/hfp0zi/difference_between_etl_elt/,1.0,24.0,0.0,15139.0,"I am confused. How do we differentiate between the 2 in terms of 

(1) Servers

(2) DBMS instances

(3) ETL Tools"
2582,2020-06-25 20:32:09,1593106329.0,dataengineering,"How to perform conditional operation on ""Datasets"" using Pandas &amp; Python",hfqb68,8329417966,,https://www.reddit.com/r/dataengineering/comments/hfqb68/how_to_perform_conditional_operation_on_datasets/,1.0,0.0,0.0,15142.0,https://youtu.be/sCXO7wx9W4U
2583,2020-06-25 21:15:32,1593108932.0,dataengineering,"Spark Delight - A Spark UI replacement with a better UX, new metrics, and automated performance recommendations",hfr3vr,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/hfr3vr/spark_delight_a_spark_ui_replacement_with_a/,1.0,2.0,0.0,15145.0,"Hello guys,

[This is what it looks like](https://i.redd.it/im3q4jt6l3751.gif)

It's not released yet, but we're working on a Spark UI replacement -- easier to use, with system metrics, and high-level feedback with recommendations to make your app perform better.  
You can view the details here: [https://www.datamechanics.co/blog-post/building-a-better-spark-ui](https://www.datamechanics.co/blog-post/building-a-better-spark-ui)

We would make it work on top of any Spark platform (not just ours), entirely free of charge. It's a big project, so we'd love to get your feedback on it, particularly those of you who are familiar with Spark and the Spark UI. What do you think?

Thanks 🙏  
JY, Co-Founder of Data Mechanics."
2584,2020-06-26 03:38:35,1593131915.0,dataengineering,Help translating ETL to Airflow - Dynamic tasks,hfxpu1,baddays79,,https://www.reddit.com/r/dataengineering/comments/hfxpu1/help_translating_etl_to_airflow_dynamic_tasks/,1.0,10.0,0.0,15151.0,"Hi everyone,

I have an existing ETL process and I’m attempting to migrate it to Airflow from google cloud scheduler/compute engine running Python scripts. I’m a total Airflow beginner.

I’m having a bit of a hard time wrapping my head around structuring my workflow in Airflow. My workflow starts out with an API call to get a list of n things. Depending on the attributes of each of these things, I might need to make another API call or not, and I don’t know how many things the API call will return.

In Airflow, I created a PythonOperator that successfully makes the API call every 5 minutes and prints the results. But now I want to pass each “thing” in these results to its own task.

Can I dynamically create n tasks based on the results of my first API call that each operate on a single “thing”? Or should I be thinking of each task as operating on a batch of things? How do I handle branching per thing if I’m operating on a batch?

Should Airflow even be handling this logic?

Thanks in advance for your help! This community has been great getting me up and running."
2585,2020-06-26 05:45:21,1593139521.0,dataengineering,BaDSV — BaDly-Separated Values,hfzkow,drecklia,,https://www.reddit.com/r/dataengineering/comments/hfzkow/badsv_badlyseparated_values/,1.0,0.0,0.0,15157.0,
2586,2020-06-26 07:39:20,1593146360.0,dataengineering,I desperately need help designing a data model!,hg14x3,makecents91,,https://www.reddit.com/r/dataengineering/comments/hg14x3/i_desperately_need_help_designing_a_data_model/,1.0,10.0,0.0,15162.0,"Hello everyone, I need help and feedback in designing a (better) data model. I currently store data that is put onto an FTP for me to download. I have Python scripts (automated with Windows Task Scheduler) that download that data into a network drive and then it inserts it into a table in a Data Warehouse (PostgreSQL). From here, I have SQL scripts (executed manually) that formats and inserts the data into a star schema. Then I have a report builder python script that uses Pandas and psycopg2 to query metrics from the star schema and writes it into an Excel template. The report builder is generalized to use configuration for each report type to figure out what data is needed and how it should be formatted. The end product is a production report in the form of an Excel sheet. 

I know that this whole process can be automated from end-to-end, and I’ve been reading up on some data engineering articles and technologies to figure out the best way. I’ve only started to read the Data Engineering handbook by Andreas Kretz. At this point, I think I can use Airflow to automate the Python scripts that pipe the data through. It also sounds like Airflow can do what Kafka does, am I right about that? Also, our postgres server runs pretty slow (maybe a few minutes querying a year of data). Our server runs is a box at the office. Inserting a year or two of data into the star schema takes a whole day. It’s dealing with 300 GB of data right now, and we expect to expand our dimensions from 30 columns to 200 columns at some point in the future. Would a columnar data warehouse solution like Redshift be the right move? How much does something like that cost?

If you have other ideas as to how I should implement this, please I’m all ears and open to suggestions! Also, if you can recommend any books or articles I should read to help me build a strong foundation as a data engineer and would be useful, I’m all for it.

&amp;#x200B;

Some background – I don’t have experience as a data engineer. I was hired on as a data analyst and the team used to do these reports from scratch, from downloading source data, to cleaning the data in Excel, then to pasting it into a template. I realized that much of their process could be simplified with some Python scripts and a data warehouse system, so I proposed it as a solution and volunteered to do this, and I got to work on building it. I am now their “Data Engineer”. The company I’m at lacks talent and expertise in this area, and they look to me when I also really have no clue. I lack much needed mentorship and guidance. I have enough freedom to implement this in whichever way I want, but I have no idea what I’m doing and I’m learning and implementing things on the fly."
2587,2020-06-26 08:16:10,1593148570.0,dataengineering,"Student here, super confused about the future of the industry",hg1loi,ericblobb,,https://www.reddit.com/r/dataengineering/comments/hg1loi/student_here_super_confused_about_the_future_of/,1.0,32.0,0.0,15163.0,"I'm currently a student majoring in Computer Engineering looking to get into Data Engineering as a career. Putting aside the fact that a lot of companies seem to just have ""Software Engineer, Data/Data Infrastructure/Data Platform/Big Data"" as the title, after reading through this sub and /r/bigdata and Medium posts here and there I've become *really* confused about where this industry is headed, specifically regarding SQL and Big Data.

[This post](https://medium.com/skills-matter/why-you-cant-do-all-of-your-data-engineering-with-sql-294137b88479) and [this post](https://www.jesse-anderson.com/2018/06/the-two-types-of-data-engineering/) seem to push the view that data engineering in the Big Data world is moving towards programming languages (with frameworks like Spark) doing the processing and not SQL. There's even this thing called [Kappa architecture](https://milinda.pathirage.org/kappa-architecture.com/) which sounds crazy to me (like, no databases? what?!).

But comments in [this post](https://www.reddit.com/r/dataengineering/comments/gh2674/ideas_for_etl_pipeline_architecture/) seem to push the exact opposite, that you should ""embrace SQL"" as the top comment literally says. And [this post](https://www.reddit.com/r/dataengineering/comments/g0bjq7/sql_to_de_pipeline_transition/), whose top comment literally says that ""data modeling with SQL is hot right now,"" linking to [this blog post](https://medium.com/dataform/consider-sql-when-writing-your-next-processing-pipeline-5167f67afb16).

WTF is going on? Is it just all about use cases? Maybe SQL is good for low amounts of data but Big Data should use Spark and stuff?

And then what is it with all this stuff about ETL/ELT? Some posts mention ETL pipelines but it really seems like they're doing ELT. But how does ELT even work? Like, sure, you do the Load before the Transform, but you gotta Load it again right? You don't Transform the data and just leave it there in memory, that's preposterous. So it should really be called ELTL shouldn't it? Fuck me, this is so confusing.

From what I can tell, ETL is like: source -&gt; transformation scripts/programs like Informatica -&gt; data warehouse, and ELT is like: source -&gt; data lake/data warehouse (Redshift, Snowflake) -&gt; transform in Redshift -&gt; ??? (back to Redshift again?)

Can anyone shine some light on these contradictions and inconsistencies? I swear though, I'm going to get even more confused after reading you guys' comments."
2588,2020-06-26 10:33:19,1593156799.0,dataengineering,"What tools/skills should I learn to apply to this job ""Web Data Farming Engineer"" ?",hg37wd,fiarth,,https://www.reddit.com/r/dataengineering/comments/hg37wd/what_toolsskills_should_i_learn_to_apply_to_this/,1.0,0.0,0.0,15166.0,"I want to get into the data engineering field and there are not a lot of entry level jobs at the moment. This job doesn't require technologies like Spark, Kafka, which I've used. I'm starting this course now:  [https://www.coursera.org/learn/python-network-data](https://www.coursera.org/learn/python-network-data) , what else do I need to get hands on experienced with to be eligible for this job?

![img](gui8x5szi7751)"
2589,2020-06-26 14:14:38,1593170078.0,dataengineering,Chicken Egg Production Ranking | TOP 10 Country from 1961 to 2018,hg5l05,Edwardkwan,,https://www.reddit.com/r/dataengineering/comments/hg5l05/chicken_egg_production_ranking_top_10_country/,1.0,0.0,0.0,15177.0,
2590,2020-06-26 21:18:10,1593195490.0,dataengineering,Effective Temp Services by SERIGOR to Fill Quick Roles,hgcvhi,serigorinc,,https://www.reddit.com/r/dataengineering/comments/hgcvhi/effective_temp_services_by_serigor_to_fill_quick/,0.0,0.0,0.0,15197.0,
2591,2020-06-26 23:21:08,1593202868.0,dataengineering,How do I create and work on multiple Airflow Projects?,hgf7l5,DatKalvin,,https://www.reddit.com/r/dataengineering/comments/hgf7l5/how_do_i_create_and_work_on_multiple_airflow/,1.0,11.0,0.0,15198.0," Hi,   
I started learning to use Airflow for workflow orchestration lately  on Windows using WSL. I have created a DAG to scheduled my job.   
The  challenge now is, how do I create multiple airflow projects each running  its own DAG folder? Do I have to do a fresh Airflow installation for  every new project I want to work on?   
Is it possible to create  virtual  environments and have each projects running in separate virtual  environments with just one airflow installation? Kindly help me with what is possible."
2592,2020-06-27 06:44:03,1593229443.0,dataengineering,Data Engineers Fear to Data Scientist lol,hgmgzz,DataFreakk,,https://www.reddit.com/r/dataengineering/comments/hgmgzz/data_engineers_fear_to_data_scientist_lol/,1.0,18.0,0.0,15211.0,"1.Does Apacha Spark big data Developer will be merged with Data scientists roles I heard in future many companies will.merge DS +DE as some does now .I'm interested into DE roles but verey bad at Stats and Maths .I don't want DS roles Can I get DE based roles alone without DS in future 
2.Why are very few Projects on Apache spark does that make my growth as Big data Developer limited in future.
2.How would typical Data Engineer need Devops tools like Docker , Kubernets,etc Do He need manage AWS services ? 
Could some one give me some answers regarding above questions ."
2593,2020-06-27 10:44:46,1593243886.0,dataengineering,S3 eventual consistency issue,hgpd7m,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hgpd7m/s3_eventual_consistency_issue/,1.0,3.0,0.0,15215.0,"Hello everyone, 

usecase : we have 2000 files each 5 to 6 MB and trying to process the files using Glue + S3 . We are facing issues like S3 eventual consistency and failed to Move/Rename the files when writing parquet file format.   


Is there any workaround for this . Using the  job parameter: “–enable-s3-parquet-optimized-committer” set to true wont help to process files. Overall job runtime is taking 2-3 hours to process 10 to 15 GB files. Not sure Is glue giving the same performance to everyone."
2594,2020-06-27 13:38:32,1593254312.0,dataengineering,How can companies and individuals start with Machine Learning without or with little internal/own data?,hgrbwa,Letstuit,,https://www.reddit.com/r/dataengineering/comments/hgrbwa/how_can_companies_and_individuals_start_with/,1.0,0.0,0.0,15216.0,"To answer this question I have set up a survey in the scope of my Master Thesis for which I need the help of those who are training Machine Learning Models with external data or who have daily touch points with such models.

[https://www.umfrageonline.com/s/external\_data\_sources\_ML](https://www.umfrageonline.com/s/external_data_sources_ML)

Participation is anonymous and takes roughly 10 minutes. The results will be shared afterwards to enable as many as possible to profit of what has been collaboratively created - a guide on how to select external data sources across countries, industries and organizational functions.

I am a student at Berlin School for Economics and Law, if you have any questions, feel free to reach out to me. I usually get back to you right away.

Thank you for your support."
2595,2020-06-27 13:52:43,1593255163.0,dataengineering,Broadband Penetration Ranking | TOP 10 Country from 1998 to 2016,hgrhwo,Edwardkwan,,https://www.reddit.com/r/dataengineering/comments/hgrhwo/broadband_penetration_ranking_top_10_country_from/,1.0,0.0,0.0,15218.0,
2596,2020-06-27 16:20:31,1593264031.0,dataengineering,most popular mobile phone brand in the world || most popular smartphone ...,hgtdfs,ayushkedia123456,,https://www.reddit.com/r/dataengineering/comments/hgtdfs/most_popular_mobile_phone_brand_in_the_world_most/,1.0,0.0,0.0,15221.0,
2597,2020-06-27 21:07:51,1593281271.0,dataengineering,Top 15 wicket takers in ODI cricket (1971-2020),hgybum,pheonix_bird,,https://www.reddit.com/r/dataengineering/comments/hgybum/top_15_wicket_takers_in_odi_cricket_19712020/,2.0,2.0,0.0,15236.0,
2598,2020-06-27 21:16:03,1593281763.0,dataengineering,Why the ability to concentrate is the most important skill in 2020,hgyh5m,Karlos224,,https://www.reddit.com/r/dataengineering/comments/hgyh5m/why_the_ability_to_concentrate_is_the_most/,17.0,10.0,0.0,15236.0,"Many of us usually have at least one thing that we know we need to do. And if somehow we managed to sit down and do it from start to finish. Our life would be better because of it. The problem is that people put off that thing, they do anything under the sun to distract themselves.  


Being a person who naturally gets distracted easily and was surely one of the worst procrastinators. I can confidently say it's never too late to make a change. Because if somehow even I managed to find little strategies and create little short cuts to become someone who can concentrate for long periods of time. Then you can too!  


\#1 Why it's so important?  


First of all, it's probably not a secret that getting sidetracked nowadays is easier than ever. We are constantly bombarded with ads and online marketing. In fact, according to research, it takes around 15-20 min. to get back to your 100% concentration after getting distracted. Basically, if we cut to the chase - this new distracting digital age creates a huge demand for people who can resist distraction and concentrate.  


\#2The bar is so lower than you think  


If you can dive in even for one hour on your most important thing for the day with a ruthless and intense focus. You will make substantial progress in your life. And as you get used to that hour of concentration. You can upgrade that to 2 or 3 hours. Just think how much intense focus that is. You will skyrocket past your goals!  


\#3 Guilt-free pleasure and balance  


I know that many of us want to have a balanced life. We want to achieve something or do something meaningful but still enjoy life. For example, maybe you want to work on your personal projects, but at the same time, you don't want to give up video games. This was one of the biggest pains I struggled myself. I would play a lot of video games but then at the same time I would feel guilty for not making progress on my personal goals. And it's funny because the solution is so simple. You can play the crap out of those video games after you put a tremendous amount of focus on something else. This way you don't feel guilty and can fully immerse yourself into video games.  


And if the perks of mastering concentration don't entice you, you can stop here...  


But if it interests you, consider reaching out to me or comment below - I'd be happy to answer all of your questions!  


On a side note, I'm starting my own small business as a consultant for professionals. I’m basically helping people seat and achieve their goals so they can get their projects under control and live a more balanced lifestyle. But before getting serious about it, I’d like to find one or two more test clients to really perfect my method. If you’re interested send me a message."
2599,2020-06-28 00:38:08,1593293888.0,dataengineering,An Exhaustive Article on Data Science,hh22v4,princepatni,,https://www.reddit.com/r/dataengineering/comments/hh22v4/an_exhaustive_article_on_data_science/,0.0,0.0,0.0,15238.0,"I was planning to learn Data Science and found that there are so many Amazing Resources on it one must read. I tried to bring them all in one here [in this blog](https://princepatni.com/blog/tech/what-is-data-science-best-resources-courses-for-data-scientists/).

\-Data Science Terms &amp; Technologies  
\-Best Data Science Blogs (REALLY USEFUL)  
\-Best Books on Amazon  
\-Best Certificate Courses &amp; Degrees offered by Universities (Harvard, Berkeley)  
\-Data Scientist Skills &amp; Salaries  
\-Common Data Science Questions and Comparisons"
2600,2020-06-28 04:14:27,1593306867.0,dataengineering,Can someone help me with the job search in current condition?,hh5mlz,motta_bull,,https://www.reddit.com/r/dataengineering/comments/hh5mlz/can_someone_help_me_with_the_job_search_in/,1.0,2.0,0.0,15242.0,"Hello Reddit,

I just graduated from US(Bay Area) and looking for full time opportunities. I have 5 years of experience prior to my master's degree and 2 Internship experience is US. Despite all these, I hardly get any calls from companies at this moment.I am looking for Software Engineer - Backend, DataEngineer, ETL, BI developer roles.

I primarily apply through LinkedIn and careers page of whatever company I came across. Also, I am applying through referrals from my friend circle.

Is there anything I can do better with my job search? I understand current market is harsh. Need experts advice to do things wisely to swim in the given condition.

Thanks!"
2601,2020-06-28 13:34:06,1593340446.0,dataengineering,Honey Production Ranking | TOP 10 Country from 1961 to 2018,hhc8t3,Edwardkwan,,https://www.reddit.com/r/dataengineering/comments/hhc8t3/honey_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,15254.0,
2602,2020-06-28 16:42:08,1593351728.0,dataengineering,What are Informatica's capabilities for Data Virtualization?,hheket,knotordie,,https://www.reddit.com/r/dataengineering/comments/hheket/what_are_informaticas_capabilities_for_data/,1.0,0.0,0.0,15256.0,"i'm trying out data virtualization (making views from different sources and joining them together, meaning no physical movement of data) and have seen that Informatica is the leader in the space (per Forrester research of data virtualization market Q4 2017). 

I wanted to try Informatica's free trial of Informatica PowerCenter (which is Informatica Cloud), which contains Data Integration service, but it seems to me there are not data virtualization capabilities in the service (or in the other Informatica Cloud services) and it's simply an ETL tool, not a virtualization one. My question is, how is it possible to create unified views of source data with Informatica Cloud?

I have also found that they used to have Informatica Data Services product, which allowed for creation of logical data objects, but as far as I have looked, this product is not available anymore and it's capabilities should be integrated into Informatica PowerCenter?"
2603,2020-06-28 17:34:45,1593354885.0,dataengineering,"Read, write simple Ui",hhfe4o,hantt,,https://www.reddit.com/r/dataengineering/comments/hhfe4o/read_write_simple_ui/,1.0,12.0,0.0,15258.0,"Hey guys, I currently have a few execl online setup to ingest user input and write them to a Sql database. The data is then transformed and ultimately displayed through Tableau. The excel essintally act as a way for none sql team members to ""comment"" on certain records. The problem with this is that it isn't dynamic, I basically pasted a static list of the Primary key in excel and people would use those to find records to add or change comments on. People also like to fiddle with the columns or change the keys so the process is very brittle. 
I'm wondering if there might be a unified and simpler alternative where records can be pulled, updated and written back to the DB.  The ideal app would allow users to define some simple parameters, fetch records based those, and allow users to update the records and recommit them back to the DB. 
I've considered just making a custom web app but was hoping there might already be a solution out there."
2604,2020-06-28 18:47:55,1593359275.0,dataengineering,"Simple web UI tool to schedule, execute and monitor SQL queries for analysts",hhgn7c,koteikin,,https://www.reddit.com/r/dataengineering/comments/hhgn7c/simple_web_ui_tool_to_schedule_execute_and/,1.0,14.0,0.0,15263.0,"Hey guys,

looking for a simple tool with easy web UI for data analysts to create SQL-based pipelines.

I am aware and played with Airflow and alike, and also aware of DBT but still not happy.

Our data analysts are not technical at all, heck they even struggle to write good SQL, and our data engineering team wants to provide them self-service hand-off environment so they can create their own tables in Impala/Hive without moving data to other tools. I like the idea for them to define SQL SELECT statements and then creating tables using CTAS.

We have Alteryx, Informatica, BO Data services etc. but all these tools are not really making this task easy.

Ideally, it should be DBT-like tool but way more easier that does not involve running code via cli or having them messing with YAML configs and such. Just a very simple web-based UI, but with the ability to chain SQL commands with dependencies, monitoring/alerting, logging previous runs etc.

Another thing is security and multi-tenancy for various teams (so they do not step on each others jobs).

Does something like that exist really? I know it sounds like an ETL tool but my idea is to leave data ingest/more complex tasks to data engineering team while giving more freedom with SQL only to analysts, moving to ELT concept for them. This also allows them to really benefit from our Big Data systems without moving data into Alteryx and similar tools."
2605,2020-06-28 19:12:27,1593360747.0,dataengineering,"Should I do go for MNC with NO idea about job role Or go for ""Data Engineer"" role at 30 people startup in Bangalore, India ?",hhh2jy,ajinkyajawale,,https://www.reddit.com/r/dataengineering/comments/hhh2jy/should_i_do_go_for_mnc_with_no_idea_about_job/,1.0,8.0,0.0,15269.0,"Hello guys, I am 2020 CS Bachelor just passed out, I got Campus placement offer from MNC (biggest IT service company in India , hired 30k grads in corona, giving x amount ctc. No idea about the Job Role just basic entry-level position )  so I always wanted to give a shot for better offer, and DS/ML  related role, so i am about to get offer of almost 2x ctc and ""Data Engineer""  role in thier data science team with total 30 people startup in Bangalore, so what do you thing what should do? which offer should I go for (while interviewing on of my manager said they have enough funding for next 2 years and they are looking for a data engineer who could transition into data science) so basically I'm in confusion what should i do. I always wanted to do data engineering, Please provide your input considering covid effect on job-laid off, future growth"
2606,2020-06-28 19:38:49,1593362329.0,dataengineering,Azure Synapse vs. Snowflake,hhhjl5,koteikin,,https://www.reddit.com/r/dataengineering/comments/hhhjl5/azure_synapse_vs_snowflake/,1.0,15.0,0.0,15270.0,"is there anyone here who evaluated both? we are moving into Azure and considering both. We have to support 100s of projects, \~1PTb in parquet (both data lake and dw), should serve ad-hoc interactive queries and sub-second OLAP-like queries to BI tools (mostly Qlik and PowerBI).

Ideally, we should also support near real-time uses (but latency can be minutes not seconds).

I did read quite a bit about both platforms and I understand differences in architecture but wondering if someone had the same choice and what were the reasons to go with Snowflake or Synapse.

Maybe I am wrong, but my general impression that Synapse is just a bunch of stitched MS legacy solutions (much like Oracle's autonomous data warehouse) while Snowflake was really designed for cloud."
2607,2020-06-28 19:46:25,1593362785.0,dataengineering,Spark architecture confusion (If not for storage ),hhhoe8,DataFreakk,,https://www.reddit.com/r/dataengineering/comments/hhhoe8/spark_architecture_confusion_if_not_for_storage/,1.0,6.0,0.0,15270.0,"Why spark Developer are called as DE . Amazon Redshift ,DWH,ETL,elt come under data Engineering. Spark now is used for Analysis of large Data spread over clusters in Ram and analyse data and tell answers related to data. isn't this Data scientist work for Analysis .as DE build things and see proper flow of data and pipleines . Am I missing something ? 

If u just want on building things and pipelines  .Do I need spark ?  Or Spark is used for Spread data over multiple clusters and storing on hdfs  and later used for Analysis

 If majority of data is migrated to redshift which distributed cluster based data warehouse does spark will be relevant 

Note : I'm super confused and not interested in Data scientist role.Guide me usage of spark ,interms of DE"
2608,2020-06-28 22:43:38,1593373418.0,dataengineering,Databricks delta lake,hhkyl2,StockFault7,,https://www.reddit.com/r/dataengineering/comments/hhkyl2/databricks_delta_lake/,1.0,28.0,0.0,15274.0,"Can we use databricks delta lake as a data warehouse kind of thing where business analysts can explore data according to their needs ? 

Delta lake provides following features which I think supports this idea 

1) support to sql syntax
2) provide ACID guarantees 
3 ) can handle big data workloads
4) can do some kind of basic visualisation with the reports 
5) parquet formats provide better performance in analytical queries 

Any opinions or suggestions ? Anything I am missing ? 

P.s : I am a junior Data engineer so any help is appreciated . We have a certain poc going on and I am thinking of providing this as a solution"
2609,2020-06-28 23:48:02,1593377282.0,dataengineering,A slow Glue - AWS,hhm4he,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hhm4he/a_slow_glue_aws/,2.0,11.0,0.0,15277.0,"Anyone experiencing slow Glue experience . We were trying to process sma files of 10 gb data . Glue is taking 4 to 8 hours to process which increases the cost and basically delays the pipeline . Is there anyway we can increase the pipeline. 

1. What is your thoughts of glue 2.0 spark instead of glue 0.9"
2610,2020-06-29 05:20:11,1593397211.0,dataengineering,Looking DAG Scheduler that can be installed from APT or SNAP,hhrh8u,perceptionsmk,,https://www.reddit.com/r/dataengineering/comments/hhrh8u/looking_dag_scheduler_that_can_be_installed_from/,2.0,10.0,0.0,15279.0,"As the title states does anyone know of any scheduling app that can be installed from a system package manager like SNAP or APT?

I run all my python scripts and ETL within a docker python 3.7 container. I execute them via shell on the host machine. Would love to use a modern DAG type scheduler to ensure everything runs, dependencies are managed and retries are automatically attempted.

Airflow, Luigi etc all seem to require I install python on the host. I would like to avoid this. I am curious if a package exists to make the install of one of these schedulers a one liner.

Alternatively has anyone found a way to have a docker container version of one of these schedulers execute commands on the host to run other containers?"
2611,2020-06-29 09:23:24,1593411804.0,dataengineering,Apache griffin helm chart,hhupt6,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/hhupt6/apache_griffin_helm_chart/,1.0,0.0,0.0,15281.0,"Hi,

I am looking at some of the open source tools for data quality to be used in my organisation and one of the tool I found was Apache griffin. Would like to give it a try. Some of them as of now I have looked at is great expectations and deequ. I would like to use Kubernetes deployment. Has anyone used it and is there a helm chart available for the same?"
2612,2020-06-29 13:32:02,1593426722.0,dataengineering,Prometheus or influx for alerting,hhxhgj,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/hhxhgj/prometheus_or_influx_for_alerting/,1.0,1.0,0.0,15286.0,"Hi,

For my alerting and monitoring needs I generally tend to use grafana and influx. I recently came across prometheus and had few questions about it from the users who are using it. 

I used telegraf to push custom metrics to influxdb. It is basically a push model. When I was going through prometheus architecture, it seems to be a pull model. Please help me understand how does it work

Let say I have installed prometheus. I have few machines behind scalesets which come and go depending on the load. How does prometheus know that there are new machines behind the scaleset which it needs to scrape. Also, Isn't a push better model than a pull one ? 

Do we need to add new server connections in prometheus for it to scrape metrics from new server added?"
2613,2020-06-29 13:54:51,1593428091.0,dataengineering,Data Engineer Interview Questions | Data Engineer Interview Preparation | Intellipaat,hhxqwx,akshaylak,,https://www.reddit.com/r/dataengineering/comments/hhxqwx/data_engineer_interview_questions_data_engineer/,1.0,0.0,0.0,15286.0,
2614,2020-06-29 14:01:10,1593428470.0,dataengineering,Internet User Ranking | TOP 10 Country from 1990 to 2016,hhxto4,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hhxto4/internet_user_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,15286.0,
2615,2020-06-29 15:13:08,1593432788.0,dataengineering,Batching vs. Streaming - Scale &amp; Process Millions of Measurements a Second,hhyqpz,mto96,,https://www.reddit.com/r/dataengineering/comments/hhyqpz/batching_vs_streaming_scale_process_millions_of/,1.0,1.0,0.0,15289.0,
2616,2020-06-29 17:27:07,1593440827.0,dataengineering,100% OFF Udemy Databricks &amp; Apache Spark 3.0.0 Course,hi0tme,gwadson,,https://www.reddit.com/r/dataengineering/comments/hi0tme/100_off_udemy_databricks_apache_spark_300_course/,2.0,15.0,0.0,15290.0,"Hi Data enthusiasts,

If you are new to **Databricks** and **Apache Spark** and want to learn it step by step, then I have a brand-new course on Udemy for you.

[The course](https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/?couponCode=B0F1F71238845CB68C4C) covers all you need to know to get started with Apache Spark and Databricks

[ENROLL NOW FOR FREE](https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/?couponCode=B0F1F71238845CB68C4C)"
2617,2020-06-29 18:01:50,1593442910.0,dataengineering,Snowflake vs BigQuery,hi1fdo,mango_sorbet13,,https://www.reddit.com/r/dataengineering/comments/hi1fdo/snowflake_vs_bigquery/,1.0,8.0,0.0,15294.0,"We are currently using a data warehouse called Panoply and are thinking about switching to either Snowflake or BigQuery. I've been doing a some research on the pros/cons of each and the differences between the two, but haven't been able to find a lot of useful information.

Is there anyone here that can enlighten me on this topic?"
2618,2020-06-29 18:04:32,1593443072.0,dataengineering,"Treat your datasets like cattle, not pets",hi1h7o,mildbyte,,https://www.reddit.com/r/dataengineering/comments/hi1h7o/treat_your_datasets_like_cattle_not_pets/,3.0,0.0,0.0,15294.0,
2619,2020-06-29 19:19:01,1593447541.0,dataengineering,"Kafka, for your data pipeline? Why not?",hi2v4w,tuankid,,https://www.reddit.com/r/dataengineering/comments/hi2v4w/kafka_for_your_data_pipeline_why_not/,1.0,3.0,0.0,15294.0,
2620,2020-06-29 21:42:15,1593456135.0,dataengineering,Top 5 De books or one book that's awe-SUM,hi5tje,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/hi5tje/top_5_de_books_or_one_book_thats_awesum/,1.0,2.0,0.0,15301.0,"Wagwaan!! ( that's ""what's going on"" in Jamaican creole) 

Hey everybody , all the DE enthusiasts, what are your top 5 must read books for DE . 

I find the book by Martin Kleppmann , Building data intensive applications, quite interesting. A very holistic tome which rather stitches up quite nicely all the loosely coupled, floating pieces of DE knowledge. 

And as you can see I'm a hard hitting evangelist of that book. Any woman/man out there who champion other books vehemently? 

Regards 
Psy"
2621,2020-06-30 14:08:15,1593515295.0,dataengineering,Coffee Production Ranking | TOP 10 Country from 1961 to 2018,hilapn,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hilapn/coffee_production_ranking_top_10_country_from/,1.0,0.0,0.0,15328.0,
2622,2020-06-30 15:05:57,1593518757.0,dataengineering,"Data modeling interview question - 1.Design a data warehouse for an e-commerce website. 2.Also, assume there would be millions of rows coming in every hour into the data warehouse. Design keeping scalability in mind.",him34e,ppalety,,https://www.reddit.com/r/dataengineering/comments/him34e/data_modeling_interview_question_1design_a_data/,1.0,20.0,0.0,15328.0,This was asked in one of the interviews and I was wondering how do you approach such questions.
2623,2020-06-30 16:04:01,1593522241.0,dataengineering,Free Webinar on Introduction to Data Science: How to Get Started,himyb0,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/himyb0/free_webinar_on_introduction_to_data_science_how/,1.0,0.0,0.0,15331.0,
2624,2020-06-30 17:46:29,1593528389.0,dataengineering,Serving TensorFlow models in Rust using Actix-Web,hiopf1,kykosic,,https://www.reddit.com/r/dataengineering/comments/hiopf1/serving_tensorflow_models_in_rust_using_actixweb/,1.0,0.0,0.0,15334.0,
2625,2020-06-30 18:07:24,1593529644.0,dataengineering,Need advice for a Payment Backend,hip3bx,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/hip3bx/need_advice_for_a_payment_backend/,0.0,0.0,0.0,15335.0,"Hi everyone, hope you're all doing well.

I would like to get some insights regarding how to design a back-end for payments. We have a client who would want to get some reports about their sales (this is FMCG with lots of daily transactions). They are asking us to design a cloud-based pipeline architecture on either Azure or AWS. Basically these payments come in different POS and we need to collect and transform them so we can make aggregated analysis on the performance of each product. Would you have any suggestion on what tools to use and how it could be implemented? Thank you for your time"
2626,2020-06-30 19:51:59,1593535919.0,dataengineering,Help designing architecture,hir4ec,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/hir4ec/help_designing_architecture/,1.0,3.0,0.0,15336.0,"Hi! We have a client who wants to have a payment back-end designed. They are an e-commerce company with data coming from various POS (JSON format). Data will be served to data scientists to do some customer segmentation analysis.

I would like to get some suggestions

1. What resources to use in Azure/AWS that would have a best fit for this? 
2. What tools to use to operationalize a pipeline?
3. How often should this workflow be triggered?"
2627,2020-06-30 23:04:43,1593547483.0,dataengineering,Handling eventual consistency,hiv2oi,hugorocha,,https://www.reddit.com/r/dataengineering/comments/hiv2oi/handling_eventual_consistency/,1.0,0.0,0.0,15341.0,
2628,2020-06-30 23:44:48,1593549888.0,dataengineering,Dev/QA/Prod isolation in S3 datalake,hivw6j,raginjason,,https://www.reddit.com/r/dataengineering/comments/hivw6j/devqaprod_isolation_in_s3_datalake/,1.0,1.0,0.0,15341.0,"I'm interested in finding out how different teams create environment isolation in their S3 datalakes. How data transitions from its raw state into something structured and useful more or less makes sense, but I'm not clear how dev/QA/prod isolation is generally done in regards to data being stored in S3. Do you usually see a bucket for each (datalake-dev, datalake-qa, datalake-prod S3 buckets) and apply permissions at that level, or one bucket with various permissions on the paths? Or is there some other way that I'm not thinking of?"
2629,2020-07-01 01:09:26,1593554966.0,dataengineering,Maximizing the Value of Your Cloud-Enabled Enterprise Data Lake by Tracking Critical Metrics,hixglf,distinct_name,,https://www.reddit.com/r/dataengineering/comments/hixglf/maximizing_the_value_of_your_cloudenabled/,1.0,0.0,0.0,15347.0,
2630,2020-07-01 02:35:09,1593560109.0,dataengineering,What do you want to see in a 6-month Data Engineering course?,hiyyhg,datamewhat,,https://www.reddit.com/r/dataengineering/comments/hiyyhg/what_do_you_want_to_see_in_a_6month_data/,1.0,49.0,0.0,15349.0,"Hi all,

I've built the data infrastructure from the ground up at a couple of small, successful startups. I like teaching and mentoring and I believe my position as a lead and in startup world has given me the experience to really teach people ""core"" and future-proof data engineering knowledge.

I'm looking to write a 6-month long data engineering course with the goal of taking someone with no tech/programming experience to a place where they could get a junior DE position. I think there's a huge knowledge gap for someone to become a data engineer, and most online course I have seen are not great (including Udacity's).

My main questions are:

* Are most people wanting to get into data engineering complete beginners to coding or are they general SWE's looking to transition? What bucket do you or any of your friends who are interested in data engineering fit into?
* More generally, what would you want to see in a data engineering course?

Just to give an idea of what I'm thinking:

1. Curriculum will be: A heavy emphasis on SQL/Python for ETL and ELT, data warehouses and data modeling, distributed ETL tools (Spark and Hive in EMR, serverless tools like Athena), Airflow, some RDBMS, some BI tools/analysis, maaaaybe a little NoSQL. And general cloud knowledge (EC2, S3, RDS etc). Not sure about streaming.
2. Heavily cloud-based. We will work with rdbms, data warehouses, and ETL tools in the cloud. This will be billed on the students cloud accounts, so we can coast on free tier, or we can easily limit usage to under a $100 using certain instances. If something can be learned locally (like Airflow and Python), it will be. I will host data in S3 and snapshots. Leaning towards AWS. 
3. The course itself might be $100 or broken into chunks so you can only pay for what you need to learn.
4. The course will be **very** project-based. I will provide helpful code, data, assignments, guidance etc. The course content itself will be videos/text. I may see if I can get short multiple-choice quizzes or code evaluation. 
5. I'll create a discord to answer questions and to create a student community. I'm not sure how feasible real person evaluation of students work is possible. But if I can figure out a good solution, I will.
6. Students should graduate with interesting projects on their resume as a result of the course.

&amp;#x200B;

Thoughts?"
2631,2020-07-01 05:12:06,1593569526.0,dataengineering,Optimizing Spark code,hj1e51,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/hj1e51/optimizing_spark_code/,1.0,7.0,0.0,15351.0,"Hey everyone, 

  I see a lot of tutorials which teach Apache Spark syntax, dataframes, RDD, datasets. But not a lot on distributed system internals like what partitions are, what data shuffling is and how to improve the performance and reduce time/cost of your Spark job. In this post I talk about 3 techniques you can use to improve Spark performance [https://www.startdataengineering.com/post/how-to-optimize-your-spark-jobs/](https://www.startdataengineering.com/post/how-to-optimize-your-spark-jobs/)

They are appropriate partitioning, caching and broadcast and bucket joins. It turned out a bit longer than I expected. Please take a look if you are interested in this topic and would love any feedback and/or share."
2632,2020-07-01 06:03:47,1593572627.0,dataengineering,Know how gradient descent algorithm helps to improve performance in gradient boosting models,hj2557,Hussain_Mujtaba,,https://www.reddit.com/r/dataengineering/comments/hj2557/know_how_gradient_descent_algorithm_helps_to/,1.0,0.0,0.0,15354.0,
2633,2020-07-01 11:03:51,1593590631.0,dataengineering,What kind of person would enjoy work as a DE?,hj625l,Aegyx,,https://www.reddit.com/r/dataengineering/comments/hj625l/what_kind_of_person_would_enjoy_work_as_a_de/,1.0,9.0,0.0,15366.0,"To give some context, I'm a junior at a top ~15 university studying in a CS-related major, and have been involved mostly with SWE internships up until now. 

Following a string of courses I recently took among other factors, I've become more interested in the work of a data scientist. I think it'd feel really nice to be able to formulate questions from your domain-specific knowledge and to be able to extract profitable insights from applying models to data. However, I realize that this is a very naive view of the data scientist's role, and that getting a DS job as someone with only a Bachelor's and a not extensive math/statistics background (just the typical core CS math/stats courses including discrete math and the intros for calc/linalg/multi/prob theory/etc.) will entail a long journey from starting off as something like a data analyst, or going to get a MS in a quantitative field.

I have instead been reading up a bit about what a DE does and it is to my understanding that the work of a DE revolves around formation of pipeline infrastructures / ETC / data munging / etc., and is much heavier in the preprocessing of data. I also get the impression that my CS / SWE background is much more suited for a DE role (at least at this moment). This could be completely off from what an actual DE does - if so, I apologize in advance for my ignorance, and would really welcome a better explanation. 

As the title reads and as is probably apparent from my post, I have  rather limited knowledge of what a DE does and what I could expect from the role, and thus don't really know what kind of person would like the job of a DE. Can anyone shed some insight? Thanks!"
2634,2020-07-01 12:50:38,1593597038.0,dataengineering,Running Apache Airflow locally with the Kubernetes Executor on a multi-node cluster,hj7954,marclamberti,,https://www.reddit.com/r/dataengineering/comments/hj7954/running_apache_airflow_locally_with_the/,1.0,0.0,0.0,15368.0,
2635,2020-07-01 13:35:25,1593599725.0,dataengineering,Set up dev environment with docker-compose and Python,hj7s2q,andodet,,https://www.reddit.com/r/dataengineering/comments/hj7s2q/set_up_dev_environment_with_dockercompose_and/,1.0,0.0,0.0,15368.0,"In the past few weeks I've been playing around with Airflow and one thing that I've struggled with while studying was the lack of a playground environment where to get comfortable with DAGs.

I've decided to build my own through a mix of `docker-compose`, MySQL and Python. The whole thing is quite simple:

1. Spawn a MySQL db
2. Create db schema and needed tables.
3. Fill tables with mock data.
4. [Optional] Test everything has been brought up correctly.

I quite like this approach as making any change to the columns or schema is as easy as editing a Python script and it spares the time and money to resort to GCP to provision a micro instance with a db instance.

I'd like to understand alternative approaches and whether I might have overlooked anything."
2636,2020-07-01 13:37:01,1593599821.0,dataengineering,Set up prepopulated dev environment with docker-compose and Faker,hj7ssh,andodet,,https://www.reddit.com/r/dataengineering/comments/hj7ssh/set_up_prepopulated_dev_environment_with/,1.0,0.0,0.0,15368.0,"In the past few weeks I've been playing around with Airflow and one thing that I've struggled with while studying was the lack of a playground environment where to get comfortable with DAGs.

I've decided to build my own through a mix of `docker-compose`, MySQL and Python. The whole thing is quite simple:

1. Spawn a MySQL db
2. Create db schema and needed tables.
3. Fill tables with mock data.
4. [Optional] Test everything has been brought up correctly.

I quite like this approach as making any change to the columns or schema is as easy as editing a Python script and it spares the time and money to resort to GCP to provision a micro instance with a db instance.

I'd like to understand alternative approaches and whether I might have overlooked anything.

[Here](https://www.anddt.com/post/mock-environment/) you can find a more detailed write up."
2637,2020-07-01 13:52:53,1593600773.0,dataengineering,Set up prepopulated dev environment with docker-compose and Faker,hj7zjw,andodet,,https://www.reddit.com/r/dataengineering/comments/hj7zjw/set_up_prepopulated_dev_environment_with/,1.0,2.0,0.0,15369.0,
2638,2020-07-01 14:10:04,1593601804.0,dataengineering,Mobile Browser Ranking | TOP 10 from 2009 Q1 to 2020 Q2 (updated),hj874o,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hj874o/mobile_browser_ranking_top_10_from_2009_q1_to/,1.0,0.0,0.0,15369.0,
2639,2020-07-01 15:32:47,1593606767.0,dataengineering,Webinar on Introduction to Data Science: How to Get Started,hj9aum,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/hj9aum/webinar_on_introduction_to_data_science_how_to/,1.0,0.0,0.0,15372.0,
2640,2020-07-01 17:14:46,1593612886.0,dataengineering,"Python library for advanced Google News data mining: get news data by topics, geolocation, full-text search. Plus, clusters of similar topics",hjazll,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/hjazll/python_library_for_advanced_google_news_data/,2.0,0.0,0.0,15376.0,
2641,2020-07-01 18:26:22,1593617182.0,dataengineering,Udacity nanodegree,hjcdrw,akeebismail,,https://www.reddit.com/r/dataengineering/comments/hjcdrw/udacity_nanodegree/,1.0,14.0,0.0,15379.0,"I want to start udacity data engineering nanodegree this week, I want to ask does it worth it ? What should I look forward too?
I’ve experienced building software with Python, Nodejs, SQL, NoSQL, cloud infrastructure.
Also, I’m reading the book Designing data intensive application. 
And if you’ve other resources that can help mi learn data engineering from start or a learning path to explain where I need to start from."
2642,2020-07-01 23:01:21,1593633681.0,dataengineering,Unemployment rate for different race (1948-2020),hjhty0,daily3mindatas,,https://www.reddit.com/r/dataengineering/comments/hjhty0/unemployment_rate_for_different_race_19482020/,1.0,0.0,0.0,15394.0,
2643,2020-07-01 23:26:58,1593635218.0,dataengineering,How we migrated our data warehouse from Redshift to BigQuery,hjibr1,rahulj51,,https://www.reddit.com/r/dataengineering/comments/hjibr1/how_we_migrated_our_data_warehouse_from_redshift/,1.0,7.0,0.0,15397.0,
2644,2020-07-02 00:40:12,1593639612.0,dataengineering,Script to Enlarge AdventureWorksDW2019 - Anyone?,hjjr50,Not-NedFlanders,,https://www.reddit.com/r/dataengineering/comments/hjjr50/script_to_enlarge_adventureworksdw2019_anyone/,1.0,0.0,0.0,15399.0,I've been working with AdventureWorksDW2019 and I was hoping to enlarge the dataset. I've seen scripts thrown around for this purpose in the past for older iterations of AW but was wondering if anyone had anything for 2019?
2645,2020-07-02 01:00:59,1593640859.0,dataengineering,The expert way of structuring a project for Python ETL.,hjk5bt,adinoriya,,https://www.reddit.com/r/dataengineering/comments/hjk5bt/the_expert_way_of_structuring_a_project_for/,1.0,0.0,0.0,15398.0,
2646,2020-07-02 04:07:15,1593652035.0,dataengineering,High level interview question advice,hjnecx,unbundler,,https://www.reddit.com/r/dataengineering/comments/hjnecx/high_level_interview_question_advice/,1.0,9.0,0.0,15403.0,"I'm designing an interview question for a mid-level data engineer (3+ years experience in the field).

**Background context:** 

* The project stage for which they are interviewing involves accessing different museum APIs to grab unstructured and culturally-sensitive collection data. 
* Importantly, the project also entails preserving the museum's internal data storage idioms. 
* There won't be much data, each museum likely only has 1-30 objects of interest and there are about 50 museums. 
* It's a Greenfield project — not a line of code has been written.

*Can you at a high level discuss how you would go about planning the construction of a data pipeline for this task? What might be the general phases or steps that you take? If you can list frameworks or procedures that you are already comfortable working with, please do.*

I want to understand how they would think about completing this task. But since I'm a novice in the field myself, I don't know what a good response entails. Can you offer up some responses you'd trust?"
2647,2020-07-02 04:42:16,1593654136.0,dataengineering,[Tutorials] Sleep detection system [useful for cars drivers],hjnym1,manav1918,,https://www.reddit.com/r/dataengineering/comments/hjnym1/tutorials_sleep_detection_system_useful_for_cars/,1.0,0.0,0.0,15406.0,
2648,2020-07-02 13:46:39,1593686799.0,dataengineering,Cattle Meat Production Ranking | TOP 10 Country from 1961 to 2018,hjusxf,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hjusxf/cattle_meat_production_ranking_top_10_country/,1.0,0.0,0.0,15419.0,
2649,2020-07-02 14:11:16,1593688276.0,dataengineering,Apache Spark on DataProc vs Google BigQuery,hjv3cx,Darth_Programmer,,https://www.reddit.com/r/dataengineering/comments/hjv3cx/apache_spark_on_dataproc_vs_google_bigquery/,1.0,0.0,0.0,15419.0,
2650,2020-07-02 15:11:04,1593691864.0,dataengineering,Discussion: Spark SQL -- Merge vs queries,hjvv15,Umesh-Sharma,,https://www.reddit.com/r/dataengineering/comments/hjvv15/discussion_spark_sql_merge_vs_queries/,1.0,5.0,0.0,15419.0,"Say I have tables like this

Users

|userId|address|
|:-|:-|
|1|q|
|2|w|
|3|e|
|4|t|

Updates

|userId|address|
|:-|:-|
|5|q|
|2|a|

*(Please spare the syntax errors.)*

**Query Set 1 :**

`create or replace table users_final as`

`select * from`

`(select * from users where id not in (select id from updates)) temp`

`union all updates;`

`create or replace table users as select * from users_final;`

`drop table users_final;`

Query Set 2 :

`MERGE INTO users`

`USING updates`

`ON users.userId = updates.userId`

`WHEN MATCHED THEN`

`UPDATE SET address = updates.addresses`

`WHEN NOT MATCHED THEN`

`INSERT (userId, address) VALUES (updates.userId, updates.address)`

&amp;#x200B;

Now both the queries result the same output. I am trying to understand which of the above is a better way to approach. In terms of performance, time, processing.

&amp;#x200B;

What I am thinking is Merge on a smaller data set will perform slower than the set-1. And as the data size increases the performance will be similar. So what I am thinking is the Set-1 queries are better to use over merge in most scenarios. Please can anyone explain me this.

This syntax is for Spark-SQL."
2651,2020-07-02 15:21:51,1593692511.0,dataengineering,Datavault staging area,hjw0d8,mrcool444,,https://www.reddit.com/r/dataengineering/comments/hjw0d8/datavault_staging_area/,1.0,0.0,0.0,15420.0,"Hello All,

We have a non-persistent staging area to load our Raw vault from S3. Due to various reasons some times we are getting multiple full files copies into same partition in S3.

When our Datavault batch runs it's copying all the files into staging area. We have extract date coming from source, so I am thinking to use it to order the data in staging and insert into satellites.

Is it a right way to do and if not, could you please suggest best practices to handle multiple full files in staging?

Thanks,

mc"
2652,2020-07-02 16:11:36,1593695496.0,dataengineering,Accelerate your career as a Senior Big Data Engineer,hjwqqt,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/hjwqqt/accelerate_your_career_as_a_senior_big_data/,1.0,0.0,0.0,15422.0,
2653,2020-07-02 19:35:37,1593707737.0,dataengineering,Mediocre developer trying to get into FAANG.,hk0bdm,encodej,,https://www.reddit.com/r/dataengineering/comments/hk0bdm/mediocre_developer_trying_to_get_into_faang/,1.0,5.0,0.0,15432.0,"Anybody trying to get into or already in FAANG? 
What’s it’s like? I am a mediocre developer at best, I am good at SQL and Spark, but logical thinking is where I lack, that’s where most of my mediocrity lies. I am pretty happy where I currently am right now and I think with 6 months to a year work I can get into big N, the question is will I get fired for mediocre performance? Should I even risk what I got right now?"
2654,2020-07-02 19:46:12,1593708372.0,dataengineering,Data Warehousing,hk0ihf,SuccessfulFarmer,,https://www.reddit.com/r/dataengineering/comments/hk0ihf/data_warehousing/,1.0,2.0,0.0,15432.0,"I'm setting up general data architecture for the company I work at and created a data warehouse but after reading [this ](https://docs.microsoft.com/en-us/archive/blogs/sqlcat/azure-sql-data-warehouse-workload-patterns-and-anti-patterns)Microsoft blog and realizing that my data is too small, I'm confused about how I should store my transformed data. Currently using the Azure platform for most of the deployment.

My questions are:

1. Should my database be highly normalized or follow a star type schema? I don't need the extra processing power of a warehouse and my boss is the only other person who will need to query and transform our data
2. I am currently using the ELT model and am wondering where I should store my transformed data. My plan was to transform the data and load it to the warehouse from a data lake but without the warehouse, I'm confused if I should load it directly to the production database or load it back to the data lake as a staging area
3. What tools are best to perform incremental loading? I've seen mentions of data factory for Azure but am not sold on this"
2655,2020-07-02 20:03:51,1593709431.0,dataengineering,"Example use cases for Apache Flink including scenarios such as Event-driven Applications, Data Analytics Applications, Data Pipeline Applications",hk0uhc,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hk0uhc/example_use_cases_for_apache_flink_including/,1.0,0.0,0.0,15432.0,
2656,2020-07-02 20:27:54,1593710874.0,dataengineering,"A scalable solution, calculating Geo Distance?",hk1b6z,sam_butt,,https://www.reddit.com/r/dataengineering/comments/hk1b6z/a_scalable_solution_calculating_geo_distance/,1.0,3.0,0.0,15432.0,"I have two CSV files. 

airport 150kb 7000 records

```
    iata_code	latitude	longitude
    AAA	-17.352606	-145.509956
    AAB	-26.69317	141.0478
    AAC	31.07333	33.83583
```

```
user ~75MB ~1million records

    uuid	geoip_latitude	geoip_longitude
    DDEFEBEA-98ED-49EB-A4E7-9D7BFDB7AA0B	-37.8333015441895	145.050003051758
    DAEF2221-14BE-467B-894A-F101CDCC38E4	52.5167007446289	4.66669988632202
    31971B3E-2F80-4F8D-86BA-1F2077DF36A2	35.685001373291	139.751403808594
```

I want to find which airport is nearest to the user based upon the geo-distance. 

The output should have two columns **UUID and corresponding iata_code**

I have haversine utility function for calculating the geo-distance
```
    def distance(
          startLon: Double,
          startLat: Double,
          endLon: Double,
          endLat: Double,
          R: Double
      ): Double = {
        val dLat = math.toRadians(endLat - startLat)
        val dLon = math.toRadians(endLon - startLon)
        val lat1 = math.toRadians(startLat)
        val lat2 = math.toRadians(endLat)
    
        val a =
          math.sin(dLat / 2) * math.sin(dLat / 2) +
            math.sin(dLon / 2) * math.sin(dLon / 2) * math.cos(lat1) * math.cos(lat2)
        val c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    
        R * c
      }
```

My code in Spark. 

```
    userDF
     |-- uuid: string (nullable = true)
     |-- geoip_latitude: double (nullable = true)
     |-- geoip_longitude: double (nullable = true)
    
    airportDF
     |-- iata_code: string (nullable = true)
     |-- latitude: double (nullable = true)
     |-- longitude: double (nullable = true)
    
    
    transformations(spark, userDF, airportDF).show()
    
    def transformations(spark: SparkSession, userDF: DataFrame, airportDF: DataFrame) = {
        val airports = broadcastDF(spark, airportDF)
        userDF.transform(findNearestAirport(spark, airports.value))
      }
    
      def broadcastDF(spark: SparkSession, df: DataFrame) = {
        spark.sparkContext.broadcast(df.collect())
      }
    
      def findNearestAirport(spark: SparkSession, airports: Array[Row])(
        userDF: DataFrame
      ): DataFrame = {
        import spark.implicits._
    
        var distance = Double.MaxValue
        var minDistance = Double.MaxValue
        var nearestAirportID = """"
    
        userDF.flatMap { user =&gt;
          airports.foreach { airport =&gt;
            distance = Haversine.distance(
              user.getAs[Double](""geoip_longitude""),
              user.getAs[Double](""geoip_latitude""),
              airport.getAs[Double](""longitude""),
              airport.getAs[Double](""latitude"")
            )
            if (minDistance &gt; distance) {
              minDistance = distance
              nearestAirportID = airport.getAs[String](""iata_code"")
            }
          }
          println(s""User ${user.getAs[String](""uuid"")} is closest to airport $nearestAirportID"")
          Seq((user.getAs[String](""uuid""), nearestAirportID))
        }.toDF(""uuid"", ""iata_code"")
      }
```

I finished the code but have a few questions.

 1. I used DF.transform function instead of UDF. Is it better or the same?
 2. Most/all broadcast examples on the internet were with map-like structure/json/case class. I just broadcasted with the DF as it is. is there any advantage/disadvantage of one over another.
 3. Any way I can improve the code?  
 4. Is this a good enough scalable solution? I choose to use spark myself as if the data is streaming it can also deal with it easily. What could have been some other scalable options(in Scala), without using some engine like Spark, considering there could be hundreds or thousands of events per second?

I'd be happy to hear your arguments and feedback."
2657,2020-07-02 20:44:04,1593711844.0,dataengineering,Are These the Droids I'm Looking For?,hk1m5x,therealtibblesnbits,,https://www.reddit.com/r/dataengineering/comments/hk1m5x/are_these_the_droids_im_looking_for/,1.0,2.0,0.0,15432.0,"I've been working as an analyst for the past 10ish years, but I think I actually want to be a data engineer when I grow up. Do the projects I've enjoyed working on align with what data engineering is?

For pretty much all of my career, I've worked under the title of analyst, data scientist, or investigator, but in each of those roles I've found myself navigating towards the niche role of creating in-house solutions to help analysts do their jobs better. Allow me to give some examples of projects I've worked on that I really enjoyed.

* At my first job, I built a script that ingested the data from the surveys we sent out, parsed it out, and put it into data formats that could be used in our models, which reduced the time it took to build a model from \~60 minutes down to \~12 and automated the whole process.
* In my next role, I built a web-application that ingested tens of thousands of reports in XML format, parsed the data, stored it in a simple sqlite database, and performed record linkage on the reports to cluster those that were similar to each other so investigators could quickly scan through clusters looking for activity they wanted to investigate. Resulted in analysts going from reviewing 5-6 PDF reports/day to analyzing 10-15k very quickly.
* In that same role, worked with a couple other people to build a web app that ingested phone records in a number of formats (.txt, .pdf, .csv, and .html), stored in them in a MSSQL Sever database, and performed rudimentary analyses, displaying them in a dashboard. Took data ingestion from several days to almost instant, and provided investigators with immediate insights.
* In a later role, automated a report using R that connected to Splunk via its API, pulled data, processed it and performed a series of anomaly detection analyses and programmatically created a PDF with visualizations, tables, and conclusions. The table was then pushed to Microsoft Teams via an email interface. End result was a daily report that was delivered at 9 am, which allowed analysts to identify any suspicious activity they wanted to investigate that day.
* In that same role, created a script with Python that scraped data from Twitter's streaming API, looked for indicators of compromise (URLs, IP addresses, emails, hashes, etc), extracted them, and stored them in Splunk using its API. This provided an entirely new source of data for the organization.
* Most recently, built an API in Python that streamed data from a sqlite database to the client, and then built a Python script that connected to the API, processed the streaming data, and pushed it to Google BigQuery to demonstrate to a friend how to avoid writing data to disk when processing data from an API.

I really like building solutions that help analysts do their jobs better and faster. It's not always a pipeline per se, though. For example, I wouldn't consider the record linkage web app to be a data pipeline. So my main question is whether or not these are the types of tasks that a data engineer would do. A lot of the stuff I read online gives me mixed feelings about whether or not DE has a lot of overlap with DBA, which I would not want to do. 

I'd really appreciate hearing your thoughts on this, as well as maybe what your day-to-day looks like."
2658,2020-07-02 21:04:19,1593713059.0,dataengineering,What could be my task working as a data engineer?,hk201q,ajinkyajawale,,https://www.reddit.com/r/dataengineering/comments/hk201q/what_could_be_my_task_working_as_a_data_engineer/,1.0,6.0,0.0,15433.0,"Hey guys, I'm new college grad and got offered from a startup as a data engineer. they've product as a finance management app. I'm bit confused about my job role. Can you please explain what's the Lifecycle and what could be a typical day or work or tasks associated with attached Job description. TIA you kind people gbu!"
2659,2020-07-02 22:33:54,1593718434.0,dataengineering,An interview about how Audio Analytic is building a data set of high quality audio samples from scratch to power their sound recognition technology.,hk3p15,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hk3p15/an_interview_about_how_audio_analytic_is_building/,1.0,0.0,0.0,15434.0,
2660,2020-07-02 23:08:46,1593720526.0,dataengineering,"Koalas, or PySpark disguised as Pandas",hk4dle,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/hk4dle/koalas_or_pyspark_disguised_as_pandas/,1.0,0.0,0.0,15435.0,
2661,2020-07-03 02:07:52,1593731272.0,dataengineering,Personal Project on AWS Free Tier,hk7on6,SnooCompliments9388,,https://www.reddit.com/r/dataengineering/comments/hk7on6/personal_project_on_aws_free_tier/,1.0,15.0,0.0,15439.0,"Hey guys! I’m working on a personal project during my free time and wanted to get some input to see if the overall workflow I’m implementing for a streaming data pipeline makes sense or can be done better using the resources I have on the free tier of AWS.

So right now, because free tier doesn’t give access to Amazon Kinesis, I am just using a Kafka broker running on an EC2 instance. I currently have a web socket connection streaming somewhere between 100-300 records every minute into a topic that has a retention of just 5 minutes because I don’t have as much storage space to utilize.

What I’m trying to implement right now is a spark streaming application running at the same time on that EC2 instance that pulls a batch of data every minute, aggregates it all into one record and then inserts that record into a relational database on Amazon RDS. I don’t need all of the raw data from that batch afterwards hence why the retention for the Kafka topic is only 5 minutes. From there I’m going to be visualizing the processed data in QuickSight. 

I know there’s a lot of other tools on AWS that could be potentially useful like streaming the data from Kafka into S3 and analyzing it in batches afterwards, or running a spark application to process everything on EMR so I wanted to know if my approach still made sense for what I’m trying to accomplish right now, or if I should redesign it before I move further along with it.

TLDR:

On EC2 Instance, Websocket connect streams data into Kafka and Spark application aggregates new data every minute and appends a record with a summary into a database. Is it okay running it all on EC2 instance, or can the design be improved?"
2662,2020-07-03 02:51:25,1593733885.0,dataengineering,Any good videos on building Data Lakes,hk8ffq,vsmatcha,,https://www.reddit.com/r/dataengineering/comments/hk8ffq/any_good_videos_on_building_data_lakes/,1.0,2.0,0.0,15440.0,I am trying to learn about building Data Lakes(preferably on AWS). I want to go indepth into it. There are tons of videos on youtube. Dont know which one is a good one. looking for suggestions please
2663,2020-07-03 09:25:06,1593757506.0,dataengineering,Cloud Certifications for Data Engineering?,hke2xt,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/hke2xt/cloud_certifications_for_data_engineering/,1.0,22.0,0.0,15454.0,"For a Data Engineer role, which is actually a DevOps Software Engineering role, which Cloud Certifications are required? I'm looking at AWS, GCP and Azure; and I would like to get just ONE certificate from each.

For AWS, I think the Solutions Architect (Associate) is enough to perform any kind of operation. But what about GCP and Azure?"
2664,2020-07-03 13:51:21,1593773481.0,dataengineering,Difference between delta lake and Iceberg,hkgz3s,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/hkgz3s/difference_between_delta_lake_and_iceberg/,1.0,6.0,0.0,15464.0,How is Apache Iceberg different from delta lake : [https://iceberg.apache.org/?utm\_campaign=Evergreen&amp;utm\_medium=email&amp;\_hsmi=90688023&amp;\_hsenc=p2ANqtz-9Ck1LZEAEgRhuD4eKBHYYQeSOCQwsHvfQBdqlfd0KLAojHotvsFCt69DOdCIm0czHGhXR3mhpJVxRf72RNJKm\_4yb5uQ&amp;utm\_content=90687529&amp;utm\_source=hs\_email#user-experience](https://iceberg.apache.org/?utm_campaign=Evergreen&amp;utm_medium=email&amp;_hsmi=90688023&amp;_hsenc=p2ANqtz-9Ck1LZEAEgRhuD4eKBHYYQeSOCQwsHvfQBdqlfd0KLAojHotvsFCt69DOdCIm0czHGhXR3mhpJVxRf72RNJKm_4yb5uQ&amp;utm_content=90687529&amp;utm_source=hs_email#user-experience)
2665,2020-07-03 15:11:36,1593778296.0,dataengineering,Senior Big Data Engineer Certification - Because Big Data Doesn’t Build Itself,hkhyxn,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/hkhyxn/senior_big_data_engineer_certification_because/,1.0,0.0,0.0,15470.0,
2666,2020-07-03 15:29:45,1593779385.0,dataengineering,Zoe: Discover the new release of the command line tool for Kafka (with a new Katacoda environment to try it out from the browser),hki7q4,wlezzar,,https://www.reddit.com/r/dataengineering/comments/hki7q4/zoe_discover_the_new_release_of_the_command_line/,1.0,0.0,0.0,15470.0,
2667,2020-07-03 16:06:39,1593781599.0,dataengineering,Free Webinar on Introduction to Data Science: How to Get Started,hkiq4l,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/hkiq4l/free_webinar_on_introduction_to_data_science_how/,1.0,0.0,0.0,15471.0,
2668,2020-07-03 18:15:44,1593789344.0,dataengineering,Men Obesity Ranking | TOP 10 Country from 1975 to 2016,hkkt9p,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hkkt9p/men_obesity_ranking_top_10_country_from_1975_to/,1.0,0.0,0.0,15481.0,
2669,2020-07-03 19:36:07,1593794167.0,dataengineering,Modern Machine Learning Tooling,hkm9ae,linkerzx,,https://www.reddit.com/r/dataengineering/comments/hkm9ae/modern_machine_learning_tooling/,2.0,0.0,0.0,15491.0,
2670,2020-07-03 20:33:30,1593797610.0,dataengineering,Advice on Data migration to snowflake,hknak4,navinkp16,,https://www.reddit.com/r/dataengineering/comments/hknak4/advice_on_data_migration_to_snowflake/,3.0,11.0,0.0,15494.0,"Hi,

I am looking for advices / suggestions on data migration approach from AWS Postgres RDS to  Snowflake. Its for the very big partitioned tables which contain 2-3 billion records.

1. Historical/initial data sync as well as daily automatic update sync(near real-time) between the postgres RDS and Snowflake. Tried with one Snowflake partner ELT tool, which is going forever for the large big partitioned tables and its not feasable solution.
2. Looking for an advice/suggestion on the solution by using AWS tools for the history load and near-realtime incremental loads. What AWS tools can make this work easily in an optimized way. Please suggest if you have any solutions in mind using aws tools.

Thanks"
2671,2020-07-03 22:09:27,1593803367.0,dataengineering,Looking for data modelling interview questions/resource,hkoz7x,sabalibruh,,https://www.reddit.com/r/dataengineering/comments/hkoz7x/looking_for_data_modelling_interview/,14.0,6.0,0.0,15497.0,"Hi guys,
I want to practice on my data modelling skills. I am trying to find resources where there are questions focused on creating a data model for relational or dimensional DW. For ex.I didn't find any resource where you can practice creating a data model or a data warehouse. 

I found that for DE or BI roles , some companies ask for creating a data model given a scenario where you have to come up with schema and then maybe write scripts for that and create a database.

If you can point me to any of the resources that will be helpful. I am searching and didn't find any good resource. 

Thanks for your help."
2672,2020-07-04 06:23:44,1593833024.0,dataengineering,IT Engineer with 2 yrs exp. Data Analyst Resume. Would appreciate your feedback.,hkwivn,vishalw007,,https://www.reddit.com/r/dataengineering/comments/hkwivn/it_engineer_with_2_yrs_exp_data_analyst_resume/,5.0,24.0,0.0,15502.0,"Hey guys,

Hope you are doing well. I got laid off from my job due to COVID-19. Please give me any tips/pointers on my resume. I have 2 years of work experience and I am looking for Data Analyst/Data Engineer/ BI Developer roles.

Stay safe. Thanks :)

&amp;#x200B;

https://preview.redd.it/r8et1lwcer851.png?width=744&amp;format=png&amp;auto=webp&amp;s=32a74e69a9d4b063b70c7692f92eeedfb6a7319b"
2673,2020-07-04 10:35:13,1593848113.0,dataengineering,"Aspiring DE, but completely lost on where to begin",hkzgws,Aegyx,,https://www.reddit.com/r/dataengineering/comments/hkzgws/aspiring_de_but_completely_lost_on_where_to_begin/,1.0,12.0,0.0,15505.0,"Hi, I’m a rising undergrad senior and have recently become more interested in pursuing a job as a DE after graduating. I’m a CS major with a slightly more SWE-related background, but I have recently taken classes regarding database design, distributed computing, and business intelligence (pipelining, data warehousing, etc.). After reading through posts on this sub and searching on Google, I think I have a broad picture of what a DE generally does, although I understand it can vary wildly from company to company. 

Despite being a CS student, I really have nothing going for me at the moment besides the fact that I go to a top ~12 undergrad school. My SWE internship this summer got cancelled, so I’ve been looking to create some side projects to build my understanding on DE concepts and technologies (and to also increase my chances of actually being considered for a DE job), but I’m having a lot of trouble conceptualizing how exactly I should go about doing this.

For data science projects, you can just generally start off with a problem that you’re interested in and collect data / do some exploratory data analysis / apply some ML models to the data / explain whatever insights you gained from your results and bang! you have a decent data science project. However, I don’t see how I can do the same for DE, especially considering that any project I’d want to work on would be on such a small scale that it would be kind of funny or weird to use technology or software that is specialized for big data purposes. Besides this, I’m also not exactly sure how one actually goes about formulating a DE project. It’s just much harder for me to think about the utility or feasibility of a personal side project for DE than it is for DS. 

One more problem is that there are just so many things for me to learn. I don't know what kind of theoretical knowledge / technologies I should have under my belt in order to have a chance at getting a junior data engineering role out of undergrad. I feel like I'd need more than the average candidate especially because I don't have any internship experience. 

I would really appreciate some pointers or resources to guide me. Thanks!"
2674,2020-07-04 12:53:17,1593856397.0,dataengineering,Spotify: Most Streamed Songs (June 2020),hl0uzl,Dataism_channel,,https://www.reddit.com/r/dataengineering/comments/hl0uzl/spotify_most_streamed_songs_june_2020/,1.0,0.0,0.0,15510.0,
2675,2020-07-04 13:51:22,1593859882.0,dataengineering,Chicken Meat Production Ranking | TOP 10 Country from 1961 to 2013,hl1gdy,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hl1gdy/chicken_meat_production_ranking_top_10_country/,1.0,0.0,0.0,15510.0,
2676,2020-07-04 18:32:34,1593876754.0,dataengineering,Thoughts on Home Lab OS,hl56hp,therealtibblesnbits,,https://www.reddit.com/r/dataengineering/comments/hl56hp/thoughts_on_home_lab_os/,1.0,3.0,0.0,15520.0,"My desktop is an 8-core AMD Ryzen 3900x with 64 GB of RAM, a 1TB M2, and a 10 TB external HDD, and I want to take full advantage of it when working on projects. However, I'm bumping up against some trade offs and would like to hear other people's opinions on what I should do.

Currently, I'm running Windows 10 Home and whenever I want to drop into Linux, I open up VirtualBox. This was working fine until I got to a point where I wanted to get more familiar with containers (yes, I realize I'm very far behind the curve on this). Docker won't run on Windows 10 Home, so I'd have to upgrade to Pro. So here are what I see as my options:

* I could of course run it in a Linux VM, but then I'm restricting myself on the resources I have available since I can't have the VM use all 8 cores or all 64 GB of RAM. 
* I could wipe my OS and just run Linux on the host, but then I lose some of my functionality not related to DE, such as full access to gaming and Roll20 (for DnD) doesn't seem to work as well on Ubuntu. I'm also worried that if I wipe the entire drive to install Linux, I lose my ability to go back to Windows if I want. I still have the Windows 10 USB installer, but I'm not sure if the product key would work again?
* I could theoretically dual boot. This would be my preferred option, but apparently AMD architecture doesn't let you turn off secure boot? And when trying to install Ubuntu next to Windows 10, even after shrinking my C:/ partition, wouldn't work. I couldn't even select the M2 drive and Ubuntu would only offer my external hard drive. Perhaps due to file system format?
* I could forgo all of this and just do my DE projects in the cloud, since that is something I need to learn anyway. This is my least favorite option because it basically means my computer sits idle a lot. I obviously intend to work with cloud, but I'd like to make use of the home machine as well.

With all of that said, here are my questions: 

* Am I right that assuming running a Linux OS would be an advantage in DE projects?
* If I were to run a Linux VM, how big of a difference would it make if the VM only had 6 cores and 60 GB of RAM, compared to 8 cores and 64 GB of RAM?
* Is there another option I haven't thought of?

TIA for your help!"
2677,2020-07-05 04:39:39,1593913179.0,dataengineering,Any spark schema automation technique/library?,hlemk9,humphreyahn,,https://www.reddit.com/r/dataengineering/comments/hlemk9/any_spark_schema_automation_techniquelibrary/,1.0,4.0,0.0,15532.0,"Hi, I am a noob data engineer in 200+ size startup. I used to write ETL script in python using sqlalchemy and pandas, taking advantage of ORM and schema validation. But after moving on to Spark, it seems super tedious job to write schema using Spark Sql Types. Not to mention no schema versioning, unlike sqlalchemy + alembic. How do you guys manage table schema with spark?"
2678,2020-07-05 04:42:05,1593913325.0,dataengineering,Simple data engineering platform for a small analytics team,hlenlt,numice,,https://www.reddit.com/r/dataengineering/comments/hlenlt/simple_data_engineering_platform_for_a_small/,3.0,46.0,0.0,15532.0,"I've been thinking about how to setup a simple data engineering/data science infrastructure for our team. We do mainly analytics (Tableau) plus some small additional initiative data science projects.

The data sources are from system data mainly from CI/CD and devops tools such as Jenkins, Bitbucket, CI tools, and a little extra bit more from Sharepoint. No customer data is involved.

What we have right now for the data is mostly just databases like Postgres or SQLite but we have a roadmap to actually come up with a real infrastructure in the future.

The point is none of us has any experience with data engineering and the amount of data is not going to be huge. My estimation of the amount of data is somewhere around 10-50GB.

We're a small team of 6-7 so we'd like to keep things simple but also prefer to use industrial standard tools since it's a big corporate where software approvals take quite a long time. 

What would you guys suggest for this? Is Spark going to be overkill for us?"
2679,2020-07-05 06:22:35,1593919355.0,dataengineering,Big data projects?,hlfwsb,v2thegreat,,https://www.reddit.com/r/dataengineering/comments/hlfwsb/big_data_projects/,1.0,2.0,0.0,15532.0,
2680,2020-07-05 11:52:06,1593939126.0,dataengineering,DATA DOMAIN HIRING TRENDS BY MEDIOCRE BI ANALYST,hljh8u,DataFreakk,,https://www.reddit.com/r/dataengineering/comments/hljh8u/data_domain_hiring_trends_by_mediocre_bi_analyst/,1.0,3.0,0.0,15537.0,"  

Lately I have been analyzing Job positions in Data Domain and figured out that . Data Domain is Divided in 5 positions  


**1. DATA SCIENTIST (30%)**  
 **2. BIG DATA ENGINEER (15%)**  
 **3. DATA ENGINEER (ETL DEVELOPER) (25%)**  
 **4. DATA BASE ADMINISTRATOR (20%)**  
 **5.DATA ANALYST/BI ANALYST (20%)**  


***% : percent of openings*** 

My Estimation is that  **BI and Data scientist** merge in future  . Mediocre and low level work of Data scientists (ML,python,SQL,Stats) will be replicated and done in BI tool and advanced level of work will be done by Data scientists where accuracy of data models make developer sustain their life. Every company will adopt one Analytic team for their organisation  trends and insights whew every team will have BI analyst(Tableau,Power bi ) and Data scientist.    


**Big data Developer** who relies on extracting data and transformation will be in good position for near future as  MNC companies wont rely on Enterprise ETL tool  and rely on writing code  in Scala ,Spark and Airflow,Streaming data  kafka,etc but storage layer may shift to Cloud like s3,data lake,etc and hadoop will be replaced for major part.  


Data Enginner aka **ETL Developer** will be in also demand as Many organisation  has already using good GUI  etl too like inofrmatica ,talend,ssis  these people  may become consultants.   


**DBA** : These pople are crucuial and has sound knowledge on Data bases like SQL SERVER , TERA DATA, ORACLE ,RDS, etc but  major portion of dba work will be automated like daily backups,etc by vendors itself and less pain for Clients.  


***Note*** *: These are my insight from observing hiring pattern and I m Tableau BI Analyst  and Don't bash me If  i m worng.*"
2681,2020-07-05 13:31:46,1593945106.0,dataengineering,Duck Meat Production Ranking | TOP 10 Country from 1961 to 2013,hlkiei,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hlkiei/duck_meat_production_ranking_top_10_country_from/,1.0,0.0,0.0,15541.0,
2682,2020-07-05 22:01:14,1593975674.0,dataengineering,Udacity Nanodegree help,hls27e,juan-ms07,,https://www.reddit.com/r/dataengineering/comments/hls27e/udacity_nanodegree_help/,2.0,13.0,0.0,15555.0,I'm looking for the Github repository with the templates of the projects in the nanodegree program. I want to run my projects in my machine and can't find the clean repo.
2683,2020-07-05 23:52:13,1593982333.0,dataengineering,Patterns for deploying Approximate Nearest Neighbour (ANN) libraries as part of ETL?,hltzr2,ratatouille_artist,,https://www.reddit.com/r/dataengineering/comments/hltzr2/patterns_for_deploying_approximate_nearest/,3.0,5.0,0.0,15560.0,"I was wondering if there were any common patterns to deploy ANN libs (in particular as part of an ETL). My use case is around doing text based ANN search for tokens in 30 million documents  


I have come across [Milvus](https://milvus.io/) which seems to provide a simple docker interface for ANN libs   


Currently the ETL system I am working on uses Google Dataflow and I would like to better understand what patterns do others use to integrate ANN libs with their ETL."
2684,2020-07-06 03:36:01,1593995761.0,dataengineering,Looking for a better way to run jobs,hlxnvp,The_Amp_Walrus,,https://www.reddit.com/r/dataengineering/comments/hlxnvp/looking_for_a_better_way_to_run_jobs/,1.0,1.0,0.0,15568.0,
2685,2020-07-06 07:23:22,1594009402.0,dataengineering,"New Grad here! Today's is my first day at my first ever job as a "" Data Engineer "" at a startup. So exited... Can you give some tips?",hm0w4i,ajinkyajawale,,https://www.reddit.com/r/dataengineering/comments/hm0w4i/new_grad_here_todays_is_my_first_day_at_my_first/,5.0,33.0,0.0,15574.0,
2686,2020-07-06 09:55:21,1594018521.0,dataengineering,Junior Data Engineering job accessibility compared to SWE,hm2rj9,feistyteacup,,https://www.reddit.com/r/dataengineering/comments/hm2rj9/junior_data_engineering_job_accessibility/,1.0,2.0,0.0,15576.0,"I'm an undergrad senior in CS. I have one SWE internship from last summer where I was working primarily in React JS. I've become more interested in data engineering and I was wondering how accessible junior data engineering positions in comparison to junior SWE jobs. 

To elaborate, for SWE my impression is that you can come from pretty much any background as long as you display knowledge of some base CS concepts through a mixture of classes taken / technical interview questions on data structures and algorithms, as well as some side projects. Strong projects can often make up for a lack of an internship although it would obviously be much easier to land a job if you had an internship in a SWE related role.

I don't know if this holds true for junior data engineering jobs. Firstly, your personal projects are unlikely to be large in scale - you won't really be working with 'big data'. I don't have an internship in DE or anything big data and I think my front-end internship experience isn't very helpful either. How feasible would it be for me to land a job in such an area? 

I think your undergrad school matters much less in the tech industry than in something like consulting, but for what it's worth I do go to an Ivy and there are plenty of recruiters (although unfortunately there won't be any career fairs this upcoming fall due to covid)."
2687,2020-07-06 10:28:55,1594020535.0,dataengineering,Ditto: A dag transformation framework for Airflow DAGs!,hm35jj,singhangad,,https://www.reddit.com/r/dataengineering/comments/hm35jj/ditto_a_dag_transformation_framework_for_airflow/,1.0,1.0,0.0,15577.0,
2688,2020-07-06 10:34:01,1594020841.0,dataengineering,"Airflow HDInsight operators, sensors and hooks",hm37m0,singhangad,,https://www.reddit.com/r/dataengineering/comments/hm37m0/airflow_hdinsight_operators_sensors_and_hooks/,1.0,0.0,0.0,15577.0,
2689,2020-07-06 12:17:11,1594027031.0,dataengineering,"How to ""Predict"" my friends weight using ""Machine Learning"" &amp; ""Sci-Kit Learn""??",hm4d6q,8329417966,,https://www.reddit.com/r/dataengineering/comments/hm4d6q/how_to_predict_my_friends_weight_using_machine/,1.0,1.0,0.0,15578.0,https://youtu.be/A4JwnkTFEXI
2690,2020-07-06 14:03:16,1594033396.0,dataengineering,Mobile Social Media Ranking | TOP 10 Popularity from 2009 Q3 - 2020 Q2,hm5lj3,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hm5lj3/mobile_social_media_ranking_top_10_popularity/,1.0,0.0,0.0,15580.0,
2691,2020-07-06 16:41:34,1594042894.0,dataengineering,Announcing Early Access Program for Flink SQL in Ververica Platform,hm7urs,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hm7urs/announcing_early_access_program_for_flink_sql_in/,1.0,0.0,0.0,15586.0,
2692,2020-07-06 18:16:27,1594048587.0,dataengineering,Telegram channel with remote senior Data Engineering jobs every day [OC],hm9hvk,xlpz,,https://www.reddit.com/r/dataengineering/comments/hm9hvk/telegram_channel_with_remote_senior_data/,2.0,12.0,0.0,15592.0,
2693,2020-07-06 20:33:06,1594056786.0,dataengineering,"Data Analysis internship to build some cred for students - fully online, reputable, and comes with endorsements and review on Linkedin!",hmc30p,taimoor2,,https://www.reddit.com/r/dataengineering/comments/hmc30p/data_analysis_internship_to_build_some_cred_for/,0.0,2.0,0.0,15594.0,"This is a good internship. Teaches basics of data analysis using python, gives a certificate, and an endorsement and review on LinkedIn. Requirements are very simple. Just complete a Udemy course and complete some basic assignments.

TakenMind Global Managerial Internship Application in the field of Data Analytics(Finance) is now open for aspiring students.

Applications are open at https://internship.takenmind.com/p/76022

TakenMind Global Internship Program is recognized under United Nations SDG catering to the candidates aspiring a high-end career in Finance/Analytics. This is a remote internship. This means you will do your assigned work, long-distance, as per your own convenience. You will interact with your Internship Manager by means of email, phone and/or other digital media. Students and Professionals can take part in this internship alongside their University work or Professional work. The Internship Program will help the candidates learn and implement live financial analytics project that can boost career opportunities to a higher level.

Note:


1. No prior knowledge in data analytics is needed to apply for the Internship Program.

1. Top 5 selected Interns selected based on performance will represent TakenMind and will be sponsored a free entry to the Strata's Data Conference.

Applications are open at https://internship.takenmind.com/p/76022"
2694,2020-07-06 20:53:43,1594058023.0,dataengineering,How do you enable your users upload ad hoc data into Snowflake?,hmchs2,tedfahrvergnugent,,https://www.reddit.com/r/dataengineering/comments/hmchs2/how_do_you_enable_your_users_upload_ad_hoc_data/,3.0,11.0,0.0,15594.0,"I’ve looked at a couple methods and currently `snowsql` -&gt; `@~/stage` -&gt; `DW.ADHOC.&lt;USERNAME&gt;_TABLE` seems the best but is a little complex especially for first time setup. I’ve thought about using an S3 bucket, syncing OneDrive to S3, using the web ui, etc. 

Also how do you manage ad hoc data to keep it from becoming a problem? (Maybe this should be a separate post)"
2695,2020-07-06 21:37:35,1594060655.0,dataengineering,Structure - Building SQL Pipelines for Snowflake,hmd8t2,macklemores_hair,,https://www.reddit.com/r/dataengineering/comments/hmd8t2/structure_building_sql_pipelines_for_snowflake/,1.0,4.0,0.0,15598.0,"Hey everyone, 

We've been working on an analyst-friendly tool for building SQL pipelines for Snowflake. 

Instead of writing ad-hoc SQL or manually managing tables, building SQL pipelines allows your team to transform data in Snowflake in a way that is version-controlled and allows your team to leverage  single source of truth definitions for future analysis. (If you are familiar with DBT or Dataform - you are familiar with the idea of building SQL as a DAG / pipeline)

With Structure - we've made building SQL pipelines absolutely dead simple for everyone on the team, with an editing environment specifically built for SQL pipelines. There isn't any compilation or command line tools, in Structure, just a dedicated analytics environment so everyone on the team can understand and contribute to the analytics process, together.

Here's a short video - [https://www.youtube.com/watch?v=V1tmeFgWMLs](https://www.youtube.com/watch?v=V1tmeFgWMLs) and our website - [www.structure.rest](https://www.structure.rest)

I'm here to answer any questions you have! cheers, -michael"
2696,2020-07-07 06:46:38,1594093598.0,dataengineering,Most mothers of newborns are up at midnight because the baby can't sleep...,hmn4tz,BostonPanda,,https://www.reddit.com/r/dataengineering/comments/hmn4tz/most_mothers_of_newborns_are_up_at_midnight/,1.0,15.0,0.0,15613.0,"I'm not most mothers though. My baby is asleep deeply. However, the new sprint begins tomorrow and my pipeline (baby of these two weeks) decided to start spitting out an error message thanks to a minor code change. Sure I can roll over work but I'm stubborn and told basically everyone at the end of the day how great it was that this pipeline was all set 😅🤦

The end is near, running a test now, but wanted to vent to those who understand!

Delete if not allowed :)"
2697,2020-07-07 09:03:39,1594101819.0,dataengineering,"Computers don't understand textual data, so how do they process textual data? Word embedding is used to represent these words.Click here to know more",hmoz2s,Hussain_Mujtaba,,https://www.reddit.com/r/dataengineering/comments/hmoz2s/computers_dont_understand_textual_data_so_how_do/,1.0,0.0,0.0,15620.0,
2698,2020-07-07 13:37:13,1594118233.0,dataengineering,Wine Production Ranking | TOP 10 Country from 1961 to 2014,hms5hs,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hms5hs/wine_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,15627.0,
2699,2020-07-07 14:04:28,1594119868.0,dataengineering,Top Data Engineering Company,hmshwk,Poojakhana52314,,https://www.reddit.com/r/dataengineering/comments/hmshwk/top_data_engineering_company/,1.0,0.0,0.0,15627.0,
2700,2020-07-07 15:32:39,1594125159.0,dataengineering,"An interview about how the Lenses.io platform addresses the DataOps challenges for streaming systems to power observability, discovery, and governance of your real time data.",hmtojy,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hmtojy/an_interview_about_how_the_lensesio_platform/,1.0,0.0,0.0,15627.0,
2701,2020-07-07 16:41:43,1594129303.0,dataengineering,"Machine learning algorithms don't understand textual data, so how do they process textual data? Word embedding is used to represent these words.Click here to know more",hmuqs2,Hussain_Mujtaba,,https://www.reddit.com/r/dataengineering/comments/hmuqs2/machine_learning_algorithms_dont_understand/,1.0,0.0,0.0,15627.0,
2702,2020-07-07 17:17:31,1594131451.0,dataengineering,How to provide failover for Logstash or other log collector using keepalived,hmvcng,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/hmvcng/how_to_provide_failover_for_logstash_or_other_log/,1.0,0.0,0.0,15627.0,
2703,2020-07-07 17:43:42,1594133022.0,dataengineering,Apache Flink 1.11 is out now! Read the release announcement for all new features and improvements,hmvtnv,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hmvtnv/apache_flink_111_is_out_now_read_the_release/,1.0,0.0,0.0,15627.0,
2704,2020-07-07 19:13:05,1594138385.0,dataengineering,U.S. Unemployment Rates History (1970 -2020),hmxibm,Dataism_channel,,https://www.reddit.com/r/dataengineering/comments/hmxibm/us_unemployment_rates_history_1970_2020/,1.0,0.0,0.0,15630.0,
2705,2020-07-07 20:04:28,1594141468.0,dataengineering,Apache Spark Performance Benchmarks show Kubernetes has caught up with YARN,hmyhlz,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/hmyhlz/apache_spark_performance_benchmarks_show/,1.0,6.0,0.0,15631.0,
2706,2020-07-07 20:43:20,1594143800.0,dataengineering,"GCP Cloud Functions, Cloud SQL and Pandas Help",hmz8r5,ElethorAngelus,,https://www.reddit.com/r/dataengineering/comments/hmz8r5/gcp_cloud_functions_cloud_sql_and_pandas_help/,1.0,0.0,0.0,15635.0,"Hi All!

I have been working on a Cloud Function that will trigger when a file is uploaded on cloud storage. For the time being almost everything works, but it seems that I am unable to write data into Cloud SQL using the function.

The current approach I am using to write to SQL is just to use pandas built in .to\_sql method, the error doesn't seem descriptive but I am assuming that it might be an issue with how we use a unix socket to connect to MySQL using Cloud Functions.

Is this the case ? Or am I seeing things ?

Traceback below

`Traceback (most recent call last):`

`File ""/env/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1258, in _execute_context`

`cursor, statement, parameters, context`

`File ""/env/local/lib/python3.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py"", line 148, in do_executemany`

`rowcount = cursor.executemany(statement, parameters)`

`File ""/env/local/lib/python3.7/site-packages/pymysql/cursors.py"", line 197, in executemany`

`self._get_db().encoding)`

`rows += self.execute(sql + postfix)`

`File ""/env/local/lib/python3.7/site-packages/pymysql/cursors.py"", line 170, in execute`

`result = self._query(query)`

`conn.query(q)`

`File ""/env/local/lib/python3.7/site-packages/pymysql/connections.py"", line 517, in query`

[`result.read`](https://result.read/)

    ()

`File ""/env/local/lib/python3.7/site-packages/pymysql/connections.py"", line 1075, in read`

`first_packet = self.connection._read_packet()`

`File ""/env/local/lib/python3.7/site-packages/pymysql/connections.py"", line 684, in _read_packet`

`packet.check_error()`

`File ""/env/local/lib/python3.7/site-packages/pymysql/protocol.py"", line 220, in check_error`

`undefined`"
2707,2020-07-07 20:47:22,1594144042.0,dataengineering,Migrating to Snowflake,hmzbox,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hmzbox/migrating_to_snowflake/,1.0,5.0,0.0,15635.0,"My team wrote this article on tips and tricks for migrating to Snowflake from legacy data centers (a common pain point for our customers). Any advice we missed?

[https://towardsdatascience.com/migrating-to-snowflake-like-a-boss-6163293f0bcb#6926-c6a3c39d7234?utm\_source=blog&amp;utm\_medium=reddit&amp;utm\_campaign=snowflake\_migration](https://towardsdatascience.com/migrating-to-snowflake-like-a-boss-6163293f0bcb#6926-c6a3c39d7234?utm_source=blog&amp;utm_medium=reddit&amp;utm_campaign=snowflake_migration)"
2708,2020-07-07 21:20:32,1594146032.0,dataengineering,Data Engineering: Which course should i take,hmzyre,sdqafo,,https://www.reddit.com/r/dataengineering/comments/hmzyre/data_engineering_which_course_should_i_take/,1.0,6.0,0.0,15636.0,"As part of my progress after passing SAA-C02, I plan to start a career as a Data Engineer and later go into Big Data Specialty. However, i am looking for a course with very deep training not just certification. I will appreciate your recommendation. I have seen some on Simplilearn and Udacity but not sure. Kindly advise"
2709,2020-07-07 23:36:26,1594154186.0,dataengineering,Dremio &amp; Snowflake,hn2k3q,DataNubNub,,https://www.reddit.com/r/dataengineering/comments/hn2k3q/dremio_snowflake/,1.0,3.0,0.0,15644.0,"Hello All,

As a background, I have no data science degree and I'm more or less a cog BI analyst in a severely corporate consulting machine. 

I'd like to really understand the differences between the features of Dremio vs Snowflake. It seems like a lot of these big data companies structure how they want to monetize their services and I'm a bit of a dummy to really get the differences.

This article somewhat helped but I would really appreciate an ELI5:  [https://www.forbes.com/sites/danwoods/2018/07/31/saving-your-data-lake-how-dremio-delivers-a-new-data-as-a-service-paradigm/#273ed0c1405c](https://www.forbes.com/sites/danwoods/2018/07/31/saving-your-data-lake-how-dremio-delivers-a-new-data-as-a-service-paradigm/#273ed0c1405c) 

 Does it make sense to use both to maintain your data as a company? My understanding if you go snowflake, the snowflake company provisions a warehouse for you and you can do ETL within the warehouse without need for PowerBI's powerquery or Alteryx (which my company heavily uses for ETL). Apparently from the article, Dremio can also ETL as well, so they wouldn't need to ETL with another application before using visualization software. My company's looking to adopt Dremio, but I don't understand the differences. The terminology of data lakes, data warehouses and self-service is also kind of confusing..

Thanks for your response!

Best Regards,"
2710,2020-07-08 02:14:44,1594163684.0,dataengineering,Anyone heard of Subsurface Conf - The Cloud Data Lake conference,hn5grb,WranglingData,,https://www.reddit.com/r/dataengineering/comments/hn5grb/anyone_heard_of_subsurface_conf_the_cloud_data/,1.0,0.0,0.0,15651.0,"Hi Folks,

A message on the Spark users mailing list caught my attention this morning - an announcement for a online Cloud based data lake conference called Subsurface.  I had a quick look (cautiously clicking on a link, as Google turned up zero) and it seems that have a reasonable list of speakers.

So, before I give them my details, I'm curious to know whether people had heard about it before?  I know its free and they are not going to steal my money, but I still am cautious giving away my deets.

Link if you are game:  [https://subsurfaceconf.com/](https://subsurfaceconf.com/) 

Cheers"
2711,2020-07-08 04:09:59,1594170599.0,dataengineering,Docker + Airflow + scrapy + Selenium,hn7dw7,digichap28,,https://www.reddit.com/r/dataengineering/comments/hn7dw7/docker_airflow_scrapy_selenium/,1.0,9.0,0.0,15657.0,"Hey there! I hope everything is going great with you 👍

I developed a scraper using scrapy and selenium. My script opens up a web app which prompts a windows authentication pop up on the browser, logs in, and then gets several HTML pieces of code (example: tables) which are finally stored as html files.

To accomplish the authentication step I had to import the win32com package. And, as the Browser UI is needed for the log in to happen, the headless option can’t be used.

As I’m trying to automate an entire process where this step or task would be the beginning, and also other pipelines I have in mind, I thought using Airflow would fir perfectly. 

Unfortunately I didn’t have any experience with Airflow, neither docker. So I had to start reading a lot, watching videos, etc to be able to implement my stuff.

At this moment I’m stuck on how should the Selenium have to be ran. If Selenium is containerized, would I be able to run the browser with the UI so the Win32com would work ? I have read a few articles where Airflow and selenium run on different containers, and the first one spins up the latter one, but the web drivers they have used were set up to be headless. 

I would appreciate if someone could point me to the right direction or maybe give me a clue.

I’m learning hitting myself and now I’m feeling lost :/


Thanks!!"
2712,2020-07-08 09:06:39,1594188399.0,dataengineering,Need help I'am getting confused in my career path,hnbq3l,ZashYang,,https://www.reddit.com/r/dataengineering/comments/hnbq3l/need_help_iam_getting_confused_in_my_career_path/,1.0,9.0,0.0,15665.0,Should I need to learn some Data Security or not? is a data engineer only considered a data engineer if he create pipelines or data integration coded in python?
2713,2020-07-08 11:52:58,1594198378.0,dataengineering,Web Browsers War in the Last Decade,hndlcq,mostafaelmahdy,,https://www.reddit.com/r/dataengineering/comments/hndlcq/web_browsers_war_in_the_last_decade/,1.0,0.0,0.0,15671.0,
2714,2020-07-08 13:32:12,1594204332.0,dataengineering,Stroke Death Rate Ranking | TOP 10 Country from 1990 to 2017,hneow8,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hneow8/stroke_death_rate_ranking_top_10_country_from/,1.0,0.0,0.0,15671.0,
2715,2020-07-08 13:41:24,1594204884.0,dataengineering,student with intership opportunity.. lost in data analysis.. need help!,hnesum,TikiCruise,,https://www.reddit.com/r/dataengineering/comments/hnesum/student_with_intership_opportunity_lost_in_data/,1.0,1.0,0.0,15671.0,"Opportunity for internship.. need help!

Hi.. i just want to first mention that english is not my native language.
I have an internship/job (if I do well) opportunity at a tech company in the cloud department.
They need a guy who can take data from IoT sources that will go on the azure platform and make analysis on it.
I know some basic linux, networking, sql and programming.. I know how to use the azure platfrom but im a data analysis noob.. I've read a bit about it.. about hadoop and ETL pipelines but im still confused.. my question is what are the options to analyze data? I know im not being specific but right now I dont have more information on the software i will have to work with. Is hadoop overkill? what should i learn to get a basic understading
of this topic? Im a bit lost and I dont want to get into the hadoop world if I wont need it.. what is the basic? I know I'am not being very clear but as I said before.. I'm kinda lost on this one.
I'll appreciate any help!
thx in advance"
2716,2020-07-08 15:03:31,1594209811.0,dataengineering,What every data engineer should know...,hnftuu,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hnftuu/what_every_data_engineer_should_know/,1.0,10.0,0.0,15673.0,"Are you new to data  engineering and want to share some advice to other newcomers? Are you an  old hand and data wrangling and want to leave some pointers to the next  generation? I'm working with O'Reilly Media on 97 Things Every Data Engineer Should Know and we need your help to make it a reality.

If you have a blog post,  presentation, or white paper that is useful for data engineers, then  send them along. We can work it into shape for the book. Share your wisdom and help educate data engineers everywhere!

https://www.dataengineeringpodcast.com/97things"
2717,2020-07-08 16:09:12,1594213752.0,dataengineering,A local AWS EMR to help with development,hngrgx,davlum,,https://www.reddit.com/r/dataengineering/comments/hngrgx/a_local_aws_emr_to_help_with_development/,1.0,0.0,0.0,15675.0,
2718,2020-07-08 16:37:40,1594215460.0,dataengineering,What they don’t tell you about event sourcing,hnh75l,hugorocha,,https://www.reddit.com/r/dataengineering/comments/hnh75l/what_they_dont_tell_you_about_event_sourcing/,1.0,0.0,0.0,15678.0,
2719,2020-07-08 17:26:35,1594218395.0,dataengineering,An open source local AWS EMR to help with development,hnhzp6,davlum,,https://www.reddit.com/r/dataengineering/comments/hnhzp6/an_open_source_local_aws_emr_to_help_with/,1.0,0.0,0.0,15684.0,
2720,2020-07-08 17:42:58,1594219378.0,dataengineering,What are the use cases for DataBricks Delta Lake? Doe sit make sense to use it?,hni9pz,micjosmcc,,https://www.reddit.com/r/dataengineering/comments/hni9pz/what_are_the_use_cases_for_databricks_delta_lake/,1.0,1.0,0.0,15684.0,"Hey all,

I've recently moved into a data engineering role from software engineering, and I'm currently the only one in our data team 😬 

I'm in the process of understanding our current data commons which consists of a number of PostgreSQL databases on AWS connected by airflow pipelines. A director in another team is very interested in data versioning as well as a better way to store and work with a growing number of large single cell experiment objects (these are usually extremely large sparse matrices). He's interested in looking into Quilt, but I've been told to also check out DataBricks Delta Lake.

However, even after looking into it, I'm not sure what it's use cases are? We're a biotech company, and we generate large amounts of data from instrument runs, we also scrape open source data from online databases. I think we generate enough data to make the case for DataBricks, but I'm curious what other companies look to get out of Delta Lake when implementing it? I think with a better knowledge of this I can better make the case to go with it"
2721,2020-07-08 18:04:12,1594220652.0,dataengineering,Permissions for users on redshift are acting rather strange,hnin5n,psychuil,,https://www.reddit.com/r/dataengineering/comments/hnin5n/permissions_for_users_on_redshift_are_acting/,1.0,4.0,0.0,15685.0,"Hi everyone, I'm sorting out my cluster and as part of that people are starting to use their own users, so I've given everyone access to their own schemas and all is well.

However, despite setting up `alter default privileges` to all on a schema for a group - when one person creates a table it doesn't allow someone else from the same group to select from it.

And another thing I've noticed is that I would look into has_table_privilege for a user on a specific table, and they would only have create+use+select, but no insert/update/delete.
And even after I run a grant, giving them either all or specific permissions it doesn't change anything.

Has anyone ever encounter something like this? Googling keeps bringing me to the grant and alter default privileges docs and related questions, but nothing relevant."
2722,2020-07-08 18:47:10,1594223230.0,dataengineering,Data Warehouse REST API?,hnjf4m,goobdin,,https://www.reddit.com/r/dataengineering/comments/hnjf4m/data_warehouse_rest_api/,1.0,8.0,0.0,15687.0,"Hi everyone,

I'm currently trying to integrate some of our company's data from our warehouse to the rest of the organization without giving direct access to our Redshift cluster. My initial thoughts were to build out a REST API that users can just call into (provided they have an API Key).

 Would you recommend just going server-less with AWS Lambda or building a REST API and deploying it as a Container or ECS?

Thanks you!"
2723,2020-07-08 18:47:58,1594223278.0,dataengineering,How do you and your team catalog data?,hnjfmg,robmacanderney,,https://www.reddit.com/r/dataengineering/comments/hnjfmg/how_do_you_and_your_team_catalog_data/,1.0,0.0,0.0,15687.0,"Hi all,

Please can you help my team and I with some research?

I am pulling together some thoughts on how analytics teams surface and then gain context on data in their organizations.

Full transparency - I run a data science consultancy, and we are trying to enhance our understanding of the area.

I am aware commercial and open-source data catalogs offer a solution to this, however, I have still seen:

\- Organizations often don’t have a handle on all the data they have. There is often low awareness amongst business users of what data is available

\- Time is wasted reinventing the wheel as calculations are not proactively shared among team members

\- There are often inconsistencies in metric definitions. Not knowing how metrics and terms are defined can cause confusion

\- It is not easy for new analysts / infrequent data users to get up to speed with data schemas

Questions:

1. Have you experienced problems like this?

2. How do you solve these problems?

3. Would you be happy to talk to me for 20 minutes on the subject?

Thanks!"
2724,2020-07-08 19:29:50,1594225790.0,dataengineering,Switching Careers from Data Analyst to Data Engineer,hnk7v0,Culpgrant21,,https://www.reddit.com/r/dataengineering/comments/hnk7v0/switching_careers_from_data_analyst_to_data/,1.0,43.0,0.0,15688.0,"I am currently a data analyst and have been for a year since graduation in 2019 (the University of Illinois in Economics). I want to make the switch from data analyst to data engineer in my career path - I enjoy the more technical skills. 

The question I am struggling with is how to do I achieve this?

My current skills are Power Platform (Power BI, Powerapps, Power Automate), SQL, and Python.

**Has anyone made the switch from Data Analyst to Data Engineer? And how did you do it?**

I am currently considering graduate school and online courses.

&amp;#x200B;

Thank you!"
2725,2020-07-08 22:23:23,1594236203.0,dataengineering,Amazon Data Engineer recruitment process,hnnkjz,eeshitaverma,,https://www.reddit.com/r/dataengineering/comments/hnnkjz/amazon_data_engineer_recruitment_process/,1.0,9.0,0.0,15694.0,"Hi,
I have completed technical written round and expecting a positive response, but it’s been one month already I haven’t heard back. Can anyone help me with the following: 
1. How much time does it takes to hear back, post written assessment? 
2. How virtual hiring process is being conducted during ongoing condition of covid? ( detailed explanation requested) 
3. What should I focus on, to get through? Please suggest resources also. 

Thanks in advance."
2726,2020-07-08 22:41:22,1594237282.0,dataengineering,Humble Book Bundle: Data Science Essentials by Taylor &amp; Francis (pay what you want and help charity),hnnxkq,G4M1NG,,https://www.reddit.com/r/dataengineering/comments/hnnxkq/humble_book_bundle_data_science_essentials_by/,1.0,0.0,0.0,15696.0,
2727,2020-07-09 01:49:26,1594248566.0,dataengineering,Wiki for the DE sub,hnrfj8,sabalibruh,,https://www.reddit.com/r/dataengineering/comments/hnrfj8/wiki_for_the_de_sub/,1.0,2.0,0.0,15699.0,"People of DE sub. I am interested in becoming a DE and I am sure there are many people who have joined this sub for learning about the same. People have come across the links and articles on how to become one. The courses to take etc. There are already DE in the group as well who can contribute majorly in the wiki. I am proposing this because I have posted a question' How to become a DE or transition from one'. 

Every now and then there are quest posted around this. I think if we have a wiki around this it will be beneficial for newbies and also we can include a many sections to help navigate the current responsibilities by pointing them to right resources.

Please voice your opinion on the pole below. I am interested in wiki and even contributing to it.

[View Poll](https://www.reddit.com/poll/hnrfj8)"
2728,2020-07-09 03:21:39,1594254099.0,dataengineering,Interview prep advice: Need help understanding data life cycle concepts and source schema design &amp; storage methods for high volume/velocity consumer websites and applications.,hnsy81,sillysally09,,https://www.reddit.com/r/dataengineering/comments/hnsy81/interview_prep_advice_need_help_understanding/,1.0,0.0,0.0,15702.0,"Hi all, I have a FAANG data modeling interview coming up and am expected to know how to design source and target data model schemas for various popular consumer applications, eg LinkedIn, Airbnb, Instagram, etc. I could use some help understanding the relevant data lifecycle concepts here, particularly at the collection and storage step as I'm lacking in source system and web-based data architecture design experience. Any dimensional model I've built in the past has always been from an already existing data warehouse, so I am unfamiliar with upstream data collection and processing steps before this part of the lifecycle.

Let's take the LinkedIn News Feed for example, if I was asked to design a source schema and target schema for this application, to me a target schema sounds like a dimensional model in a relational database, but could also be one or more NoSQL database tables depending on the data use cases. I am learning how to design target schemas to answer questions like, what was the text string value of User ID 123's news feed post yesterday (probably a NoSQL table), as well as questions like, give me the number of daily active users and their average session length yesterday in Canada (probably a relational star schema); I'm using the e-commerce chapter of Kimball's Data Warehouse Toolkit to get a feel for dimensionally modeling this sort of activity.

I'm a bit confused about what upstream data capture, design, and storage processes might look like for this sort of application.

* What structure would a source schema look like here and what medium would I initially store it in? Is the storage format a raw JSON file? How would it be the file structured: how would multiple events get stored, would each event be separate lines in the same JSON file, or does each event get its own file?
* What would be the first platform I use to store each event? Should it first be on s3, on HDFS, pushed directly to NoSQL database like Cassandra, pushed directly to a data warehouse in Snowflake? Is there a common order of operations here or are all of these mutually exclusive? Are these choices affected if data is batch processed vs streaming?
* How should data be partitioned and when? Should we group events into files by day, hour, region, etc? I’ve seen recommendations online suggesting knowing answers to questions like, how would you partition your Hive table. I don't have Hadoop ecosystem experience and have mostly just worked with relational databases so I understand table indexes, and recently learned about partitions / clustering in Cassandra. Should I spend some time brushing up on Hive partitions? When should I anticipate this type of question?
* If the interviewer asks for me to design both a NoSQL system and a relational star schema model based on the set of questions they want to answer, how should I understand which steps to take and in what order?

I know there are a lot of questions here so anyone interested in responding, don't feel like you need to address all of it. Please feel free to correct any of my implied misunderstandings of these systems as well. Any thoughts would be appreciated!"
2729,2020-07-09 07:57:37,1594270657.0,dataengineering,Need help with Reddit integration with Apache Kafka.,hnx269,DevZeusGru1602,,https://www.reddit.com/r/dataengineering/comments/hnx269/need_help_with_reddit_integration_with_apache/,1.0,0.0,0.0,15711.0,
2730,2020-07-09 09:41:24,1594276884.0,dataengineering,Facebook Business Manager/ Google data studio api,hnyasp,audyoga,,https://www.reddit.com/r/dataengineering/comments/hnyasp/facebook_business_manager_google_data_studio_api/,1.0,0.0,0.0,15715.0,Has anyone used fb business manager and google data studio as producers and if yes can you share any implementation details on if it can be configured making graphql or rest calls.
2731,2020-07-09 12:57:38,1594288658.0,dataengineering,ENTRY LEVEL/FRESHER,ho0feu,KIENBACH12,,https://www.reddit.com/r/dataengineering/comments/ho0feu/entry_levelfresher/,1.0,1.0,0.0,15719.0,"hello guys , i want to find  data engineer jobs , i know about mysql, basic machine learning algorithms, worked in some deep learning projects (object detections.).But i realize i like data engineer more than data scientists ,  so what skills do i need to improve and learn to work in data engineering field."
2732,2020-07-09 13:51:19,1594291879.0,dataengineering,Labor Productivity Ranking | TOP 10 Country from 1970 to 2017,ho10qw,datavtworld,,https://www.reddit.com/r/dataengineering/comments/ho10qw/labor_productivity_ranking_top_10_country_from/,1.0,0.0,0.0,15719.0,
2733,2020-07-09 15:41:08,1594298468.0,dataengineering,One skill that to you need if you want to be a good data engineer,ho2dvy,A1M94,,https://www.reddit.com/r/dataengineering/comments/ho2dvy/one_skill_that_to_you_need_if_you_want_to_be_a/,1.0,23.0,0.0,15722.0,"**You must be able to look for answers on your own!**

If you become software engineer / data engineer / whatever engineer (or any other profession), nobody is going to hold your hand and answer your questions 24/7. 

This sub is flooded with posts like ""Recent undergrad, how do I become data engineer?"", ""Which course should I take?"", ""Transitioning from XYZ to DE"". Does not matter if you are transition from XYZ or ABC, almost all answers are still valid. If you can not gather enough information from a subreddit, where this question appears every day, how are you going to solve other problems? How are you going to fix that mysterious bug, that was solved by randomDude at stackoverflow, who just closed the issue with ""Nevermind, solved it."" and does not tell you how? Or are you going to ask a question on reddit and hope for an answer? What are you going to say to your boss, when you miss a deadline? ""I asked on the internet but nobody answered.""?

Most of your answers are one search query away and usually top search result. Do your research and then ask if something is unclear. 

Ok, rant over, now you can downvote me."
2734,2020-07-09 22:03:39,1594321419.0,dataengineering,Video Series: Streaming Concepts &amp; Introduction to Flink - Part 1,ho92wj,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ho92wj/video_series_streaming_concepts_introduction_to/,1.0,0.0,0.0,15743.0,
2735,2020-07-10 00:50:14,1594331414.0,dataengineering,Advise on data extraction from PDF files,hoc5mr,AliGMaye,,https://www.reddit.com/r/dataengineering/comments/hoc5mr/advise_on_data_extraction_from_pdf_files/,1.0,8.0,0.0,15745.0,"Hello wonderful people , I am stuck on how to extract data from a pdf file.The pdf file is a bank statement. The statement has transactions in a table but the table does not have column lines.I want to convert the data to csv format for easier processing in the data pipelines. 
 I have tried using python's tabula package but the extracted format looks really misaligned with the table headers merging onto one another.

Any advise would be highly appreciated. 
Thank you."
2736,2020-07-10 02:55:33,1594338933.0,dataengineering,Handling GDPR/CCPA user deletion requests?,hoecmt,yxjxy,,https://www.reddit.com/r/dataengineering/comments/hoecmt/handling_gdprccpa_user_deletion_requests/,1.0,10.0,0.0,15746.0,"My company is soon having to delete user data from GDPR &amp; CCPA deletion requests, I was wondering how other companies are handling these requests when tables involving user activity are massive and can be very expensive to scan and delete by user.

Here was one of our ideas - 

Hashing the user\_id when ingested into the raw event tables and having a lookup table to join to with any relevant PII/user info, and then deleting that row in the lookup table when given a deletion request, thus rendering that user's hashed user\_id useless while keeping the activity intact

What are the solutions you guys have come up with?"
2737,2020-07-10 08:30:19,1594359019.0,dataengineering,The need for building a Dev-ops platform for Spark and Big Data,hoj9op,shad-rocks,,https://www.reddit.com/r/dataengineering/comments/hoj9op/the_need_for_building_a_devops_platform_for_spark/,1.0,9.0,0.0,15758.0,"We are a small team of Spark Developer and UX Designer looking to build an awesome experience for working with distributed systems, starting with Apache Spark.

Using Spark APIs are fun and straight forward, but the tasks involved in developing and running in production can be quite daunting and time consuming.

Some of the common tasks involved are -

* Setup local dev environment
* Automate deployment on multiple test and staging environment
* Run unit/integration tests
* Find the right size of ephemeral cluster
* Debug the logs
* Monitoring the performance and tune it for optimal run time and cost
* Rollback in case of failures
* Backfill data

In this blog, I explain what really got me started - [https://blog.gigahex.com/why-we-are-building-gigahex](https://blog.gigahex.com/why-we-are-building-gigahex)"
2738,2020-07-10 12:56:50,1594375010.0,dataengineering,How do create nested JSON with relational data.,hom8ah,ovary_laid,,https://www.reddit.com/r/dataengineering/comments/hom8ah/how_do_create_nested_json_with_relational_data/,1.0,8.0,0.0,15763.0,"I have crores of records in multiple tables, and I wanted to create a json object based  on each entity's one view. This json has to be in nested form for married records. 

    [{""cust_id"":1,
      ""orders"":{""order_id"":1,
              ""order_details"":""details""},
      ""return"":{""return_id"":1,
              ""return_details"":""details""}
    },
    ...
    ]
    where customer, order and returns are from different tables.

I tried using pandas in python and it worked.

Problem with pandas is it process in memory

Just searching for other options.."
2739,2020-07-10 14:02:39,1594378959.0,dataengineering,Airflow Tutorial has me stumped,homyj0,SK4nda1,,https://www.reddit.com/r/dataengineering/comments/homyj0/airflow_tutorial_has_me_stumped/,1.0,1.0,0.0,15765.0,"Hey all, 

I have a problem. And I really hope someone with more experience in docker and airflow can help be. I just don't know what to google or what direction to search in anymore. I'm trying to get airflow to generate a dummyfile from within a docker.

After I installed docker I got the image, spinned it up and attached volumes to it.

    sudo docker pull puckel/docker-airflow
    
    sudo docker run -d -p 8080:8080 --name airflowContainer -v /home/someuser/dags:/usr/local/airflow/dags/ -v /home/someuser/output:/usr/local/airflow/output puckel/docker-airflow webserver

To eliminate permission troubles I set the permissions of /someuser/dags and /someuser/output to 777. 

I create a dag based upon a tutorial that goes: 

    from airflow import DAG
    from airflow.operators.python_operator import PythonOperator
    from airflow.utils.dates import days_ago
    
    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': days_ago(2),
        'retries': 1}
    
    dag = DAG(
        'generateTestFile',
        default_args=default_args,
        description='trial for reading dags and writing output')
    
    def generateData():
            with open( ""/usr/local/airflow/output/testfile.txt"" , ""a"" ) as f:
                    f.write( ""This is a file"" )
    
    t1 = PythonOperator(
            task_id =""generateFileOperation"",
            python_callable=generateData)
    
    t1

Everything seems to work, the GUI works fine on localhost:8080. The DAG shows up and airflow doesn't report any syntax errors. But when I trigger the dag manually.... nothing happens. No file is created. 

I tried to create files manually both from the host and from within the container. Both work fine and in both cases the file is visible from the other end. 

I tried `docker attach` but there too nothing weird shows up. 

Can anyone help me find what I'm missing?"
2740,2020-07-10 14:14:39,1594379679.0,dataengineering,Spark Vs Hadoop For Big Data Professionals: What’s Your Pick,hon3kg,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/hon3kg/spark_vs_hadoop_for_big_data_professionals_whats/,1.0,0.0,0.0,15766.0,
2741,2020-07-10 14:45:37,1594381537.0,dataengineering,Geese Meat Production Ranking | TOP 10 Country from 1961 to 2013,honhd2,datavtworld,,https://www.reddit.com/r/dataengineering/comments/honhd2/geese_meat_production_ranking_top_10_country_from/,1.0,0.0,0.0,15766.0,
2742,2020-07-10 18:12:04,1594393924.0,dataengineering,Azure Data Factory without DataBricks?,hoqnp0,Zyklon00,,https://www.reddit.com/r/dataengineering/comments/hoqnp0/azure_data_factory_without_databricks/,1.0,23.0,0.0,15770.0,"Hello everyone,

I'm a BI Analyst in a production firm. We produce and sell bricks. I have been hired to ""give more insights to the data that is available"". I have had some ETL experience and I know my way around various tools but I wouldn't call myself an expert in any of them... So the way to go is to use a tool to make it less technical for me. The tool that they were looking at when I started was TimeXtender. The experience was quite pleasant but the tool is rather expensive for us (30k/y). This combined with the fact that we would like to be future-ready for near-realtime data analytics  makes us consider Cloud solutions. We have been advised to check out Azure Data Factory and we got a ball park estimate of 10k/y. 

I have been doing some research and something that came up a lot is that Azure Data Factory is mainly used to get data from A to B and that Azure DataBricks is used for transformations. Which, I fear, will increase the price.

We need to do an ETL process 1 time at night. Loading a few 10 million records. I will need do some basic transformation and joins on them. Then we put the completed tables in Azure Cloud storage. From here I will use Power BI to make some dashboards and use R to do more advanced stuff.

Will Azure Data Factory on its own be enough for us?
Is the ballpark estimate they made for us realistic (including ADF and storage)?"
2743,2020-07-10 20:49:10,1594403350.0,dataengineering,Titles for very senior level data engineering roles,hotj58,BeThreeCoifs,,https://www.reddit.com/r/dataengineering/comments/hotj58/titles_for_very_senior_level_data_engineering/,1.0,22.0,0.0,15780.0,"I have been asked to come up with an appropriate title for a senior level data engineering role (i. e., more senior than a senior data engineer).  Think salary range $200k+, candidates with several years of experience and advanced degrees.  Principal Data Engineer has been floated, but I am not really sure if it is senior enough.  What kind of title would make sense to you, or do you think Principal Data Engineer is sufficient?"
2744,2020-07-10 21:50:36,1594407036.0,dataengineering,Please give your feedback on my CV.,houpr7,vishalw007,,https://www.reddit.com/r/dataengineering/comments/houpr7/please_give_your_feedback_on_my_cv/,1.0,12.0,0.0,15781.0,"Hello,

Hope you are doing well. I am from India and recently because of COVID-19 I got laid off from my job. I have 2 years of work experience as a Data Analyst. I am looking for a Data Analyst/Data Engineer/ BI Developer role. Please let me know how can I improve my CV.

Stay safe. Thanks :) 

&amp;#x200B;

[https://docs.google.com/document/d/12tHIlHju6J8YWSUFKdxThkccHDM9fVaHFm9g4eQsZuo/edit?usp=sharing](https://docs.google.com/document/d/12tHIlHju6J8YWSUFKdxThkccHDM9fVaHFm9g4eQsZuo/edit?usp=sharing)"
2745,2020-07-10 22:46:18,1594410378.0,dataengineering,Confused Support Guy,hovrs0,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hovrs0/confused_support_guy/,1.0,10.0,0.0,15782.0," Hello everyone,

Story of the Stupid(Support) IT Guy - India

I feel like I'm a stupid fellow. I'm really having a very hard time in my personal life and professional life. Today I have to let go of a few team members in my team because of ramping down due to Corona. My manager told me they are useless, will remove them from the team. The only reason I'm staying back in the team is I'm workaholic like anything ( do all the monotonous work ) which manager believes. The team member called me and said, few of them were under him for four years for the same support project. he threw me away just like he don't know me with a question what do you know Python, AWS, etc ... Because the previous project doesn't demand any of the skills he is asking now. He is also like same me ( stupid fellow ). he also does monotonous work but will not extend not more than 10 mins. That's the prime reason. I told my manager that we can retain him for another project, he said no we can't use him ( he is useless ). The point is 4-5 years he used him, this guy also didn't upgrade himself and thought he would become a manager and get a promotion. now he got another project and individual contributor on the project with another manager. He has to do coding, he has to design, he has to do Query, he has to use Talend ( none of them he did in the past four years). He almost felt the frustration and going through hell managing the clients and unable to meet the expectations.

my worry is down the line, I also will be thrown out because I'm not learning anything and not doing any developer work. Only thing I'm doing managing a team and guide them for any technical work(on small enhancements) They are really smart than me but just I will help them.

I'm being frustrated because everything I take I cannot complete it because it leads to another small thing, then It leads to another big thing and stuck. no motivation and nowhere I can use in the current SUPPORT PROJECT. Due to heavy competition, becoming a manager is also tough in the current organization. because I'm not a developer so becoming an architect is not an option. Now I'm staying in the 10th year of IT.

**How do I fresh start and become very good at least among 10000 people in the current organization?**

**Aim without Plan is a dead end.** **time is evil speed is god**. I want to improve my efficiency and at the same time my speed in doing that. Whatever I learn without real-world experience is making me feel worried. How to get real-world Exp just by learning online courses in Udemy, Coursera, edX etc...

My Exp is fully in production Support from the beginning throughout the 10 years. I want to become good at

1. Python/Go Lang -- Which one to choose and plan to become intermediate level
2. AWS -- Plan to become Intermediate level
3. SQL -- Plan to become Intermediate level
4. DevOps - Plan to become Intermediate level ( optional )
5. Or Anything which has good future but gaining traction slowly

I want to accomplish the first 3 to intermediate level. How can I achieve this . can someone mentor me on this. I would pay them once I attain the level in any one of the categories(1,2,3).

Is there any paid mentorship available online. I seriously wanted to do something and stand out.

Ours is a love marriage and came out of the house to get married. Due to the reason she is Christian and I'm Hindu and she is elder than me and she is a dark-toned and I'm fair. Our parents are against us and married her on confidence that I will definitely win one day. It's been 3 years of marriage we still can't earn to manage a family without EMIs and close our loan when we took during the struggling days. Now, I'm depressed due to the fact that I do not know anything in IT, and to survive for the next 10 years in IT and learn cutting edge technologies it is misleading to each and every dead end and I'm honestly and deeply stuck.

Yours lovingly,

Stupid IT MAN."
2746,2020-07-11 03:22:22,1594426942.0,dataengineering,Career Help,hp0goa,ymuribbi,,https://www.reddit.com/r/dataengineering/comments/hp0goa/career_help/,1.0,4.0,0.0,15790.0,"Hello everyone,

I have been working as a BI Engineer(Consultant) for the last four years in the United States. I am pretty good with T-SQL. I have designed several data marts and SSAS cubes, and created ETL operations by using SSIS. I have dabbled with SSRS and Tableau a little bit, created couple reports(not too fancy ones) Oh and I have a bachelors degree in computer science.

Recently I got my green card, and completed my masters and I have decided to look for a full time position, maybe a career change. I am currently working for a mid size home healthcare company, but I am not really happy. I am not learning anything new.

I am thinking to move towards data engineering but I am not exactly sure where to start. From my understanding solid Python skills is a must in this field. Is this correct? Are there any specific python libraries that I need to have a good grasp ?(like pandas) What else should I focus on? Should I chase certification programs? Would you recommend any books/training programs?

I might be a little bit lost, I am not sure. I would appreciate any help."
2747,2020-07-11 03:41:47,1594428107.0,dataengineering,test,hp0r6g,uicpt,,https://www.reddit.com/r/dataengineering/comments/hp0r6g/test/,1.0,0.0,0.0,15790.0,test
2748,2020-07-11 03:44:53,1594428293.0,dataengineering,Data scientists and engineers deserve a better notebook,hp0stp,uicpt,,https://www.reddit.com/r/dataengineering/comments/hp0stp/data_scientists_and_engineers_deserve_a_better/,1.0,42.0,0.0,15790.0,"As a data platform engineer, I have heard enough complaints from data scientists and data engineers about the tool they use daily. I know my fellow teammates feel painful, unproductive, frustrated. That is why I am starting an open source project, Bayesnote, for data scientists and data engineers:

Bayesnote = Notebook + workflow + machine learning platform + dashboard.

Or

Bayesnote ≈ Jupyter notebook + Airflow + Mlflow + Tableau 

My goal is to make every data scientist and data engineer productive and happy in their day job.

This is not a small project. It takes enormous engineering hours.

Here is the article that I introduce Bayesnote.  

[https://towardsdatascience.com/bayesnote-redefine-notebook-2ab00277bbc](https://towardsdatascience.com/bayesnote-redefine-notebook-2ab00277bbc)

Claps for the article appreciated. Star our repo to support us. Thanks!"
2749,2020-07-11 14:01:30,1594465290.0,dataengineering,Agriculture Productivity Ranking | TOP 10 Country from 1991 to 2017,hp86kd,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hp86kd/agriculture_productivity_ranking_top_10_country/,1.0,0.0,0.0,15807.0,
2750,2020-07-11 15:47:58,1594471678.0,dataengineering,Clarifications of Terminologies: Data Lake/Warehouse/Anything else?,hp9f8j,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/hp9f8j/clarifications_of_terminologies_data/,1.0,12.0,0.0,15812.0,"As a newbie, I have confusions regarding the meaning of the terms such as Data Lake or Warehouse or Anything else similar. I was wondering how many different terms related to Data warehousing are there and what they mean individually.

Thanks."
2751,2020-07-11 17:53:57,1594479237.0,dataengineering,"Data-related careers that don’t involve cleaning, organizing, and collecting data",hpb9uc,Dudeguybrochingo,,https://www.reddit.com/r/dataengineering/comments/hpb9uc/datarelated_careers_that_dont_involve_cleaning/,1.0,19.0,0.0,15815.0,"Hi, I’m a university student taking data analytics, but I really hate cleaning, organizing, and collecting data. What are some data-related jobs that don’t involve doing these? 

Thank you."
2752,2020-07-11 18:12:10,1594480330.0,dataengineering,The Application Server Could not be Contacted (pgAdmin 4),hpbksi,sdqafo,,https://www.reddit.com/r/dataengineering/comments/hpbksi/the_application_server_could_not_be_contacted/,1.0,3.0,0.0,15815.0,"I need help please. I have done all google search, stack Overflow etc but no solution yet. I already downloaded PostgreSQL and PgAdmin but whenever I open PgAdmin I get the error message “The Application Server Could not be Contacted”. I already uninstalled and reinstalled lower version but still same error. I even installed app instead of downloader still the same. I have tried to start PostgreSQL from the terminal, same error. I don’t know what to do anymore . Please I need help with this"
2753,2020-07-11 23:29:44,1594499384.0,dataengineering,Using Science to find Noah's Ark,hph40n,liamlucas993,,https://www.reddit.com/r/dataengineering/comments/hph40n/using_science_to_find_noahs_ark/,1.0,0.0,0.0,15825.0,
2754,2020-07-12 08:13:55,1594530835.0,dataengineering,Does anyone here run a 1 person show,hpoyru,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/hpoyru/does_anyone_here_run_a_1_person_show/,1.0,33.0,0.0,15841.0,Anyone just manage etl and data admin and all data used for the biz maybe some analytics but possibly there is a biz analyst or you give biz leaders a CSV dump. Your just the data person and you get a good salary mid 100k range and no one really knows what you do.
2755,2020-07-12 13:50:16,1594551016.0,dataengineering,Cancer Death Rate Ranking | TOP 10 Country from 1990 to 2017,hpsjax,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hpsjax/cancer_death_rate_ranking_top_10_country_from/,1.0,0.0,0.0,15850.0,
2756,2020-07-12 19:27:58,1594571278.0,dataengineering,Any Azure Data Eng certificate holders ?,hpx6og,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/hpx6og/any_azure_data_eng_certificate_holders/,1.0,7.0,0.0,15851.0,"Hi Azurians / anyone who passed the dp201/200,

What's your advice for me, who is about to prepare for the exams .

What were the resources that you used for the certification .

I have been working on adf , blob, synapse analytics, databricks and a lot of SQL and .net ."
2757,2020-07-12 20:17:26,1594574246.0,dataengineering,Most efficient data engineering tools in Python?,hpy1c5,dhruvmk,,https://www.reddit.com/r/dataengineering/comments/hpy1c5/most_efficient_data_engineering_tools_in_python/,1.0,24.0,0.0,15856.0,"Whenever I do any machine learning, I end up spending 90% of my time loading and shaping my data, and only 10% on actually building ML/DL models. 

I've noticed that my pipelines are extremely inefficient and not robust at all. For example, the other day when I was doing an image classification project, I used a for loop to cycle through each image in a specific folder on my computer and add it to an array. This process took 1 hour, and I ended up using a large amount of RAM. For that reason, I want to know how to efficiently load my data (ETL, maybe?) without eating up my RAM and time.

 For context, the only data manipulation libraries that I'm proficient in are NumPy and Pandas. Thanks in advance, and sorry if this is a stupid question."
2758,2020-07-12 22:22:24,1594581744.0,dataengineering,Any tips for transitioning to a DE role?,hq0921,bull_chief,,https://www.reddit.com/r/dataengineering/comments/hq0921/any_tips_for_transitioning_to_a_de_role/,1.0,13.0,0.0,15865.0,"I’m currently a new PM for a large internet services company. I don’t have a CS or related degree (I have a BS in BIS) however, interned while I was in college as a software engineer for a start up and the last half was basic DE work and I really enjoyed it. 

Does anyone have advice on transitioning to a role as a Data Engineer? Or anyone who has experienced transitioning from a business role to a DE role?  

For background, I’ve designed and deployed a sql database for a startup, performed basic ETL, and constructed an API pipeline in python/pymsql."
2759,2020-07-13 00:10:17,1594588217.0,dataengineering,Streaming with kafka,hq24ya,owila,,https://www.reddit.com/r/dataengineering/comments/hq24ya/streaming_with_kafka/,1.0,6.0,0.0,15868.0,Please can anyone point me to some good resources for learning streaming with apache kafka. I am trying to work on a project that will require streaming data from twitter.
2760,2020-07-13 01:58:27,1594594707.0,dataengineering,How To Run Airflow on Windows (with Docker),hq3y9c,jhizzle4rizzle,,https://www.reddit.com/r/dataengineering/comments/hq3y9c/how_to_run_airflow_on_windows_with_docker/,1.0,0.0,0.0,15872.0,
2761,2020-07-13 06:36:53,1594611413.0,dataengineering,DE Landscape Course recommendation for Managers,hq82ub,pknpkn21,,https://www.reddit.com/r/dataengineering/comments/hq82ub/de_landscape_course_recommendation_for_managers/,1.0,1.0,0.0,15874.0,"Hi, 

I am looking for courses or learning track for IT managers from software engineering or traditional data warehousing background to understand the current DE landscape covering different DE technologies/product/purpose etc. with limited focus on hands-on activities. 

Please share if any recommendations are there for such courses. Thanks in advance."
2762,2020-07-13 07:21:58,1594614118.0,dataengineering,Data Engineering Manager experience,hq8oz8,van_d39,,https://www.reddit.com/r/dataengineering/comments/hq8oz8/data_engineering_manager_experience/,1.0,3.0,0.0,15875.0,"I started at my current job as a Senior Data Engineer and soon found myself working with Associate Data Engineers and Data Engineers a lot more often such that they now report to me. Now, I'd like to be proficient and hands on too (more like 50% individual contributor and 50% leading a team), but am struggling to find time and balance these 2 aspects right. I don't have much experience in management per say, so, I am particularly interested to know some guidelines on how you (if you are a manager) or your manager (if you a Data Engineer) run things in your team

* Apart from code reviews and technical abilities/prowess, how do you evaluate your team? 
* Are there some frameworks that you leverage in order to get the individuals to succeed in the team and in their career in general? 
* How frequent are you 1:1s with the team members? What are those conversations usually around?
* For DEs, if there was something you wish your manager understood (apart from the code you write ;) ), what would that be?  


Any advise that you'd like to share with someone who is getting into engineering management would be really appreciated.   


Thank you"
2763,2020-07-13 08:35:59,1594618559.0,dataengineering,Between Designing Data-Intensive Applications and Kimball's Data Warehousing,hq9mwz,feistyteacup,,https://www.reddit.com/r/dataengineering/comments/hq9mwz/between_designing_dataintensive_applications_and/,1.0,17.0,0.0,15874.0,"If I'm just getting started with self-learning data engineering and want to get started on projects relatively quickly, which book should I read first (I will of course eventually read both)?

My beginner project idea just revolves around creating a web-scraping script for multiple different websites, transforming/joining/cleaning the data from these sources and then loading it into something like Postgres, and then maybe using Tableau or something to show some trends in the data that are related to the hypothetical question I'm seeking to answer in collecting this data. 

I'm aware that a project like this doesn't require me to read either of these books, but I'd like to get some theoretical background as I go and I'm just wondering which would be more beneficial to start with first. Both books are loaded with information and would take some time to digest, and I'm also working part-time / will be going back to university come September so this is something that I'd like more input on."
2764,2020-07-13 13:40:21,1594636821.0,dataengineering,Pork Production Ranking | TOP 10 Country from 1961 to 2013,hqd150,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hqd150/pork_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,15882.0,
2765,2020-07-13 14:08:19,1594638499.0,dataengineering,Facebook Onsite DE - Data Analytics,hqdcly,joeen10,,https://www.reddit.com/r/dataengineering/comments/hqdcly/facebook_onsite_de_data_analytics/,1.0,21.0,0.0,15882.0,"Hi everyone,

&amp;#x200B;

I will have an onsite interview with Facebook in a couple weeks (virtual). The structure of the onsite will be an ownership interview and 3 technical ones. The recruiter provided information about it and he mentioned that it would be adapted to similar tasks I would have in the job. From my research online, it would be something along these lines:

2 ETL rounds  
1 Data Modelling round  
1 Ownership round 

&amp;#x200B;

 I don't have any experience with real-time data or with Hadoop/Spark. It was mentioned the possibility of creating end-to-end pipelines after modelling them. Will this be a red flag? Is this something that could be asked to write end-to-end? Can anyone provide an actual example of an end-to-end pipeline (Python preferably)

&amp;#x200B;

In addition, can anyone provide a regular SQL end-to-end pipeline example in python ? I believe it's not required to use pandas, just plain python.

&amp;#x200B;

Any other advice would be more than useful, thanks"
2766,2020-07-13 14:29:58,1594639798.0,dataengineering,Download Free Guide to Become BIG DATA ENGINEER – 2020,hqdlzq,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/hqdlzq/download_free_guide_to_become_big_data_engineer/,1.0,0.0,0.0,15882.0,
2767,2020-07-13 17:24:15,1594650255.0,dataengineering,Survey: External Data Sources for the Training of Machine Learning Models,hqg5g2,FreeElChapo7,,https://www.reddit.com/r/dataengineering/comments/hqg5g2/survey_external_data_sources_for_the_training_of/,1.0,0.0,0.0,15886.0,"Hey guys, my name is Chris and im helping a friend out who has no Reddit account yet.  
Hes doing a survey on external data sources. Everything else in the text below!

""Hey,         You still have a little over a week left to help democratize   the     use    of AI through the participation in a 5-10 minute survey.

[https://www.surveyhero.com/s/external\_data\_sources\_ML](https://www.surveyhero.com/s/external_data_sources_ML)

The         survey has been posted here a couple weeks back and is directed     at     all  those who are currently using external data to   train Machine       Learning  models for the use in a company.

It's         aim is to provide disadvantaged enterprises who are poor in   data      with   the potential to start with Machine Learning  nevertheless. 

The survey is anonymous, the participation period ends on 22. July at 9am CET and its outcome will be shared afterward. 

For         those of you who have participated but were not able to fill it      out     completely for whatever reason, you can return to the  question      where    you left off by clicking the survey link.

Thank you very much for your support."""
2768,2020-07-13 17:37:33,1594651053.0,dataengineering,What's for Next 5 years,hqgdsc,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hqgdsc/whats_for_next_5_years/,1.0,3.0,0.0,15887.0,"Hi All,

I'm assuming no knowledge on the IT industry. I would like to learn the trending technologies which will definitely have good pay in the future + good scope in the future until 2025. 

1. Which programming lang (go/python) 
2. What DevOps learning path
3. AWS or Azure 
4. Conterinization is required or not? 
5. Is Streaming going to the future instead of ETL Batch"
2769,2020-07-13 17:38:06,1594651086.0,dataengineering,July 2020 - Databricks Documentation,hqge4w,RayisImayev,,https://www.reddit.com/r/dataengineering/comments/hqge4w/july_2020_databricks_documentation/,1.0,0.0,0.0,15887.0,"Perhaps I've missed that, but Databricks now allows to Undo deleted notebooks' commands! https://docs.databricks.com/release-notes/product/2020/july.html#restore-cut-notebook-cells"
2770,2020-07-13 21:08:29,1594663709.0,dataengineering,Data Governance tools &amp; frameworks,hqkf48,kelseyfecho,,https://www.reddit.com/r/dataengineering/comments/hqkf48/data_governance_tools_frameworks/,1.0,4.0,0.0,15894.0,What are some of your favorite data gov tools or frameworks?
2771,2020-07-13 21:43:03,1594665783.0,dataengineering,[Webinar] How 360 Degree Data Integration Enables the Customer-centric Business,hql3cr,iqbalahmedalvi,,https://www.reddit.com/r/dataengineering/comments/hql3cr/webinar_how_360_degree_data_integration_enables/,1.0,0.0,0.0,15897.0," 

Looking to build a customer-centric business strategy to create tailored marketing, efficient sales processes, and product offerings that serve your enterprise needs? Tune in to our free webinar to learn how you can create a 360-degree customer-view to improve your business processes.

[Save Your Spot Now](https://www.brighttalk.com/webcast/17771/424512#/utm_source=reddit&amp;utm_medium=Social&amp;utm_campaign=Organic)

&amp;#x200B;

https://preview.redd.it/hebie2rk6oa51.jpg?width=898&amp;format=pjpg&amp;auto=webp&amp;s=d9d6bd0e4024e92623512d28dede3d01fbda2b70"
2772,2020-07-14 00:45:17,1594676717.0,dataengineering,Example fuzzy data matching projects,hqojfz,matalab,,https://www.reddit.com/r/dataengineering/comments/hqojfz/example_fuzzy_data_matching_projects/,1.0,0.0,0.0,15905.0,"Some example fuzzy data matching projects (record linkage, data de-duplication), with QDeFuZZiner software
https://matasoft.hr/QTrendControl/index.php/qdefuzziner-fuzzy-data-matching-software/demo-fuzzy-match-projects

#data #DataScience #DataAnalytics #datamanagement #FuzzyMatch #RecordLinkage #DataDeduplication #deduplication #DataMatching #FuzzyDataMatching"
2773,2020-07-14 02:49:57,1594684197.0,dataengineering,Data modeling resources,hqqpar,leocharm,,https://www.reddit.com/r/dataengineering/comments/hqqpar/data_modeling_resources/,1.0,4.0,0.0,15911.0,"Hi There,

Can you suggest some good websites for brushing up on data modeling? I have an interview coming up next week and this is an area of improvement for me. Thank you. 

Ravi."
2774,2020-07-14 06:09:39,1594696179.0,dataengineering,How to build an internal search engine,hqtrb0,tezoswanchain,,https://www.reddit.com/r/dataengineering/comments/hqtrb0/how_to_build_an_internal_search_engine/,1.0,3.0,0.0,15916.0,"We do a lot of information extraction from text data, but it has become monotonous and I got an idea to build an internal tool which enables user to do Google search kind of on text mostly in csv files.i am sound really vague but I am looking for possible options"
2775,2020-07-14 06:15:18,1594696518.0,dataengineering,Data engineering project on Github,hqtu8r,ilya-g-,,https://www.reddit.com/r/dataengineering/comments/hqtu8r/data_engineering_project_on_github/,1.0,29.0,0.0,15916.0,"Howdy all - just posting this in hopes of possibly getting some feedback on a DE project I'm working on. 

Some background: I currently work as a data engineer but almost entirely in a Microsoft shop. The goal of this project was to get some experience with open source software including Airflow and the AWS ecosystem.

From my understanding, it's also just beneficial in general to have some personal projects up on GitHub when searching for a job so figured I'd kill two birds with one stone :)

The project and explanation of it is here:  [https://github.com/ilya-galperin/SF-EvictionTracker](https://github.com/ilya-galperin/SF-EvictionTracker) 

Any and all feedback is more than welcome - this is my first time doing something like this so I'm sure it's not entirely perfect.

Thanks!"
2776,2020-07-14 10:10:41,1594710641.0,dataengineering,What is Immuta and how does it improve data governance in Databricks?,hqwwc3,valdasm,,https://www.reddit.com/r/dataengineering/comments/hqwwc3/what_is_immuta_and_how_does_it_improve_data/,1.0,0.0,0.0,15919.0,
2777,2020-07-14 13:30:50,1594722650.0,dataengineering,Political Participation Ranking | TOP 10 Country from 1950 to 2000,hqz49n,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hqz49n/political_participation_ranking_top_10_country/,1.0,0.0,0.0,15938.0,
2778,2020-07-14 15:53:47,1594731227.0,dataengineering,"Supercharging dbt with Splitgraph: versioning, sharing, cross-DB joins",hr0ym6,mildbyte,,https://www.reddit.com/r/dataengineering/comments/hr0ym6/supercharging_dbt_with_splitgraph_versioning/,3.0,1.0,0.0,15941.0,
2779,2020-07-14 19:11:59,1594743119.0,dataengineering,Streaming Data Pipeline Strategy for Largish Small JSON files (1-3MBs),hr4d7t,Strade,,https://www.reddit.com/r/dataengineering/comments/hr4d7t/streaming_data_pipeline_strategy_for_largish/,2.0,13.0,0.0,15943.0,"**TLDR**: I'm pulling back \~100k records/day that can be 1-3MB (JSON) each and the majority of the events will be updates.  I need to trigger operational flows to sync data to other systems as well as transformations for data warehousing/operational data store.  The DW flow could be micro batches, 5-15 minutes apart.  How would you approach this, staging and pipeline wise? (Tools available: everything AWS, Possibly Snowflake, open to others)  Thanks.

I've done a lot of research about streaming pipelines but much of what I find is to solve higher velocity/smaller payloads with mostly inserts.  This problem differs in that it's lower velocity with a larger payload and mostly updates so I'm wondering how you would approach this.

The source systems provides a webhook subscription to notify when a record has been created or updated.  It doesn't provide the changes. In order to get the changes a separate API call needs to be made to get the record, which is returned in JSON form. This JSON can be between 1-3 MBs in size and daily volume is a little over 100k/day.  The majority of the transactions will be updates.

Currently, I have a subscription to the webhook that routes to Amazon SQS.  This triggers a Lambda that makes the API call to get the record JSON and, at the moment, saves the record to SQL Server. I used SQL Server as the current stage as that's the DB we have running and what I'm most familiar with and this process was a POC.  So on to the next stage of the POC...

I am open to any suggestions to the above strategy but what I'm really curious to hear are thoughts about next stage of the flow and storage.  Once the data in staged, it needs to be split into different flows.

1. Operational:  This may be multiple flows, taking each record and pushing a set of fields to update another system.  
2. Analytics: Lets say taking the last 5-15 minutes of new/updated records and running transformations on them to update an operational data store and/or data warehouse. 

I've thought through a few approaches and do have a set of tools available to me.  We have access to Snowflake but it's not being used in production yet.

1. Let's say I stick with SQL Server.  Leveraging Amazon SNS: 
   1. The operational flow can fan out as necessary and use Lambda to load other systems.
   2. The analytical flow could be SPs in SQL Server (the majority of our reporting is done from here now so this does serve as an ELT approach).
   3. Thoughts: The big challenge here with some of my testing is querying and parsing the JSON on the server is fairly costly since the JSON is so large.  Is it worth loading that data into another ETL tool, processing there, and loading back to DB?
2. Could staging to S3 work?
   1. I've read some concerns with smaller files on S3 and costly read/write.  Has anyone else come across this?  Same approach could be done as above with S3 triggering the next steps.  
3. Utilizing Snowflake?
   1. Correct me if I'm wrong but Snowflake would not be good to go directly to if we need to support the operational flows?
4. Should the original Lambda that receives the JSON payload just push to multiple locations (S3 for operational trigger, and Snowflake for DW/transformations)?
   1. The total population of loans is \~900k so at 3MBs a record that's close to 3TB of data.  Is it worth duplicating the JSON at this stage?

My biggest mental block is the size of the JSON, as parsing such a large string seems to add quite a bit of overhead but it has to be done.  Also, we don't have any ETL orchestration type tools yet.  Any recommendations?

Thanks for reading and your feedback."
2780,2020-07-14 21:17:10,1594750630.0,dataengineering,Open Source Production Grade Data Integration With Meltano - Episode 141 - Data Engineering Podcast,hr6r0n,MeltanoDouwe,,https://www.reddit.com/r/dataengineering/comments/hr6r0n/open_source_production_grade_data_integration/,1.0,2.0,0.0,15950.0,
2781,2020-07-15 01:31:23,1594765883.0,dataengineering,Spark Vs Snowflake: The Cloud Data Engineering debate!,hrbk9f,nonab8,,https://www.reddit.com/r/dataengineering/comments/hrbk9f/spark_vs_snowflake_the_cloud_data_engineering/,0.0,12.0,0.0,15955.0,"For Enterprises, navigating the cloud transition can be tricky. With both **Spark** and **Snowflake** available for Cloud DataEngineering, how do you choose?

We've penned down our thoughts [here](https://www.prophecy.io/blogs/spark-vs-snowflake-the-cloud-data-engineering-etl-debate). I'm keen to know what you think!

&amp;#x200B;"
2782,2020-07-15 01:33:47,1594766027.0,dataengineering,Tesla Reliability Data Engineer,hrblq8,c0wabungaItIs,,https://www.reddit.com/r/dataengineering/comments/hrblq8/tesla_reliability_data_engineer/,0.0,4.0,0.0,15955.0,"Anyone recently interviewed for Tesla Reliability Data Engineer, or completed a take home assignment for the same? Looking for preparation tips, anything helps! Thanks, kind redditors!"
2783,2020-07-15 05:24:43,1594779883.0,dataengineering,How has your work-life been since Covid?,hrf9ez,mac-0,,https://www.reddit.com/r/dataengineering/comments/hrf9ez/how_has_your_worklife_been_since_covid/,21.0,46.0,0.0,15963.0,"I feel like since Covid hit, my days have been so long and unproductive. Does anyone else have a similar (or opposite) experience?

Meetings go on longer (because there's nobody to kick you out of the meeting room) and are more numerous (because bosses want more planning meetings, more knowledge transfer meetings). I probably meet for like 4-5 hours a day, every day. And the worst part is that the meetings are staggered, e.g. a meeting from 10:00 - 10:30 and another from 11:00 - 11:30. It's impossible to get in the zone in just 30 minutes so that 10:30 - 11:00 slot is often wasted."
2784,2020-07-15 08:48:03,1594792083.0,dataengineering,Need ideas on how to extract data from a image,hri1r9,Ntoms,,https://www.reddit.com/r/dataengineering/comments/hri1r9/need_ideas_on_how_to_extract_data_from_a_image/,1.0,7.0,0.0,15964.0," Hello, my provider shows their offers on their website in this way:  

&amp;#x200B;

https://preview.redd.it/3bd5jtk1mya51.jpg?width=1000&amp;format=pjpg&amp;auto=webp&amp;s=7d30832eaf9187309c4584a2c545731aa9febe4b

 [https://www.yaguar.com.ar/cordoba/](https://www.yaguar.com.ar/cordoba/) 

 Basically it is like a magazine with images of each product and next to it a description (product name, price and product code).  

   
What I am looking for is to be able to extract these descriptions to put them in tables and thus compare them with those of the other providers, is there a way to do it automatically?"
2785,2020-07-15 10:19:38,1594797578.0,dataengineering,Big Data without Hadoop/HDFS? MinIO tested on Jupter + PySpark,hrj44q,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/hrj44q/big_data_without_hadoophdfs_minio_tested_on/,1.0,0.0,0.0,15965.0,
2786,2020-07-15 13:46:38,1594809998.0,dataengineering,Cow's Milk Production Ranking | TOP 10 Country from 1961 to 2018,hrld8q,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hrld8q/cows_milk_production_ranking_top_10_country_from/,1.0,0.0,0.0,15971.0,
2787,2020-07-15 15:55:36,1594817736.0,dataengineering,Top 10 Biggest Companies Market Capitalization (1996 – 2020),hrn1bu,mostafaelmahdy,,https://www.reddit.com/r/dataengineering/comments/hrn1bu/top_10_biggest_companies_market_capitalization/,1.0,0.0,0.0,15970.0,
2788,2020-07-15 16:07:10,1594818430.0,dataengineering,Webinar on How to Learn Data Science: The Path to Becoming a Data Scientist,hrn7oe,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/hrn7oe/webinar_on_how_to_learn_data_science_the_path_to/,1.0,0.0,0.0,15970.0,
2789,2020-07-15 16:50:32,1594821032.0,dataengineering,The Flink &lt;-&gt; Kafka Connector for reading and writing data from/to Kafka topics,hrnwid,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hrnwid/the_flink_kafka_connector_for_reading_and_writing/,1.0,0.0,0.0,15970.0,
2790,2020-07-15 17:07:11,1594822031.0,dataengineering,Three point plan for rounding out resume - Thoughts?,hro6vf,ThinkChest9,,https://www.reddit.com/r/dataengineering/comments/hro6vf/three_point_plan_for_rounding_out_resume_thoughts/,3.0,8.0,0.0,15970.0,"I'm currently employed as a data engineer, but I want to invest some of my free time rounding out my resume to make sure that, 2-4 years down the line when the time comes to move on, I can be a stand-out candidate. In addition, I want to mitigate the fact that \~50% of the tools I use at work were developed in-house.

I was thinking the following three side projects would probably give me the most bang for my metaphorical buck:

* Earn a single cloud vendor certification (i.e. AWS Data Analytics or Google Cloud Data Eng.).
* Build and maintain an AWS-based, end-to-end sample project, ideally with some interesting final output in a dash.
* Contribute to at least two key data eng. open source projects (Spark? Airflow? Some streaming tool?).

Does anyone have any thoughts or experience with this kind of approach? Comments on certifications to take or avoid? I saw a great sample end-to-end project here a day ago so I'm good on that one. Anyone have experience with OSS contributions? Even if it's just bug fixing, I think it'll really stand out."
2791,2020-07-15 18:31:14,1594827074.0,dataengineering,Don't Send Money to Dead People (And Other Data Lessons),hrpopz,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hrpopz/dont_send_money_to_dead_people_and_other_data/,1.0,0.0,0.0,15970.0,"""*In June 2020, it was reported that* [*bad data*](https://fcw.com/articles/2020/06/25/johnson-gao-cares-waste-report.aspx) *hampered the U.S. government’s ability to roll out its COVID-19 economic recovery programs. In addition to other grievous errors, this* [***data downtime***](https://towardsdatascience.com/the-rise-of-data-downtime-841650cedfd5) *incident sent over* ***$1.4 billion*** *in COVID-19 stimulus checks to dead people.""*

Interested to hear how others ensure data integrity and reliability, and you know, avoid sending money to dead people.

[https://towardsdatascience.com/dont-send-money-to-dead-people-7653bed80892?source=friends\_link&amp;sk=68581aea28e947ab718a2d182dfd4bdb](https://towardsdatascience.com/dont-send-money-to-dead-people-7653bed80892?source=friends_link&amp;sk=68581aea28e947ab718a2d182dfd4bdb)"
2792,2020-07-15 18:48:22,1594828102.0,dataengineering,Insight Fellows Program - Now?,hrpzww,clueless3867,,https://www.reddit.com/r/dataengineering/comments/hrpzww/insight_fellows_program_now/,1.0,4.0,0.0,15970.0,"Hey guys!

I was thinking of applying to the Insight Fellows program for DE. It seems like a really good opportunity for the point of my career I'm at, but have some concerns around leaving a well paying job with benefits in the middle of a COVID-driven recession. I've currently been applying for roles and have only heard back from two...One of which decided to not fill due to COVID.

Has anyone on here done the fellowship? Would it be worth doing considering this climate?"
2793,2020-07-15 21:05:05,1594836305.0,dataengineering,Python for Data engineer,hrslp7,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hrslp7/python_for_data_engineer/,1.0,5.0,0.0,15971.0,"Hello everyone,

I don't have any python experience and very much a beginner. what course should I learn to focus more on the data engineer role(Intermediate Python + advanced SQL)? 

like analyzing the data of different formats, loading the data to the database , Analytics of your data, Manipulating the data, Crunching the data, and get the desired results, Building an ETL pipeline. How can I start and where to start. I know this is a very basic question. Can someone please advise me on this. 

I have a few options which I have googled but not sure it is right: data camp, Dataquest, plural sight 

I already have a job, but I need to learn this realtime and should be ready to implement this in the project."
2794,2020-07-15 21:23:32,1594837412.0,dataengineering,Anyone else get way more frustrated/worried when a complicated derived value is only slightly off rather than way off?,hrsyhl,drumkeys,,https://www.reddit.com/r/dataengineering/comments/hrsyhl/anyone_else_get_way_more_frustratedworried_when_a/,1.0,3.0,0.0,15971.0,"When a value is way off base, it’s usually a simple logic error in my experience. Whenever a value is just a little bit off, it seems to turn out to be a way more complicated issue. Is this just Baader-Meinhof effect at work or do others experience the same?"
2795,2020-07-15 21:25:18,1594837518.0,dataengineering,Tool for Creating SQL Pipelines – Structure.rest,hrszoo,punknight,,https://www.reddit.com/r/dataengineering/comments/hrszoo/tool_for_creating_sql_pipelines_structurerest/,2.0,10.0,0.0,15971.0,"Hi All! We've spent a few months on getting an MVP together, and would love to get some feedback on whether this tool meets you needs.

* Here is a link to a demo video: https://www.youtube.com/watch?v=FBLi3vdKB-4&amp;feature=emb_rel_pause
* Here's a link to our website: https://www.structure.rest
* And here's a blog article, I published today in the space: https://www.structure.rest/blog/using-a-data-analytics-stack-to-gain-business-insights"
2796,2020-07-16 00:10:08,1594847408.0,dataengineering,Airflow Setup Help - K8s operator,hrw6lb,DeusExVeritas,,https://www.reddit.com/r/dataengineering/comments/hrw6lb/airflow_setup_help_k8s_operator/,1.0,4.0,0.0,15975.0,"Working on setting up our first instance of Airflow and running into an issue when trying to run a combination of the CeleryExecutor/KubernetesPodOperators. I can get everything up and running, but seeing this issue when actually trying to run a dag, for any of the tasks using the K8s operator:  

    Traceback (most recent call last):
      File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 983, in _run_raw_task
        result = task_copy.execute(context=context)
      File ""/usr/local/lib/python3.7/site-packages/airflow/contrib/operators/kubernetes_pod_operator.py"", line 200, in execute
        config_file=self.config_file)
      File ""/usr/local/lib/python3.7/site-packages/airflow/contrib/kubernetes/kube_client.py"", line 103, in get_kube_client
        client_conf = _get_kube_config(in_cluster, cluster_context, config_file)
      File ""/usr/local/lib/python3.7/site-packages/airflow/contrib/kubernetes/kube_client.py"", line 41, in _get_kube_config
        config.load_incluster_config()
      File ""/usr/local/lib/python3.7/site-packages/kubernetes/config/incluster_config.py"", line 94, in load_incluster_config
        cert_filename=SERVICE_CERT_FILENAME).load_and_set()
      File ""/usr/local/lib/python3.7/site-packages/kubernetes/config/incluster_config.py"", line 45, in load_and_set
        self._load_config()
      File ""/usr/local/lib/python3.7/site-packages/kubernetes/config/incluster_config.py"", line 51, in _load_config
        raise ConfigException(""Service host/port is not set."")
    kubernetes.config.config_exception.ConfigException: Service host/port is not set.

This is using pretty much stock setup from [https://github.com/puckel/docker-airflow](https://github.com/puckel/docker-airflow) and running it locally. I'm sure this error is pretty simple to debug, but I can't find the way to resolve it unfortunately. 

Anyone run into something similar before that could help?"
2797,2020-07-16 01:02:33,1594850553.0,dataengineering,268 Billion Events with Snowplow Analytics and Snowflake at CarGurus,hrx59y,jakebuilds,,https://www.reddit.com/r/dataengineering/comments/hrx59y/268_billion_events_with_snowplow_analytics_and/,1.0,0.0,0.0,15978.0,
2798,2020-07-16 02:47:50,1594856870.0,dataengineering,are any current DE's implementing data governance best practices for their company,hryz5s,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/hryz5s/are_any_current_des_implementing_data_governance/,1.0,2.0,0.0,15984.0,just want to understand if there were any resources that you had that helped with implementing these. I'm currently leading the charge for my new company with a pretty young databricks infrastructure.
2799,2020-07-16 03:07:40,1594858060.0,dataengineering,Transformations in Azure,hrzatq,HansSlinger,,https://www.reddit.com/r/dataengineering/comments/hrzatq/transformations_in_azure/,1.0,12.0,0.0,15984.0,"We are getting started on our Azure journey and I'm interested to know what you're using for your transformations (as part of ELT).  We have structured and unstructured data with daily deltas of &lt;5gb and total size of \~5tb.  We are using ADF for orchestration - are you using tools like SSIS through adf, Databricks (or Dattaflow) through ADF, Informatca or Loome or others?  Many thanks"
2800,2020-07-16 05:20:21,1594866021.0,dataengineering,The Analytics Setup Guidebook: How to Build Scalable Data Analytics and BI Stacks in Modern Cloud Era,hs1cer,anhthong00,,https://www.reddit.com/r/dataengineering/comments/hs1cer/the_analytics_setup_guidebook_how_to_build/,1.0,6.0,0.0,15990.0,
2801,2020-07-16 11:57:04,1594889824.0,dataengineering,Application Deployment in Apache Flink: Current State and the new Application Mode,hs6808,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hs6808/application_deployment_in_apache_flink_current/,1.0,0.0,0.0,16009.0,
2802,2020-07-16 13:40:09,1594896009.0,dataengineering,Military Size Ranking | TOP 10 Country from 1816 to 2020,hs7cbe,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hs7cbe/military_size_ranking_top_10_country_from_1816_to/,1.0,0.0,0.0,16011.0,
2803,2020-07-16 16:16:20,1594905380.0,dataengineering,Anyone up for a small ETL process review ?,hs9g9r,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/hs9g9r/anyone_up_for_a_small_etl_process_review/,1.0,9.0,0.0,16018.0,"Oh cool.
Keep reading.


I have a scenario where a SQL proc in azure  DW is taking 4 hours to execute(it's a big union all proc basicay). To reduce the time , I'm planning to move the tables into databricks and create a hive table in it, do the processing in databricks and dump the results back to the DW destination table as many reports depend on it.Does this approach sound good ? 

SQL optimization is very hard I feel as there are more than 15 mil rows , 25 columns at this point .

Any cost efficient or better approaches to take ?"
2804,2020-07-16 17:38:16,1594910296.0,dataengineering,How Data Is Stored: Down the Rabbit Hole,hsargi,schoolgurllou,,https://www.reddit.com/r/dataengineering/comments/hsargi/how_data_is_stored_down_the_rabbit_hole/,1.0,2.0,0.0,16018.0,
2805,2020-07-16 19:29:11,1594916951.0,dataengineering,Best practices for Data Governance,hscpzw,advaitha_ram,,https://www.reddit.com/r/dataengineering/comments/hscpzw/best_practices_for_data_governance/,1.0,1.0,0.0,16027.0,"I am doing a study of data governance best practices. How to ensure security of the data, access controls, quality of the data, lineage, maintaining meta store, good documentation for the datasets, audit logs, improving the trust for the data etc. Please help with practices which you found to be beneficial. Please share any good resources you came across, open source tools which will help."
2806,2020-07-16 20:23:49,1594920229.0,dataengineering,A short and simple example of running apache airflow within a docker environment,hsdwt3,OliverRobie,,https://www.reddit.com/r/dataengineering/comments/hsdwt3/a_short_and_simple_example_of_running_apache/,1.0,0.0,0.0,16028.0,
2807,2020-07-16 20:27:16,1594920436.0,dataengineering,HOW TO CHOOSE THE RIGHT DSP (DEMAND SIDE PLATFORM) - martechlive,hsdzf1,MartechLive,,https://www.reddit.com/r/dataengineering/comments/hsdzf1/how_to_choose_the_right_dsp_demand_side_platform/,1.0,0.0,0.0,16027.0,
2808,2020-07-16 21:18:39,1594923519.0,dataengineering,Python ETL pipeline and testing,hsf17z,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/hsf17z/python_etl_pipeline_and_testing/,1.0,3.0,0.0,16029.0,"Hi, I’m currently looking for resources on best practices on creating a Python ETL pipeline and doing some unit and integration tests. Is there any video/github repo I could check to learn? Thanks"
2809,2020-07-17 00:31:50,1594935110.0,dataengineering,Can I join your project / have any ideas?,hsivey,to_future_me,,https://www.reddit.com/r/dataengineering/comments/hsivey/can_i_join_your_project_have_any_ideas/,1.0,0.0,0.0,16038.0,"So I have a decent amount of Python &amp; R experience. I'm solid at SQL. I'm very comfortable working with data. I'm pretty much a data analyst at work and am en route to take on a more technical role within the next year. 

I lack experience with tools like Spark, Airflow, Docker, etc. Your big data / coud tools I guess. Even though I'm headed towards a DE / DS role soonish, my projects don't need / require me to understand these tools or use them currently. I want to get familiar with these products before I change roles. I don't have a ton of experience working / collaborating on code. I understand git basics but haven't ever really worked on code as a team.

I'm curious if anyone has projects that I can join or had any ideas for projects that I can take on outside of work to help me develop these missing skills."
2810,2020-07-17 00:32:57,1594935177.0,dataengineering,Can I do this on Zeppelin? Mix Interpreters,hsiw56,chesus05,,https://www.reddit.com/r/dataengineering/comments/hsiw56/can_i_do_this_on_zeppelin_mix_interpreters/,1.0,0.0,0.0,16038.0," Hi,

I have a Zeppelin Notebook edge node, running and administer by IT.

They have been configuring the interpreter for different databases.

How can my team work and mix this data.

Ex.

Select data from %mysql create a pandas Data frame in %python and write the results in %jdbc.

What's the best approach to do this kind of work? We cant have the user and password of the databases.

Thanks"
2811,2020-07-17 04:52:07,1594950727.0,dataengineering,Free Learning Spark 2nd Edition ebook,hsn1z2,aleebit,,https://www.reddit.com/r/dataengineering/comments/hsn1z2/free_learning_spark_2nd_edition_ebook/,1.0,10.0,0.0,16040.0,
2812,2020-07-17 07:10:52,1594959052.0,dataengineering,ETL process question - insert to temp table first or write directly to table?,hsp1hr,12mZBmBJMy6VnzkJhjXn,,https://www.reddit.com/r/dataengineering/comments/hsp1hr/etl_process_question_insert_to_temp_table_first/,1.0,8.0,0.0,16045.0,"As the title suggests, I want to know which method is better in the context of an hourly etl process.

This is a part of a procedure which reads data from a bunch of application sql tables and then joins them in a data warehouse table for use in reporting and analytics systems. Please note: the table it writes to is not a reporting table but there are procedures which build reporting tables based on data from this table. So there is not a high risk of this table being locked or something due to a database job.

The procedure has two parts: inserts and updates. The current implementation simply checks for new data based on an update time field being greater than the last update time in the dwh, does some logic which determines whether the data is an insert or an update, unions these two types together into a temp table with a flag on each part of the union to determine which part they are in. 


For example: 


    create temporary table [name] as
    select id, ... , ""I""  from ... where updatetime &gt; @time and createtime &gt; @time
    union
    select id, ... , ""U"" from ... where updatetime &gt; @time and createtime &lt; @time;


It then takes this and creates another temp table for the ""I"" data to insert to the data warehouse table. 

After that it creates another temp table for the ""U"" data and performs an update on the data warehouse table.

Finally, it drops the temp tables. 

I am going through the process of simplifying this to increase speed and reduce unecessary computing power expenditure. My thought is that it has a couple of unecessary steps. Couldn't I just write the first part of the union query to the table? There is no additional filtering on it so I don't see why i need to create multiple temp tables. For the second part of the union, the update, I could probably still use a temp table but this would not only reduce size of the temp table since it doesn't have the insert data, it would also reduce processing resources as I wouldn't have to filter it based on the ""U"" field during the UPDATE.

Are my solutions valid from a theoretical point of view? I know I will have to actually make the change/test it out to see for sure."
2813,2020-07-17 07:28:34,1594960114.0,dataengineering,"Need advice for bigdata self learning Hive, Hadoop and Spark . I'm somewhat lost on doing self study on Bigdata",hsp9wr,HalfChineseHalfTito,,https://www.reddit.com/r/dataengineering/comments/hsp9wr/need_advice_for_bigdata_self_learning_hive_hadoop/,1.0,12.0,0.0,16045.0,"So I'm stuck at home due to coronavirus for 4-5 months now. I joined this company last month as I left my previous company due to covid19, no work no pay so had no choice. The company is pretty big and they ask nothing of me but to learn Hive/Spark/Hbase with Java. But I'm running out of ideas now since I've already finished Spark in Action, some Spark udemy course and Hbase vids on youtube. The thing is, I can't find a good tutorial or guide, like the sakila guides for MySQL with a working java product to work with. The thing is I want to practice this skill on my own, but I don't know where to start. What kind of self made project can I do to learn this thoroughly? Not just running some random queries on VRbox. I want to actually apply the skill. Hopefully someone can help me out.

I really feel guilty as I thinkk I'm not learning much and my company is paying me pretty good by just reading stuff. I really want to actually absorb the knowledge.

Sorry for my confusing English. Thank you."
2814,2020-07-17 07:37:35,1594960655.0,dataengineering,Alteryx - Extract patterns from a column of a dataset,hspe6m,macxima,,https://www.reddit.com/r/dataengineering/comments/hspe6m/alteryx_extract_patterns_from_a_column_of_a/,1.0,0.0,0.0,16046.0,
2815,2020-07-17 09:03:24,1594965804.0,dataengineering,Migrating Hive metastore to AWS Glue,hsqggh,DevZeusGru1602,,https://www.reddit.com/r/dataengineering/comments/hsqggh/migrating_hive_metastore_to_aws_glue/,1.0,2.0,0.0,16050.0,"Hi,

I am running a single node Hadoop-Hive cluster for test purposes and now want to migrate to Hive metastore to AWS Glue for persistent storage. I couldn’t find a way to do it. Can anybody help?"
2816,2020-07-17 11:24:17,1594974257.0,dataengineering,Data Governance Q: How do share know-how?,hss0lr,OneOverNever,,https://www.reddit.com/r/dataengineering/comments/hss0lr/data_governance_q_how_do_share_knowhow/,1.0,1.0,0.0,16051.0,"I'm currently benchmarking the knowledge base for a Data Governance implementation.

Analyst from different areas use different techs, servers, databases, etc. Because they speak such different languages, my idea is to attempt to consolidate alll knowledge in three repositories:

* Requirements (workflow tool such as clickup, asana, jira, etc.)
* Code (github or some other code repo)
* Wiki (confluence?)

In the code part, the idea is that for any requirement (whether it's a project or adhoc), instead of saving the code in the user's PC, that it is saved on git with a certain standardized name structure that will make it ""easier"" to identify.  Git would be structured with three levels:

1) Functional Area

2) Type of Task

3) Tech

One of the data engineers told me that this attempt to centralize code from different areas, projects, etc. could generate too much responsibility for a single repo and ultimately generate inefficiencies.

Can anyone with experience on anything similar help me out a bit by criticizing the code model?  Should I generate as many repos as areas or projects and consolidate the knowledge base on the wiki instead?

Any suggestions are greatly appreciated as well :)"
2817,2020-07-17 12:35:20,1594978520.0,dataengineering,Best Best Data Engineering Agency,hssr6y,Poojakhana52314,,https://www.reddit.com/r/dataengineering/comments/hssr6y/best_best_data_engineering_agency/,1.0,0.0,0.0,16051.0,
2818,2020-07-17 13:56:53,1594983413.0,dataengineering,Solar Energy Ranking | TOP 10 Country from 1990 to 2018,hstnh5,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hstnh5/solar_energy_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,16054.0,
2819,2020-07-17 14:55:30,1594986930.0,dataengineering,Deep learning edge deployment,hsud3r,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/hsud3r/deep_learning_edge_deployment/,1.0,0.0,0.0,16059.0,"Hi,

I would like to know if someone has worked on a full stack open source iot platform which is cloud agnostic and can provide me the following

- device management
- edge management
- telemetry
- edge modules management

I looked at azure iot hub and edge and that is exactly something I am looking at in open source.
https://github.com/Azure-Samples/Custom-vision-service-iot-edge-raspberry-pi"
2820,2020-07-17 15:11:19,1594987879.0,dataengineering,Engineering analytics dashboard out of plain CRUD app,hsukqe,serkanozer,,https://www.reddit.com/r/dataengineering/comments/hsukqe/engineering_analytics_dashboard_out_of_plain_crud/,1.0,5.0,0.0,16060.0,"I lost my way in a project where we want to create analytic dashboards from a plain CRUD database that only stores **current state** of entities  in a normalized way**.**  I have been searching and found  pieces that I am not able to connect properly to craft a solution. Some of terms I was encountered was ETL, CDC, stream processing. I will provide one of our scenarios to make the question more concrete.

We have a slack like application where users post questions and answers to different channels, and thus channel component is important for the dashboard we create, we want to provide each metrics by channel. One dashboard we will have is the number of users per channel **through time which may be monthly yearly or daily through months quarters or years.** The problem is  we only keep a UserChannel table to represent channels of a user, roughly

*UserChannel*

* user\_id
* channel\_id

This ofc puts a limitation to provide historical analytics, as user leaving a channel simply means deletion of a UserChannel record, and we have no information to calculate what was the number of users in a channel, say 30 days ago

&amp;#x200B;

My question is in general how should I attack this problem but specifically:

1. Should we change the application database itself, maybe use soft deletes or use separate tables like JoinChannel LeaveChannel ?
2. Should we separate analytics process, maybe create a separate audit log, or use CDC via debezium or any other tool ? I guess we would create two events for joins and leaves then how  would we compute users per channel, lets say per day, would we create pipelines to store metrics per day, or would we always compute metrics out of all events after each query to dashboard page

I am open to any basic recommendation/term to look up or and more open to a ""step by step guide"" :)"
2821,2020-07-17 15:44:00,1594989840.0,dataengineering,Thoughts on DE management,hsv0rc,fithrowaway379,,https://www.reddit.com/r/dataengineering/comments/hsv0rc/thoughts_on_de_management/,1.0,6.0,0.0,16062.0,"Hey all, this is a throwaway acct, but want to ask something. I've been a DE for 4 years at a great company. Started there after my masters and have gotten a few promotions and I think i've done pretty well.

My manager told me there is an opportunity to act as a lead for a team of DEs where i would essentially be the mentor for the whole and manager of the junior members of the team (including 1-1's reviews, etc..). He called it something like a manger training program. This is without actually getting promoted to a director, just still being a lead DE. I love the idea of this because a lot of what i do is help out other teams with DE questions, so it seems like a good role. But I am a little worried about getting more into management and more away from the tech since i'm still early in my career. I feel that is not as marketable as still being a developer.

Has anyone else gone through this and have any advice?"
2822,2020-07-17 17:40:36,1594996836.0,dataengineering,Webinar on How to Learn Data Science: The Path to Becoming a Data Scientist,hswu22,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/hswu22/webinar_on_how_to_learn_data_science_the_path_to/,1.0,0.0,0.0,16065.0,
2823,2020-07-17 18:21:36,1594999296.0,dataengineering,Is Spark required to read .parquet files?,hsxjw2,freebird348,,https://www.reddit.com/r/dataengineering/comments/hsxjw2/is_spark_required_to_read_parquet_files/,1.0,6.0,0.0,16067.0,"First of all, you all are awesome -- I love this community.

I am new to data engineering and working on my final project for the Udacity course, and I have a some questions regarding .*parquet* files.

**My goal is such: To load several .parquet files into a dataframe (with the end goal of uploading that data to a redshift database).**

In the current workspace I'm using, there is a folder called **sas\_data**. Within **sas\_data**, there is one SUCCESS files and several .*parquet* files. My initial attempt was to just plainly use a function in pandas to load the .*parquet* data into a dataframe. I tried the following code:

`df = pd.read_parquet('sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet')`

I ran this code and it was taking forever, so I assumed Spark was necessary to read the .parquet file (but please correct me if I'm wrong).

The course instantiated the Spark session with the following code:

`from pyspark.sql import SparkSessionspark = SparkSession.builder.\config(""spark.jars.packages"",""saurfang:spark-sas7bdat:2.0.0-s_2.11"")\.enableHiveSupport().getOrCreate()df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')`

To be honest, I'm pretty confused on several part of instantiating a Spark session, but that's a question for a different time (like for example, where is the spark code installed? How do I incorporate AWS?).

Then there was one additional cell with the following code (pre-written by Udacity):

`#write to parquetdf_spark.write.parquet(""sas_data"")df_spark=spark.read.parquet(""sas_data"")`

I am confused on why the comment mentions ""write to parquet"" when we have a write function and then a read function right below. What do these two lines of code do?

&amp;#x200B;

**My questions are as such:**

1. Is Spark required to read *.parquet* files? I am assuming that is the reason my first line of code with **pd.read\_parquet()** is taking so long.
2. Why is the **df\_spark.write.parquet()** function calling the name of the folder ""**sas\_data**""? Does the function know to sort through every *.parquet* file in the folder?
3. What do the **df\_spark.write.parquet()** and **spark.read.parquet()** functions do?

&amp;#x200B;

I the context for this is a little confusing since most of you aren't familiar with the course, but any help is appreciated!"
2824,2020-07-17 18:23:49,1594999429.0,dataengineering,What is data reliability?,hsxlgp,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hsxlgp/what_is_data_reliability/,1.0,2.0,0.0,16067.0,"There are a lot of definitions out there when applied to statistics, but how do teams measure the reliability of their data? I found this article useful, but curious to hear what others think:

[https://towardsdatascience.com/what-is-data-reliability-66ec88578950?source=friends\_link&amp;sk=1e68914ac3f3e1bbb38f0f94c48074f0](https://towardsdatascience.com/what-is-data-reliability-66ec88578950?source=friends_link&amp;sk=1e68914ac3f3e1bbb38f0f94c48074f0)"
2825,2020-07-17 23:27:11,1595017631.0,dataengineering,What tags of leetocde to practice for data engineering coding rounds?,ht380h,groversarthak29,,https://www.reddit.com/r/dataengineering/comments/ht380h/what_tags_of_leetocde_to_practice_for_data/,1.0,19.0,0.0,16077.0,"I am looking for internships in the data engineering domain and wanted to know what tags (like arrays, strings, matrices etc.) to practice for coding rounds in interviews?"
2826,2020-07-17 23:54:13,1595019253.0,dataengineering,"Adapting to a new Data Source, SAP HANA",ht3p9j,poppinstacks,,https://www.reddit.com/r/dataengineering/comments/ht3p9j/adapting_to_a_new_data_source_sap_hana/,1.0,11.0,0.0,16077.0,"Hi Hello DEs!

I was just on boarded onto a migration project which involves migrating away from SAP HANA into Snowflake. I have no experience with SAP, and this raised two questions...

1. Does anyone have any recommendations to learn SAP (from 1000ft view) so that I can make sense of the tables it creates in HANA. 
2. How do you personally approach learning just enough about a new upstream to become dangerous? (Obviously, its not time efficient to learn the entirety of SAP, and SAP HANA just to start planning a migration)."
2827,2020-07-18 01:10:21,1595023821.0,dataengineering,SCD2 with spase data,ht52ph,Resident_Author,,https://www.reddit.com/r/dataengineering/comments/ht52ph/scd2_with_spase_data/,1.0,7.0,0.0,16078.0,"For a Slowly Changing Dimension Type 2 dataset, how does it work with inconsistently populated data? Most of the examples I've seen online have fully populated data I'm unsure on if the current row is supposed to represent to sum of all data about an entity or just the most recent version of it.

For example:

Existing data

|id|name|weight|height|start\_time|end\_time|is\_current|
|:-|:-|:-|:-|:-|:-|:-|
|1|Andy|150|63|1577836800|null|true|

&amp;#x200B;

Incoming data

|name|weight (modification)|height (missing value)|
|:-|:-|:-|
|Andy|160|null|

&amp;#x200B;

Which should be the result?

A)  


|id|name|weight|height|start\_time|end\_time|is\_current|
|:-|:-|:-|:-|:-|:-|:-|
|1|Andy|150|63|1577836800|1577923200|false|
|2|Andy|160|null|1577923201|null|true|

&amp;#x200B;

B)

&amp;#x200B;

|id|name|weight|height|start\_time|end\_time|is\_current|
|:-|:-|:-|:-|:-|:-|:-|
|1|Andy|150|63|1577836800|1577923200|false|
|2|Andy|160|63|1577923201|null|true|"
2828,2020-07-18 01:50:47,1595026247.0,dataengineering,BIG 4/Financial Services DE Salary and TC,ht5r1t,Westb0r0Faptist,,https://www.reddit.com/r/dataengineering/comments/ht5r1t/big_4financial_services_de_salary_and_tc/,1.0,0.0,0.0,16079.0,"Hi all I am looking for compensation data for associate (L1) level roles within the financial sector, specifically big4. If anyone could link me resources or PM me I would be very grateful."
2829,2020-07-18 03:13:16,1595031196.0,dataengineering,How Do I Become a Data Engineer?,ht73fs,Playba1133,,https://www.reddit.com/r/dataengineering/comments/ht73fs/how_do_i_become_a_data_engineer/,1.0,1.0,0.0,16082.0,"Hi All,

I am posting this hoping to gain some advice on what steps I should take in trying to becoming a data engineer.

**Previous:** Graduated in 2016 with a Bachelors Degree in Economics (graduated with a 2.8 GPA) . Worked as a Business Analyst for 2yrs and a Data Consultant for a year. Gained the AWS SAA certification and taught myself SQL, Tableau, R, and Python (mostly through various MOOCs) . Applied to the OMSA graduate program at GT and was rejected.

**Currently:** Working as a Senior Business Analyst for the past year where I gather requirements, perform testing, automate some processes in python and run a bunch of queries in SQL. Enrolled in a graduate program in Data Analytics at the University of Maryland Global Campus and have completed 12 credits.

**Goal:** Looking to land a row as a data engineer or scientist where I am able to use machine learning, AI, and cloud technologies to build data pipelines and mathematical models.

**Additional Background:** Do not really feel like I am gaining anything from my current graduate program as I have previously taught myself a lot of the things we are covering in class. However; I did not take high level math or any coding courses in college so would probably be tough to get into a different program (especially with my low GPA). Would like to pursue a more fulfilling path that allowed me to accomplish the goal referenced above.

**Next Steps:** ?????

A. Continue to pursue my graduate degree program.

B. Drop Out of the graduate program and instead enroll in a coding boot camp

C. Drop of the graduate program and teach myself the skills needed to become a data scientist/engineer.

D. Try to apply to a different graduate program (suggestions welcome)

E. Something else, feel free to suggest anything."
2830,2020-07-18 07:55:01,1595048101.0,dataengineering,Explorative Data Analysis using Pandas Profiling,htaz0k,8329417966,,https://www.reddit.com/r/dataengineering/comments/htaz0k/explorative_data_analysis_using_pandas_profiling/,1.0,0.0,0.0,16090.0,https://youtu.be/IezuD2e13tU
2831,2020-07-18 13:47:33,1595069253.0,dataengineering,Goat Meat Production Ranking | TOP 10 Country from 1961 to 2018,hteog4,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hteog4/goat_meat_production_ranking_top_10_country_from/,1.0,0.0,0.0,16098.0,
2832,2020-07-18 14:09:20,1595070560.0,dataengineering,"Hi, can someone point me to a couple of messy datasets ?",htewkm,newbie9232,,https://www.reddit.com/r/dataengineering/comments/htewkm/hi_can_someone_point_me_to_a_couple_of_messy/,1.0,3.0,0.0,16098.0,
2833,2020-07-18 14:23:39,1595071419.0,dataengineering,Serverless on GCP,htf233,Dminor77,,https://www.reddit.com/r/dataengineering/comments/htf233/serverless_on_gcp/,1.0,0.0,0.0,16099.0,"[We're going live today 📡]

𝗦𝗲𝗿𝘃𝗲𝗿𝗹𝗲𝘀𝘀 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝘀 𝗼𝗻 𝗚𝗖𝗣
 
We are going to learn what are several products for Serverless computing and different use cases, facilited by *Guillame Blaquiere*, Google Developer Expert for Google Cloud Platform and *Jigar Navadiya*, Co-Organizer GDG Cloud Ahmedabad.

*Date*: 18th July 2020
*Time*: 5:00 PM IST

*Join in here:* https://youtu.be/B3agAsFtU94"
2834,2020-07-18 16:40:10,1595079610.0,dataengineering,Dezyre big data projects,htgro0,Iffexibility1,,https://www.reddit.com/r/dataengineering/comments/htgro0/dezyre_big_data_projects/,1.0,11.0,0.0,16103.0,"Hi
I am just starting to develop myself as a data engineer and I came across dezyre.com, a platform that offers big data projects. I want to know if it worth paying for. 

Should I subscribe?"
2835,2020-07-18 17:26:14,1595082374.0,dataengineering,Qlik (Attunity) Replicate &amp; Compse costs?,hthg57,youderkB,,https://www.reddit.com/r/dataengineering/comments/hthg57/qlik_attunity_replicate_compse_costs/,1.0,3.0,0.0,16105.0,"Has anyone experience with Qlik Replicate &amp; Compose and can say something about the pricing, because it’s not listed on the website..."
2836,2020-07-18 23:54:32,1595105672.0,dataengineering,SQL Reference/Cheat Sheet for Interviews,htnzsx,[deleted],,https://www.reddit.com/r/dataengineering/comments/htnzsx/sql_referencecheat_sheet_for_interviews/,1.0,0.0,0.0,16184.0,
2837,2020-07-19 12:25:41,1595150741.0,dataengineering,Anyone interested in online coding class?,htxsod,K12ITCamp,,https://www.reddit.com/r/dataengineering/comments/htxsod/anyone_interested_in_online_coding_class/,6.0,6.0,0.0,16203.0,"Led by college Ph.D. professor (**of Mount Saint Mary’s University; PhD from UCI; more than 10 years experiences in data science field**), suitable for 11th+12th graders, college freshmen+sophomores, and people simply interested in comsci/coding. You can receive **rec letters, app background enhancement, and appear at press release**. DM me for more information! :)"
2838,2020-07-19 13:44:08,1595155448.0,dataengineering,Electricity Generation Ranking | TOP 10 Country from 1985 to 2018,htyk4j,datavtworld,,https://www.reddit.com/r/dataengineering/comments/htyk4j/electricity_generation_ranking_top_10_country/,1.0,0.0,0.0,16205.0,
2839,2020-07-19 14:21:19,1595157679.0,dataengineering,Advice needed - Should I quit my new DE job?,htyxnh,[deleted],,https://www.reddit.com/r/dataengineering/comments/htyxnh/advice_needed_should_i_quit_my_new_de_job/,2.0,8.0,0.0,16205.0,
2840,2020-07-19 16:24:19,1595165059.0,dataengineering,Interview prep materials/problems sets with answers for DE skills (leetcode for DE?),hu0fkm,AdministrativeCell84,,https://www.reddit.com/r/dataengineering/comments/hu0fkm/interview_prep_materialsproblems_sets_with/,14.0,13.0,0.0,16209.0,"I am a SWE who does not have any CS training, I started as a programmer after a science degree 3 years ago. I have discovered I much more interested in Data Engineering and would like to get a job as a data engineer. My best skills in this area are in python and AWS, which I am reasonably confident in (I don't design the systems in AWS just implement them). I have used other DE tools like spark and some CI/CD tools etc, but I have just learned what I needed for the task and I don't feel confident in those (partly my personality as well as skill level, I want to be able to check my knowledge and get feedback). I have put together some personal projects using DE tools but I worry that I don't know if they are very good/how to improve them.

What I am after here is something like leetcode but more suited to my interests/career ambitions. I have done the easy and medium SQL questions on leetcode which are offered free and found them really helpful, the format of studying the topic/attempting a question/getting the feedback/relooking at the topic if necessary is perfect for me and my SQL improved massively. I also learned a lot doing the practice papers with answers for the AWS Associate exams. I don't think leetcode is right for me because I don't think I should focus on DS/algorithms as a priority over other aspects right now.

I am looking for something like this but for data engineering interview questions on data modelling, data pipelines, systems design etc. I don't mind paying a leetcode-similar amount of money for this. I have seen SystemsExpert advertised to me but it says it only provides 8 questions and I don't know what topics they will be on so I am not sure about that being what I want.

Does anyone know of any such resources? 

The main thing I hope to gain from this is confidence in my abilities and feedback on how to improve so I think looking for interview prep is where I am most likely to find problem sets/feedback which is what I want. Plus I do also want to pass a DE interview as well. I will of course continue to do my own projects. In terms of actually learning, I am not really looking for just instructional videos etc as I can learn by myself reading docs and articles, watching youtube videos, trying solutions and that seems to work OK for me, although if you do have any recommendations for learning these things I will check them out as well.

Oh also, I am not looking at FANG or anything super elite if that matters, just normal DE jobs.

tldr:

Interview prep/question and answer banks for data modelling, data pipeline design, other topics in data engineering."
2841,2020-07-19 20:04:31,1595178271.0,dataengineering,Challenges of Open Data,hu3usl,nfrankel,,https://www.reddit.com/r/dataengineering/comments/hu3usl/challenges_of_open_data/,1.0,0.0,0.0,16216.0,
2842,2020-07-19 20:59:42,1595181582.0,dataengineering,What is the name for this kind of data?,hu4tcr,NewStandards,,https://www.reddit.com/r/dataengineering/comments/hu4tcr/what_is_the_name_for_this_kind_of_data/,2.0,19.0,0.0,16221.0,"*The context: A water networks monitoring solution. Think sensors and meters installed on a water network that send readings to a server for analysis.*

I've noticed while working on this project that I'm mainly dealing with two kinds of data: one where a time **period** is required for the value to make sense, and the other where only a **single point in time** is needed to make sense of that value.

As an example, water consumption needs a start and end time for the value to make sense. For instance: ""between 1:00 pm and 2:00 pm the consumption was 100 m^(3)"".   
But a tank's water level is different. You only need one time value for that. E.g.: ""At 1:00 pm the tank was at 78%"".

I'm no data engineer, nor a data scientist, so I apologize if this is a silly question but I don't even know what to look up on google to research this. Is there a name for these two kinds of data? Are there any resources that you can point me to that talk about the best practices of storing such data in a database? Are there ""other kinds of data"", whatever that means...?

Also, I'd appreciate any general tips from someone who has ever dealt with a similar project.

&amp;#x200B;

Thanks."
2843,2020-07-19 22:46:51,1595188011.0,dataengineering,I want to find the time complexity of the following algorithm; what is the procedure?,hu6rfr,[deleted],,https://www.reddit.com/r/dataengineering/comments/hu6rfr/i_want_to_find_the_time_complexity_of_the/,0.0,2.0,0.0,16228.0,
2844,2020-07-19 23:36:18,1595190978.0,dataengineering,[Apache Spark] Imputation Techniques and Research Papers,hu7mrr,Kubacka,,https://www.reddit.com/r/dataengineering/comments/hu7mrr/apache_spark_imputation_techniques_and_research/,1.0,0.0,0.0,16229.0,"Hi everyone!

I'm working on making imputation methods for Spark, and was wondering if anyone was aware of what the current SOTA research is, and if any teams have made progress towards implementing methods such as MICE.

Thank you!"
2845,2020-07-20 00:35:05,1595194505.0,dataengineering,"Anyone here a data engineer in the EU ? Would love to bear about salary , hours and where you work",hu8nvc,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/hu8nvc/anyone_here_a_data_engineer_in_the_eu_would_love/,35.0,106.0,0.0,16230.0,"Bonus points if you moved from the US. I am a us citizen with eu citizenship. Work as a data engineer (aws /snowflake). Prior exp was data analyst. Only speak English. Interested in working in EU for culture , ideally also continue to grow my data engineering exp. (rather than just moving to eu for fun and taking any job I’d still like to grow my career )"
2846,2020-07-20 09:07:08,1595225228.0,dataengineering,What do I look for when hiring a Data Engineer?,hug53i,tanmaydeshpande,,https://www.reddit.com/r/dataengineering/comments/hug53i/what_do_i_look_for_when_hiring_a_data_engineer/,1.0,0.0,0.0,16247.0,
2847,2020-07-20 10:49:01,1595231341.0,dataengineering,GCP Cloud Composer: managed Airflow on the cloud. Anyone here using?,huhapt,kotartemiy,,https://www.reddit.com/r/dataengineering/comments/huhapt/gcp_cloud_composer_managed_airflow_on_the_cloud/,1.0,4.0,0.0,16252.0,"I've been using AWS so far. Decided to go through what GCP is proposing. 

&amp;#x200B;

You should have seen my face when I saw that GCP has a managed Airflow service. I briefly went through how it's done. Seems like they did a great job making all the ops ob for me. 

&amp;#x200B;

Of course, there will be a limitation (as it always is with managed solutions) but I am OK giving them up in exchange for not managing Airflow. 

&amp;#x200B;

So, anyone? Did you try it? your thoughts?"
2848,2020-07-20 13:39:06,1595241546.0,dataengineering,Daily Smoker Ranking | TOP 10 Country from 1980 to 2012,huj2tf,datavtworld,,https://www.reddit.com/r/dataengineering/comments/huj2tf/daily_smoker_ranking_top_10_country_from_1980_to/,1.0,0.0,0.0,16257.0,
2849,2020-07-20 13:59:55,1595242795.0,dataengineering,How to Simulate Web Events?,hujazb,AndreSionek,,https://www.reddit.com/r/dataengineering/comments/hujazb/how_to_simulate_web_events/,23.0,6.0,0.0,16258.0,
2850,2020-07-20 16:44:24,1595252664.0,dataengineering,what core skill i should upgrade when working in data engineer jobs ?,hulhhd,KIENBACH12,,https://www.reddit.com/r/dataengineering/comments/hulhhd/what_core_skill_i_should_upgrade_when_working_in/,0.0,2.0,0.0,16263.0,
2851,2020-07-20 18:07:24,1595257644.0,dataengineering,ETL Auditing in ODI,humvnm,loonina,,https://www.reddit.com/r/dataengineering/comments/humvnm/etl_auditing_in_odi/,3.0,16.0,0.0,16265.0,
2852,2020-07-20 18:58:02,1595260682.0,dataengineering,"Explore ""Data"" using ""Sweetviz"" &amp; ""Python"".",hunrw5,8329417966,,https://www.reddit.com/r/dataengineering/comments/hunrw5/explore_data_using_sweetviz_python/,1.0,0.0,0.0,16266.0,https://youtu.be/9UqLd2k1aSk
2853,2020-07-20 19:10:55,1595261455.0,dataengineering,An interview about the Meltano project and their goal of building a fully open source data integration platform that is competitive with commercial systems.,huo0f9,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/huo0f9/an_interview_about_the_meltano_project_and_their/,1.0,0.0,0.0,16266.0,
2854,2020-07-20 19:13:49,1595261629.0,dataengineering,Best non-SaaS extract-load tools on AWS,huo2g2,Alert_Dragonfly,,https://www.reddit.com/r/dataengineering/comments/huo2g2/best_nonsaas_extractload_tools_on_aws/,3.0,32.0,0.0,16266.0,"Hi all,

What would you choose for daily / semi-daily extract batch jobs from AWS RDS (Postgres) to AWS S3?

Tools such as Fivetran or Stich are a no-go as the data should no go outside our AWS account. We currently use Airflow to pg\_dump/pg\_restore (as you would do for backup) which is not perfect but was good for a quick start.

Different possibilities I have in mind:

1/ Custom bash scripts (/COPY) to offload the data as CSV files and upload them to S3, everything orchestrated by Airflow

2/ Same as 1/ but use custom Python scripts using [singer.io](https://singer.io) open-source library (Stitch seems to use it internally?)

3/ Use managed services such as LakeFormation, or AWS Glue (which ones?)

1/ and 2/ seem to be the way to go, as we would have total control over what happens and the capability to tune the extract logic if needed. 1/ is easier and faster than 2/ as I would the logic already exists with /COPY native commands and AWS s3 cp CLI. However, 2/ is still good as we would have a Python codebase. I try to stay away from bash scripting as it is not easily understandable and maintainable.

Both 1/ and 2/ might result in a large codebase that would add a large overhead in term of dev time.

I have no experience with the tools in 3/. I have read some mixed reviews about AWS Glue as the service is not mature at all (1 year ago). According to what I have read, it uses Spark to perform the ETL logic, and sometimes it can be tricky to understand why the jobs are slow as the users have not total control on the scripts. Moreover, Spark seems way too overkill for our volumetry (less than 20GB daily) and having worked on Spark, I try to stay away if possible because of its complexity.

I would be thankful for any feedback on the above ! :) Thanks"
2855,2020-07-20 19:33:13,1595262793.0,dataengineering,resources for learning TDD in DE,huof3x,audyoga,,https://www.reddit.com/r/dataengineering/comments/huof3x/resources_for_learning_tdd_in_de/,7.0,14.0,0.0,16268.0,"Any links, literature, sample code would be really helpful"
2856,2020-07-21 01:13:01,1595283181.0,dataengineering,Question about Clickhouse and parquet,huv0xi,jaredw,,https://www.reddit.com/r/dataengineering/comments/huv0xi/question_about_clickhouse_and_parquet/,1.0,4.0,0.0,16282.0,"Hello Reddit 

I've searched everywhere on how to export clickhouse data as partitioned parquet files. 

Obviously we could write some script that exports the data using dimensions. However, I was hoping someone might know of a way to do this some other or better way. 

Thanks"
2857,2020-07-21 02:10:20,1595286620.0,dataengineering,Question on how to use Spark in order to process and upload data to Redshift,huw1pu,freebird348,,https://www.reddit.com/r/dataengineering/comments/huw1pu/question_on_how_to_use_spark_in_order_to_process/,11.0,13.0,0.0,16285.0,"Forgive my ignorance as I am what you call a **newbie**. 

I'm currently learning Spark and I'm attempting to understand its importance when uploading data to Redshift. Let's say I have some data in the form of a json response in S3. I pull the data from S3, and then use Spark to create a dataframe of the json data. 

From that point forward, how do I upload the data to Redshift? Do I just create a loop that goes through every row of the Spark dataframe and upload that using a SQL Insert statement? If that is the case, then how does Spark's processing power play into effect?

Is the other option to clean the data using Spark's data frame, then re-upload the clean data to S3, and then create a separate script which loops though and imports to Redshift using using a SQL Insert statement?

Any help would be appreciated!"
2858,2020-07-21 05:47:49,1595299669.0,dataengineering,Recommended system / specs for full stack DE dev machine,huzfw8,jonscrypto,,https://www.reddit.com/r/dataengineering/comments/huzfw8/recommended_system_specs_for_full_stack_de_dev/,1.0,14.0,0.0,16292.0,"TLDR; what is sufficient hardware to buy for running several data engineering (ie DB servers - rdb, olap, nosql; reporting, BI, ETL, big data, orchestration) applications in a development environment?

I'd like to install a full suite of data engineering tools for self training and building portfolio projects. I have an old Windows 10 box (2gb ram, 1.7ghz cpu, 1TB HDD). I tried installing SQL Server, SSIS (in VS), SSAS, SSMS, SSRS on top of Power BI. It worked, but was very slow, even just running one app at a time. Then I installed a 2nd instance of SQL Server for OLAP and the machine stopped dead in it's tracks. I checked sys req's for SQL Server and realized I was way short on ram. At best I can use that machine for Power BI and SSIS against an Azure SQL DB since I uninstalled the rest of the SQL Server suite. I would also use it for backups.

I'd like to learn other tools as well (eg Spark/Databricks, Kafka, Airflow) and I'm not tied to MS so I'll consider other open source alternatives to those mentioned above as well.  I'm just not sure what to look for in a dev machine. I'm assuming an SSD for the OS and software would run much faster and guess I need at least 8gb of ram. 

Is that about right? I don't want to go and buy something that turns out coming up short but also don't want to get something unnecessary. Do I need Windows Server or is Professional ok? I use pro now for RDP and not sure what else Server gets me for this purpose. Would a new high end laptop be sufficient? Any specific recommendations would be appreciated.

TIA!"
2859,2020-07-21 07:25:27,1595305527.0,dataengineering,How to prepare for DE System Design and Modelling Interviews,hv0tb6,floydhead11,,https://www.reddit.com/r/dataengineering/comments/hv0tb6/how_to_prepare_for_de_system_design_and_modelling/,1.0,0.0,0.0,16296.0,"As the question suggests - 
I have spent a decent amount of time reading up on system design. I watched videos, have used Grokking, and had discussions on blogs or peer meetings.

My DE interview did not have an SDA round but had way more coding rounds. Hence, I have not given an SDA round that is successful (have only appeared for 2).

I am trying to apply for more technical DE roles and have grinded leet and prepared otherwise (I know DSA basics, understand the MVC, server issues, load balancers, consistent hashing, SQL/NOSQL, Star/Snowflake, Mart/Warehouse, etc) but I am unsure how to actually prepare and improve my SDA skills for interview so that I can apply for a higher skill higher pay job at the premier companies. 

I end up with a choice paralysis when I am in an SDA.
In a normal conversation, I can discuss different aspects with a decent understanding.
In interviews, I either want to discuss EVERYTHING or my mind freezes and I cannot even make a proper schema.

How do I tackle this?"
2860,2020-07-21 07:26:49,1595305609.0,dataengineering,Looking for some thoughts on how to approach a data problem conceptually,hv0txm,Rey661199,,https://www.reddit.com/r/dataengineering/comments/hv0txm/looking_for_some_thoughts_on_how_to_approach_a/,1.0,2.0,0.0,16296.0,"Currently we have a database that contains users that have been redirected to us by partners. This table of users we need to filter it down to eligible users -&gt; trigger a callback function on a url for every user in the filtered database table (every callback makes a payment to a partner).  The callback is basically sending a get request to a URL. 

The acceptance criteria (AC) is to build this in a way that it is idempotent. So for all users that the callback (get request) has been triggered for, we have to mark them as done and not pay again to them. The second AC is to log payments (probably get request responses). 

The task is not that complicated, but I am wondering if you have had similar problems? Is there a gold standard for approaching these problems? The objective is to have high integrity and correctness. Not performance."
2861,2020-07-21 12:58:49,1595325529.0,dataengineering,Snowflake Cloud Data Warehouse Review – TPC-DS Benchmark Performance Analysis and Why It’s Becoming the Post-Hadoop Big Data Nirvana,hv4p3h,[deleted],,https://www.reddit.com/r/dataengineering/comments/hv4p3h/snowflake_cloud_data_warehouse_review_tpcds/,1.0,4.0,0.0,16303.0,
2862,2020-07-21 13:32:16,1595327536.0,dataengineering,Turkey Meat Production Ranking | TOP 10 Country from 1961 to 2018,hv52hb,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hv52hb/turkey_meat_production_ranking_top_10_country/,1.0,0.0,0.0,16304.0,
2863,2020-07-21 15:07:02,1595333222.0,dataengineering,"$1 Trillion Dollar Club [Companies that has Market Cap Exceeds $1,000,00...",hv676s,mostafaelmahdy,,https://www.reddit.com/r/dataengineering/comments/hv676s/1_trillion_dollar_club_companies_that_has_market/,1.0,0.0,0.0,16306.0,
2864,2020-07-21 17:30:52,1595341852.0,dataengineering,Top 10 Examples of Data Lineage,hv8bfe,cwadamsmith,,https://www.reddit.com/r/dataengineering/comments/hv8bfe/top_10_examples_of_data_lineage/,0.0,0.0,0.0,16309.0," 

Here, we will cover the top 10 real-life data lineage examples. This blog will focus on the significance and benefits of data lineage for the below-mentioned companies.

* **Examples of Data lineage in Financial Services**

1. **Standard Chartered**
2. **Slovenská Sporiteľňa**

* **Examples of Data Lineage in Technology**

1. **NCR Corporation**
2. **Spiral Universe**

* **Examples of Data Lineage in Marketing &amp; Media**

1. **Sky Deutschland**
2. **Allant Group**

* **Examples of Data Lineage in Government**

1. **Georgia Department of Transportation**
2. **Gendarmerie Nationale**

* **Examples of Data Lineage in Transportation**

1. **Air France**
2. **Keolis**

 Read the in-depth [post here](https://www.knowledgenile.com/blogs/data-lineage-examples/?utm_source=RD&amp;utm_medium=SM&amp;utm_campaign=RS)."
2865,2020-07-21 17:42:02,1595342522.0,dataengineering,Full pipeline with pyspark and airflow,hv8ii6,owila,,https://www.reddit.com/r/dataengineering/comments/hv8ii6/full_pipeline_with_pyspark_and_airflow/,3.0,33.0,0.0,16310.0,"Hello everyone, please can someone point me to an example of a full data pipeline using airflow and pyspark for data transformation.

Some quick edit
The project workflow I plan on going with is this
1. Load data to s3
2. pull data from s3 and transform with pyspark
3. save transformed data as parquet format and write to an s3 bucket.
5. Pull transformed data and load to Redshift DB.

Airflow will will run the dag for step 2 to 5 but I am having issues setting up a pyspark job, I know I will have to use an EMR cluster but can't find a way connecting the job to the airflow pipeline.

I am also open to suggestions on how I can go about the project.


Note: I want to use pyspark for the transformation for learning purposes."
2866,2020-07-21 19:09:23,1595347763.0,dataengineering,Advanced Flink Application Patterns Vol.2: Dynamic Updates of Application Logic,hva52w,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hva52w/advanced_flink_application_patterns_vol2_dynamic/,1.0,0.0,0.0,16313.0,
2867,2020-07-21 19:34:21,1595349261.0,dataengineering,How to do ETL pipeline DAG visualization,hvamuh,stym06,,https://www.reddit.com/r/dataengineering/comments/hvamuh/how_to_do_etl_pipeline_dag_visualization/,9.0,18.0,0.0,16313.0,"So we've got this setup for MySQL &lt;&gt; Hive

Maxwell (CDC) -&gt; Kafka -&gt; Secor -&gt; S3 

And then reconciliation jobs on top of the S3 data.

Whats the best way to build a dashboard for the overall visibility and observability of the pipeline?"
2868,2020-07-21 21:54:16,1595357656.0,dataengineering,Split Insert and Updates separately,hvdckg,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hvdckg/split_insert_and_updates_separately/,1.0,10.0,0.0,16317.0,"Hello everyone,

I have table with 90 million record, I want to separate insert values , update , delete values separately and convert to a file to S3 . 

Right now we are doing a full comparison of the table and doing full outer join to split the Insert and update values which takes lot of time . 

How can we do this better. any inputs."
2869,2020-07-21 22:14:35,1595358875.0,dataengineering,DYI: How do I set up a database (on server),hvdqln,lakenp,,https://www.reddit.com/r/dataengineering/comments/hvdqln/dyi_how_do_i_set_up_a_database_on_server/,0.0,10.0,0.0,16317.0,"I would like to set up a database.

To store prices from stores I come across. 

Such data seems quite structured, so I guess I need a plain SQL database (not noSQL)? 

Moreover, I'd like to be able to read and write data from/to the database flexibly. Like from different desktops, phones, etc., and preferably at any time using the interwebs.

So I guess I'd need to put the database on a server, and develop some API for access as well.

I have never done this before. By background is in data science / statistics, and I know R, Python, SQL and some other languages to a lesser extent.

\*\*Where do I start?\*\* \*What do I need to consider?\*"
2870,2020-07-21 23:28:15,1595363295.0,dataengineering,Need help deciding,hvf5i6,haagimus,,https://www.reddit.com/r/dataengineering/comments/hvf5i6/need_help_deciding/,1.0,0.0,0.0,16321.0,"[removed]

[View Poll](https://www.reddit.com/poll/hvf5i6)"
2871,2020-07-21 23:52:31,1595364751.0,dataengineering,Why do you enjoy being a Data Engineer? What is your background?,hvfmit,ZeWaffleStomp,,https://www.reddit.com/r/dataengineering/comments/hvfmit/why_do_you_enjoy_being_a_data_engineer_what_is/,32.0,88.0,0.0,16320.0,"I'm really curious about why people chose this profession (or aspire to be in this profession).

A lot of people seem to think that Data Engineers are just wannabe Software Engineers that didn't quite make the cut.  While that statement isn't entirely false for me, I did intend to use DE as an eventual transition to a back-end software engineer, but I think I just liked Data Engineering so much more, simply because it's not 90% heads down coding.

My background:

I'm 6 years out of school with a Bio science major, who got lucky coming out of university and landed at a small tech company in operations role (business system analyst, data analyst, database engineer, programmer, etc.) in Texas and then put it all together by completing a Data Engineering bootcamp in the Bay Area.

Curious how other people have got here and what motivated their Data Engineering aspirations.

EDIT:

I haven't really commented on specific responses, but I think it's incredibly refreshing how diverse the backgrounds and skillsets are for fellow Data Engineers. Not a single CSci major!!"
2872,2020-07-22 00:29:21,1595366961.0,dataengineering,Has anyone integrated a data quality monitoring tool with their stack?,hvgbb2,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hvgbb2/has_anyone_integrated_a_data_quality_monitoring/,11.0,17.0,0.0,16322.0,"My company is mulling over whether or not it makes sense to:

* Invest in a third-party data quality monitoring solution like Toro Data, Databand, or Soda Data.
* Build an ad hoc solution (least likely option, but it's not off the table... we currently have a selection of ad hoc SQL alerts but are trying to upgrade)
* Use Great Expectations, or another open source tool.

Any notes from the field?"
2873,2020-07-22 03:47:16,1595378836.0,dataengineering,Sunburst Presto,hvjrha,[deleted],,https://www.reddit.com/r/dataengineering/comments/hvjrha/sunburst_presto/,1.0,0.0,0.0,16330.0,
2874,2020-07-22 03:49:28,1595378968.0,dataengineering,Starburst presto,hvjsqg,koteikin,,https://www.reddit.com/r/dataengineering/comments/hvjsqg/starburst_presto/,2.0,2.0,0.0,16330.0,"Anyone using it in production successfully? And if you do, what did you compare it to? Is it as fast as Impala? Any pain?"
2875,2020-07-22 10:55:25,1595404525.0,dataengineering,Datavault questions,hvpnvd,mrcool444,,https://www.reddit.com/r/dataengineering/comments/hvpnvd/datavault_questions/,1.0,0.0,0.0,16339.0,"Hi All,

If you have implemented DV2.0 or having ideas for below questions, I will appreciate your help.

* How do you handle deltas with late arriving full snapshots? Do we have to reload satellite starting from the late arrived file date? We load full snapshot to staging and compare it with the latest record from satellite using hashdiff and load it only when there is a change in the record.
* Is there any testing framework for DV2.0? What are the generic test cases for Datavault?

Thanks,

Mc"
2876,2020-07-22 13:27:21,1595413641.0,dataengineering,Vocabulary for different data stages?,hvrdgq,brokenindu,,https://www.reddit.com/r/dataengineering/comments/hvrdgq/vocabulary_for_different_data_stages/,2.0,6.0,0.0,16342.0,"Here's a great debate: what is a good vocabulary for describing the different stages data goes through  that reflects its quality and effectively communicates to the users what they can expect?

For instance, the term **raw** data is widely used and understood to mean data that has no quality checks on it, no transforms, is unstructured, and the user will need to perform any such changes to the data if they want to use it.

As data engineers we then process the raw data into something more consumable by users and downstream applications. The data may go through many intermediate steps before it's presented in its final form. 

How do you describe the data in its various stages?

The best description I've read has been from DataBricks which describe data in 3 stages: [Bronze, Silver, and Gold](https://databricks.com/blog/2019/08/14/productionizing-machine-learning-with-delta-lake.html), **gold** being the best quality data ready for public consumption. At my current company we use the term **processed** or **master** to describe the final form of data. 

What's been your experience?"
2877,2020-07-22 13:49:54,1595414994.0,dataengineering,Which is less time consuming !?,hvrmwh,Mhayc_en,,https://www.reddit.com/r/dataengineering/comments/hvrmwh/which_is_less_time_consuming/,3.0,15.0,0.0,16343.0,"My supervisor wants  me to make an ETL that feeds data into a web dashboard. However we are in the early stages so he wants me to see if it's better to use PowerBi data integration system . 

Or is it better to use a web dev like flusk or django + the etl ?

i dont know what's better in terms of time .

thank you in advance"
2878,2020-07-22 13:54:27,1595415267.0,dataengineering,Military Expenditure Ranking | TOP 10 Country from 1949 to 2016,hvrosb,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hvrosb/military_expenditure_ranking_top_10_country_from/,1.0,0.0,0.0,16343.0,
2879,2020-07-22 14:32:36,1595417556.0,dataengineering,Any advice to become a data engineer?,hvs5dt,PM_ME_YOUR_DONUT_PLS,,https://www.reddit.com/r/dataengineering/comments/hvs5dt/any_advice_to_become_a_data_engineer/,21.0,48.0,0.0,16345.0,"Hi everyone. I am a bio grad (1 yr now) which is totally unrelated to data engineering. I have learnt some python, not too much, but I am very eager. I was wondering has anyone here made it into or have any advice on how I could break in to data engineering? I am not talking about being a data scientist but an engineer which are programmers.

What courses I could take/bootcamps? What sort time frame it will require? Costs? What sort of things I would need to achieve to prove to employers such as in a portfolio?

What I know is that in summary it would require proficiency in python, SQL, maybe R, knowledge of cloud services such as AWS and knowledge of data storage solutions.

Any advice? Thanks. I'd like to add that I am interested as there seems to be high demand for juniors and it's future proof."
2880,2020-07-22 23:14:45,1595448885.0,dataengineering,An interview with the founder of Turbit Systems about how they are improving the efficiency and sustainability of wind energy through real time analysis of data collected from turbines.,hw16wm,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hw16wm/an_interview_with_the_founder_of_turbit_systems/,1.0,0.0,0.0,16357.0,
2881,2020-07-23 03:48:47,1595465327.0,dataengineering,DE Portfolio project advice,hw60x7,andresg3,,https://www.reddit.com/r/dataengineering/comments/hw60x7/de_portfolio_project_advice/,14.0,21.0,0.0,16363.0,"Hello all,  
I've been a DBA for 6yrs. These last few  months I've been taking courses and studying to become a data engineer. I've taking online courses/tutorials on python, airflow, spark, aws... etc  
I feel that it's time to start working on a project that I can show on any future interviews.  


On a very high level I would like to build a ETL pipeline. I've been thinking to:  
1. Fetch data from an API(possibly yelp).  
2. Save data on local disk  
3.  Move data to landing zone s3 buckets  
4. Copy data from landing zone to working zone  
5. Use spark to perform transformation operations on the dataset (time stamp formatting, blank space removal, etc)  
6.  Move data to the Processed Zone  
7. Data is picked up from processed zone and stored in Redshift tables  
8. ETL job execution completed  


ETL jobs will be written in spark and scheduled in airflow to run every N minutes.  


I was looking at the yelp api data but I can't see how I can use that data on my project. Like a ""use case that would make sense"" Can anyone think of some ideas. Or maybe another API that would allow me to pull meaningful data to be used on my pipeline?  


Any and all feedback is more than welcome"
2882,2020-07-23 08:50:39,1595483439.0,dataengineering,Google ads raw data to adls gen2,hwaak4,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/hwaak4/google_ads_raw_data_to_adls_gen2/,1.0,0.0,0.0,16374.0,"Hi,

Has anyone setup pipeline to copy google ads raw data to adls gen2 without using bigquery? I do not want to use another pricing layer in between and would like to copy the data directly from google ads to adls gen2"
2883,2020-07-23 11:27:52,1595492872.0,dataengineering,How To Start with Apache Spark and Apache Cassandra,hwc19s,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/hwc19s/how_to_start_with_apache_spark_and_apache/,13.0,0.0,0.0,16379.0,
2884,2020-07-23 14:03:23,1595502203.0,dataengineering,Visitor Arrivals Ranking | TOP 10 Country from 1995 to 2018,hwdpjs,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hwdpjs/visitor_arrivals_ranking_top_10_country_from_1995/,1.0,0.0,0.0,16381.0,
2885,2020-07-23 15:55:47,1595508947.0,dataengineering,Does it make sense to move data from one RDBMS to another to do analysis.,hwf5ga,thor123321,,https://www.reddit.com/r/dataengineering/comments/hwf5ga/does_it_make_sense_to_move_data_from_one_rdbms_to/,2.0,19.0,0.0,16385.0,"Hey,  


Where i work we are using PowerBI, which is set to update once a day. The reports pulls data using sql directly from our ERP(Dynamic NAV). We only run these reports at night, because when the reports run the performance of our ERP suffers a great deal - almost not possible to work in it.  


However, i have begun to pull data using python, through NAV webservices, which dosn't affect performance at all, no matter how much i pull.

So, i have this idea to make sort of a datawarehouse, using another RDBMS(mabey MySQL) - and feed this datawarehouse only with the data i wish to use for analysis, by pulling it from the production SQL, using python and webservices.  
Another thing is, that webservices has a limit in the amount of rows which can be pulled in one pull request, so i only need to gather historical data once, and then just update it afterwards.  


As i see it, this would give me the possibility to update our PowerBI reports more often, many times aday.

The consultants which have set up the current powerbi reports, don't think a datawarehouse is necessary.. But im not sure they are right.  


I trust you guys opinion more ;-)   
so, does it make sense?  


Thank you in advance."
2886,2020-07-23 18:29:35,1595518175.0,dataengineering,Sharing is caring - Catalogs in Flink SQL,hwhncl,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hwhncl/sharing_is_caring_catalogs_in_flink_sql/,1.0,0.0,0.0,16388.0,
2887,2020-07-23 19:09:28,1595520568.0,dataengineering,Nubank Data Analysts are now called Analytics Engineers,hwidum,pepachino,,https://www.reddit.com/r/dataengineering/comments/hwidum/nubank_data_analysts_are_now_called_analytics/,27.0,11.0,0.0,16388.0,
2888,2020-07-23 20:42:25,1595526145.0,dataengineering,"Data Visualization using ""Matplotlib"" &amp; Python | Part-I",hwk70f,8329417966,,https://www.reddit.com/r/dataengineering/comments/hwk70f/data_visualization_using_matplotlib_python_parti/,1.0,0.0,0.0,16388.0,https://youtu.be/v212XEmj9ek
2889,2020-07-23 22:56:27,1595534187.0,dataengineering,Linked Lists In DE?,hwmszh,clueless3867,,https://www.reddit.com/r/dataengineering/comments/hwmszh/linked_lists_in_de/,2.0,17.0,0.0,16390.0,"Hey Everyone!

I'm preparing for an interview, and am trying to get very comfy with data structures and algorithms. While I understand conceptually what a linked list is, I'm having a hard time imagining situations where a DE would choose a linked list data structure over other ones. I haven't worked as a DE yet, and I primarily use Python...so maybe my scope isn't as well informed as some of you on here.

Do any of you have any real-life examples of using linked lists in your work?"
2890,2020-07-24 02:11:06,1595545866.0,dataengineering,Automating S3 to Tableau,hwqca8,BIEIntern,,https://www.reddit.com/r/dataengineering/comments/hwqca8/automating_s3_to_tableau/,1.0,11.0,0.0,16393.0,"I will be getting JSON files uploaded to my S3 bucket a few times each day, and my thought process was to build out tables in Athena from those JSON files. I have done this, but now I came to the realization that when extracting/updating the data from tableaus end it is not updating my tables because my tables are created out of a source table to read the nested JSON structure.

Any way to automate this process? I have been looking at GLUE, but honestly kind of confused what it is doing considering it just seems like a different way to create the source table. I am also worried that whichever way I go with this each query from the bucket will just get more and more costly with each file uploaded. Any way to only query from the most recent files?

Appreciate any help. Thank you! (Not a DE btw)"
2891,2020-07-24 03:17:08,1595549828.0,dataengineering,How to Fix Your Data Quality Problem,hwrg60,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hwrg60/how_to_fix_your_data_quality_problem/,2.0,3.0,0.0,16397.0,
2892,2020-07-24 10:10:10,1595574610.0,dataengineering,Amazon sql coding interview,hwxa73,ronela29,,https://www.reddit.com/r/dataengineering/comments/hwxa73/amazon_sql_coding_interview/,7.0,37.0,0.0,16406.0,"Laid off due to COVID-19 recently. I have a sql coding interview with amazon for a BA role. The interviewer is a Data engineer. 

1. Any tips or suggestions on how to practice? Any online websites or resources I should refer? 

2. What questions can I expect? 

3. Any one who has done this in the past - some do’s and don’ts?

TIA"
2893,2020-07-24 13:50:23,1595587823.0,dataengineering,Trump vs Biden | 2020 U.S. Presidential Election,hwzmnw,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hwzmnw/trump_vs_biden_2020_us_presidential_election/,1.0,0.0,0.0,16412.0,
2894,2020-07-24 16:30:42,1595597442.0,dataengineering,Here's how to use data science techniques to get insights on PUBG data.,hx1tmg,Vaishali_Advani,,https://www.reddit.com/r/dataengineering/comments/hx1tmg/heres_how_to_use_data_science_techniques_to_get/,2.0,0.0,0.0,16418.0,
2895,2020-07-24 17:30:51,1595601051.0,dataengineering,Using Python to detect people wearing face masks in real-time webcam feed,hx2rzx,Hussain_Mujtaba,,https://www.reddit.com/r/dataengineering/comments/hx2rzx/using_python_to_detect_people_wearing_face_masks/,1.0,0.0,0.0,16419.0,
2896,2020-07-24 21:02:05,1595613725.0,dataengineering,S3 to Snowflake,hx6nvk,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hx6nvk/s3_to_snowflake/,2.0,26.0,0.0,16444.0,"Hello everyone,

What is the best and cost-efficient way to load the full load and incremental load from S3 to Snowflake?

Options I have is :-

1. Snowpipe
2. Bulk load approach
3. Using Glue and using Spark Dataframe
4. any other ? Please suggest .

The size of the data we are talking about daily is 50GB per day "
2897,2020-07-24 22:37:37,1595619457.0,dataengineering,Experience with choosing AWS Glue as an ETL platform,hx8h1j,raginjason,,https://www.reddit.com/r/dataengineering/comments/hx8h1j/experience_with_choosing_aws_glue_as_an_etl/,13.0,60.0,1.0,16453.0,"I am tasked evaluating AWS Glue for a new data engineering project. I have a good bit of experience with data engineering, but not much experience with Spark. The volume of data for this project is relatively low, and this project is expected to operate in a nightly batch mode (i.e. no realtime or streaming requirements). Because of the relatively low performance requirements, Glue PySpark seemed like a good fit. I've been working with it for a few weeks and the deeper I go the more I'm doubting how robust this solution is:

* Local development is a hassle (`Unresolved reference 'awsglue'` for any Python Glue libraries)
* Local testing is not officially supported although I was able to create a Docker image that performs reasonably enough
* The Glue libraries themselves don't seem particularly robust. For example, adding datatypes to a DynamicFrame NULLs out anything that fails to cast, without warning. To me, this means that adding structured to data and working with data quality will be more or less hand spun. That's fine, but if one of the selling points of Glue is ""quicker to market"", it feels very shortsighted.

So, now I'm looking at Glue with Scala, and even that seems a little odd relative to vanilla Spark. Namely in that you cannot send a fat/uber JAR to the Spark executor. You *have* to include a .scala file, and Glue will compile this for you and then send it to the executors. That can be worked around a bit using something like [this](https://manta-innovations.co.uk/2020/02/01/aws-glue-quickstart/), but it seems odd to me that for a compiled language, I'm going to have to set my CI/CD up to include a naked .scala file.

At this point, AWS Glue (both PySpark and Spark + Scala) in general sort of feels like the Ruby on Rails for data engineering. Good for quick and dirty, but as soon as you start to paint outside the lines, it's going to be hell and be a battle every step of the way.

I'm really concerned with the long term viability and support of our data platform. I know PySpark can move plenty of data and be made to work here, but PySpark always kind of has a ""smell"" to it. I also know that Scala, while a bit more of a barrier to entry, will work. What I don't know is how much of a mess either will be say 1 or 2 years down the road. Some future concerns would be: how painful the codebase is to maintain/test/fix, having a developer experience that isn't extremely painful, and hiring talent as this team expands (it'll be way easier to get Python people than Scala people).

I'm interested in how some of you have made the decision between ""PySpark vs Scala"" as well as how that decision changes (if at all) when going with AWS Glue. What did you do and did it work? If not, what do you wish you would have done different?"
2898,2020-07-25 06:21:13,1595647273.0,dataengineering,How do you stay updated ?,hxftn0,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/hxftn0/how_do_you_stay_updated/,1.0,8.0,0.0,16468.0,"Hi Guys, 

It's  Saturday morning and I grabbed the newspaper to see what's happening around .

And I wanted to do the same with big data , data engineering.

So I wanted to make a collection of different approaches people take to stay ""chic"" and updated in the data engineering industry.

Comment below any and every thing you do to stay abreast."
2899,2020-07-25 06:39:44,1595648384.0,dataengineering,DE Facebook-Onsite Interview,hxg2h5,DE_Data,,https://www.reddit.com/r/dataengineering/comments/hxg2h5/de_facebookonsite_interview/,0.0,10.0,0.0,16468.0,"Hi All,  


Can anyone help me with the preparation for DE position interview at Facebook for final round (onsite). Or if anyone recently appeared for that.

I just want to understand what kind of questions can i expect there.

Thanks in advance."
2900,2020-07-25 11:19:47,1595665187.0,dataengineering,Best Data Engineering Consulting Services,hxjaxq,Poojakhana52314,,https://www.reddit.com/r/dataengineering/comments/hxjaxq/best_data_engineering_consulting_services/,1.0,0.0,0.0,16477.0,
2901,2020-07-25 11:47:33,1595666853.0,dataengineering,8 Best Online Courses on Big Data Analytics You Need to Know in 2020,hxjktt,MlTut,,https://www.reddit.com/r/dataengineering/comments/hxjktt/8_best_online_courses_on_big_data_analytics_you/,2.0,11.0,0.0,16477.0," 

Hi Guys,

According to one report, By **2025**, it’s estimated that **463 exabytes** of data will be created each day globally – that’s the equivalent of **212,765,957** **DVDs** per day!  
**Shocking! Right?**

But it's good news for Big Data Learners. As Data is growing, the demand for Big Data Analysts is also growing.

So if you are planning to learn Big Data, but unable to find good resources for learning, then read this article-[https://www.mltut.com/best-online-courses-on-big-data/](https://mltut.us18.list-manage.com/track/click?u=daed40bf0a68af806ccb51607&amp;id=ce9d097f21&amp;e=3d7f03b9f8)

In this article, I have mentioned the 8 Best Online Courses on Big Data Analytics. I hope these courses will help you to start your Big Data Journey. 

All the Best!

Happy Learning!"
2902,2020-07-25 13:37:24,1595673444.0,dataengineering,Nuclear Energy Ranking | TOP 10 Country from 1970 to 2018,hxko90,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hxko90/nuclear_energy_ranking_top_10_country_from_1970/,1.0,0.0,0.0,16479.0,
2903,2020-07-25 14:43:54,1595677434.0,dataengineering,Docker Cluster of Apache Spark 3.0.0 and Hadoop 3.2,hxleft,pknerd,,https://www.reddit.com/r/dataengineering/comments/hxleft/docker_cluster_of_apache_spark_300_and_hadoop_32/,1.0,9.0,0.0,16480.0,
2904,2020-07-25 15:48:47,1595681327.0,dataengineering,ETL with AWS Lambda + SQS pipelines?,hxm6ir,rmhsilva,,https://www.reddit.com/r/dataengineering/comments/hxm6ir/etl_with_aws_lambda_sqs_pipelines/,1.0,5.0,0.0,16482.0,"Wondering whether this is a common / smart approach for ETL / data processing workflows.

[View Poll](https://www.reddit.com/poll/hxm6ir)"
2905,2020-07-25 16:47:12,1595684832.0,dataengineering,5 points that I need help about.,hxmzne,subhayang,,https://www.reddit.com/r/dataengineering/comments/hxmzne/5_points_that_i_need_help_about/,1.0,0.0,0.0,16484.0,"Hello guys,

This is my first post on reddit so wish me luck.

Now let's go straight to the point.

Let me tell briefly tell you guys about me. I am an average guy working as an Oracle EBS R12 ERP developer for the last 2 years. This job of mine requires, SQL , PL/SQL as coding languages. You might know, market for PLSQL is not growing and Oracle EBS itself is a dying technology. So I am willing to learn something new.

As an alternative, I started learning Python and my aim is to become a data engineer within the next 6 months or so.

But, to become a data engineer, one needs to master a lot of technologies like hadoop, spark, hive, AWS/Azure, Kafka, Apache Airflow, NoSQL databases, Shell/Python scripting etc. and learning these technologies will take time and not an easy task.

Considering my experience in SQL and PLSQL, and Python, what should be my next area of focus?

Let me point out my queries:

1. What are the basic skills that can get me a job as an entry level data engineer in India, say in 6 months? Please give me a list. I went through a bunch of job postings and the requirements given are daunting to say the least.
2. I have some familiarity with Python and can create basic scripts but I came to know that knowing Scala (Spark-Scala) is required. Is it really needed or pyspark is enough?
3. What about Jenkins, Docker, Kubernetes? Are those needed too?
4. Can you please provide me a list of resources?
5. This one is what I need the most: can you hook me up with someone who can mentor people like me. Reached out to many people via linkedin but these days people are busy so...

I know there are a lot of similar threads present in reddit but writing this post somewhat comforts me so I did it anyway.

Thank you very much for your kind attention and I hope I have not offended anyone."
2906,2020-07-25 17:23:51,1595687031.0,dataengineering,Combining multi-tenant source data into single warehouse,hxnixy,jaredstufft,,https://www.reddit.com/r/dataengineering/comments/hxnixy/combining_multitenant_source_data_into_single/,1.0,4.0,0.0,16486.0,"Hi everyone,

&amp;#x200B;

I have a multi-tenant source postgres database architecture hosted on AWS. Each tenant has at least their own postgres database, and many have their own RDS. I'm working to combine these tenant schemas into a single analytics warehouse. All tenants have the same data model/schema so combining the data sets is not too hard, but I wanted to reach out to some communities and see if anyone has any best practices for this type of process.

&amp;#x200B;

Most specifically, I'm wondering what people do with the primary keys from the source tenant tables once they all get combined into the single-tenant warehouse. They're auto-incremental integers, so they'll be duplicated across the tenants i.e. if there's an \`Events\` table with an \`id\` primary key field, pretty much all tenants will have an event \`id == 1\`. My current solution is to re-assign an auto increment id field as the primary key for each table, then having an additional \`source\_id\` field and a \`tenant\` field which are unique together and can be used to re-construct the relationships from each tenant. It also allows us to check parity between the source and warehouse, i.e. if someone sees something wrong with an \`Event\` record in the warehouse, we can use the source id to check it out in the source and see if there's a bug in the ETL or a bug in the application.

&amp;#x200B;

Is this how other people solve this problem? Are there any other best practices or tips you take in this situation?

&amp;#x200B;

Thanks!!"
2907,2020-07-25 20:13:53,1595697233.0,dataengineering,Data Leak,hxqa5h,ProfessionalClub1003,,https://www.reddit.com/r/dataengineering/comments/hxqa5h/data_leak/,1.0,0.0,0.0,16493.0,
2908,2020-07-25 20:44:20,1595699060.0,dataengineering,"After SQL and Python, what language is next?",hxqtb8,Luukv93,,https://www.reddit.com/r/dataengineering/comments/hxqtb8/after_sql_and_python_what_language_is_next/,1.0,19.0,0.0,16495.0,"Hello,

I feel confident about my SQL and Python knowledge that I gained during 2 years of studying and working with the languages doing ELT and data analysis related work.

Now that I want develop myself towards more Data Engineering related work (using the Azure Cloud) I wonder what language should be next to learn.

Tasks for the upcomming years include setting up a data platform for around 300/400 business users. Platform requires batch and streaming pipelines and most likely also an IoT Hub to extract IoT data from the production plant. We use the Azure Cloud.

What language/skills would you recommend me to learn/develop?"
2909,2020-07-26 04:20:19,1595726419.0,dataengineering,Is Github Actions with AWS Lambda Python Functions and API Gateway really a modern foundation for building and deploying data-driven applications?,hxy3vh,asheriff91,,https://www.reddit.com/r/dataengineering/comments/hxy3vh/is_github_actions_with_aws_lambda_python/,1.0,1.0,0.0,16506.0,
2910,2020-07-26 04:26:20,1595726780.0,dataengineering,Data architecture resources,hxy6y9,aleebit,,https://www.reddit.com/r/dataengineering/comments/hxy6y9/data_architecture_resources/,1.0,7.0,0.0,16507.0,"Not an English-speaking native here. 
 I just started reading the highly recommended Desing Data Intensive applications and would like more study materials (courses, books, articles) about data architecture.
Can you guy's help me?"
2911,2020-07-26 13:49:14,1595760554.0,dataengineering,Women Obesity Ranking | TOP 10 Country from 1975 to 2016,hy4ynq,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hy4ynq/women_obesity_ranking_top_10_country_from_1975_to/,1.0,0.0,0.0,16515.0,
2912,2020-07-26 19:26:47,1595780807.0,dataengineering,AutoViz: A new tool for Automated Visualization in Data Science.,hy9sru,8329417966,,https://www.reddit.com/r/dataengineering/comments/hy9sru/autoviz_a_new_tool_for_automated_visualization_in/,1.0,0.0,0.0,16523.0,https://youtu.be/aiKa6QCV0qQ
2913,2020-07-26 19:38:01,1595781481.0,dataengineering,Big BPM is coming,hy9zim,MaximFateev,,https://www.reddit.com/r/dataengineering/comments/hy9zim/big_bpm_is_coming/,1.0,0.0,0.0,16524.0,
2914,2020-07-26 22:46:14,1595792774.0,dataengineering,Any advice for an Data Analyst transitioning to a Solutions Architect position?,hyddsj,rowdyllama,,https://www.reddit.com/r/dataengineering/comments/hyddsj/any_advice_for_an_data_analyst_transitioning_to_a/,1.0,7.0,0.0,16535.0,"I’m a self-taught data analyst who started in the field about 5 years ago. Today I’m an analyst supporting a sales team at a Big N tech company. I’m highly proficient in SQL, Python, and Linux. I’m also in client facing, which includes interpreting business problems and building data solutions for those problems.

I have some some experience building ETLs, and over the last 2 years I’ve worked as a liaison between my team and our Data Engineering team. My primary responsibility on this project was to prioritize requests and secure resourcing from our DE team.

The new role is internal-only. My primary responsibility will be collecting requirements from data consumers and translating them into a data model that meets those requirements. I will then work with our DS/DE teams to build the solutions we design. Essentially I’m the project manager for the data model that powers the strategy/operations of our business.

I’ve never worked in a non-sales position. I’m looking for general advice about best practices and big do’s/don’ts of a role like this. If you’ve worked with/as a Solutions Architect before, what should I be careful to do or not do?

Any advice would be greatly appreciated. Thanks in advance."
2915,2020-07-27 00:26:23,1595798783.0,dataengineering,Data Engineer Switch,hyf5r6,hyperandaman,,https://www.reddit.com/r/dataengineering/comments/hyf5r6/data_engineer_switch/,1.0,10.0,0.0,16542.0,"I currently work as a data analyst and have wanted to switch into a data engineering role. I know cloud is very important these days and have been seeing lot of courses from Udemy, Coursera, Udacity and Linux academy for their their cloud certification programs. I have proficient knowledge and experience using SQL but no other languages. 


Does it make sense to take these courses without having proficient understanding of a programming language (i.e. java, python, etc.)

Or should I start learning a language and then jump on these courses after?"
2916,2020-07-27 06:44:31,1595821471.0,dataengineering,Databricks: Save a variable value even when cluster is off,hykysb,rapp17,,https://www.reddit.com/r/dataengineering/comments/hykysb/databricks_save_a_variable_value_even_when/,1.0,5.0,0.0,16553.0,I want to save a variable value in a Databricks notebook even when I change clusters or the cluster is detached. What is the best way to achieve this?
2917,2020-07-27 11:12:32,1595837552.0,dataengineering,Data Cleaning in Python Which Enhances Accuracy,hyo7ge,instigator-001,,https://www.reddit.com/r/dataengineering/comments/hyo7ge/data_cleaning_in_python_which_enhances_accuracy/,1.0,5.0,0.0,16558.0,
2918,2020-07-27 16:21:20,1595856080.0,dataengineering,Webinar on How to Learn Data Science: The Path to Becoming a Data Scientist,hys579,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/hys579/webinar_on_how_to_learn_data_science_the_path_to/,1.0,0.0,0.0,16567.0,
2919,2020-07-27 18:09:29,1595862569.0,dataengineering,Choosing the right tech,hyu0gj,Spare_cog,,https://www.reddit.com/r/dataengineering/comments/hyu0gj/choosing_the_right_tech/,1.0,19.0,0.0,16573.0,"Hi guys,

I'm building my first data engineering project and it basically should extract from an APi some data about job postings, go through a message queue, get connected to a processing framework and then get stored in a Postgre database and later on display some basic analytics about the data that I've collected. The problem is that I'm having some issues balancing what technologies could be more noob friendly, achievable on a single 5 years old laptop (if this is an actual restriction) and would actually make a good project to display some level of competency.

Any help would be greatly appreciated and I would be putting the project out there with some tutorials for whoever would like to learn data engineering."
2920,2020-07-27 18:40:42,1595864442.0,dataengineering,Visualization of ML Models Metrics,hyulfw,Mhayc_en,,https://www.reddit.com/r/dataengineering/comments/hyulfw/visualization_of_ml_models_metrics/,1.0,0.0,0.0,16575.0,"In the context of my internship i wanted to use and learn as much of The data engineering Hot tech as possible , my supervisor assgined me the project of creating a visualization of machine learning metrics with Grafana and told me to explore an architecture suited for such a project . 

I know Grafana works well with Prometheus to monitor systems , however the metrics that i'm gooing to etl are MRE, SD, and Mean basically machine learning prediction metrics . 

I want to ingest the data into Kafka  and use it as a datasource on Grafana all in a web app using Flask.

but i have no experience , my vision is lacking details . If there is any help as to how to setup the current tech stack and if how to add airflow to the equation if its not going to be useless or unenecessary , thank you"
2921,2020-07-27 18:41:24,1595864484.0,dataengineering,Survey about data annotation market,hyulxu,yongen96,,https://www.reddit.com/r/dataengineering/comments/hyulxu/survey_about_data_annotation_market/,1.0,1.0,0.0,16575.0,"Hi, everyone. We are a startup that provides data annotation service to customers that need to build deep learning models. We need some feedbacks from the market.

Could you take a few minutes to fill in the form? Form: [https://airtable.com/shrDqTkbaqB0BLgw0](https://airtable.com/shrDqTkbaqB0BLgw0)

Thank you."
2922,2020-07-27 20:20:36,1595870436.0,dataengineering,VIM vs VSCode (with shortcut proficiency) or other editors,hywiw6,kvotheTHEinquisitor,,https://www.reddit.com/r/dataengineering/comments/hywiw6/vim_vs_vscode_with_shortcut_proficiency_or_other/,1.0,4.0,0.0,16579.0,"Would love to hear some perspectives on this.  I'm fairly new to DE and engineering and find myself mostly using VSCode native shortcuts (plus some custom ones) for code editing and dbeaver for SQL.  

Wondering how essential it is to know VIM, do you think employers or other engineers judge you if you don't know VIM?"
2923,2020-07-27 21:01:53,1595872913.0,dataengineering,Data Engineering Weekly Newsletter [https://dataengineeringweekly.substack.com],hyxc9o,vananth22,,https://www.reddit.com/r/dataengineering/comments/hyxc9o/data_engineering_weekly_newsletter/,1.0,0.0,0.0,16585.0,"I started a newsletter to share what I’m learning every week. Please subscribe to [https://dataengineeringweekly.substack.com](https://dataengineeringweekly.substack.com/).  
The first week’s issue is out. [https://dataengineeringweekly.substack.com/p/data-engineering-weekly-1](https://dataengineeringweekly.substack.com/p/data-engineering-weekly-1)"
2924,2020-07-27 22:42:17,1595878937.0,dataengineering,How to work with huge dataset (600 GB+) stored in nested JSON format in S3?,hyzaew,mdocvar,,https://www.reddit.com/r/dataengineering/comments/hyzaew/how_to_work_with_huge_dataset_600_gb_stored_in/,1.0,1.0,0.0,16592.0,
2925,2020-07-27 22:50:58,1595879458.0,dataengineering,What is wrong in just using a single python script as an ETL solution?,hyzggv,thor123321,,https://www.reddit.com/r/dataengineering/comments/hyzggv/what_is_wrong_in_just_using_a_single_python/,1.0,39.0,0.0,16593.0,"Hey,

I am looking through some ETL solutions such as Data Factory, but why not just use a single python script to do all of Extract, Transform and Load?

Extraxt data from an API(requests), Transform it using Pandas, json, and then just Load it in to a database using pyodbc..

Im i missing some fundamental best practices here?"
2926,2020-07-27 22:51:41,1595879501.0,dataengineering,What different ways your company send and retrieve data ?,hyzgz6,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/hyzgz6/what_different_ways_your_company_send_and/,1.0,0.0,0.0,16593.0,"I am genuinely interested on how everyone company implement this ? My company infra is with AWS, We retrieve data from  external company via sftp  and provide them a key to do this  and actually we mount the EC2 to an S3 bucket  and the data get stored in S3.  
Within our company, to retrieve data from different team either they give my team read permission of a S3 bucket or  we use kinesis data stream for them to send data and create IAM role in our side for them to assume.  What are the security requirement do you undertake  when sending and receiving data ?"
2927,2020-07-27 23:50:08,1595883008.0,dataengineering,Looking for guidance on a solution to cleaning data,hz0ktr,MisterGlasss,,https://www.reddit.com/r/dataengineering/comments/hz0ktr/looking_for_guidance_on_a_solution_to_cleaning/,1.0,2.0,0.0,16592.0,"I'm trying to get into more data engineering with the group I'm in, I have more of an IT background.  I'm learning python and SQL and have a couple AWS certs.  I'd like to grow more by doing real world problems and came across this issue we are having.

Issue: We are putting .csv's that are in S3 into Redshift and there is a particular column if where values are slightly different but are and should be changed to be the same.  As an example, say the column is gaming consoles and you just want to have the values be Xbox or Playstation but the values could be xbox, XBOX, XBOX1, Xbox Series X, Xbox One, Playstation, playstation, Playstation3, playstation 4 pro, etc. 

Should the data get cleaned while still in S3 and what tools/methods would be most efficient or are there advantages of having it in Redshift and cleaning it there?  Redshift gets updated daily by these CSV's that could be tens of GB and the Redshift database is about 8TB at the moment."
2928,2020-07-28 05:04:09,1595901849.0,dataengineering,"My study plan, please give an opinion",hz5xp1,aleebit,,https://www.reddit.com/r/dataengineering/comments/hz5xp1/my_study_plan_please_give_an_opinion/,1.0,10.0,0.0,16609.0,"Hey guys, first of all, non a native speak englesh here.

I'm a ex BI/ETL developer, with 4yr of IBM DataStage and 3yr on Hadoop.   
Can you guys look at my study plan below and give some tips?

https://preview.redd.it/x3gpwq8g9id51.png?width=1404&amp;format=png&amp;auto=webp&amp;s=90d2b65c9bd053aef97ae689966f7e091e2da7cb"
2929,2020-07-28 08:03:59,1595912639.0,dataengineering,Apache Spark &amp; Databricks Tutorial : Read Data From Azure Blob Storage |...,hz8ifz,gwadson,,https://www.reddit.com/r/dataengineering/comments/hz8ifz/apache_spark_databricks_tutorial_read_data_from/,1.0,0.0,0.0,16619.0,
2930,2020-07-28 08:53:45,1595915625.0,dataengineering,Best Comparison of Data Analyst vs Data Scientist,hz94wd,codeavail_expert,,https://www.reddit.com/r/dataengineering/comments/hz94wd/best_comparison_of_data_analyst_vs_data_scientist/,1.0,0.0,0.0,16620.0,
2931,2020-07-28 09:19:00,1595917140.0,dataengineering,The Most Powerful Data Science Tools in 2020,hz9g18,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/hz9g18/the_most_powerful_data_science_tools_in_2020/,1.0,0.0,0.0,16620.0,
2932,2020-07-28 09:34:18,1595918058.0,dataengineering,The Most Important Statistics for R to Get Started With Data Science,hz9mn5,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/hz9mn5/the_most_important_statistics_for_r_to_get/,1.0,0.0,0.0,16620.0,
2933,2020-07-28 13:19:25,1595931565.0,dataengineering,Hong Kong vs Singapore | GDP from 1960 to 2018,hzc60p,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hzc60p/hong_kong_vs_singapore_gdp_from_1960_to_2018/,1.0,0.0,0.0,16624.0,
2934,2020-07-28 14:35:17,1595936117.0,dataengineering,Free Webinar on How to Learn Data Science | In Collaboration With IBM,hzd2yn,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/hzd2yn/free_webinar_on_how_to_learn_data_science_in/,1.0,0.0,0.0,16626.0,
2935,2020-07-28 15:36:53,1595939813.0,dataengineering,An interview with Jepsen creator Kyle Kingsbury about what he has learned about distributed systems by breaking them.,hzdw5j,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/hzdw5j/an_interview_with_jepsen_creator_kyle_kingsbury/,1.0,0.0,0.0,16628.0,
2936,2020-07-28 16:30:05,1595943005.0,dataengineering,Is Kafka and Node.js a bad combination?,hzeob0,smpvlc,,https://www.reddit.com/r/dataengineering/comments/hzeob0/is_kafka_and_nodejs_a_bad_combination/,1.0,5.0,0.0,16630.0,"Good afternoon,

In"" Data Lake for Enterprises"" by Tomcy John; Pankaj Misra, they mention Kafka and Node.js as a combination to avoid.

I have read about Node.js being a bad choice for processes that are CPU intensive, common in Big Data , but I was wondering if this would also be the case for webapps that feed on real-time data via sockets, that are used to create real time dashboards or relative simple stream processing using KSQL.

I haven't found that many examples on the web regarding node.js and Kafka, so I would appreciate if somebody had some input on this.

Many thanks!"
2937,2020-07-28 19:03:30,1595952210.0,dataengineering,Data mesh architecture -- does it make sense?,hzhaqd,mkvor8,,https://www.reddit.com/r/dataengineering/comments/hzhaqd/data_mesh_architecture_does_it_make_sense/,1.0,9.0,0.0,16640.0,"Last year, Zhamak Dehghani, an engineer from ThoughtWorks, published an article introducing the **data mesh**, a database architecture that borrows principles of microservice architectures by leveraging self-serve, domain-driven design. Learn more: [https://towardsdatascience.com/what-is-a-data-mesh-and-how-not-to-mesh-it-up-210710bb41e0?source=friends\_link&amp;sk=ac9b28f381e16ea2f33a193ad77c7568](https://towardsdatascience.com/what-is-a-data-mesh-and-how-not-to-mesh-it-up-210710bb41e0?source=friends_link&amp;sk=ac9b28f381e16ea2f33a193ad77c7568)

My question to you: **to mesh or not to mesh?** Has anyone implemented the data mesh paradigm and if so, what was your experience like? 

&amp;#x200B;

[View Poll](https://www.reddit.com/poll/hzhaqd)"
2938,2020-07-28 20:24:21,1595957061.0,dataengineering,Real-time Streaming Pipeline vs Web Service,hzitgk,addictzz,,https://www.reddit.com/r/dataengineering/comments/hzitgk/realtime_streaming_pipeline_vs_web_service/,1.0,1.0,0.0,16646.0,"Hi folks, I am trying to find value of using real-time streaming pipeline over simple web service.

Here is my understanding &amp; research for the 3 types of data pipeline so far (overly compressed):

* Batch: Process numerous data in certain time interval.
* Micro-batch: Similar to batch however shorter time interval (probably in 3 digits miliseconds) and much smaller batch of data.
* Real-time Streaming: Immediate data processing. Main difference with microbatch is that streaming processes in 1 single continuous job, micro-batch processes in jobs (due to lots of small batches to process).

Then I am thinking, how real-time streaming pipeline differs from a simple web service that runs continuously? Let's say I have this scenario:

    I have a stream of tweets (string data) going into a data-streaming pipeline. In the pipeline, I am adding the text `processed` into every single of those tweets. I then dump the output into a PostgreSQL database.

I think I can accomplish those with a simple Flask API (AWS Lambda probably works too) which runs a function that adds \`processed\` text and store it in PostgreSQL. The stream of tweets will be inputted into the Flask API using HTTP api call. 

&amp;#x200B;

So i am confused what's the value of using real-time data pipeline over having simple web service like above?"
2939,2020-07-28 21:03:54,1595959434.0,dataengineering,Storage Engine Considerations for Your Apache Spark Applications - Mladen Kovacevic,hzjko9,youareafakenews,,https://www.reddit.com/r/dataengineering/comments/hzjko9/storage_engine_considerations_for_your_apache/,1.0,0.0,0.0,16650.0,
2940,2020-07-28 22:34:26,1595964866.0,dataengineering,Dynamic task generation in Airflow: how can I schedule a subsequent task after dynamically generated tasks?,hzlarb,tylerjaywood,,https://www.reddit.com/r/dataengineering/comments/hzlarb/dynamic_task_generation_in_airflow_how_can_i/,1.0,2.0,0.0,16652.0,"I want to have the for loop generating tasks dynamically so that this portion is extensible. After all of those are completed, I need to run some SQL. 

When I run `list_task` all the tasks show up. Also, `airflow test &lt;dag_id&gt; vdd_sql &lt;date&gt;` to test the task runs with no issues.

However, the scheduler doesn't seem to be picking it up, and there are no traces of this task in the Airflow UI and I wonder if there is something I am missing about how scheduling works to put a defined task behind a set of dynamically generated tasks. 


    # imports pull_n_push
    # boilerplate dag definition
    
    collections = ['foo', 'bar']
    
    for c in collections:
        task = PythonOperator(
            task_id='pull_push_'+c,
            python_callable=pull_n_push,
            op_kwargs={...},
            dag=dag
        )
    
    vdd_sql = PostgresOperator(
        task_id='vdd_sql',
        postgres_conn_id='...',
        sql='''
            SELECT 1+1;
            ''' ,
        dag=dag
        )
    
    task &gt;&gt; vdd_sql"
2941,2020-07-28 23:42:02,1595968922.0,dataengineering,Insert and Update into a table Databricks R,hzmlvx,rapp17,,https://www.reddit.com/r/dataengineering/comments/hzmlvx/insert_and_update_into_a_table_databricks_r/,1.0,4.0,0.0,16655.0,"I can insert new data into an existing parquet table using saveAsTable(.., mode = ""append""). However, this keeps adding data to the table even if the data already exists in the table. How can I avoid adding duplicate data to the table?"
2942,2020-07-29 03:29:44,1595982584.0,dataengineering,R in data engineering,hzqjzp,hankyun,,https://www.reddit.com/r/dataengineering/comments/hzqjzp/r_in_data_engineering/,1.0,6.0,0.0,16666.0,How prevalent is R in the world of Data engineering? Is it worth learning more than the beginner level? is it beneficial from a career perspective?
2943,2020-07-29 04:55:10,1595987710.0,dataengineering,How to break through as a consultant,hzrvuz,CesQ89,,https://www.reddit.com/r/dataengineering/comments/hzrvuz/how_to_break_through_as_a_consultant/,1.0,20.0,0.0,16672.0,"There is a massively high demand in my market for Data Engineering (data work in general) and even though I'm very happy with my current employer I wouldn't mind trying to moonlight as a consultant part time. My current work is fully remote till likely next summer and we are encouraged to take as much time off as we need for mental health purposes so I see this as an opportunity.

What's the first step? How do I build a portfolio if most my work has been with an employer full time?

I'm pretty bang average in a wide context but above average if scope is limited to local talent.

Tech stack is: Spark (Python and SQL), AWS/Azure/GCP (I work for a big segmented shop), Docker, Terraform, Oracle and SQL Server DBs, Java (very little tbh), Teradata, and a lot more.

No recruiters. Would like to be fully independent."
2944,2020-07-29 06:29:17,1595993357.0,dataengineering,Serverless Real-time Indexing: A Low Ops Alternative to Elasticsearch for Real-time Analytics,hztb0z,ssb61,,https://www.reddit.com/r/dataengineering/comments/hztb0z/serverless_realtime_indexing_a_low_ops/,1.0,0.0,0.0,16678.0,"**Tech Talk**

Save your spot: [https://rockset.com/elasticsearch-talk/](https://rockset.com/elasticsearch-talk/)

Both Rockset and Elasticsearch are queryable datastores that can store data and serve queries. Both of them index data and use the index to serve queries. Both systems are document-sharded. But that is where their similarities end. Rockset is a serverless realtime indexing database built to exploit cloud elasticity with minimal ops, while Elastic requires special expertise and effort to manage the ELK stack.

Ben Hagan, a former solutions architect from Elasticsearch, and Shruti Bhat will go over some of the ops considerations for deploying and managing elastic clusters at scale, as we compare and contrast to Rockset’s serverless ops in the cloud

* Collecting real-time events, managing change data capture and denormalization: With Elasticsearch, you manage different ingestion and input source pipelines to denormalize data. In contrast, Rockset supports click and connect integrations for continuously indexing data from MongoDB, DynamoDB, Kafka, S3 etc. It has native support for JOINs, so there is no need to denormalize your data.
* Configuring clusters and managing node types: When deploying Elasticsearch, it is important to configure master nodes, data nodes, ingest nodes, coordinating nodes, alerting nodes in your cluster and optimize them based on the use case and requirements. In contrast, Rockset is a modern serverless system that is highly optimized for fast queries out-of-the-box
* Scaling writes, sharding and re-indexing: Elasticsearch uses a primary-backup model for replication so each replica re-indexes the data locally again. As your data size grows, you may will typically increase the shard size and re-index your data in elasticsearch. In contrast, Rockset uses RocksDB remote compaction and micro-sharding to eliminate the need for re-indexing overhead.
* Scaling reads and isolating workloads: The Elastic Cloud offers different types of nodes each with fixed compute/memory ratios such as io-optimized and storage-optimized nodes, and moving between these requires a data migration. In contrast, Rockset separates compute from storage to allow seamless scaling of reads by increasing the compute allocation in the form of fully isolated virtual compute for each workload.
* Managing data durability and performance: Elasticsearch assumes a shared-nothing storage architecture where data durability is guaranteed via replication among data nodes, and you manually configure the resiliency of new writes. Rockset uses the cloud’s storage model with automatic S3-backed durable storage already configured in the cloud.

Save your spot: [https://rockset.com/elasticsearch-talk/](https://rockset.com/elasticsearch-talk/)"
2945,2020-07-29 07:16:40,1595996200.0,dataengineering,Big Data / Blockchain Project,hztz77,daniel75007,,https://www.reddit.com/r/dataengineering/comments/hztz77/big_data_blockchain_project/,1.0,1.0,0.0,16681.0,"Looking for ideas for a project building an ETL pipeline, using both big data and blockchain.

From what I've read, the main applications are fraud detection and price prediction.

Any idea of other interesting and not too complicated projects mixing the 2?

Thanks !"
2946,2020-07-29 07:48:03,1595998083.0,dataengineering,Snowflake COPYINTO,hzuerc,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/hzuerc/snowflake_copyinto/,1.0,3.0,0.0,16684.0,"We would want to execute Snowflake copy into command . 

 COPY INTO TEST\_DATA FROM u/TEST/test.csv.gz FILE\_FORMAT = (   TYPE=CSV , FIELD\_DELIMITER = ';'  , FIELD\_OPTIONALLY\_ENCLOSED\_BY='""' );

Which is good, Glue or Lambda or EC2 is the best option to execute this in terms of **COST .**

Overall file size is approx 50GB. Snowflake is having Medium size cluster.

OR

Docker"
2947,2020-07-29 08:09:28,1595999368.0,dataengineering,What is the point of logical data modeling?,hzuozk,Basketball_data,,https://www.reddit.com/r/dataengineering/comments/hzuozk/what_is_the_point_of_logical_data_modeling/,1.0,4.0,0.0,16684.0,Does this just show the ultimate tables that we want in the end? Do all the separate entities represent tables we want in our database?
2948,2020-07-29 11:06:26,1596009986.0,dataengineering,Suggestions,hzwsaj,pratzeh,,https://www.reddit.com/r/dataengineering/comments/hzwsaj/suggestions/,1.0,4.0,0.0,16692.0,Hey guys! I'm a college student graduating in about less than a year. I'm still confused as to what I want to do after I graduate. What are the skills I'll need for a career in data engineering.?I'm kind of noob but I kinda hoping I get some clarity on what are the responsibilities involved in a career such as this. Thanks
2949,2020-07-29 13:21:21,1596018081.0,dataengineering,Smoking Death Rate Ranking | TOP 10 Country from 1990 to 2017,hzy9t1,datavtworld,,https://www.reddit.com/r/dataengineering/comments/hzy9t1/smoking_death_rate_ranking_top_10_country_from/,1.0,0.0,0.0,16697.0,
2950,2020-07-29 13:41:02,1596019262.0,dataengineering,Azure data factory pricing history and future,hzyht9,Zyklon00,,https://www.reddit.com/r/dataengineering/comments/hzyht9/azure_data_factory_pricing_history_and_future/,1.0,0.0,0.0,16697.0,"We are considering ADF as a solution. It is not easy to get a price estimate because pricing is all over the place. But with the tests we are doing now, we are able to get an idea. We are looking for a long-term solution and since some products (like data flows) are very new, I expect prices to go up significantly. I would like to get an idea how much this price will go up.
Suppose today I have a solution that costs 100$/ month. How much (roughly!) would the same solution have cost me 1 or 2 years ago? How much do you expect the cost will be after 1, 2 years."
2951,2020-07-29 14:16:46,1596021406.0,dataengineering,Flink SQL Demo: Building an End-to-End Streaming Application,hzyx08,Marksfik,,https://www.reddit.com/r/dataengineering/comments/hzyx08/flink_sql_demo_building_an_endtoend_streaming/,1.0,0.0,0.0,16699.0,
2952,2020-07-29 17:32:28,1596033148.0,dataengineering,Semantic Versioning of Data Models - Structure.rest,i01pa6,punknight,,https://www.reddit.com/r/dataengineering/comments/i01pa6/semantic_versioning_of_data_models_structurerest/,1.0,0.0,0.0,16703.0,"Hi everyone, this is Daniel with the Structure.rest team. We make analyzing your data easier using a graph-based editor to organize your queries into pipelines. Here's a quick [Youtube video] (https://www.youtube.com/watch?v=8uaov4xm764&amp;feature=reddit) to get a better idea of what we do.

We’ve been doing customer interviews for the past couple weeks, and the one feature that is a “table stakes”, “must-have”, “basic need” for all of the data engineers that we interviewed was version control. I made this [video] (https://youtu.be/gVx4JhugCUc) showing how we implemented version control. I built a simple version control menu that connects up to the GitHub Rest API (v3). At first, I thought this would be enough, but as I have talked to more people, the picture becomes clear that this is not a simple problem. If any of you guys or gals have similar problems please reach out. We’d be interested in learning about the problem, so we can offer better solutions in the future.

In data engineering, version control can be useful for situations such as when data sources change, ETL automation services change, schemas change, or when business goals change. The big problem is that you don’t want to either start from scratch or refresh all of your tables from scratch when some change happens upstream of the models you are currently working on. I think semantic versioning is an excellent solution to this problem.

Here’s the idea:
	You start with a simple graph, and each model in that graph can be labeled or tagged as version 0.0.1. Downstream models can be added to the graph with impunity, and they will be similarly labeled with the version tag 0.0.1. Now, say I’m working with Google Analytics data, and I realize that I need to add a dimension to my data source, such as referralPagePath in order to have access to that column downstream. Adding this column to my data source doesn’t actually affect any of my data models yet, but the data source is different, so I should at least label it with a new version 0.1.1 and make a commit message explaining why the new version exists. Next, I add my downstream model to aggregate analytics based on referalPagePath. This model doesn’t work until all of my models in the SQL pipeline between my new model and my data source have been updated, so I will relabel the version of each of these models to version 0.1.1 as well. 

Now a team member can easily look at my graph, and my commit history to understand the work I did that day. This follows the convention of semantic versioning (Major.Minor.Patch), with Major changes being non backward compatible for the entire graph (such as swapping data sources), minor changes being backward compatible for most of the graph, and patch changes being small updates of individual models within the graph.

Here's a [blog article] (https://www.structure.rest/blog/semantic-versioning-of-data-models) that talks about the problem just a little bit more. If this kind of stuff excites you please free to check us out at https://structure.rest or visit or [slack] (https://join.slack.com/t/structuresupport/shared_invite/zt-ddx04ho4-_q43i5o3zQ9jv00qx~dx8A)"
2953,2020-07-29 20:14:15,1596042855.0,dataengineering,"So do anyone use DigitalOcean, Heroke, Linode cloud platforms for professional solutions, or are they only loved by youtubers and podcast hosts?",i04flk,thor123321,,https://www.reddit.com/r/dataengineering/comments/i04flk/so_do_anyone_use_digitalocean_heroke_linode_cloud/,1.0,12.0,0.0,16707.0,"If you have used them for professional use, what do you find as the benefit?"
2954,2020-07-29 21:01:51,1596045711.0,dataengineering,"Data Visualization using ""Matplotlib""| Part_2",i05bd4,8329417966,,https://www.reddit.com/r/dataengineering/comments/i05bd4/data_visualization_using_matplotlib_part_2/,1.0,0.0,0.0,16708.0,https://youtu.be/GNV-ozvV7dc
2955,2020-07-29 21:16:51,1596046611.0,dataengineering,Reasons for serveless data infrastructure,i05l8z,CogLuca,,https://www.reddit.com/r/dataengineering/comments/i05l8z/reasons_for_serveless_data_infrastructure/,1.0,3.0,0.0,16708.0,"To make a long story short, while doing a data related project of mine I started wondering the reasons for which a company would choose to have a cloud infrastructure for their data vs the reasons for which they should have their own infrastructure and maybe just rent a data center ? Is there a clear distinction between these choices ? Is it infrastructure size dependent ? Could it be about compliance to Gdpr laws ? Lower costs on the short-term ?"
2956,2020-07-29 22:40:56,1596051656.0,dataengineering,Developing DE skills,i0761i,Luukv93,,https://www.reddit.com/r/dataengineering/comments/i0761i/developing_de_skills/,1.0,4.0,0.0,16709.0,"Hello,

Over the last 2 years I have studied Python and SQL and used them in my work as a data analyst. Now that I figured data engineering is much more exciting than data analysis I wonder where I could develop my Python coding skills on the following topics:

\- Batch processing:

Extracting data from API's, sensors and databases and move them to a data lake.

\- Stream processing:

Streaming sensor data and do some anomaly detection.

\- Data wrangling

I often face data that I need to process in order to use it. 

&amp;#x200B;

I hope you can provide me with tips on how to improve my skills on above topics. Maybe you know some projects to practise on that can be reviewed by a data engineer? All help is welcome."
2957,2020-07-30 00:23:22,1596057802.0,dataengineering,How to research which database/data warehouse is right for business?,i0927i,phl3gmatic,,https://www.reddit.com/r/dataengineering/comments/i0927i/how_to_research_which_databasedata_warehouse_is/,1.0,8.0,0.0,16716.0,"Hi, sorry if this is not the right place to post. I would appreciate it if someone can point me in the right direction. 

I will start soon as a data analyst for a financial services company. The company has no data team, let alone a database. I will be the sole data analyst. I will mostly be a reporting analyst at the start but they are expecting me to start looking into researching/setting up a database/data warehouse with the IT department 6 months from now. 

Problem is, I have zero database experience. I was mostly an end-user in the data pipeline. So, where do I start? Where can I begin learning on what data warehouse would be right for the business? What are some questions I should start asking? 

I’m trying to get a head start. The company is willing to send me off for training, but what should I be looking for? 

Some context from what I gathered: financial services industry. I’ll be working with Salesforce data and financial data from clients.

Thank you in advance!"
2958,2020-07-30 00:27:43,1596058063.0,dataengineering,Folder/table structure for persistence in Airflow pipeline,i0951c,baddays79,,https://www.reddit.com/r/dataengineering/comments/i0951c/foldertable_structure_for_persistence_in_airflow/,1.0,0.0,0.0,16716.0,"Hi everyone,

I made a post a bit ago about storing intermediate results of pipeline stages and your responses were very helpful, so I was hoping to receive some input on another problem.

I am reconfiguring a Python script into an Airflow pipeline that does the following:

* Request data from API and save as .json to S3
* Load from S3, transform, load into RDS Postgres
* Load from RDS Postgres, transform, load into RDS Postgres

Most of the Airflow tutorials I've seen involve creating temp tables that don't persist after the workflow has completed, but I want to persist the results so I can easily locate where my algorithms are failing and improve them.

Here are some options I'm evaluating:

* Monolithic table per pipeline stage, e.g. stage\_##.table\_##
   * Pros
      * Can easily query all historical data with no joins
      * Simple - fewer tables
   * Cons
      * Must somehow indicate to next stage which records to process, maybe by pushing list of row IDs through XCOM, then next stage filters using query WHERE id IN (a, b, c)
* Temp/Archive tables per pipeline stage, e.g. stage\_##.table\_##\_temp and stage\_##.table\_##\_archive
   * Pros
      * Don't have to pass anything through XCOM, and no WHERE id IN (a, b, c)
      * Can still query historical data with no joins
   * Cons
      * Need to copy records from temp table into archive table, maybe requires additional operator

What are your best practices for keeping persistent data per stage in your data warehouse? Is there something obvious I'm missing?

Thanks for your help!

Resources:

[http://cognosl.blogspot.com/2016/01/data-warehouse-data-warehouse-is.html](http://cognosl.blogspot.com/2016/01/data-warehouse-data-warehouse-is.html)

[https://www.sqlchick.com/entries/2019/1/20/faqs-about-organizing-a-data-lake](https://www.sqlchick.com/entries/2019/1/20/faqs-about-organizing-a-data-lake)"
2959,2020-07-30 01:56:49,1596063409.0,dataengineering,low effort data engineering platform,i0aog7,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/i0aog7/low_effort_data_engineering_platform/,1.0,9.0,0.0,16715.0,"Hi everyone, If you are looking to easily create a ""low cost"", ""low complexity"" ELT system with alerting and monitoring capabilities I wrote an article here [**https://www.startdataengineering.com/post/build-a-simple-data-engineering-platform/**](https://www.startdataengineering.com/post/build-a-simple-data-engineering-platform/) **.** This article uses stitch and DBT services to make it easy to do CDC and ELT. Any feedback would be appreciated."
2960,2020-07-30 05:26:18,1596075978.0,dataengineering,Learning spark - is it okay to ignore RDD/GraphX APIs?,i0dy2o,cstaway1990,,https://www.reddit.com/r/dataengineering/comments/i0dy2o/learning_spark_is_it_okay_to_ignore_rddgraphx_apis/,1.0,7.0,0.0,16721.0,"I'm using the [free book sponsored by Databricks](https://databricks.com/p/ebook/learning-spark-from-oreilly) to learn Spark. I'm a data engineer with a few years experience, but I've yet to use distributed batch processing professionally.

The book notes: 

&gt; While there are many topics we have chosen to cover, there are a few that we have
&gt; opted to not focus on. These include the older low-level Resilient Distributed Dataset
&gt; (RDD) APIs and GraphX, Spark’s API for graphs and graph-parallel computation.

In some intro videos I watched I heard about both of these APIs, so I assume that things like DataFrame are superseding RDDs. 

Is it okay to proceed and ignore RDDs/GraphX for now, at least initially? Or will not learning these APIs/concepts early on cause headaches later?"
2961,2020-07-30 09:41:14,1596091274.0,dataengineering,eXplainable Artificial Intelligence(XAI): A Welcome Step Towards Responsible AI,i0h8vx,tanmaydeshpande,,https://www.reddit.com/r/dataengineering/comments/i0h8vx/explainable_artificial_intelligencexai_a_welcome/,1.0,1.0,0.0,16728.0,
2962,2020-07-30 11:20:03,1596097203.0,dataengineering,How do you write unit tests for a data engineering-oriented project?,i0ibhz,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/i0ibhz/how_do_you_write_unit_tests_for_a_data/,1.0,17.0,0.0,16729.0,"I have recently been asked to do R&amp;D on unit testing for our data-engineering oriented project.

Our project has different workflows and pipelines which involve fetching, processing/transforming, storing data from a complicated structure.

Now as long as simple unit tests are concerned, I know how to write unit tests but in this case, how do we do it?

How do we prepare the data? How do we check output after a certain workflow is complete? How do we verify the data? How do we verify if the data looks at how it's supposed to?    


I am really confused if anyone could help that would be great."
2963,2020-07-30 11:31:35,1596097895.0,dataengineering,5 Best Practices for Data Lineage,i0ifyy,MichaelF823,,https://www.reddit.com/r/dataengineering/comments/i0ifyy/5_best_practices_for_data_lineage/,1.0,0.0,0.0,16731.0,"1. Assigning the owner for the data.
2. Keeping a track of modifications/updation to data.
3. Critical data sets should be identified.
4. Ability to find errors in the data.
5. Proper storage of data.

Check out the [in-depth post here](https://www.yourtechdiet.com/blogs/best-practices-data-lineage/?utm_source=RD&amp;utm_medium=SM&amp;utm_campaign=SB)."
2964,2020-07-30 12:58:01,1596103081.0,dataengineering,Validating large XML file against complicated XSD,i0jd32,The_Fog_Bandit,,https://www.reddit.com/r/dataengineering/comments/i0jd32/validating_large_xml_file_against_complicated_xsd/,1.0,2.0,0.0,16731.0,"What tools are people using to validate xml?

I have used Python's lxml package for small files, but it seems to grind to a halt on larger files.

There is spark-xml available, but I think the complicated nature of the xsd is causing issues with validation as known correct files fail.

So just looking for recommended strategies for validating xml, if you have one then you'll probably make my day."
2965,2020-07-30 13:22:33,1596104553.0,dataengineering,Hydro Energy Ranking | TOP 10 Country from 1970 to 2018,i0jmw7,zoehoky,,https://www.reddit.com/r/dataengineering/comments/i0jmw7/hydro_energy_ranking_top_10_country_from_1970_to/,1.0,0.0,0.0,16733.0,
2966,2020-07-30 14:03:14,1596106994.0,dataengineering,Top 5 database documentation tools for any team in 2020,i0k3im,duyenla257,,https://www.reddit.com/r/dataengineering/comments/i0k3im/top_5_database_documentation_tools_for_any_team/,1.0,0.0,0.0,16734.0,
2967,2020-07-30 15:43:13,1596112993.0,dataengineering,Top 10 Interview Questions for Data Scientist,i0ldnu,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/i0ldnu/top_10_interview_questions_for_data_scientist/,1.0,0.0,0.0,16739.0,
2968,2020-07-30 16:20:55,1596115255.0,dataengineering,Deploying Pipelinewise,i0lx8d,allan_w,,https://www.reddit.com/r/dataengineering/comments/i0lx8d/deploying_pipelinewise/,1.0,0.0,0.0,16740.0,"Has anyone here deployed Pipelinewise to production? If so, what platform did you choose?"
2969,2020-07-30 16:24:47,1596115487.0,dataengineering,"Career suggestion for newbie data engineer, ex backend engineer",i0lzca,pieofprotons,,https://www.reddit.com/r/dataengineering/comments/i0lzca/career_suggestion_for_newbie_data_engineer_ex/,1.0,5.0,0.0,16740.0,"Hi all! I should have looked for this reddit BEFORE I switched to DE because I am not sure at this point if I choose the right field or not. I need to know if this is what data engineering looks like or not, I am hoping for some directions and suggestions from the experienced DEs here as to where to go from here.

A little bit background: I have been mostly a Python backend engineer for the last couple of years, with slight front end work at the beginning. I switched to data engineering in a previous workplace where the pipelines were extremely simple (at best one join and almost never any temporary table). We used Google Dataproc and BigQuery at core but we had a ton of cron jobs and bash scripts, I enjoyed converting things to Python and Airflow. I joined the data team entirely out of curiosity and my interest was mostly in ""big data"" area, i.e. the techniques and tools of processing huge amount of data.

I usually learn things on the job as I go forward and wasn't satisfied with the extremely simplistic pipelines at that workplace, the bar of requirements was too low. So I had applied to several companies and eventually moved to Germany as a data engineer. I am adding this bit of information because from what I see, the tech culture vary a lot from my home country and the problems that I am facing might just be because of this.

So now my point of frustrations. The business requirement for me as a data engineer in my team is more close to data analysis than actually engineering anything, i.e. working with the big data technologies. For example, I had not been required to write any Python code, or any programming language as a matter of fact, for the data pipelines or any other task. All we do day in day out is write SQL queries, be it to make our pipelines, or create tables for ad hoc requests or even for quality checks. Coming from a TDD practicing background, I am not even sure if SQL is the correct tool to see if the pipeline produces intended results or not. On the other hand I am required to understand how the data is being transformed, from the beginning when it's collected in the front end to the table that I am using as basis for my own tables. We use Oozie as our scheduler and although I have learned to use it more or less, it's very jarring for me and I am still trying to convince my seniors to convert to Airflow as it is more intuitive and has a visual approach. Not to mention the logging and debugging is vastly easier. We have Hive and Impala, with Hue on top of it, and the whole data warehousing is maintained by a team of more experienced and mature date engineers who basically setup everything my team and all other stakeholders use.

I specially suck in the analysis portion and as I mentioned before, it's not what interested me in data engineering in the first place, it was the technologies, concepts like distributed computing, big data storage etc. As a result when trying to debug errors in SQL queries with 4-8 CTEs that use temporary tables which in turn have their own CTEs, I get entirely lost... But then again, there are many other data engineers in my company who are perfectly comfortable with the setup we have, for example almost none of them have ever tried to connect to our Impala instance with Python, even to do some basic analysis with Pandas.

I am still in my probationary period and I am questioning my decision of switching to the field. I still want to learn hands on about big data, not just be a database administrator with some business intelligence knowledge, who simply uses pre-existing Hadoop instance. Just because of how I am not enjoying my work, my learning process has almost halted and my performance took a huge hit, so much so that I am considering going back to backend engineering without taking way too much risk of being out of practice. With the Corona crisis and moving to a new country at the same time, I am feeling like I am walking on a tightrope. 

What do I do? Do I keep searching for DE opportunities where I can explore the parts that I find interesting parts, or just switch to backend as it will give me job reliability? Or do you guys suggest something different altogether?"
2970,2020-07-30 18:02:01,1596121321.0,dataengineering,"What is Text augmentation, Numerical data augmentation, Image augmentation? Also, know how we achieve data augmentation using GANs",i0nk8d,Vaishali_Advani,,https://www.reddit.com/r/dataengineering/comments/i0nk8d/what_is_text_augmentation_numerical_data/,1.0,0.0,0.0,16743.0,
2971,2020-07-30 18:38:45,1596123525.0,dataengineering,"Getting a junior DE job, and personal projects with Spark",i0o7tx,jotdwaetda,,https://www.reddit.com/r/dataengineering/comments/i0o7tx/getting_a_junior_de_job_and_personal_projects/,1.0,11.0,0.0,16745.0,"Hi all, I'm a rising college senior at a top ~13 university. I want to hopefully get a job as a junior data engineer after I graduate next year, but I do not have previous internship experience as a DE (I did do 1 internship as a frontend SWE). 

I've been making projects to showcase that I'm interested in data engineering and to hopefully make myself a better candidate. I have one decent project so far, where I used Selenium/pandas/s3/postgres/airflow/d3.js to scrape multiple job posting websites and visualize the most in demand skills for DE positions depending on industry/seniority label/company size/etc. 

I have a different project idea but with my first project I realized that probably the most sought after technology/skill after SQL and Python is Spark. I understand that with the size of my data, it'd be pretty dumb to use Spark over pandas for transforming. However, given that I have no prior internship experience as a DE, I feel anxious about not using it in a project or two.

In terms of getting that junior DE job, would it be fine to just know of Spark and when it would be appropriate to use it, and justify not having used it myself in my projects by explaining that my projects were too small in size? Or, would it be better for me to go and find massive amounts of data for a project and use Spark in the ETL process?"
2972,2020-07-30 19:09:47,1596125387.0,dataengineering,"Loading thousands of images from a specific folder into some form of data structure (preferably a NumPy array, but I'd be fine using a database)",i0ortc,dhruvmk,,https://www.reddit.com/r/dataengineering/comments/i0ortc/loading_thousands_of_images_from_a_specific/,1.0,1.0,0.0,16745.0,"Basically I've got this folder that consists of 30,000 images. I want to read the images, resize them to a 30x30x3 array, and append it to a data structure. As it stands, here is my inefficient code:

    import numpy as np
    from skimage.io import imread
    from skimage.transform import resize
    import os
    
    images = np.empty([30000, 30, 30, 1])
    directory = '/path'
    index = 0
    
    
    for file in os.listdir(directory):
        img = imread(os.path.join(directory, file))
        img = resize(img, (30, 30, 3))
        images[i] = img

How do I make this more efficient? I don't mind using another library, like Dask, Spark, etc."
2973,2020-07-31 04:43:42,1596159822.0,dataengineering,"I teach ""Hands on Machine Learning with Scikit-learn and Tensorflow 2.0"" - Course - Looking for feedback :)",i0ystq,IcarusGenius,,https://www.reddit.com/r/dataengineering/comments/i0ystq/i_teach_hands_on_machine_learning_with/,1.0,0.0,0.0,16757.0,
2974,2020-07-31 10:53:01,1596181981.0,dataengineering,Random word (order) generator. (What's the best way to do it?),i13knw,eujinski,,https://www.reddit.com/r/dataengineering/comments/i13knw/random_word_order_generator_whats_the_best_way_to/,1.0,3.0,0.0,16769.0," \[Newbie in programming\]

&amp;#x200B;

I would like to make a program that takes an input of a number of words and then randomly chooses and displays three at a time.  

&amp;#x200B;

It can be a data set preloaded or as an alternative data fed from an outside source (does this makes things more interesting? What benefits occur?).  

What programming language do you think is best for this kind of purpose?

As I'm new to this can you suggest your personal choice in how you would approach this in terms of structure? \[I'm not expecting effort on your behalf (that you might consider I should make) in explaining what I asked for.\] That is if you think there are more ways to go about it."
2975,2020-07-31 12:54:40,1596189280.0,dataengineering,Data Engineering Career Path: Step by Step Complete Guide,i14uwe,MlTut,,https://www.reddit.com/r/dataengineering/comments/i14uwe/data_engineering_career_path_step_by_step/,1.0,10.0,0.0,16773.0,"Hi Guys,  
The Dice 2020 Tech Job Report labeled data engineer as the fastest-growing job in technology in 2019, with a 50% year-over-year growth in the number of open positions.

Data Engineer is one of the most **profitable, secure, and most demanding career**. So, if you are planning to become a Data Engineer, but stuck with lots of questions like Who is Data Engineer?”, “What does Data Engineer do?”, “What skills are required for Data Engineering?”, “Where to start Learning?”, and much more. Then read this full article- [https://www.mltut.com/data-engineering-career-path/](https://mltut.us18.list-manage.com/track/click?u=daed40bf0a68af806ccb51607&amp;id=1e7794ff5b&amp;e=3d7f03b9f8)

In this article, you will find step by step Career Path for Data Engineering. I hope you will find this article helpful.

All the Best!

Happy Learning!"
2976,2020-07-31 13:53:23,1596192803.0,dataengineering,Covid-19 Death Rate Ranking | TOP 10 Country (Updated on 30 Jul 2020),i15i29,zoehoky,,https://www.reddit.com/r/dataengineering/comments/i15i29/covid19_death_rate_ranking_top_10_country_updated/,1.0,0.0,0.0,16774.0,
2977,2020-07-31 15:45:22,1596199522.0,dataengineering,IDE for Data Engineering day to day job?,i16vx2,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/i16vx2/ide_for_data_engineering_day_to_day_job/,1.0,15.0,0.0,16774.0,"I am preparing for a DE role, and I am learning 3 languages: Python, SQL, and Scala. I am looking to select IDE for these 3 languages and get myself familiar with them for longer-term.

For Python, I have selected PyCharm and currently using it. What would be the recommendations of the community for SQL and Scala? The IDE should not only support the language alone, rather support the corresponding end-to-end DE pipelines."
2978,2020-07-31 16:45:11,1596203111.0,dataengineering,Python: Procedural or Object-Oriented Programming?,i17r6b,wiekiang,,https://www.reddit.com/r/dataengineering/comments/i17r6b/python_procedural_or_objectoriented_programming/,1.0,0.0,0.0,16775.0,
2979,2020-07-31 18:40:42,1596210042.0,dataengineering,Need Some Help with a Difficult Request,i19oi6,Magicians_Nephew,,https://www.reddit.com/r/dataengineering/comments/i19oi6/need_some_help_with_a_difficult_request/,1.0,3.0,0.0,16778.0,"I am part of a small team within a large company that does GIS Data Science. Most of my team primarily uses Alteryx for analysis.; I do as well, but handle anything that's not Windows or involves a programming language. A senior member of the team has asked that all of our spatial datasets be put in a SQL database on the same server that Alteryx runs on with a **master lookup table for joins.** Each dataset is updated periodically and never at the same time. My team member's primary goal is to use Alteryx's In-DB tools and have the time-consuming spatial joins performed once, then joined via the master lookup table afterwards. My team in general has little to no experience in SQL, and when I initially expressed my concerns about the data integrity of a 1-2 TB DB that requires frequent data updates relying on a master lookup table, there was not time to fully explain why this idea was not good (in addition to several other reasons, which I won't get into here). I would appreciate any succinct reasons why this is not a good idea, or if it is actually workable and you have advice on how to implement it, that would also be appreciated."
2980,2020-07-31 19:12:42,1596211962.0,dataengineering,DE Job Search,i1a9a1,owila,,https://www.reddit.com/r/dataengineering/comments/i1a9a1/de_job_search/,1.0,4.0,0.0,16779.0,"How long did your DE job search take? 
Did you do or change Anything during your job search that got  you faster feedback and a YES? 
How did you tailor your job search? 

I am beginning my DE job search and I am looking for some suggestions. Thanks"
2981,2020-08-01 02:54:22,1596239662.0,dataengineering,Big Data Developer vs Data Engineer?,i1ie0x,kchoi85,,https://www.reddit.com/r/dataengineering/comments/i1ie0x/big_data_developer_vs_data_engineer/,1.0,5.0,0.0,16792.0,Wondering the difference between these titles / their job responsibilites.
2982,2020-08-01 04:38:51,1596245931.0,dataengineering,Layman question: what exactly is Snowflake and how does it relate to 'exposure data' for online advertising?,i1jxx0,Broad_Management,,https://www.reddit.com/r/dataengineering/comments/i1jxx0/layman_question_what_exactly_is_snowflake_and_how/,1.0,5.0,0.0,16793.0,"Very much a layman but just trying to understand how this works when it comes to online advertising. 

Is Snowflake just the websites data 'warehouse' that holds all the exposure data? How does this fit into things like LiveRamp? Does LiveRamp take the 'exposure file' from a place like Snowflake and then match it to the real world?"
2983,2020-08-01 05:54:37,1596250477.0,dataengineering,Springboard Data Engineering Bootcamp Legit?,i1kzdm,Business-Frame43,,https://www.reddit.com/r/dataengineering/comments/i1kzdm/springboard_data_engineering_bootcamp_legit/,1.0,6.0,0.0,16793.0,"Has anyone had any experience with the [Springboard Data Engineering Bootcamp](https://www.springboard.com/courses/data-engineering-career-track) ? I saw some bad reviews about the Data Science one, but didn't see anything about the Data Engineering one? What are your opinions on these ""bootcamps""?"
2984,2020-08-01 06:12:57,1596251577.0,dataengineering,What’s the big deal with Microsoft Azure?,i1l8gb,sandhulk145,,https://www.reddit.com/r/dataengineering/comments/i1l8gb/whats_the_big_deal_with_microsoft_azure/,1.0,26.0,0.0,16794.0,"I know the three big cloud technologies are Google Cloud, AWS, and Azure. I graduate next semester so I’ve been looking through job postings to get an idea of skills to build. I see Azure a lot more than AWS and definitely more than Google Cloud (which I’ve actually done my own project in). Is it more affordable? Better performance?"
2985,2020-08-01 09:34:49,1596263689.0,dataengineering,Does meltano provide incremental loading capability or does it alway do a full extraction?,i1nqzs,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/i1nqzs/does_meltano_provide_incremental_loading/,1.0,3.0,0.0,16800.0,"[https://meltano.com/docs/command-line-interface.html#examples-3](https://meltano.com/docs/command-line-interface.html#examples-3)

Imagine I wanted to extract data from Salesforce, and I wanted to extract it every day. If I run the extraction operation at 8:00pm Jan 1st, and then I run the extraction operation at 8:00pm Jan 2nd, is that possible the 2nd run only extract the incremental part?

1. I am not a data engineer, but I feel incremental dump (extraction) is a real need because full dump wastes too much unnecessarily resource including bandwidth and etc.; however, the cmd shown on the link above does not seem to provide an incremental dump mechanism
2. If it indeed provides an incremental dump mechanism, how could the 2nd dump know at which record the last dump ended?"
2986,2020-08-01 09:54:00,1596264840.0,dataengineering,Expert Advice on How Important is Math for Data Science,i1ny7u,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i1ny7u/expert_advice_on_how_important_is_math_for_data/,1.0,0.0,0.0,16800.0,
2987,2020-08-01 12:32:23,1596274343.0,dataengineering,Top Data Engineering Company,i1pj1y,Poojakhana52314,,https://www.reddit.com/r/dataengineering/comments/i1pj1y/top_data_engineering_company/,1.0,0.0,0.0,16806.0,
2988,2020-08-01 13:19:07,1596277147.0,dataengineering,Top 10 Richest People in the world from 2000 to 2020,i1q02x,karinakongk,,https://www.reddit.com/r/dataengineering/comments/i1q02x/top_10_richest_people_in_the_world_from_2000_to/,1.0,0.0,0.0,16808.0,
2989,2020-08-01 19:18:48,1596298728.0,dataengineering,What they don’t tell you about event sourcing,i1uti9,-segmentationfault-,,https://www.reddit.com/r/dataengineering/comments/i1uti9/what_they_dont_tell_you_about_event_sourcing/,1.0,0.0,0.0,16814.0,
2990,2020-08-01 20:23:35,1596302615.0,dataengineering,What they don’t tell you about event sourcing,i1vw2n,-segmentationfault-,,https://www.reddit.com/r/dataengineering/comments/i1vw2n/what_they_dont_tell_you_about_event_sourcing/,1.0,0.0,0.0,16814.0,
2991,2020-08-01 22:09:35,1596308975.0,dataengineering,Where do I start?,i1xpfx,nomore_regrets,,https://www.reddit.com/r/dataengineering/comments/i1xpfx/where_do_i_start/,1.0,6.0,0.0,16816.0,"I've been working as a Data Analyst for more than a year now, I want to get started into Data Engineering, 
Where do I start, can you share me some resources to start and learn with.

Thank you"
2992,2020-08-02 00:31:39,1596317499.0,dataengineering,We're building a better open-source Spark UI: “The Native Spark UI is my favorite monitoring tool” — said no one ever.,i20921,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/i20921/were_building_a_better_opensource_spark_ui_the/,1.0,0.0,0.0,16818.0,
2993,2020-08-02 00:34:22,1596317662.0,dataengineering,We're building a better open-source Spark UI: “The Native Spark UI is my favorite monitoring tool” — said no one ever.,i20aof,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/i20aof/were_building_a_better_opensource_spark_ui_the/,1.0,2.0,0.0,16818.0,
2994,2020-08-02 02:44:49,1596325489.0,dataengineering,Data Engineering Influencers to follow in LinkedIn?,i22cmx,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/i22cmx/data_engineering_influencers_to_follow_in_linkedin/,1.0,14.0,0.0,16824.0,"I find it helps to connect/follow influencers in a certain domain to get latest happenings and new products or ideas. Unfortunately, the Data people that I have in my LinkedIn network are all focused in Data Science (which I believe an over hyped position and on its way to the dodos). So I was wondering what are are your recommend LinkedIn persons to get connected/followed for exclusive Data Engineering contents."
2995,2020-08-02 08:28:43,1596346123.0,dataengineering,"Data Visualization using ""Matplotlib"" &amp; ""Python""",i26xy7,8329417966,,https://www.reddit.com/r/dataengineering/comments/i26xy7/data_visualization_using_matplotlib_python/,1.0,1.0,0.0,16833.0,https://youtu.be/IUAQVk7hBJQ
2996,2020-08-02 11:55:46,1596358546.0,dataengineering,Wrote a book review of Designing Data-Intensive Applications. It was awesome!,i2922r,today_is_tuesday,,https://www.reddit.com/r/dataengineering/comments/i2922r/wrote_a_book_review_of_designing_dataintensive/,1.0,12.0,0.0,16838.0,
2997,2020-08-02 14:15:34,1596366934.0,dataengineering,Children Under 5 Ranking | TOP 10 Country from 1950 to 2100,i2afst,karinakongk,,https://www.reddit.com/r/dataengineering/comments/i2afst/children_under_5_ranking_top_10_country_from_1950/,1.0,0.0,0.0,16841.0,
2998,2020-08-02 16:49:47,1596376187.0,dataengineering,ETL process between two PostgreSQL databases with DBLink?,i2cc4e,smpvlc,,https://www.reddit.com/r/dataengineering/comments/i2cc4e/etl_process_between_two_postgresql_databases_with/,1.0,2.0,0.0,16844.0,"Good afternoon,

For a university project I have designed a set of tables mimicking an OLTP system in a PostreSQL database, and in a different database I have created a Data Mart to illustrate a star schema design.

I have previously done ETL processes with Pentaho Data Integration, but it is something I would try to avoid in this occasion since we have been advise to use SQL only where possible.

My idea is to define stored procedures that fill the temporary tables as needed and then load those into the data mart, but since both are in separated databases I am not sure what the best approach for this one would be without using an ETL tool.

It seems that DBLink is the tool I need https://www.postgresql.org/docs/9.3/dblink.html but as usual, I would appreciate any comments from people that had experience doing a similar process with this tool.

Many thanks in advance."
2999,2020-08-02 22:41:29,1596397289.0,dataengineering,Data engineering weekly #2,i2i9qh,vananth22,,https://www.reddit.com/r/dataengineering/comments/i2i9qh/data_engineering_weekly_2/,1.0,3.0,0.0,16862.0,
3000,2020-08-03 02:10:47,1596409847.0,dataengineering,Why monitoring your big data analytics pipeline is important (and how to get there),i2luwc,elixirofhope,,https://www.reddit.com/r/dataengineering/comments/i2luwc/why_monitoring_your_big_data_analytics_pipeline/,1.0,2.0,0.0,16868.0,
3001,2020-08-03 10:53:12,1596441192.0,dataengineering,Advice on orchestration platform for Pentaho jobs,i2szy3,OCTANE_,,https://www.reddit.com/r/dataengineering/comments/i2szy3/advice_on_orchestration_platform_for_pentaho_jobs/,1.0,0.0,0.0,16881.0,"Hi all,

I‘m looking for a sort of orchestration platform which we can use to schedule/execute our existing pentaho jobs.  This should be extended to a customer view in the future where he can upload files and view results/metrics. I‘m looking for some advice on which tools fit best. 

First of all our current stack exists of our developed pentaho jobs hosted on a centralized GitLab instance. I setup a gitlab-runner with shell executor on a server with a local instance of pentaho. We can now use a CI/CD pipeline to trigger/schedule a job and view the logs afterwards on GitLab. 
This approach does not scale well in my opinion, because we have no centralized view of all pipeline. And each user has to setup specific notification settings to get notified in case of a pipeline failure. Parameter passing is also pretty clunky. 

The ideal tool would serve both an admin and a „customer“ view. It should have a straightforward way to trigger the pipelines. A rest api would be nice too. The customer view would include some basic options like view/trigger pipelines and should have some role based access control. 

I have yet no luck of finding a tool that comes close to this. The customer view is not mandatory, we can probably hack something together in mendix. 

I tried prefect.io, Meltano, airflow and some others. 

Airflow seems the most promising, but now we would need to centralized place to store all dags and we would always need a bashoperator that „git pull“s to make sure we’re using the latest version of the pentaho job.  Automatically retrying a pipeline is a very nice feature, but if a pipeline fails, it will likely also fail next time. 

A simple ui that centralizes the GitLab pipelines would probably already to the trick. Then we wouldn‘t have to adapt a new technology and could focus on our development. 

Maybe someone has an idea/suggestion

Cheers!"
3002,2020-08-03 12:52:25,1596448345.0,dataengineering,Advanced Flink Application Patterns Vol.3: Custom Window Processing,i2u90x,Marksfik,,https://www.reddit.com/r/dataengineering/comments/i2u90x/advanced_flink_application_patterns_vol3_custom/,1.0,0.0,0.0,16882.0,
3003,2020-08-03 13:13:43,1596449623.0,dataengineering,Most Popular Data Analytics Software to Learn in 2020,i2uhbg,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i2uhbg/most_popular_data_analytics_software_to_learn_in/,1.0,0.0,0.0,16883.0,
3004,2020-08-03 14:00:56,1596452456.0,dataengineering,Hong Kong vs Singapore | Population from 1950 to 2100,i2v0lz,karinakongk,,https://www.reddit.com/r/dataengineering/comments/i2v0lz/hong_kong_vs_singapore_population_from_1950_to/,1.0,0.0,0.0,16884.0,
3005,2020-08-03 14:24:31,1596453871.0,dataengineering,Recent Elec/CompEng Grad looking for advice on how to upgrade my skills/training for DE roles,i2vavj,TagThing,,https://www.reddit.com/r/dataengineering/comments/i2vavj/recent_eleccompeng_grad_looking_for_advice_on_how/,1.0,0.0,0.0,16884.0,"Hey all, I've posted similar posts on other subeditors but wanted to specifically get insight from those in the data engineering field. 
 
I am a recent graduate from an undergrad ECE program and am looking for some advice on how to transition and market myself for entry-level data engineering. As an ECE undergraduate, while I know the fundamentals of CS(Algorithms/Data Structures, core programming concepts) and have learned many different languages(C, Java, MATLAB, SQL, Python, Verilog) for homework and school projects, I haven't utilized them to build a full-scale, end-to-end applications. e software stack and build a demonstrable portfolio with that(thoughts on this?)

I do have a full year of internship experience in a role that mostly involved writing SQL procedures for business logic, reporting/dashboards and some DBA work(Traces, scripting jobs etc) along with it, but it's hard to leverage that on github without the full-stack development experience to turn that into an app. I also have a business minor with courses in finance/economics so I also aim to use that to emphasize domain experience for dev roles in finance/accounting/consulting firms.

I am looking at a local data science boot camp that includes some data engineering aspects as well. While I would probably benefit from a structured learning environment and accountability , I really don't like the idea of spending out of my savings to pay for it in a pandemic. Any thoughts?

I need some advice on how to build upon my current skills to make myself competitive for developer roles. What area should I focus my energies on, and strategies on how should I go about upgrading my skills from primarily scripting to application development. Any recommended books, resources or courses would also be very appreciated. Thanks for reading!"
3006,2020-08-03 16:45:00,1596462300.0,dataengineering,"Introducing lakeFS: an atomic, versioned data lake on top of object storage",i2xacz,eior71,,https://www.reddit.com/r/dataengineering/comments/i2xacz/introducing_lakefs_an_atomic_versioned_data_lake/,1.0,0.0,0.0,16889.0,
3007,2020-08-03 20:00:16,1596474016.0,dataengineering,Data Warehousing - Begineer,i30ro6,joalltrades,,https://www.reddit.com/r/dataengineering/comments/i30ro6/data_warehousing_begineer/,1.0,13.0,0.0,16899.0,What is Data Warehousing? I am trying to get into cloud and data and want to learn how to do that. I am a beginner and might not know what i am talking about as well. Thanks for the patience.
3008,2020-08-03 20:29:44,1596475784.0,dataengineering,"Stored procedures in DE, your opinion.",i31cx3,Luukv93,,https://www.reddit.com/r/dataengineering/comments/i31cx3/stored_procedures_in_de_your_opinion/,1.0,6.0,0.0,16899.0,"Hello,

Are you using SQL stored procedures in DE related tasks?

Can you describe scenario's when you choose stored procedures and what are the advantages/disadvantages? I can't find clear answers on google."
3009,2020-08-03 21:46:17,1596480377.0,dataengineering,Projects from the 5 trends in Data.,i32vjp,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/i32vjp/projects_from_the_5_trends_in_data/,1.0,2.0,0.0,16903.0,"Anyone have a positive experience with Prefect? Seeing a lot of competition for Airflow pop up. Should be interesting to see how it shakes out:

   

[https://twitter.com/le\_james94/status/1288953068441174018?s=20](https://twitter.com/le_james94/status/1288953068441174018?s=20)"
3010,2020-08-03 23:04:53,1596485093.0,dataengineering,Join big data Hackathon,i34eow,kolaol22,,https://www.reddit.com/r/dataengineering/comments/i34eow/join_big_data_hackathon/,1.0,2.0,0.0,16905.0,"To join the Hackathon,you must complete free 45 hours of  big data course (Hadoop,Java,Spark and Scala) within one month. Good luck! 
https://www.onlinelearningcenter.in/course-details/big-data-fundamentals?ref=2539"
3011,2020-08-03 23:27:34,1596486454.0,dataengineering,I'm confused on how to set my IAM Access and Secret Key in order to access my S3 Bucket using the S3Hook operator in Airflow?,i34uzq,freebird348,,https://www.reddit.com/r/dataengineering/comments/i34uzq/im_confused_on_how_to_set_my_iam_access_and/,1.0,5.0,0.0,16906.0,"Hi all! Forgive me as I am a newbie and trying to learn some data engineering.

Basically I've been following along on this post ( [https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/?fbclid=IwAR07y1C1zwbOb4CMS0cBFX0FaC-ww1o5YZTKQ2GS01RFeNuYuvvEAecLQEE](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/?fbclid=IwAR07y1C1zwbOb4CMS0cBFX0FaC-ww1o5YZTKQ2GS01RFeNuYuvvEAecLQEE)) in order to practice a simple DE project.

I am at a point where I have a file I am attempting to upload to S3. From my understanding, normally you need an IAM **Access Key** and **Secret Key** in order to upload to the bucket. My current code looks as such:

    BUCKET_NAME = '&lt;your-bucket-name&gt;' 
    temp_filtered_user_purchase_key= 'user_purchase/stage/{{ ds }}/temp_filtered_user_purchase.csv' 
    
    def _local_to_s3(filename, key, bucket_name=BUCKET_NAME):     
        s3 = S3Hook()   
        s3.load_file(filename=filename, bucket_name=bucket_name, replace=True, key=key)  

My error refers to the fact that the script can not locate the credentials (which makes sense). My question is where and how do I input my IAM **Access Key** and **Secret Key**.

&amp;#x200B;

Let me know if any of that didn't make sense!"
3012,2020-08-03 23:44:26,1596487466.0,dataengineering,Presto on AWS,i356vg,haloworlds,,https://www.reddit.com/r/dataengineering/comments/i356vg/presto_on_aws/,1.0,4.0,0.0,16906.0,"We are happy to announce a Presto on AWS deployment with self healing, metrics agent, autoscaling setup based on metrics and graceful shutdown of Presto workers.


GitHub Link https://github.com/atlanhq/presto-on-aws"
3013,2020-08-04 03:20:17,1596500417.0,dataengineering,Is a Civil Engineering to Data Engineer Transition even possible?,i390l2,PlasticExit,,https://www.reddit.com/r/dataengineering/comments/i390l2/is_a_civil_engineering_to_data_engineer/,1.0,12.0,0.0,16912.0,"Hi everyone,

I would like to ask the data engineers on this subreddit for some advice and wisdom:

I am currently going to my 4th year of Civil Engineering at a top school. I have never really found myself attracted to most topics in Civil Engineering, however, I eventually found transportation engineering and modelling within civil which is really heavily data, probability, math, and stats based.

Fast forward and I have taken many online courses (mostly off of udemy) on web dev and python for data, been a **transportation data research assistant** with a top professor, a **Data scientist/Machine Learning research intern** for the same professor analyzing huge datasets and creating housing simulations(python jupyter, pandas, numpy, sklearn, data visualization libraries, etc.) , and a **Software Engineering Intern focused on data** for a different research team at my University (basically I'm making data pipelines using python, spark, sql, aws), however, I'm not really getting much exposure to best practices as my supervisor isn't the most.. passionate person.

In a month, I'm going to be starting as a consulting engineer and product development intern at a Tech and infrastructure firm for a co-op and I suspect that it will be somewhat data and programming related (supervisor made sure I knew python) but a lot more civil engineering related. I have to make a decision of whether to do this for a complete 12 months, or cut it off at 8 months and look for a software engineering/data engineering internship for next summer. I really would like to find a data job, because I feel as if I have found my passion, and I actually enjoy learning data intensive and mathematical concepts.

My question is, would I be a competitive applicant at decent companies for data engineering internships for next summer, given my experience, and granted I keep improving my portfolio? Is data engineering even a field where there are many entry level jobs? And finally, would it be feasible, in your opinion, that I keep pursuing a field in which I don't have the suited degree, even after University, and even though I have a really strong civil eng background? 

TL;DR: civil engineering student from a good school with a decent amount of data related experience, need to make a decision on whether to pursue data engineering or civil and would like advice on the data engineering field feasibility for myself or how to improve myself as an applicant.

Sorry for the super loaded post, and thank you very much for your time :)"
3014,2020-08-04 04:48:41,1596505721.0,dataengineering,Job Expansion Pitch - Advice / Confidence Needed,i3aeov,onestupidquestion,,https://www.reddit.com/r/dataengineering/comments/i3aeov/job_expansion_pitch_advice_confidence_needed/,1.0,4.0,0.0,16916.0,"I've been working as a data analyst for going on 3 years now, 2 of which have been at my current employer, a small manufacturing company. We have very immature data pipelines that will soon be undergoing some substantial changes; our 20-year-old ERP will be phased out for a new one over the next 1-2 years, and it's possible that we'll be moving to an integrated CRM solution from Salesforce. Likewise, we have a number of smaller systems that will also have to be migrated over for historical analysis.

Soon, I'll be meeting with my boss and big boss to discuss how my career will unfold, as they realize I'm getting bored and am capable of doing much more in my role (which is already a huge expansion over its original scope). I believe that I can deliver a ton of value in developing a good data strategy, and I'd like to start taking manageable bites off of the elephant.

To that end, I envision my first major project to be creating a well-designed, scalable data lake. While we're waiting for the ERP / CRM transition, I think this would involve landing our legacy systems in Azure to give myself and the IT team time to build competency in cloud data management. We're a Microsoft shop and already use O365, and we may be looking into AzureAD, assuming it's not already in place, and in general, IT has been forward-thinking with migration to the cloud. In general, these are my overarching priorities:

1. Choose appropriate ELT / data warehousing technologies with a preference for SaaS / PaaS and code-based solutions with version control. First, our IT department is very small, and it's likely that I would be primarily responsible for administration / management of our data systems, so I'd like to offload as much of that as possible to give me room to expand capabilities, not just administer systems. Second, we're heavily reliant on spaghetti SQL, SSIS, and Task Scheduler / cron, and it makes management, performance monitoring, and debugging very challenging. This would require some buy-in from IT, but it may be less necessary if I'm building out or managing creation of our data pipelines.
2. Choosing / developing a data quality management system. Aside from a single, well-designed DW (with limited functionality due to its age and scope), we have no sort of data quality checks (nulls, value ranges, referential integrity, etc.). We react to data inconsistencies only once they're noticed in production reporting. That has to change!
3. Choosing / developing technologies to build / maintain a data lineage / data dictionary system. Right now, data lineage is handed off through meetings and story-telling; this is time-consuming and very ineffective. Power users should be able to know where their data is coming from and when it was last refreshed, at the very least. Measures / derived KPIs should available so that nobody has to guess what definition of ""margin"" is being used in a  report.

For someone with a little bit of coding background (R, some Python, remotely some C++ / shell scripting), limited database administration, and a lot of motivation, is this reasonable, or am I biting off too much at once? I feel like this is a huge opportunity both to help my company and advance my career, but most of this would be self-directed, and I'd be relying mostly on research since nobody in-house has much experience other than with SQL, SSIS, and SSRS.

I've been given some great insights from the Discord, but I figured I could expand a bit more in a post. Any perspective, either positive or negative, would be greatly appreciated."
3015,2020-08-04 08:06:46,1596517606.0,dataengineering,Is Apache Airflow more comparable to AWS Step Function or AWS Glue?,i3d8yk,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/i3d8yk/is_apache_airflow_more_comparable_to_aws_step/,1.0,12.0,0.0,16924.0,"I am new to the data pipeline orchestration, and people recommended Airflow -- but I wanted to avoid managing infrastructure as much as possible.

I found AWS provide two services that may be relevant: 

1. Step Function (Step Function vs Airflow is discussed here [https://www.1strategy.com/blog/2020/03/26/aws-step-functions-for-data-orchestration/](https://www.1strategy.com/blog/2020/03/26/aws-step-functions-for-data-orchestration/))
2. Glue (Glue vs. Airflow is discussed here [https://www.reddit.com/r/dataengineering/comments/f3ap4l/choosing\_a\_batch\_orchestration\_tool\_looking\_into/](https://www.reddit.com/r/dataengineering/comments/f3ap4l/choosing_a_batch_orchestration_tool_looking_into/))

Each article indicate that either Step function or Glue can replace Airflow if I don't want to manage infrastructure (HA, Failover, Capacity ...)

&amp;#x200B;

My question is which one should I pick? Step function or Glue?"
3016,2020-08-04 11:41:56,1596530516.0,dataengineering,8 Best Tableau Courses Online,i3fpi3,MlTut,,https://www.reddit.com/r/dataengineering/comments/i3fpi3/8_best_tableau_courses_online/,1.0,0.0,0.0,16931.0," 

Hi Folks,

As a Data Analyst or Data Scientist, you have to showcase your findings in a visual form, so that stakeholders can understand it properly. That’s why the knowledge of **Data Visualization** is important. 

**Tableau** is the most powerful, secure, and flexible end-to-end analytics platform for Data Visualization. **Audi, Bank of America, Amazon, Burger King, EY, and Kimberly-Clark Corporation** are few of the top companies using Tableau.

I have chosen **8 Best Tableau Courses Online** for you. I hope these courses will give you good command in **Tableau.**

For More details, read this article- [https://www.mltut.com/best-tableau-courses-online/](https://mltut.us18.list-manage.com/track/click?u=daed40bf0a68af806ccb51607&amp;id=52e533b9fe&amp;e=3d7f03b9f8)

All the Best!

Happy Learning!"
3017,2020-08-04 14:02:59,1596538979.0,dataengineering,Years of Education Ranking | TOP 10 Country from 1990 to 2017,i3hazt,karinakongk,,https://www.reddit.com/r/dataengineering/comments/i3hazt/years_of_education_ranking_top_10_country_from/,1.0,0.0,0.0,16934.0,
3018,2020-08-04 14:07:12,1596539232.0,dataengineering,Will data engineering as a profession last for long?,i3hd06,loonina,,https://www.reddit.com/r/dataengineering/comments/i3hd06/will_data_engineering_as_a_profession_last_for/,1.0,0.0,0.0,16934.0,"As more and more advancements happen in the Data Engineering space, do any of you ever wonder whether data engineering as a separate discipline will remain? If yes, what would data engineers be coveted for?"
3019,2020-08-04 15:36:55,1596544615.0,dataengineering,"Atomic, versioned data lake on top of object storage (e.g. S3)",i3ijgb,eior71,,https://www.reddit.com/r/dataengineering/comments/i3ijgb/atomic_versioned_data_lake_on_top_of_object/,1.0,0.0,0.0,16935.0,"lakeFS is an open source platform that delivers resilience and manageability to object-storage based data lakes.

With lakeFS you can build repeatable, atomic and versioned data lake operations - from complex ETL jobs to data science and analytics.

lakeFS is [API compatible with AWS S3](https://docs.lakefs.io/reference/s3.html) and works seamlessly with all modern data frameworks such as Spark, Hive, AWS Athena, Presto, etc."
3020,2020-08-04 15:38:24,1596544704.0,dataengineering,Top 10 Interview Questions for a Data Scientist,i3ik98,Reginald_Martin,,https://www.reddit.com/r/dataengineering/comments/i3ik98/top_10_interview_questions_for_a_data_scientist/,1.0,1.0,0.0,16935.0,
3021,2020-08-04 16:51:32,1596549092.0,dataengineering,Need Help Moving on From Junior Role,i3jo4q,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/i3jo4q/need_help_moving_on_from_junior_role/,1.0,31.0,0.0,16942.0,"I was stuck as Data Analyst for a couple years desperately trying to get any job with more programming. After applying to literally hundreds of jobs, I got a DE job at a small company, that I've been at for just over a year.

&amp;#x200B;

My current team is very small and I am the ONLY DE. I've learned pretty much everything on my own and faked it til I made it. I feel this has really hurt me cause I pretty much just make shit up as I go along, and nobody is actually checking my work. My current tech stack is the following:

* Python
* Airflow
* Azure (basically just creating a VM and ssh'ing in. no crazy admin stuff)
* Postgres
* Pandas

And that's really it. I basic automation such as importing data from an SFTP or pulling from a REST API. I do some basic cleaning and shove it into a database that is heavily underused. 

&amp;#x200B;

My fear is that my knowledge isn't enough to move. I'd like to move on from this company, mostly because I think it's giving me bad habits and it's hard to keep learning on my own. I'd like to work somewhere where I can learn from more senior devs and have people on my team that actually understand what I'm doing. 

&amp;#x200B;

The only problem is, a lot of job postings I see want more than the experience I have. Things like machine learning and distributed computing which has been really hard to do on my own. Basically anytime there's a new tool I want use at work, it means its another service I would need to setup and maintain. There's no budget for managed services.

&amp;#x200B;

Am I truly fucked in the position I'm in? Did I dig myself a hole I can't get out of?"
3022,2020-08-04 17:31:50,1596551510.0,dataengineering,Tips to architect and separate Data Lake in Dev/Prod ?,i3kcjm,francamacdowell,,https://www.reddit.com/r/dataengineering/comments/i3kcjm/tips_to_architect_and_separate_data_lake_in/,1.0,2.0,0.0,16945.0,"Hi people. I have an Airflow pipeline ingesting data from different sources (Views, DBs, crawlers...) directly on my Data Lake (Storage). We are going to start daily batch processing, and airflow server will be on production.

&amp;#x200B;

Do you have any tips, good patterns or solutions to Data Lake architecture in separated environments (development and production)?"
3023,2020-08-04 18:37:12,1596555432.0,dataengineering,3rd Party data integration,i3lii5,insecteblond,,https://www.reddit.com/r/dataengineering/comments/i3lii5/3rd_party_data_integration/,1.0,9.0,0.0,16946.0,"Fellow Data Engineers, how do you integrate 3rd party data (GitHub, SFDC, Zendesk, Jira...) into your data lake/warehouse?

Do you have custom scripts that fetch data from the API directly, or do you use services like Singer, AWS Eventbridge, Pipelinewise etc?

I’m researching a bit on this at the moment and very keen to know how the community is integration their 3rd party data. 

Thanks a lot!"
3024,2020-08-04 19:37:39,1596559059.0,dataengineering,"Data Visualization using ""Python"" with ""Seaborn""",i3mne6,8329417966,,https://www.reddit.com/r/dataengineering/comments/i3mne6/data_visualization_using_python_with_seaborn/,1.0,1.0,0.0,16947.0,https://youtu.be/X400eIcV-So
3025,2020-08-04 21:06:16,1596564376.0,dataengineering,Azure Synapse DB Integration,i3ocs3,ksubrent,,https://www.reddit.com/r/dataengineering/comments/i3ocs3/azure_synapse_db_integration/,1.0,1.0,0.0,16955.0,"Hello DE,

I am looking for some guidance on the best approach to integrating two Synapse instances within the same subscription. It appears that there is a limitation on reading an external database in Synapse, and our current ETL tool resides in AWS. I want to stay contained within Azure to increase performance. Has anyone done something similar to this?"
3026,2020-08-05 02:28:20,1596583700.0,dataengineering,Lazy Data Scientists,i3ubfq,YourAncestorIncestor,,https://www.reddit.com/r/dataengineering/comments/i3ubfq/lazy_data_scientists/,1.0,0.0,0.0,16962.0,"""Can I copy your homework?""

""Yeah just change it up a bit so it doesn't look obvious you copied""

""Ok"""
3027,2020-08-05 02:29:26,1596583766.0,dataengineering,Lazy Data Scientists,i3uc2q,YourAncestorIncestor,,https://www.reddit.com/r/dataengineering/comments/i3uc2q/lazy_data_scientists/,1.0,0.0,0.0,16962.0,
3028,2020-08-05 02:31:51,1596583911.0,dataengineering,Lazy Data Scientists,i3udh6,YourAncestorIncestor,,https://www.reddit.com/r/dataengineering/comments/i3udh6/lazy_data_scientists/,1.0,1.0,0.0,16962.0,
3029,2020-08-05 02:54:23,1596585263.0,dataengineering,Can someone explain to me the difference between SparkSession and SparkContext?,i3uqj6,freebird348,,https://www.reddit.com/r/dataengineering/comments/i3uqj6/can_someone_explain_to_me_the_difference_between/,1.0,4.0,0.0,16962.0,"I have a simple goal -- essentially I created an EMR using AWS (which has spark and hadoop downloaded on it), and using the Junyper notebook connected to the EMR, I am looking to:

1) Parse a CSV file I have uploaded in my S3 bucket  
2) Use spark to manipulate the dataframe  
3) Upload the manipulated dataframe to my S3 bucket in a csv format

Now note that I taught myself everything through the Udacity course. Basically in order to download and upload to my S3 bucket, I need to give spark my IAM credentials. 

I am using the following code to do that:

`sc.hadoopConfiguration.set(""fs.s3a.access.key"", ....)`  
`sc.hadoopConfiguration.set(""fs.s3a.secret.key"", .....)`

In order to do this I have to have a **spark context** session. However, I was taught to use a **spark session**. What are the differences between these 2 sessions?"
3030,2020-08-05 06:44:18,1596599058.0,dataengineering,Best Method for 50 million API Calls?,i3y65l,m4329b,,https://www.reddit.com/r/dataengineering/comments/i3y65l/best_method_for_50_million_api_calls/,1.0,11.0,0.0,16969.0,"I need to run Python API calls for 50 million records where I will pass data (dict or json) and the API will return an identifier that I need to store on the record and then write it to a permanent location. The API I am using has a synchronous and asynchronous version. This API self throttles at 50,000 hits per minute. The goal is to have this run in one day (50,000\*60\*24 &gt; 50 million) The two options I have started thinking about:

A) AWS Lambda function trigger by dropping the records into an S3 bucket

B) Using some python library to take advantage of concurrent async API calls

Would love to hear some feedback"
3031,2020-08-05 06:45:38,1596599138.0,dataengineering,How do you romanticize data engineering or what makes your passionate and excited about what you do or how do you tie it to a larger meaning ?,i3y6s8,citizenofacceptance,,https://www.reddit.com/r/dataengineering/comments/i3y6s8/how_do_you_romanticize_data_engineering_or_what/,1.0,30.0,0.0,16969.0,Just curious. Or if you do it to put bread on the table that’s fine too
3032,2020-08-05 10:32:38,1596612758.0,dataengineering,Introducing Ververica Platform 2.2 with Autoscaling for Apache Flink,i40xzz,Marksfik,,https://www.reddit.com/r/dataengineering/comments/i40xzz/introducing_ververica_platform_22_with/,1.0,0.0,0.0,16973.0,
3033,2020-08-05 11:43:37,1596617017.0,dataengineering,Most Powerful Data Science Languages To Learn in 2020,i41pxe,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i41pxe/most_powerful_data_science_languages_to_learn_in/,1.0,0.0,0.0,16975.0,
3034,2020-08-05 13:49:25,1596624565.0,dataengineering,Wine Consumption Ranking | TOP 10 Country from 1961 to 2011,i433zq,karinakongk,,https://www.reddit.com/r/dataengineering/comments/i433zq/wine_consumption_ranking_top_10_country_from_1961/,1.0,0.0,0.0,16976.0,
3035,2020-08-05 16:15:01,1596633301.0,dataengineering,Data Engineering landscape in Europe,i454rj,metalcoholicfreak,,https://www.reddit.com/r/dataengineering/comments/i454rj/data_engineering_landscape_in_europe/,1.0,9.0,0.0,16978.0,"What are some of the most interesting companies in Europe for a Data Engineer?

Big or small, it doesn't matter, as long as they are mature enough to understand data engineering and build exciting stuff.

So far I am intrigued by the work at:

* Spotify (Sweden)
* Swisscom (Switzerland)
* Zalando (Germany)

I am not including companies that are not european but have offices in Europe (namely FAANG).  I am also very curious to see if any lesser-known names come up!"
3036,2020-08-05 18:07:09,1596640029.0,dataengineering,[100 seconds survey] Best practices for the development of Jupyter Notebooks,i46zld,Luigi_Q,,https://www.reddit.com/r/dataengineering/comments/i46zld/100_seconds_survey_best_practices_for_the/,1.0,2.0,0.0,16981.0,"We are trying to validate a set of **best practices for the development of Jupyter Notebooks**.  
To all data scientists out there with Jupyter Notebook experience, please consider taking this [quick \~100 seconds survey](https://forms.gle/GRbcgWJGZuD9ZYNS6). Data will be collected anonymously and analysed in aggregated form.

Thank you in advance for helping us with our research!"
3037,2020-08-05 19:00:07,1596643207.0,dataengineering,Containerization of PySpark using Kubernetes,i47ypo,Darth_Programmer,,https://www.reddit.com/r/dataengineering/comments/i47ypo/containerization_of_pyspark_using_kubernetes/,1.0,0.0,0.0,16983.0,"Read Sigmoid's detailed approach on how to use Spark on Kubernetes and a brief comparison between various cluster managers available for Spark.

Click here to read the entire blog - [https://www.sigmoid.com/blogs/containerization-of-pyspark-using-kubernetes/](https://www.sigmoid.com/blogs/containerization-of-pyspark-using-kubernetes/)"
3038,2020-08-05 19:16:59,1596644219.0,dataengineering,Why is Data Preprocessing an important phase in machine learning pipeline? Also know what are the different steps involved in Data Preprocessing.,i48aiz,instigator-001,,https://www.reddit.com/r/dataengineering/comments/i48aiz/why_is_data_preprocessing_an_important_phase_in/,1.0,0.0,0.0,16984.0,
3039,2020-08-05 21:05:13,1596650713.0,dataengineering,Postgres or MSSQL for DE?,i4acxd,Luukv93,,https://www.reddit.com/r/dataengineering/comments/i4acxd/postgres_or_mssql_for_de/,1.0,15.0,0.0,16986.0,"Hello,

What kind of database would you choose/consider when building a corporate data platform/hub ?

So far we have used the Azure Cloud for different purposes and are quite satisfied so far. 

Now I read that Data Engineers often choose for Postgres. Why would you choose Postgres over MSSQL? What would be your considerations?"
3040,2020-08-06 00:03:08,1596661388.0,dataengineering,Firebase and Real-Time Dashboards,i4dt5p,Magicians_Nephew,,https://www.reddit.com/r/dataengineering/comments/i4dt5p/firebase_and_realtime_dashboards/,1.0,1.0,0.0,16991.0,"I debated on asking this here, but my biggest concern as a prototype this for my company is that a) there is something that might slow the app being pushed to Firebase from making it to a real-time dashboard and b) I haven't found many resources on using the two together. My company has a massive amount of information being pushed into and through Firebase, but I was hoping someone here might have a word of advice."
3041,2020-08-06 04:02:11,1596675731.0,dataengineering,Any good courses on Udemy for big data?,i4hz7e,PM_ME_YOUR_DONUT_PLS,,https://www.reddit.com/r/dataengineering/comments/i4hz7e/any_good_courses_on_udemy_for_big_data/,1.0,2.0,0.0,16994.0,What do you guys think has anyone done [this](https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/) course? There seems to be a sale right now so I'm curious about courses. Particulary any good courses on tools like Spark or others that are good for DEs? Or any recommendations of other courses good for anything data eng? A lot of the courses there seem to be about data analytics. Thanks.
3042,2020-08-06 05:06:33,1596679593.0,dataengineering,What Do I Need To Learn To Break In?,i4j10z,maraskooknah,,https://www.reddit.com/r/dataengineering/comments/i4j10z/what_do_i_need_to_learn_to_break_in/,1.0,24.0,0.0,16996.0,"I was a financial analyst for 8 years, then came to a company focused on data 6 years ago. I am somewhat of a hybrid business analyst as well as programmer. I've applied to jobs, and am not getting attention.

* 4 years Python, 10 years of SQL querying. I don't have experience doing DBA work, performance tuning, designing the db, making stored procedures, etc. That is somewhat inaccessible to me since our engineering team leaves that closed off to business teams.
* I conceived of, and helped code an ETL process using Python, SQL, and Google cloud tools. There were other pieces of this pipeline that engineers took upon themselves to code. I do understand how a docker container works but not Airflow or Kubernetes. I wish someone would train me on these other pieces, but it seems like the resources aren't there.
* I see on lots of job descriptions AWS tools like Redshift, or other big data platforms like Spark and Hadoop. I don't know any of these.
* In my last interview for data analyst (not DE), I couldn't answer questions to explain what data warehousing and data modeling are. I have a feeling I may not know the precise terms, but I might know the underlying concepts.

Anyway, I know I have a lot of gaps, but I keep reading that if you know Python and SQL, you have the most important pieces. For the ETL pipeline, I got training on how to start up a linux terminal, call a bash script to build a docker container. Within the container, a template Python script was given to me that I learned how to use - it would start moving data from source (SQL Server) to destination (GCP). Once the data was in GCP, I would call a BigQuery script I made to make the proper transformations by combining data from various tables, and produce one final output table.

Any advice to address my weaknesses would be appreciated."
3043,2020-08-06 07:44:34,1596689074.0,dataengineering,How do I run a python script locally that connects to my EMR configured in AWS?,i4lc0l,freebird348,,https://www.reddit.com/r/dataengineering/comments/i4lc0l/how_do_i_run_a_python_script_locally_that/,1.0,3.0,0.0,17000.0,"I'm a little new to this so forgive my ignorance. 

Basically, I created a Spark cluster in Amazon EMR. In addition, I created a Jupyter notebook in EMR that connected to my cluster and did some data manipulation. 

My goal is essentially to export the Jupyter notebook into a python script and be able to run the python script which pulls data from S3, does some manipulation (using spark), and re-uploads the manipulated dataframe to S3. I would then like to add the python file as a task that runs in my Airflow DAG (which is configured in Docker).

The issue is I don't fully understand how I can connect to local python script to my cluster configured in Amazon EMR. I understand that Amazon does not support local mode meaning that I cannot configure the spark session to the master node IP. 

Any help would be appreciated!"
3044,2020-08-06 07:51:05,1596689465.0,dataengineering,Scaling Real-time Gaming Leaderboards for Millions of Players,i4lexm,ssb61,,https://www.reddit.com/r/dataengineering/comments/i4lexm/scaling_realtime_gaming_leaderboards_for_millions/,1.0,0.0,0.0,17000.0,"**Tech Talk**

Real-time analytics is integral to leaderboards, personalized experiences, and fraud detection in gaming and social applications, but can be challenging because of the need for complex queries at scale.

In this talk, we will discuss the growing need for real-time leaderboards in gaming and gamification scenarios, and examine various requirements when building leaderboards:

* real-time aggregations over user activity
* joins across data sets
* scalability to thousands of concurrent users
* minimal operational overhead

We will discuss a serverless stack to handle the massive scale of user activity and real-time aggregations and joins required by leaderboards.

Save your spot: [https://rockset.com/gaming-leaderboards-talk](https://rockset.com/gaming-leaderboards-talk)"
3045,2020-08-06 10:19:39,1596698379.0,dataengineering,The Most Powerful Data Science Tools in 2020,i4n7f8,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i4n7f8/the_most_powerful_data_science_tools_in_2020/,1.0,0.0,0.0,17004.0,
3046,2020-08-06 10:50:40,1596700240.0,dataengineering,Handling eventual consistency,i4nk0x,-segmentationfault-,,https://www.reddit.com/r/dataengineering/comments/i4nk0x/handling_eventual_consistency/,1.0,0.0,0.0,17005.0,
3047,2020-08-06 10:55:59,1596700559.0,dataengineering,Best practice to deal storing calculated fields in a table (PostgreSQL).,i4nm1q,smpvlc,,https://www.reddit.com/r/dataengineering/comments/i4nm1q/best_practice_to_deal_storing_calculated_fields/,1.0,2.0,0.0,17005.0,"Good morning,

Doing some research it seems that the consensus is that storing calculated values on a table is a ""no no"", the main issue being potential data consistency issues.

For my particular case I would need to create a view that calculates the value of  daily purchases for a a group of customers, so basically sales(customer,order, date, product, quantity, vat,total price), this would have to be calculated each day and I need to keep track of the purchases for each customer historically.

A materialized view seems the way to go, but I am concerned about the daily update I would have to perform on the view, specially as the data grows.

For that reason and given that I basically need to access historical information I am not sure if I just should create a separated table that stores the history of transactions. The below link discusses a similar scenario to mine:

https://www.citusdata.com/blog/2018/10/31/materialized-views-vs-rollup-tables/ 

Being the following comment the main attention point for me ""... for larger data sets and databases that have more active workloads only processing net new data from your last rollup can be a more efficient use or resources"".

Once again I would be really grateful if I could hear from somebody having more experience in this area.

Thanks a lot!"
3048,2020-08-06 12:11:34,1596705094.0,dataengineering,Getting started with Apache Avro and Python,i4ofte,pknerd,,https://www.reddit.com/r/dataengineering/comments/i4ofte/getting_started_with_apache_avro_and_python/,1.0,0.0,0.0,17005.0,
3049,2020-08-06 13:19:03,1596709143.0,dataengineering,England Premier League Ranking | Top 10 Clubs by Goals in 2019/2020,i4p7ij,elainetong,,https://www.reddit.com/r/dataengineering/comments/i4p7ij/england_premier_league_ranking_top_10_clubs_by/,1.0,0.0,0.0,17008.0,
3050,2020-08-06 14:50:19,1596714619.0,dataengineering,Udemy course alternative,i4qdpy,Martinez__,,https://www.reddit.com/r/dataengineering/comments/i4qdpy/udemy_course_alternative/,1.0,29.0,0.0,17007.0,"Hi Redditors,

I’m looking for alternative for Udemy course about Hadoop. 

https://www.udemy.com/course/hadoopinrealworld/

Why? It costs too much for me - it costs about 140 euro. Could you recommend anything please?"
3051,2020-08-06 19:15:37,1596730537.0,dataengineering,"Data Quality Challenges for Data Scientists: An Interview with Michal Klos, Part 1",i4up3w,dataculpa,,https://www.reddit.com/r/dataengineering/comments/i4up3w/data_quality_challenges_for_data_scientists_an/,1.0,0.0,0.0,17018.0,
3052,2020-08-06 20:52:02,1596736322.0,dataengineering,State vs Stateless,i4wj73,hyperandaman,,https://www.reddit.com/r/dataengineering/comments/i4wj73/state_vs_stateless/,1.0,6.0,0.0,17021.0,"This terminology keeps coming up and I am still not sure what it actually means. Does it differ based on context?

What does “Stateless” mean when we talk about Lambda functions in AWS?

What does it mean to be a “State Machine” when we talk about Step Functions? 

If you can give me a ELI5 response with some depth, that would be great. A lot of the definitions I’ve found from research end up using a lot of fancy words that don’t help you understand what it actually means."
3053,2020-08-06 22:19:52,1596741592.0,dataengineering,"Dimensional modelling, your experiences and advise",i4y8q1,Luukv93,,https://www.reddit.com/r/dataengineering/comments/i4y8q1/dimensional_modelling_your_experiences_and_advise/,1.0,4.0,0.0,17025.0,"Over the last 2 years I have used Python and SQL for data analysis. Now that I want to develop myself more towards DE related work I want to start creating dimensional models. There's a lot of theory available but I'm missing the techniques and practical examples, since I want to practise creating models myself.

&amp;#x200B;

1. I would like to hear your **experiences**, **techniques** and how/where the end users can use the dimensional models (how can they connect to them, do you use SSAS or are there other techniques available?)
2. What advise can you give me to start modeling? Maybe you have some project or resources that I can work on/try out?
3. I need someone that can show me the ropes by working together on a project. Please let me know if that's possible!"
3054,2020-08-06 22:37:19,1596742639.0,dataengineering,Guidance setting up airflow in production,i4ykuj,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/i4ykuj/guidance_setting_up_airflow_in_production/,1.0,12.0,0.0,17024.0,"Hi guys , 

We are currently using azure data factory and we want a much more flexible environment for our data pipelines to be orchestrated on( and cost efficient) , for a sequential executor can you guide me with some links or materials which would be the best and commonly used deployment model.

And will Azure provide a Airflow managed service like gcp's cloud composer ?"
3055,2020-08-07 01:26:21,1596752781.0,dataengineering,How much do you earn ? What are your credentials ? (EU),i51ps9,Fnmokh,,https://www.reddit.com/r/dataengineering/comments/i51ps9/how_much_do_you_earn_what_are_your_credentials_eu/,1.0,6.0,0.0,17031.0,"Hello everyone, this is for EU members.
It might sound a bit intrusive, but this is for everyone to have an idea about pay grades.
So where are you from ? What are your credentials/degrees/certifications/years of experience/etc... and how much do you earn, monthly or yearly ?
Myself, from the Paris region, 3 years of experience, self taught but have a degree in telecom engineering, and earn between 40 and 45k a year."
3056,2020-08-07 05:20:57,1596766857.0,dataengineering,"What is a Data Warehouse, when and why to consider one",i55gps,anhthong00,,https://www.reddit.com/r/dataengineering/comments/i55gps/what_is_a_data_warehouse_when_and_why_to_consider/,1.0,6.0,0.0,17033.0,
3057,2020-08-07 09:27:38,1596781658.0,dataengineering,Example of Data Catalog and Metadata,i58svx,irixnox,,https://www.reddit.com/r/dataengineering/comments/i58svx/example_of_data_catalog_and_metadata/,1.0,3.0,0.0,17037.0,"I get confused the difference

Can you give an example of each? Perhaps with Airplane of Flight analogy

Metadata: Boeging 787, 300 Passengers, Engine X etc (These are metadata correct?)

Data Catalog: ?"
3058,2020-08-07 09:34:02,1596782042.0,dataengineering,"""Data Warehouse"" is a poor term for what it actually does",i58vhx,LeMtEverest,,https://www.reddit.com/r/dataengineering/comments/i58vhx/data_warehouse_is_a_poor_term_for_what_it/,1.0,4.0,0.0,17037.0,"I think ""Data Warehouse"" is a poor term for what it actually does.

A more accurate term would be ""Data Analytics-Query Store""

Yes, you're warehousing data in a sense -- but -- almost every data store is warehousing data in someway.

The key difference is that ""Data Warehouses"" are ""warehousing"" data for the purpose of OLAP &amp; BI querying.

So, the question becomes: given that ""data warehouse"" is such a broad and encompassing term, does it fit the very specific use-case of BI &amp; OLAP queries?"
3059,2020-08-07 10:06:59,1596784019.0,dataengineering,Do Data Engineers have a lot of free time?,i598r6,Dudeguybrochingo,,https://www.reddit.com/r/dataengineering/comments/i598r6/do_data_engineers_have_a_lot_of_free_time/,1.0,7.0,0.0,17038.0,"I have a notion that once you’ve built the system, that you just have to sit back and relax (and fix small errors along the way)."
3060,2020-08-07 10:43:13,1596786193.0,dataengineering,The Most Powerful Data Science Tools in 2020,i59n2v,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i59n2v/the_most_powerful_data_science_tools_in_2020/,1.0,0.0,0.0,17041.0,
3061,2020-08-07 14:10:46,1596798646.0,dataengineering,What every developer should know about database consistency,i5bwb0,redblackbit,,https://www.reddit.com/r/dataengineering/comments/i5bwb0/what_every_developer_should_know_about_database/,1.0,0.0,0.0,17043.0,
3062,2020-08-07 14:33:05,1596799985.0,dataengineering,Horse Meat Production Ranking | TOP 10 Country from 1961 to 2018,i5c60y,terrencettang,,https://www.reddit.com/r/dataengineering/comments/i5c60y/horse_meat_production_ranking_top_10_country_from/,1.0,1.0,0.0,17045.0,
3063,2020-08-07 17:26:41,1596810401.0,dataengineering,"Zoe - The Kafka CLI for humans (written in Kotlin): Version 0.26.0 is out with the ability to filter consumed records based on Kafka headers, variable substitutions and more!",i5erkc,wlezzar,,https://www.reddit.com/r/dataengineering/comments/i5erkc/zoe_the_kafka_cli_for_humans_written_in_kotlin/,1.0,0.0,0.0,17049.0,
3064,2020-08-07 20:49:25,1596822565.0,dataengineering,Table without date column,i5ifxh,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/i5ifxh/table_without_date_column/,1.0,17.0,0.0,17057.0,"I have 9M records. We needed to do the following operations:-

1. daily we receive the entire file of 9M records with 150GB of file size
2. It is truncate and loads in Snowflake. Daily deleting the entire 9B records and loading

We would want to send only incremental file load to Snowflake. Meaning that: 

For example, out of 9Million records, we would only have an update in 0.5Million records(0.1 M Inserts,0.3 Deletes, and 0.2 Updates). How we will be able to compare the file and extract only delta file and load to the snowflake. How to do it **cost-effectively and fast way** in AWS native tools and load to S3. 

P.s data doesn't have any date column.  It is a pretty old concept written in 2000. We need to optimize this. The file format is fixed width. Attaching sample RAW data.

In a nutshell, I want to extract only Insert, Updates, and Deletes into a File. How do you classify this best and cost-efficient way."
3065,2020-08-07 23:44:41,1596833081.0,dataengineering,SAP ERP Change Data Capture - Incremental Loading Tables How To?,i5lqqs,poppinstacks,,https://www.reddit.com/r/dataengineering/comments/i5lqqs/sap_erp_change_data_capture_incremental_loading/,1.0,7.0,0.0,17062.0,"Hi Community,

I'm involved in an ongoing effort to connect some SAP ERP data to a Snowflake Data Warehouse.

We are leverage Informatica Cloud Data Integration to talk to SAP ERP through an ABAP table reader. Our tables have all been migrated to transparent, but there are some issues we have observed (I am entirely knew to SAP). 

Some tables (BSEG) don't show new records into the change logs (CDHDR, CDPOS), updates are reflected but if a transaction like FB60 (Vendor Invoice Creation) runs and creates an insert (I can see the creation date in the header BKPF) it doesn't log this as an insert into CDHDR.

I have a few ideas on how to ""generally"" solve this and I wonder if people have experienced similar issues...

Thoughts:

1.If a header table exists then use the creation date on the header to perform CDC

2.if a table doesn't have a header (how do I even really check this?) e.g FPLT we can maybe leverage the change log. 

3.If a table doesn't appear in the change log, and doesn't have a header / or a self contained modified attribute then maybe we can assume that the PK of the table is consistently increasing, and use that as the mechanism to monitor for CDC.

Overall I'm out of my depth on the SAP side of this equation, and any guidance is sincerely appreciated."
3066,2020-08-08 03:05:16,1596845116.0,dataengineering,MSc Data Science or MSc Computer Science?,i5p6ux,TECHTHROWAWAY1000,,https://www.reddit.com/r/dataengineering/comments/i5p6ux/msc_data_science_or_msc_computer_science/,1.0,11.0,0.0,17063.0,"Hello guys, hope everyone is doing well. I recently got offers to study MSc Computer science at the University of Bristol and also have an offer to study MSc Data science at the same university. I am having difficulty choosing a course as I do not know which course would be best for a entry level data engineering position. Which course do you guys think has the best employability post-graduation? replies from this post will help me a lot in terms of making a decision.

Thank you for reading this post."
3067,2020-08-08 07:15:22,1596860122.0,dataengineering,Data Visualization using python | Seaborn| Part-II,i5sssz,8329417966,,https://www.reddit.com/r/dataengineering/comments/i5sssz/data_visualization_using_python_seaborn_partii/,1.0,0.0,0.0,17074.0,https://youtu.be/Yzudrb6rk6Y
3068,2020-08-08 10:48:33,1596872913.0,dataengineering,Complete Guide on How Data Analytics Process Works?,i5vc91,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i5vc91/complete_guide_on_how_data_analytics_process_works/,1.0,0.0,0.0,17079.0,
3069,2020-08-08 21:58:26,1596913106.0,dataengineering,"Hi guys , I'm collating spark optimization materials from different folks , use it to learn or contribute to it if you know something.",i64mvt,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/i64mvt/hi_guys_im_collating_spark_optimization_materials/,1.0,0.0,0.0,17094.0,
3070,2020-08-08 23:49:38,1596919778.0,dataengineering,Any good online courses on shell scripting,i66jeb,st789,,https://www.reddit.com/r/dataengineering/comments/i66jeb/any_good_online_courses_on_shell_scripting/,1.0,14.0,0.0,17096.0,I'm looking for a good online course focused on unix shell scripting. I've started a new data engineering job that will require this task heavily for our data pipeline.
3071,2020-08-09 01:06:13,1596924373.0,dataengineering,"I work in both SSMS and Azure - Should I learn SSIS, Airflow, Azure Data Factory, or Azure HDInsight? Not sure where to start.",i67u1u,TheBergSource,,https://www.reddit.com/r/dataengineering/comments/i67u1u/i_work_in_both_ssms_and_azure_should_i_learn_ssis/,1.0,18.0,0.0,17098.0,"Hey all, 

I’ve posted on here recently. Im a DA looking to transition to DE. I’m good at Data Warehousing and databases, but not at the ETL side of things. I’m great in T-SQL and Python. Not sure what the most popular or the best ETL tools to use with this Microsoft Stack."
3072,2020-08-09 01:29:06,1596925746.0,dataengineering,Data Engineering - How to start,i687k0,BruToine,,https://www.reddit.com/r/dataengineering/comments/i687k0/data_engineering_how_to_start/,1.0,4.0,0.0,17099.0,"Hello everyone,

I want to start a personal project to learn more about data engineering. I need to scrape data using Scrapy/Selenium or any scraping framework from multiple sites then process it in a data pipeline to generate features for a machine learning model.

What would be the best workflow/setup to start this kind of project ? I have heard of Dagster for the data pipeline. Is it a good starting point or should I used other tools ? Should I use Docker. If so, should I put everything in the same docker container or separate the scraping part and data pipeline in different container and use something like minio for storing the data ?"
3073,2020-08-09 02:13:48,1596928428.0,dataengineering,Data Engineer job market by country?,i68wzo,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/i68wzo/data_engineer_job_market_by_country/,1.0,10.0,0.0,17098.0,"I just searched ""Data Engineer"" job ads in the Indeed job portal for the following 4 countries: Germany, Netherlands, UK, and Singapore. The UK has only 32 job ads for Data Engineer position, while all other countries have 3000-5000 job ads. What is going on with the UK market, is there no demand for Data Engineers?"
3074,2020-08-09 08:37:48,1596951468.0,dataengineering,Shuffe partitions coalesce and Great Expectations deep dive,i6e7fk,BartoszWfc,,https://www.reddit.com/r/dataengineering/comments/i6e7fk/shuffe_partitions_coalesce_and_great_expectations/,1.0,2.0,0.0,17102.0,"Hi,

That's my first post and hope that will able to share interesting things to you. Since I'm writing on [waitingforcode.com](https://www.waitingforcode.com) blog and it's easier to start, I will share 2 new blog posts published this week ;)

&amp;#x200B;

1. Shuffle partitions coalesce feature for the Adaptive Query Execution - a new Apache Spark runtime optimization to reduce the number of partitions on the reducer side. [https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-shuffle-partitions-coalesce/read](https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-shuffle-partitions-coalesce/read)
2. Great Expectations is a data validation framework pluggable to PySpark or Pandas - 2 weeks ago I presented the global picture of it. Today I added the follow-up post about some internal details, before going further and orchestrating it with Airflow in 2 weeks.   
[https://www.waitingforcode.com/big-data-problems-solutions/data-validation-frameworks-great-expectation-classes/read](https://www.waitingforcode.com/big-data-problems-solutions/data-validation-frameworks-great-expectation-classes/read) 

Best,  
Bartosz."
3075,2020-08-09 10:52:32,1596959552.0,dataengineering,Data warehouse for a simple retail business,i6fn46,hungrykoala3,,https://www.reddit.com/r/dataengineering/comments/i6fn46/data_warehouse_for_a_simple_retail_business/,1.0,9.0,0.0,17105.0,"You have a simple retail business that is growing fast and there is a clear need to start handling analytics separately from the source systems. Would you go with time-tested dimensional modelling in SQL or would you consider other approaches? You would have very limited resources in terms of people. Also, amount of data is not huge, a few tens of thousands of transactions daily at best. What other approaches would you consider?"
3076,2020-08-09 12:50:51,1596966651.0,dataengineering,Parallelism between database schemas and classes in OOP.,i6gtb0,smpvlc,,https://www.reddit.com/r/dataengineering/comments/i6gtb0/parallelism_between_database_schemas_and_classes/,1.0,4.0,0.0,17109.0,"Good morning,

Although I thought the concept of a database schema was clear to me, I realized that in practice it's a bit muddy....

I was wondering if schemas allow us for a separation of concerns in a database in the same way that classes allow us to do that under the OOP paradigm, i.e a way to organize a structure (in this case a database) into meaningful logical structures (in this case schemas containing a group of tables).

I am currently designing a database for a project, and I am wondering if I should create 2 different schemas within the same database, or I shoudl stick with a single one.

I am inclined to create two, one that would support the usual CRUD operations of the day-to-day application activities, and another schema that would contain summarized data representing the totals (sales, users etc) over a period of time, and that would be read only, but I am not sure what the ""best practice"" in this case is, since in the resources I have found they basically describe what a schema is but not much more.

If anybody has some advice I would be really grateful!"
3077,2020-08-09 13:41:01,1596969661.0,dataengineering,Hong Kong vs Singapore | GDP per capita from 1960 to 2017,i6hbws,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i6hbws/hong_kong_vs_singapore_gdp_per_capita_from_1960/,1.0,0.0,0.0,17111.0,
3078,2020-08-09 18:40:11,1596987611.0,dataengineering,Want to pursue a career in Data engineering and am looking for ways to add to my resume in order to look more appealing to hiring managers. Any advice or suggestions? What individual projects should I work on? Thanks!,i6ldj1,owter12,,https://www.reddit.com/r/dataengineering/comments/i6ldj1/want_to_pursue_a_career_in_data_engineering_and/,1.0,28.0,0.0,17121.0,
3079,2020-08-09 21:18:09,1596997089.0,dataengineering,Looking for an online Masters Degree,i6o4m8,c4rzb9,,https://www.reddit.com/r/dataengineering/comments/i6o4m8/looking_for_an_online_masters_degree/,1.0,6.0,0.0,17129.0,"Hello, I'm looking at options for an online master to get into the data engineering field. I did my undergrad in computer science at WGU and am considering their M.S. Data Analytics degree. I also applied to GA Tech's OMSA a few weeks ago. I'm just trying to gauge what my other options are if I don't get admitted into the OMSA program. I saw UCF has an online certificate/degree program:  
 [https://www.ce.ucf.edu/credit/master-data-analytics/](https://www.ce.ucf.edu/credit/master-data-analytics/) 

&amp;#x200B;

Are there any recommendations for good programs to look at that do not require the GRE?"
3080,2020-08-09 21:56:02,1596999362.0,dataengineering,Parquet partitioning for small data,i6otnr,r_mkay,,https://www.reddit.com/r/dataengineering/comments/i6otnr/parquet_partitioning_for_small_data/,1.0,8.0,0.0,17129.0," Hey guys,

Data scientist here trying to get more clued up on data engineering.

My company has historical time-series data for weather across a grid of \~50,000 latitude/longitude points. In total the data is about 50GB. Currently the data is stored as parquet files in S3, partitioned by the latitude/longitude position, so that there are 50,000 x 1MB files.  

From what I’ve been reading, having lots of small files may not be optimal. Would it be better to combine files and instead partition it as ie 50 x 1GB files? Due to the total size of data we aren’t using any big data tools like Spark - just using pyarrow to query into pandas – does this change anything?

In terms of access patterns, this data is only queried rarely, and when it is we will only query a small subset of latitude/longitude points.

Also as a more general point, when storing (small) historic data that doesn’t need to be accessed particularly fast/regularly, is S3/this approach a good way to store this data as opposed to say a regular SQL database?

Thanks!"
3081,2020-08-10 00:26:47,1597008407.0,dataengineering,Data engineering weekly - edition #3,i6rja2,vananth22,,https://www.reddit.com/r/dataengineering/comments/i6rja2/data_engineering_weekly_edition_3/,1.0,0.0,0.0,17135.0,
3082,2020-08-10 10:11:35,1597043495.0,dataengineering,Agile Methodologies in Data Engineering,i706rh,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/i706rh/agile_methodologies_in_data_engineering/,1.0,5.0,0.0,17159.0,"Do we, as a DE, need to know and practice Agile Methodologies? If yes, why and which one fits DE role best as there seem to be quite a few."
3083,2020-08-10 12:04:35,1597050275.0,dataengineering,Airflow vs Azure Data Factory,i71egr,Luukv93,,https://www.reddit.com/r/dataengineering/comments/i71egr/airflow_vs_azure_data_factory/,1.0,18.0,0.0,17166.0,"Hello,

My company is at the point of choosing an orchestration/ETL/ELT tool. 

I use Python and SQL in my daily work and feel like they are very flexible. I am afraid that choosing for Azure Data Factory will force me to use their GUI, and will lead to less flexible solutions. Also I think it will be pricy. 

Now that I've been on this reddit page I see a lot of you post about Airflow. What would you choose in my situation, azure data factory of airflow?"
3084,2020-08-10 12:34:56,1597052096.0,dataengineering,New Apache Airflow online course on Udemy,i71qjx,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/i71qjx/new_apache_airflow_online_course_on_udemy/,1.0,1.0,0.0,17169.0,
3085,2020-08-10 13:27:42,1597055262.0,dataengineering,Fish and Seafood Consumption Ranking | TOP 10 Country from 1961 to 2017,i72c1a,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i72c1a/fish_and_seafood_consumption_ranking_top_10/,1.0,0.0,0.0,17169.0,
3086,2020-08-10 18:29:03,1597073343.0,dataengineering,Deciding between Row- and Columnar-stores | Why we chose both,i76y1b,schoolgurllou,,https://www.reddit.com/r/dataengineering/comments/i76y1b/deciding_between_row_and_columnarstores_why_we/,1.0,2.0,0.0,17177.0,
3087,2020-08-10 19:58:57,1597078737.0,dataengineering,Measuring the cost of data fire drills?,i78ob6,mkvor8,,https://www.reddit.com/r/dataengineering/comments/i78ob6/measuring_the_cost_of_data_fire_drills/,1.0,1.0,0.0,17178.0,"Interested to learn how data teams measure the impact of bad data (and the associated firefighting) on their bottom line. I found this measurement for companies in the EU pretty accurate, at least to my experiences: 

&amp;#x200B;

&gt;**Labor Cost: (\[Number of Engineers\] X \[Annual Salary of Engineer\]) X 30%**  
**+**  
**Compliance Risk: \[4% of Your Revenue in 2019, per GDPR\]**  
**+**  
**Opportunity Cost: \[Revenue you could have generated if you moved faster, releasing X new products, and acquired Y new customers\]**

# = $ Annual Cost of Data Downtime

Data downtime refers to periods of time when your data is inaccurate, missing, etc. 

Here's the full article: [https://towardsdatascience.com/how-to-calculate-the-cost-of-data-downtime-c0a48733b6f0?source=friends\_link&amp;sk=7afa5091d6b76f39dc00c815ad25f975](https://towardsdatascience.com/how-to-calculate-the-cost-of-data-downtime-c0a48733b6f0?source=friends_link&amp;sk=7afa5091d6b76f39dc00c815ad25f975)

How do others measure the cost ($) of data downtime?"
3088,2020-08-10 20:23:56,1597080236.0,dataengineering,need a new AWS project or something to work on,i796e0,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/i796e0/need_a_new_aws_project_or_something_to_work_on/,1.0,3.0,0.0,17180.0,"I work on a team that isn't giving me much to do, but I have the freedom to find something interesting and educational to work on. If I tell them I'll work on something that's a little bit useful, they won't say no. 

Any ideas in AWS? We work in Redshift, s3, and EMR. You could call me a ""junior"" data engineer/analyst, intermediate SQL and Python, basic Scala (they use for EMR Spark jobs), etc. I've written Python scripts to interact with s3, for example iterating through buckets and doing stuff with objects. 

Short of a new AWS product that would cost a lot of money, is there anything really useful that I should learn, or something I should do with the above skills?"
3089,2020-08-10 20:37:32,1597081052.0,dataengineering,Data Visualization using python | Seaborn| Part-III,i79g47,8329417966,,https://www.reddit.com/r/dataengineering/comments/i79g47/data_visualization_using_python_seaborn_partiii/,1.0,0.0,0.0,17180.0,https://youtu.be/8VXGuXY2S5U
3090,2020-08-10 21:06:20,1597082780.0,dataengineering,Overview of Data Versioning use cases and tools,i7a141,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/i7a141/overview_of_data_versioning_use_cases_and_tools/,1.0,0.0,0.0,17181.0,
3091,2020-08-10 21:27:01,1597084021.0,dataengineering,Data Engineering Mentorship,i7ag2b,thehendoxc,,https://www.reddit.com/r/dataengineering/comments/i7ag2b/data_engineering_mentorship/,1.0,19.0,0.0,17181.0,"I have reached a stage (1 yrs Experience) now where I really would like some guidance on what I should be doing with my career, however data engineering being a relatively young field of expertise, I have struggled to find data engineering mentors (Mentorcruise, LinkedIn). If anyone knows anyone or has a recommendation I would love to hear it.

Also I would like to hear about anyone who is/was the ONLY Data Engineer at their company/department/team and their experience of learning.  I feel as though I have missed so much guidance, and best practices have come down to reading documentation and blog posts, then banging my head on the wall until it worked.

**About me:**

I graduated with a bachelors degree,  double majoring in CS and Stats, I have since reached 1yrs Experience as a Data Engineer at a small Software startup, being the only Data Engineer in the organization I have gained experience in many areas, and had to wear the analyst hat occasionally too. In my time some of the things I have done:

* Setup Production Airflow Deployment on Kubernetes (CI/CD with Spinnaker and Bamboo), I manage this alone, which was kinda daunting at the start.
* Setup all the ETL Pipelines and Datawarehousing Architecture in BigQuery and GCS
* Deployed an API for Data Scientists to Retrieve Complex data (think Joining Postgres + BigTable + BigQuery) using  using FastAPI
* Created and Manage All automated reporting and Dashboards
* Wearing the DA hat - Various Complex Data Analysis
* Design and Implementation of Data Analytics Pipelines (BigTable + BigQuery)"
3092,2020-08-10 22:28:32,1597087712.0,dataengineering,"An interview with the authors of the Practitioner's Guide To Graph Data about how, when, and why to use graph data algorithms and data structures.",i7bopx,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/i7bopx/an_interview_with_the_authors_of_the/,1.0,0.0,0.0,17182.0,
3093,2020-08-11 00:54:25,1597096465.0,dataengineering,Pushing data to Snowflake only allows for inserts and not updates; a good solution for this?,i7el4y,calltheambulampssir,,https://www.reddit.com/r/dataengineering/comments/i7el4y/pushing_data_to_snowflake_only_allows_for_inserts/,1.0,10.0,0.0,17186.0,"If I have a table in which the records will only ever be updated and there won't be any new inserts, what is a good approach in handling this discrepancy, seeing as how Snowflake only allows for new inserts and does not automatically delete the older version of a record, but only appends a new one to the table? (Since it doesn't support PKs)"
3094,2020-08-11 04:13:38,1597108418.0,dataengineering,How do you call this,i7i3zt,s_t_g_o,,https://www.reddit.com/r/dataengineering/comments/i7i3zt/how_do_you_call_this/,1.0,1.0,0.0,17192.0,"This tool is used to automatize a lot of simple workflows, but it's faster (faster than python, SSIS and Talend) and very performance. https://dixer.stgo.do/documentation/Jobs-types/

I'm the maintainer, but I'm investigating how to promote this and the niche. Actually is in production in 32 clients, but recently was released publicly.

Thanks."
3095,2020-08-11 07:48:47,1597121327.0,dataengineering,Data Visualization using python | Seaborn| Part-III,i7lea5,8329417966,,https://www.reddit.com/r/dataengineering/comments/i7lea5/data_visualization_using_python_seaborn_partiii/,1.0,0.0,0.0,17198.0,https://youtu.be/8VXGuXY2S5U
3096,2020-08-11 08:04:01,1597122241.0,dataengineering,What is a most related AWS service that performs the task that Meltano target for?,i7llnb,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/i7llnb/what_is_a_most_related_aws_service_that_performs/,1.0,8.0,0.0,17198.0,"https://meltano.com positions itself as the ET tool in the data pipeline. 

Does AWS have a service position like that (so that I can outsource that infrastructure to AWS)?"
3097,2020-08-11 09:52:46,1597128766.0,dataengineering,Questions about a DE Project from an absolute beginner,i7myg7,jotdwaetda,,https://www.reddit.com/r/dataengineering/comments/i7myg7/questions_about_a_de_project_from_an_absolute/,1.0,2.0,0.0,17203.0,"Hi, I'm a college student working on a small project in my spare time with a few questions for my project. I want to preface the rest of this post by saying that I am, as the title states, a total beginner to such projects / the DE field in general and would like to apologize in advance for any extremely basic questions.  

I'm scraping job postings for Business Intelligence Analysts from Glassdoor with Selenium + BeautifulSoup, writing it to a CSV file and storing that in S3 for transformation before I load the processed data to a Postgres DB. 

The use case for the data is to determine the most asked-for skills for BI Analysts overall, and also depending on the company's industry (e.g. tech, healthcare) / company size / location / etc. 

I have two large questions regarding this process:

1. 
I plan on scraping the entire job description, and then on finding the key skills asked for (e.g. Tableau, SSIS, Excel) in the transformation process. I don't know whether I should have a pre-defined list containing all the possible skills that I can think of being asked for in job descriptions so that I can go through the job description and look for words that are contained in the list. Otherwise, I'd have to do some kind of Natural Language Processing keyword recognition to find skills in the job description and I don't think I'm exactly ready to do that.
Also, I have a good idea of how I'd do the list comparison method in Python and Pandas, but I don't know how I would / how feasible it is with SQL (much less experienced with SQL than Python). Can someone tell me how simple of a task this would be in SQL? I'd prefer to do as much work in SQL to make myself more comfortable with it.

2. 
I don't know how I would store the job skills asked for in my table. I know it doesn't make sense to have a list (of skills) as a value for a column in my table - but the only alternative that's coming up in my head right now is having a column for each skill with the value being either Y or N, which seems extremely stupid."
3098,2020-08-11 13:38:12,1597142292.0,dataengineering,Death rate from Obesity Ranking | TOP 10 Country from 1990 to 2017,i7ph19,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i7ph19/death_rate_from_obesity_ranking_top_10_country/,1.0,0.0,0.0,17209.0,
3099,2020-08-11 15:09:11,1597147751.0,dataengineering,need some advice/tips on data preparation tasks,i7qn1s,pd33,,https://www.reddit.com/r/dataengineering/comments/i7qn1s/need_some_advicetips_on_data_preparation_tasks/,1.0,4.0,0.0,17212.0,"I'm a platform/java developer, new to DE. I've done some small data cleansing tasks with python in the past (mainly fixing bad, missing records, etc) and a bit familiar with pandas and ML concepts, however I haven't done any real work in this domain. 
just wanted to know what are the best practices? also any advise on not using a particular tool? hated/annoying ones?"
3100,2020-08-11 16:31:38,1597152698.0,dataengineering,Quick question,i7rv7s,chik-16,,https://www.reddit.com/r/dataengineering/comments/i7rv7s/quick_question/,1.0,0.0,0.0,17213.0,"Hello All, Had a dbt in the  data engineering part 
I have a yaml config file which has a tag that holds the name of the file which has to be loaded to the hive database.
The requirement is to pass a joined file as input in the yaml file, any idea or can suggest any such link"
3101,2020-08-11 17:46:38,1597157198.0,dataengineering,"Here are basics to advanced-level questions involving Hadoop Cluster, HDFS, MapReduce, HBase, Pig, and Hive.",i7t39v,devika_9316,,https://www.reddit.com/r/dataengineering/comments/i7t39v/here_are_basics_to_advancedlevel_questions/,1.0,0.0,0.0,17213.0,
3102,2020-08-11 19:23:00,1597162980.0,dataengineering,Thankful for this community,i7urp4,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/i7urp4/thankful_for_this_community/,1.0,16.0,0.0,17213.0,"After the final interview with CTO, I got a call from HR that unfortunately I didn’t get the job due to them having different needs as of the moment (also due to the urgency, as I would have to process my work visa), despite me doing well in all my interviews apparently. Nevertheless, I’m thankful for this community. I have learned so much and I’ll keep on trying until I get that sweet offer."
3103,2020-08-11 20:50:42,1597168242.0,dataengineering,Extract raw relational (SQL) data in data lake or extract it directly to synapse?,i7wf6g,Luukv93,,https://www.reddit.com/r/dataengineering/comments/i7wf6g/extract_raw_relational_sql_data_in_data_lake_or/,1.0,3.0,0.0,17215.0,"Hello,

We're at a point where we receive more and more semi-unstructured / unstructured data. Extracting data from API's in JSON/XML, sensor data from our production plant (IoT) and new applications that my company purchases are SaaS. Along with that also have numerous on-prem applications that provide relational (sql) data.

I am designing a new architecture using the Azure Cloud. Herebelow a link to reference of modern data warehouse I believe looks like what we need.

 [https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/modern-data-warehouse](https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/modern-data-warehouse) 

I strongly believe in having around 3 zones:

1. Raw data zone (no transformations are allowed here - future sandbox for a data scientist)
2. Analyst data zone (where analysts can query data using T-SQL)
3. OLAP models (used by business users to visualise data using Power BI)

&amp;#x200B;

Now I need your advise on the following:

1.  Should we extract relational data and load it to a data lake (con: losing relational meta-data such as data types)? 
2. When to load directly to synapse (former azure sql data warehouse) vs going to the data lake first?
3. What would be the value of extracting relational data into the data lake?"
3104,2020-08-11 23:10:50,1597176650.0,dataengineering,AWS BI Starter Project,i7z3hi,kprat2312,,https://www.reddit.com/r/dataengineering/comments/i7z3hi/aws_bi_starter_project/,1.0,0.0,0.0,17222.0,"Hello Folks,

I am a Graduate Data Analytics student. Unfortunately, my coursework allows me a little or no exposure to cloud technologies. I have some experience with AWS framework but have nothing that would add up to my profile/resume. 

I would love to have a Project to collect, store and analyze data in real time as well as in batch to exemplify both technologies. 

Anyone here who can help me with a starter project guide I can follow and take inspiration from?

Thank you!"
3105,2020-08-12 00:23:59,1597181039.0,dataengineering,Recommendations for Possible Data Federation Frameworks/Platforms,i80gh4,Divide_Unknown,,https://www.reddit.com/r/dataengineering/comments/i80gh4/recommendations_for_possible_data_federation/,1.0,1.0,0.0,17222.0,"I'm currently performing open-ended research on Data federation and consolidation for the back-end of a new enterprise application with a public facing UI and was curious as to what kind of suggestions, or recommendations this subbreddit may have in reference to available platforms, frameworks, etc.  At the highest level, the goal for the application and UI layers is to pull data from multiple disparate data sources (databases, APIs, services) and write to them as well.

  
X-Posting to r/datascience"
3106,2020-08-12 07:47:03,1597207623.0,dataengineering,Spark vs Kafka vs Flink: purpose and differences?,i87h9s,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/i87h9s/spark_vs_kafka_vs_flink_purpose_and_differences/,1.0,3.0,0.0,17233.0,"It seems to me that Spark/Kafka/Flink do the same thing: data streaming. So when do we decide to use one particular tool, and under what circumstances?

A detailed description of their purpose and the differences between them would be highly appreciated."
3107,2020-08-12 08:31:34,1597210294.0,dataengineering,Must Have Data Science Skills That You Should Learn,i881ka,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i881ka/must_have_data_science_skills_that_you_should/,1.0,0.0,0.0,17235.0,
3108,2020-08-12 08:39:37,1597210777.0,dataengineering,Looking for a mentor/collaborate for a data engineering side project,i88551,StatusStar,,https://www.reddit.com/r/dataengineering/comments/i88551/looking_for_a_mentorcollaborate_for_a_data/,1.0,5.0,0.0,17234.0,"Hello,

I just finished an AWS certification and want to use the theoretical knowledge I have gained to build a project where data is handled end to end. I am still thinking about the details of the project and would love to brainstorm. I am okay in Python. Looking to potentially use Apache Airflow in the project with spark. Please let me know if someone wants to mentor/ collaborate on this. Thanks."
3109,2020-08-12 08:55:37,1597211737.0,dataengineering,How do we manage data integrity and data security.,i88bzr,harshit173,,https://www.reddit.com/r/dataengineering/comments/i88bzr/how_do_we_manage_data_integrity_and_data_security/,1.0,0.0,0.0,17235.0,
3110,2020-08-12 10:56:44,1597219004.0,dataengineering,Has anyone recently interviewed with FB for the data engineer role? What’s the on-site like for Python?,i89pns,wowwowwow666888,,https://www.reddit.com/r/dataengineering/comments/i89pns/has_anyone_recently_interviewed_with_fb_for_the/,1.0,28.0,0.0,17237.0,
3111,2020-08-12 11:57:52,1597222672.0,dataengineering,What are your biggest time sinks?,i8acnv,whichalps,,https://www.reddit.com/r/dataengineering/comments/i8acnv/what_are_your_biggest_time_sinks/,1.0,19.0,0.0,17238.0,"Hi DE, 
coming from my own experience, I was wondering what other people sink the most time into besides writing &amp; orchestrating transforms. 

For me, those would be in declining order (longest at the top):
- custom API integrations not covered by tools
- setting up (one time) &amp; optimising monitoring/ logging 
- optimising infrastructure (frequently)
- optimising data model (metadata collection, potentially compliance topics, fine grained access policies etc.)

I do all that mostly using Python, Terraform w. Packer on AWS. Trying to leverage ECS as much as what is reasonable. 
Finally, I'm using micro batches, no deep expertise how these things might look like in a streaming scenario."
3112,2020-08-12 13:26:51,1597228011.0,dataengineering,Wind Energy Ranking | TOP 10 Country from 1980 to 2018,i8bb2d,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i8bb2d/wind_energy_ranking_top_10_country_from_1980_to/,1.0,0.0,0.0,17241.0,
3113,2020-08-12 13:46:12,1597229172.0,dataengineering,Is there Google Cloud Storage and AWS S3 cached bucket Python library?,i8bix4,ratatouille_artist,,https://www.reddit.com/r/dataengineering/comments/i8bix4/is_there_google_cloud_storage_and_aws_s3_cached/,1.0,1.0,0.0,17241.0,I am looking for a GCS and S3 cached bucket Python library that given a URI download the file to a local cache if unavailable. Is there such a library out there?
3114,2020-08-12 14:23:39,1597231419.0,dataengineering,An interview with the co-founders of Iteratively about building a platform that solves the collaboration gap for event data collection.,i8byw7,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/i8byw7/an_interview_with_the_cofounders_of_iteratively/,1.0,0.0,0.0,17241.0,
3115,2020-08-12 14:32:47,1597231967.0,dataengineering,Using airflow on Docker vs AWS EC2 instance,i8c2vz,shettyhitesh,,https://www.reddit.com/r/dataengineering/comments/i8c2vz/using_airflow_on_docker_vs_aws_ec2_instance/,1.0,3.0,0.0,17242.0,"Hi, I'm fairly new to data engineering and I have been studying airflow recently. 

I've gone through some tutorials where they've used Docker as a base for using airflow, I on the other hand have to use an EC2  instance for the same.
If someone could enlighten me on the differences of using airflow with these and why people prefer one to the other that would be a huge help.

Any suggestions on where I could get good learning resources (paid or unpaid) would be very helpful as well.

Thank you"
3116,2020-08-12 20:12:31,1597252351.0,dataengineering,Could somebody help me with ideas for a project in Data Warehousing?,i8hq38,smpvlc,,https://www.reddit.com/r/dataengineering/comments/i8hq38/could_somebody_help_me_with_ideas_for_a_project/,1.0,17.0,0.0,17251.0,"Good afternoon,

As part of my degree I need to submit a project and my area of interest (Data Warehousing) doesn't seem to be particularly interesting to my tutor, so I feel I am basically on my own trying to figure out what to do.

For the basic set-up I have implemented two databases in postgreSQL, one that simulates the OLTP for a small scale start-up and another one that simulates connexion details for users (when they logged in, duration of the session, device etc).

The idea is to integrate these two into a data mart that will contain a fact table f_customer and several dimensions country, types_of_purchases, reviews, etc, to allow the marketing and customer services team to understand their customers better.

For the data exploitation part I was thinking of connecting PostgreSQL to Metabase to provide the necessary visualization.
As for the reporting tools I was looking for something similar to Pentaho Report Designer but I haven't found anything yet.

I would perform the ETL in SQL whenever possible, since most of my data would be structured, and they need to see code, which basically means I need to do as many stored procedures and triggers as possible.

Whilst I feel that the stored procedures, triggers, data modelling, ETL processes, index optimization and data exploitation should be enough....I haven't received a particularly enthusiastic reaction, which makes me worry a bit.

I am planning on using PostgreSQL, Metabase and I would try to stick to Node.js whenever possible, since we barely did any HTTP/networking module on my degree and express.js is the only tool I feel relatively comfortable with (same goes with any cloud environment, I haven't seen any of those).

Could anybody give me something else that could add to this project that I haven't thought of?.

Many thanks!"
3117,2020-08-13 00:33:54,1597268034.0,dataengineering,How to get your data scientists and data engineers rowing in the same direction,i8mru5,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/i8mru5/how_to_get_your_data_scientists_and_data/,1.0,0.0,0.0,17255.0,
3118,2020-08-13 06:19:24,1597288764.0,dataengineering,Method to export DB (Snowflake) Table into single S3 JSON object per row,i8setn,m4329b,,https://www.reddit.com/r/dataengineering/comments/i8setn/method_to_export_db_snowflake_table_into_single/,1.0,4.0,0.0,17263.0,"I have a table of around 50 million records in Snowflake I convert to JSON and load to S3, it defaults to 1 large file of JSON in S3 eg:

    s3://snowflake_output/out.json -&gt;
    {""id"": ""1"", ""name"": ""Joe""}
    {""id"": ""2"", ""name"": ""Sue""}
    {""id"": ""3"", ""name"": ""Chris""}

What I need to do is take that JSON file in S3 and convert each row to a separate JSON object in S3:

    s3://snowflake_output/out1.json-&gt;
    {""id"": ""1"", ""name"": ""Joe""}
    
    s3://snowflake_output/out2.json-&gt;
    {""id"": ""2"", ""name"": ""Sue""}
    
    s3://snowflake_output/out3.json-&gt;
    {""id"": ""3"", ""name"": ""Chris""}

The reason is I have an AWS Lambda Function that horizontally scales and processes each individual json record on an event trigger. Doing this in just a for loop over the big JSON file is pretty slow.

Is there a way to export by row in Snowflake?

Is there an easy solution in AWS to break it out into many objects instead of 1?"
3119,2020-08-13 08:05:01,1597295101.0,dataengineering,Apache Airflow Interview questions,i8tubj,shettyhitesh,,https://www.reddit.com/r/dataengineering/comments/i8tubj/apache_airflow_interview_questions/,1.0,2.0,0.0,17267.0,What do you think are the best questions for judging a person's knowledge of airflow and workflow management systems in general?
3120,2020-08-13 08:09:40,1597295380.0,dataengineering,What's it like to work in Data Engineering?,i8twer,schlendeus,,https://www.reddit.com/r/dataengineering/comments/i8twer/whats_it_like_to_work_in_data_engineering/,1.0,36.0,0.0,17267.0,"Hey guys, I'm curious about work in data engineering. Seems fun and interesting for people who have strong engineering chops.

What's the day-to-day work like?

What are the skillsets I'd have to be good at?

What is the annoying / drudgery stuff you have to be willing to put up with?

What are the day to day problems that come up over and over again?"
3121,2020-08-13 08:53:05,1597297985.0,dataengineering,How to be expert at using SQL Window Functions?,i8uff6,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/i8uff6/how_to_be_expert_at_using_sql_window_functions/,1.0,5.0,0.0,17269.0,"I want to gain some expertise on using SQL Window Functions. I always find myself struggling to write complex queries. How do I start, or get more exercises to do?

Any courses or any suggestions would be really helpful."
3122,2020-08-13 09:55:24,1597301724.0,dataengineering,Amazon or Facebook Data Engineer Interview process. Easier one?,i8v5c6,_chickoo_,,https://www.reddit.com/r/dataengineering/comments/i8v5c6/amazon_or_facebook_data_engineer_interview/,1.0,0.0,0.0,17272.0,"Which of the two DE interviews is easier to crack(at a junior level) and why? Would love to hear personal experiences in comparison if any.

[View Poll](https://www.reddit.com/poll/i8v5c6)"
3123,2020-08-13 13:35:03,1597314903.0,dataengineering,HIV Infection Ranking | TOP 10 Country from 1990 to 2017,i8xjyf,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i8xjyf/hiv_infection_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,17283.0,
3124,2020-08-13 13:39:09,1597315149.0,dataengineering,Breaking up pipelines based on use case or based on sources,i8xlor,databass09,,https://www.reddit.com/r/dataengineering/comments/i8xlor/breaking_up_pipelines_based_on_use_case_or_based/,1.0,5.0,0.0,17283.0,"I am managing data pipelines that are starting to become complex. We have many different sources and are delivering many different datasets from those sources. Currently, we have one pipeline that takes in all sources, waits for the sources to get processed in our data lake, and then produces the business facing datasets. This is not ideal because it forces us to ingest all data sources at once leading to resource bottlenecks. 

What I'm wondering is what preferences this community has when breaking up and modularizing data pipelines. From what I can tell, there are a couple of options

* Create one, single ""mono-pipeline"" as I referenced above
   * Pros: Simple to manage
   * Cons: Business facing datasets may be delayed if upstream data source processing is delayed; Unable to run data pipelines on different cadences (e.g. daily vs. hourly). 
* Create a pipeline per business facing dataset, including ingestion of relevant sources
   * Pros: Datasets are delivered just-in-time to stakeholders; Pipelines will run faster because they will only include ingestion of relevant sources
   * Cons: What happens if two pipelines attempt to ingest the same source at the same time; There are more pipelines to manage
* Create an ingestion pipeline per data source, but then decouple pipelines that produce the business facing use cases
   * Pros: Ingestion of data sources can occur at different cadences, relieving pressure of resources
   * Cons: No longer possible to perform a DAG to see if source ingestions upstream of business facing datasets have failed or have not completed.

I would love to hear this community's thoughts!"
3125,2020-08-13 15:21:38,1597321298.0,dataengineering,HIV Infection Ranking | TOP 10 Country from 1990 to 2017,i8ywtd,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i8ywtd/hiv_infection_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,17282.0,
3126,2020-08-13 16:19:05,1597324745.0,dataengineering,Working With Iris.csv,i8zrft,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/i8zrft/working_with_iriscsv/,1.0,0.0,0.0,17284.0,
3127,2020-08-13 16:28:51,1597325331.0,dataengineering,Rejected for DE role since current DE experience more BI than SWE,i8zwvf,floydhead11,,https://www.reddit.com/r/dataengineering/comments/i8zwvf/rejected_for_de_role_since_current_de_experience/,1.0,39.0,0.0,17285.0,"Hi all,
This post is a continuation on my previous post 3 months ago - Am I a Data Engineer? (Will post the link here). I am looking for some guidance from this sub.

I have been working as a DE for over 2 years in a Microsoft shop and predominantly using SQL. I create pipelines, maintain databases over multiple servers, and have 6-7 major source systems to utilise. I also maintain reporting and create reports every once in a while. Additionally, I work with customers to figure out their needs and create business logic.

After identifying (thanks to this group and LinkedIn searches) that I need to ramp up on my Python skills, I started grinding LeetCode.
I think I must have done around 70-90 (easy-medium, few hards) questions since that post.

There was a point where I felt confident enough to start applying for ""core"" DE jobs. These are the ones that use Python, Airflow, DAGS, and all that fancy jazz.
I wanted to pivot into this since I've always liked coding and most of the high challenge high pay ""cool"" DE jobs require these skills. (Gotta break into FAANG)

One reputable company got back to me and I completed the onsite.
Overall, I had 4 technical screens (all Python) and 1 System Design (Messenger).

I was actually able to get accurate results for all for Python questions and mainly failed to optimise my last on-site question.

For System Design, I lacked that prior interviewing experience but I managed to explain all the concepts that I was aware of. It wasn't the most comprehensive design and there were flaws for sure.

Eventually, I was rejected because my work experience is more ""BI related"" than what they were looking for.

This was a huge bummer because I literally put everything to pause and worked on grinding LeetCode, practiced System Design, did projects in Python, and also was working &gt;9 hours every day.

It feels like a catch 22. I don't belong to ""core"" DE because I lack professional experience in ""core"" DE but I cannot get that at my current company or anywhere without switching, which, apparently, I cannot.

I am sure this is not unique and I was looking for some pointers!"
3128,2020-08-13 17:13:25,1597328005.0,dataengineering,Data model tools for data warehouse,i90mxd,apbcx,,https://www.reddit.com/r/dataengineering/comments/i90mxd/data_model_tools_for_data_warehouse/,1.0,5.0,0.0,17287.0,"Hi guys,

Would you know/use any tool for data warehouse design and architecture? I've been thinking about this and I believe that Visio might not be helpful for these databases. They have thousands of tables, field and etc, it'd be difficult to draw and make it useful.

Hope that the question is clear and helpful others as well.
Thanks for your suggestions and apologies if I've made anything wrong in this post.

:)"
3129,2020-08-13 19:42:08,1597336928.0,dataengineering,Data Visualization using Matrix Plot | Python| Seaborn,i93caj,8329417966,,https://www.reddit.com/r/dataengineering/comments/i93caj/data_visualization_using_matrix_plot_python/,1.0,0.0,0.0,17298.0,https://youtu.be/XU6NLA_U6_E
3130,2020-08-13 20:27:00,1597339620.0,dataengineering,Data Visualization using Matrix Plot | Python| Seaborn,i946r0,8329417966,,https://www.reddit.com/r/dataengineering/comments/i946r0/data_visualization_using_matrix_plot_python/,1.0,0.0,0.0,17304.0,https://youtu.be/XU6NLA_U6_E
3131,2020-08-13 22:19:15,1597346355.0,dataengineering,Metaplane - A metadata catalog for modern data teams,i96bgp,curvature_propulsion,,https://www.reddit.com/r/dataengineering/comments/i96bgp/metaplane_a_metadata_catalog_for_modern_data_teams/,1.0,0.0,0.0,17307.0,
3132,2020-08-13 22:29:46,1597346986.0,dataengineering,Metaplane - A metadata catalog for modern data teams,i96iof,curvature_propulsion,,https://www.reddit.com/r/dataengineering/comments/i96iof/metaplane_a_metadata_catalog_for_modern_data_teams/,1.0,2.0,0.0,17307.0,
3133,2020-08-13 23:33:19,1597350799.0,dataengineering,Prefect seems useless without Cloud offering,i97p3i,Arzela-Ascoli37,,https://www.reddit.com/r/dataengineering/comments/i97p3i/prefect_seems_useless_without_cloud_offering/,1.0,3.0,0.0,17313.0,"I went through the prefect docs, and I like the syntax and functionality for specifying DAGs, but it seems like it lacks any ability to orchestrate your pipeline (airflow handles this), unless you use their Prefect Cloud offfering.

It also appears that it doesn't have any way to track past runs, so it can't tell which tasks have been completed. 

As a demonstrative example, consider that you need to run a pipeline every day (and backfill since a historical start date if necessary). 

In airflow, you just list the tasks and they'll all run. It will schedule them to run everyday, it won't rerun things that have already happened, and it will backfill if necessary. In contrast, Prefect you would just run python ETL.py in a cron job everyday, and it would loop through all of the tasks and do them. It will redo all the old tasks everyday (no bueno). If you add more independent flows to your pipeline, just stick them all in your cron file (no bueno)

This seems abysmal. Is anyone successfully running prefect without their cloud offering? Are you running everything through cron? Are you running prefect with airflow? Any repositories with instructive examples? It feels like you can't really run prefect without airflow or Luigi, and if you're running airflow or Luigi, there's no point in going through the extra hassle of running prefect. Am I missing something about how to use Prefect Core properly? Thanks!"
3134,2020-08-13 23:39:15,1597351155.0,dataengineering,How to deploy data analytics (ETL/BI/ML/AI/...),i97t1m,buntro,,https://www.reddit.com/r/dataengineering/comments/i97t1m/how_to_deploy_data_analytics_etlbimlai/,1.0,7.0,0.0,17313.0,
3135,2020-08-13 23:41:56,1597351316.0,dataengineering,"When data breaks and no one hears it, does it make a sound?",i97ut1,mkvor8,,https://www.reddit.com/r/dataengineering/comments/i97ut1/when_data_breaks_and_no_one_hears_it_does_it_make/,1.0,0.0,0.0,17313.0,"Does this resonate? :) 

[https://towardsdatascience.com/good-tales-of-bad-data-91eccc29cbc5?source=friends\_link&amp;sk=31e7035aa3a3d08cad2e2a50e018b1e8](https://towardsdatascience.com/good-tales-of-bad-data-91eccc29cbc5?source=friends_link&amp;sk=31e7035aa3a3d08cad2e2a50e018b1e8)"
3136,2020-08-14 00:08:11,1597352891.0,dataengineering,Mock Interview Platforms for Data Engineering Roles,i98ccg,crazy_roadster,,https://www.reddit.com/r/dataengineering/comments/i98ccg/mock_interview_platforms_for_data_engineering/,1.0,12.0,0.0,17313.0,"I am currently working as a Data Engineer(mostly ETL) at a startup. I am planning to apply for DE roles in big companies (preferably FAANG). I have been prepping for the last 1.5 months. 

I thought of taking a few mock interviews to analyze and understand where I stand with respect to the preparation and get some direction to focus on the key areas where I am lagging. I looked up Pramp, [Interview.io](https://Interview.io), [expertmitra.com,](https://expertmitra.com) and a few others and all seem to be focused on SWE/Data Science but none on Data Engineering. 

Is there a website or service that offers mock interviews for Data Engineers? I would be glad if someone could point me to a website or individual. Thanks"
3137,2020-08-14 01:05:12,1597356312.0,dataengineering,How good at math do you have to be to be a data engineer?,i99dk1,shaolinswangster,,https://www.reddit.com/r/dataengineering/comments/i99dk1/how_good_at_math_do_you_have_to_be_to_be_a_data/,1.0,4.0,0.0,17316.0,So I’m good at programming but bad at math I’m applying to college for a cs major. Can someone who’s bad at math do de?
3138,2020-08-14 09:18:47,1597385927.0,dataengineering,Working With Iris.csv.,i9gsok,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/i9gsok/working_with_iriscsv/,1.0,0.0,0.0,17330.0,
3139,2020-08-14 10:50:25,1597391425.0,dataengineering,Top 10 Statistics Tools to Get Better Data Insights,i9htpk,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/i9htpk/top_10_statistics_tools_to_get_better_data/,1.0,0.0,0.0,17334.0,
3140,2020-08-14 11:22:04,1597393324.0,dataengineering,How popular are intern positions in Europe?,i9i60x,mihai_zanfir,,https://www.reddit.com/r/dataengineering/comments/i9i60x/how_popular_are_intern_positions_in_europe/,1.0,0.0,0.0,17336.0,"Hi,

Since internship / graduate positions are opening soon, I was wondering how popular are they in Europe? I'm sure they are not as popular as SWE positions but should I expect to find a decent amount of companies looking for DE interns / graduates?

In my current country of residence there are little to none openings of this sort.

Also, how does the interview process happen for an intern most of the time? Is it mostly SQL and algo &amp; ds problems?"
3141,2020-08-14 12:57:46,1597399066.0,dataengineering,Data Governance From an Engineering Perspective,i9j6e8,valdasm,,https://www.reddit.com/r/dataengineering/comments/i9j6e8/data_governance_from_an_engineering_perspective/,1.0,1.0,0.0,17338.0,
3142,2020-08-14 13:39:23,1597401563.0,dataengineering,Hong Kong vs Singapore | Merchandise exports from 1960 to 2018,i9jne6,Video_Tight,,https://www.reddit.com/r/dataengineering/comments/i9jne6/hong_kong_vs_singapore_merchandise_exports_from/,1.0,0.0,0.0,17339.0,
3143,2020-08-14 20:46:12,1597427172.0,dataengineering,r/dataengineeringjobs - New sub for those looking for work,i9qmab,claytonjr,,https://www.reddit.com/r/dataengineering/comments/i9qmab/rdataengineeringjobs_new_sub_for_those_looking/,1.0,2.0,0.0,17348.0,"Hey folks, some of you might be like me and are looking for new work now. I was furloughed a few months ago, I'm just trying to get back out there. I noticed there was not a special forum for data engineering jobs, like some other forums had. Anyway, I created a new sub, and will be from time to time posting jobs. I personally have a strong interest in remote de jobs, so I might post more of that type. I've even entertained the idea of building a bot to post jobs, just to keep it going. 

Anyway, this is my public notice. Please join up if you're interested. Please help out if you can. 

Thanks

r/dataengineeringjobs"
3144,2020-08-15 07:21:37,1597465297.0,dataengineering,Best Open-Source Tools for Data Mining Techniques,ia12xa,codeavail_expert,,https://www.reddit.com/r/dataengineering/comments/ia12xa/best_opensource_tools_for_data_mining_techniques/,1.0,0.0,0.0,17367.0,
3145,2020-08-15 10:20:24,1597476024.0,dataengineering,Which is the Best Platform to learn Data Engineering?,ia34w4,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/ia34w4/which_is_the_best_platform_to_learn_data/,1.0,0.0,0.0,17374.0,
3146,2020-08-15 14:12:41,1597489961.0,dataengineering,[Help] How does my Data Engineering training course look?,ia5jab,DataEngineeThrowaway,,https://www.reddit.com/r/dataengineering/comments/ia5jab/help_how_does_my_data_engineering_training_course/,1.0,16.0,0.0,17381.0,"Hi,   


I'm currently on a 12 week Data Engineering course designed to give people new to the field the knowledge to become a junior data engineer. Each week is a new topic, with 40 hours of training. There is also lots of training with soft skills, with heavy emphasis on teamwork.

1.Intro to curriculum and role

2.Intro to python

3.Structuring code with OOP and structuring data using encoding

4.Intro to testing

5.Intro to databases

6.Visualising data and using API's

7.Intro to cloud and containers

8.Intro to cloud services (AWS tools)

9.Intro to data warehouses, data security, data cleaning

10.Understanding ETL, serverless and logging

11.Integration &amp; deployment of data lakes

12.Introduction to data streams and snowflake  


Is there anything else I should look at that isn't on this course? My plan is to do some of the projects on this subreddit when I have the knowledge, to build a portfolio, and read through the data engineering cookbook by Andreas Kretz."
3147,2020-08-15 15:33:37,1597494817.0,dataengineering,Lemon and Lime Production Ranking | TOP 10 Country from 1961 to 2018,ia6jy5,Video_Prom,,https://www.reddit.com/r/dataengineering/comments/ia6jy5/lemon_and_lime_production_ranking_top_10_country/,1.0,0.0,0.0,17384.0,
3148,2020-08-15 17:52:55,1597503175.0,dataengineering,Data lake on GCP using Terraform,ia8om4,tuankid,,https://www.reddit.com/r/dataengineering/comments/ia8om4/data_lake_on_gcp_using_terraform/,1.0,0.0,0.0,17391.0,
3149,2020-08-15 22:05:03,1597518303.0,dataengineering,How to set up local Apache Spark environment (5 ways),iadb05,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/iadb05/how_to_set_up_local_apache_spark_environment_5/,1.0,0.0,0.0,17400.0,
3150,2020-08-16 12:05:32,1597568732.0,dataengineering,Expected Technical phone interview questions,iap9ew,AliGMaye,,https://www.reddit.com/r/dataengineering/comments/iap9ew/expected_technical_phone_interview_questions/,1.0,0.0,0.0,17420.0,"Hello awesome engineers,  hope you all are doing well.Thanks to all the advise you post on this forum,  I have been able to apply it and now I have been selected for a technical phone interview. I don't know what questions to expect but would love to have an idea of what the interviewers might ask.
Thanks and all answers will be highly appreciated."
3151,2020-08-16 13:14:45,1597572885.0,dataengineering,Skew join optimization in Apache Spark and clean up policy in Apache Kafka,iapznp,BartoszWfc,,https://www.reddit.com/r/dataengineering/comments/iapznp/skew_join_optimization_in_apache_spark_and_clean/,1.0,0.0,0.0,17424.0,"Hi,

It's the new weekly update for the [waitingforcode.com](https://www.waitingforcode.com) blog posts:

1. The Adaptive Query Execution in Apache Spark follow-up - after shuffle partitions coalesce, it's time to discover another optimization. It addresses the skewed joins: [https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-join-skew-optimization/read](https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-join-skew-optimization/read) 
2. From time to time, I also write about Apache Kafka. The ultimate goal is to understand how the transactions work but before reaching the goal, there are some topics to discover first. The next one (some others are here: [https://www.waitingforcode.com/apache-kafka](https://www.waitingforcode.com/apache-kafka) ) is clean up policy: [https://www.waitingforcode.com/apache-kafka/logs-compaction-apache-kafka-delete-cleanup-policy/read](https://www.waitingforcode.com/apache-kafka/logs-compaction-apache-kafka-delete-cleanup-policy/read) 

&amp;#x200B;

Happy reading!  
Best,  
Bartosz."
3152,2020-08-16 13:33:22,1597574002.0,dataengineering,Coal Energy Ranking | TOP 10 Country from 1965 to 2019,iaq6oe,Video_Prom,,https://www.reddit.com/r/dataengineering/comments/iaq6oe/coal_energy_ranking_top_10_country_from_1965_to/,1.0,0.0,0.0,17424.0,
3153,2020-08-16 19:13:27,1597594407.0,dataengineering,"Data Engineering Weekly #4 (Focus on Dagster, Kafka on Kubernetes and ML applications)",iav1u0,vananth22,,https://www.reddit.com/r/dataengineering/comments/iav1u0/data_engineering_weekly_4_focus_on_dagster_kafka/,1.0,1.0,0.0,17431.0,
3154,2020-08-16 19:22:05,1597594925.0,dataengineering,AUTOPLOTTER: GUI BASED EXPLORATIVE DATA ANALYSIS | PYTHON| DATA SCIENCE,iav7jz,8329417966,,https://www.reddit.com/r/dataengineering/comments/iav7jz/autoplotter_gui_based_explorative_data_analysis/,1.0,0.0,0.0,17434.0,https://youtu.be/r5uGJcp6ru0
3155,2020-08-16 21:55:30,1597604130.0,dataengineering,Data Warehouse Use Cases,iay2ci,usculler,,https://www.reddit.com/r/dataengineering/comments/iay2ci/data_warehouse_use_cases/,1.0,4.0,0.0,17439.0,"I recently joined a medium sized non profit organization as a data engineer with the task of improving and building their data and analytics infrastructure. The data I'm working with is a bunch of marketing data sprawled across different sources such as a Postgres order database, google analytics, social media data, etc.

One of the first things I noticed was that they didn't have a data warehouse. I recently submitted a paper detailing what a data warehouse is, costs, benefits, etc to our VP who is interested. He now wants me to submit a 1 page report with all the use cases for implementing this.

I just spent some time writing down some bullet points for some of the use cases for a data warehouse:

 

* Faster Data Insights
* Customizable, ad hoc reporting across multiple data sources
* Automated Reports
* Predictive Analytics - Predicting Future Growth and Bottlenecks
* Advanced Statistical Algorithms such as Machine Learning, Deep Learning, and Data Science
* Streamlined Data Access &amp; Democratization
* Scalable Data Integration
* Automated Data Cleansing and Transformation (ETL/ELT)
* A Single Consolidated Source of Truth
* Collaboration Between Data Analytics &amp; Business (BI) Users
* Advanced Data Analytics Frameworks such as Python, R, SQL, etc.
* Ability to Quickly Swap In New Tooling, Frameworks, and BI Visualization Platforms
* Query Key Performance Indicators in Real Time

Does this seem like a comprehensive list? Is there anything I should add or remove? Any hints, tips, or pieces of advice are greatly appreciated!"
3156,2020-08-17 04:59:46,1597629586.0,dataengineering,I invite all of you to a complete tutorial on how to use one of the most popular python IDE i.e. PyCharm for beginners.Here you ll know the basics of PyCharma and yo ll know why to chose PyCharm over other IDEs.,ib5a9c,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/ib5a9c/i_invite_all_of_you_to_a_complete_tutorial_on_how/,1.0,0.0,0.0,17450.0,
3157,2020-08-17 07:35:10,1597638910.0,dataengineering,"Hello everyone,as we all know businesses around the world have started making corporate decisions based on the data that they have collected over the years and most of the programming language that they use is Python and R, this article will help you to understand all the concept of Pandas in Python",ib7iou,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/ib7iou/hello_everyoneas_we_all_know_businesses_around/,1.0,0.0,0.0,17451.0,
3158,2020-08-17 14:31:47,1597663907.0,dataengineering,Start writing better DAGs by discovering Apache Airflow Best Practices!,ibceit,marclamberti,,https://www.reddit.com/r/dataengineering/comments/ibceit/start_writing_better_dags_by_discovering_apache/,1.0,6.0,0.0,17463.0,
3159,2020-08-17 14:35:12,1597664112.0,dataengineering,Avocado Production Ranking | TOP 10 Country from 1961 to 2018,ibcg3k,datavisworld,,https://www.reddit.com/r/dataengineering/comments/ibcg3k/avocado_production_ranking_top_10_country_from/,1.0,2.0,0.0,17464.0,
3160,2020-08-17 15:07:43,1597666063.0,dataengineering,Database Testing with Great Expectations,ibcw1g,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/ibcw1g/database_testing_with_great_expectations/,1.0,0.0,0.0,17464.0,
3161,2020-08-17 15:09:11,1597666151.0,dataengineering,"Understand what is learning rate and how can we tune it to increase our training speed without affecting the accuracy of the model. Here you ll know about Decaying Learning rate, Scheduled Drop Learning rate, Adaptive Learning rate and Cycling Learning Rate.",ibcwvf,MarinaChatterjee,,https://www.reddit.com/r/dataengineering/comments/ibcwvf/understand_what_is_learning_rate_and_how_can_we/,1.0,0.0,0.0,17464.0,
3162,2020-08-17 18:33:00,1597678380.0,dataengineering,Tree Schema - A data catalog that is accessible to all teams,ibgbxk,treeschema,,https://www.reddit.com/r/dataengineering/comments/ibgbxk/tree_schema_a_data_catalog_that_is_accessible_to/,1.0,0.0,0.0,17472.0,"We are a group of data engineers that have primarily worked at small and medium sized companies and we always found that the the complexity to integrate commercial data catalogs and the minimum price + per user costs of these products set the bar too high for us to adopt an off-the-shelf product. 

We recently launched our own data catalog, [Tree Schema](https://treeschema.com/), and we're currently rolling it out as a soft launch! The catalog provides, what we believe is, the most essential capabilities that all teams need and we've built it in a way that allows all users - not just the dev teams - to interact with and maintain the metadata.

Some of the key capabilities we offer are:

* [Data lineage](https://help.treeschema.com/catalog/lineage/lineage.html)
* [Support for flat schema structures and nested (JSON / struct like) objects](https://help.treeschema.com/catalog/data_schemas/schema_definition.html#embedded-object-fields)
* [Automated schema inference from your data store](https://help.treeschema.com/catalog/data_schemas/create_data_schemas_from_data_store.html)
* And of course, [metadata documentation](https://help.treeschema.com/catalog/data_fields/data_field_details.html) that enables users to have conversation about the data *with* the data

You don't need to talk to us or even provide a credit card to try the product for a month! We only want you to pay if you absolutely love it. 

For anyone who joins through this community and decides to use the product after the free month please message me directly or contact us through the website and we will give you 1/2 off the next 3 months.

[https://treeschema.com/](https://treeschema.com/)"
3163,2020-08-17 20:43:29,1597686209.0,dataengineering,When to establish a Business Intelligence team?,ibivxq,runthebaseline,,https://www.reddit.com/r/dataengineering/comments/ibivxq/when_to_establish_a_business_intelligence_team/,1.0,11.0,0.0,17476.0,"At what point does it make sense for a startup to have a Business Intelligence department?

I am a Data Analyst responsible for product analytics for a \~150 employee startup. For context, we have a separate database for each customer and my reports are generated by UDFs which basically loop through these customer databases. I primarily deliver insights via Tableau. 

Is this a typical analytics workflow for a company of this size? This is my first job after grad school and it's frustrating to run these UDFs each week. Engineering has tried to build in-house ETL tools but they don't work very well. I'm trying to understand when startups should establish more scalable analytics workflows.

I technically report to the sales team, so maybe my requests for a data warehouse aren't taken seriously because of that. I was thinking that having a BI department would refocus our efforts here."
3164,2020-08-17 20:54:49,1597686889.0,dataengineering,Table Documentation,ibj47e,ColdPorridge,,https://www.reddit.com/r/dataengineering/comments/ibj47e/table_documentation/,1.0,6.0,0.0,17477.0,"Are there any good existing tools for table documentation? We work with somewhat sizable tables, many of which have deprecated-in-place columns that are maintained for legacy use only. The people who would be writing documentation are not the table owners. How do you manage things like providing always up-to-date info on the definitions for columns/schema rationale? Possibly also monitoring for when the schema changes but the documentation is not updated would be a great feature."
3165,2020-08-17 22:05:58,1597691158.0,dataengineering,Why Data Engineering?,ibkkjt,avacado1997,,https://www.reddit.com/r/dataengineering/comments/ibkkjt/why_data_engineering/,1.0,12.0,0.0,17480.0,"Hi everyone, It's been a while since I started learning about Data Engineering. However, there are few things that I wanted to explore about this field. So I am posting few questions that I have. Please answer them in the thread or we can set up a google meet to discuss them. 
1.) What is the reason for choosing Data Engineering      as a full time profession?
2.) How long have you been working as a Data Engineer?
3.) What is your day like at work as a Data Engineer?
4.) What is your overall responsibility as Data Engineer at work?
5.) What do you think of the future of Data Engineering and it's present growth?

Your responses will be really helpful.
Thanks in advance."
3166,2020-08-18 01:53:03,1597704783.0,dataengineering,Need help with AWS RDS,iboxw1,sabalibruh,,https://www.reddit.com/r/dataengineering/comments/iboxw1/need_help_with_aws_rds/,1.0,2.0,0.0,17482.0,I am working on a python app locally. It process corona data from website. There are various websites actually. It loads into mysql locally. I created rds mysql instance and able to load the data from data frame into sql table on rds. I want to understand how do I automate this process. I am new to aws. I am aware about azure data factory and functions in Azure which can do similar thing. I think aws has glue for this or should I run the python script to load the daily new data into database. This data is later used for data analysis. Let me know if you can point me out to resources or articles where I can follow the process to implement it. Thanks.
3167,2020-08-18 04:05:10,1597712710.0,dataengineering,How to transition from Cloud Operation engineer into Data engineer.,ibr597,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/ibr597/how_to_transition_from_cloud_operation_engineer/,1.0,2.0,0.0,17485.0,"I know how to configure, administrate and debug core aws service as well as using terraform and deploying k8 cluster.However I've grown tired of doing ops stuff and really want to move into Data engineering. What skills and tools do I need to learn to do this ?"
3168,2020-08-18 11:13:23,1597738403.0,dataengineering,"The Data Janitor Letters - July 2020 (SQL, sessions, DWHs)",ibwyxk,soobrosa,,https://www.reddit.com/r/dataengineering/comments/ibwyxk/the_data_janitor_letters_july_2020_sql_sessions/,1.0,0.0,0.0,17497.0,[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-july-2020](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-july-2020)
3169,2020-08-18 11:56:37,1597740997.0,dataengineering,Top Most Reasons to do Data Science with R,ibxfe5,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/ibxfe5/top_most_reasons_to_do_data_science_with_r/,1.0,0.0,0.0,17497.0,
3170,2020-08-18 13:37:14,1597747034.0,dataengineering,Snowflake as Data Warehouse: free tier for students?,ibykzp,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/ibykzp/snowflake_as_data_warehouse_free_tier_for_students/,1.0,6.0,0.0,17498.0,"Is there a way to learn Snowflake by using it? I am a student, so I was wondering if they have any student account option."
3171,2020-08-18 14:02:59,1597748579.0,dataengineering,Homicide Rate Ranking | TOP 10 Country from 1990 to 2017,ibywra,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ibywra/homicide_rate_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,17499.0,
3172,2020-08-18 17:04:55,1597759495.0,dataengineering,Tools in the Data Management Zoo,ic1lgj,valdasm,,https://www.reddit.com/r/dataengineering/comments/ic1lgj/tools_in_the_data_management_zoo/,1.0,0.0,0.0,17504.0,
3173,2020-08-18 18:14:21,1597763661.0,dataengineering,An interview with the creator of TileDB about building a universal data engine to support cross-domain collaboration and reduce the burden of data management.,ic2v13,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ic2v13/an_interview_with_the_creator_of_tiledb_about/,1.0,0.0,0.0,17505.0,
3174,2020-08-18 18:54:42,1597766082.0,dataengineering,How Amplitude Created a Self-Service Data Democracy According to Their Executive VP of Product,ic3mr6,kelseyfecho,,https://www.reddit.com/r/dataengineering/comments/ic3mr6/how_amplitude_created_a_selfservice_data/,1.0,0.0,0.0,17508.0,
3175,2020-08-18 19:10:51,1597767051.0,dataengineering,From Zero to Versioned Data in Spark,ic3y3f,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/ic3y3f/from_zero_to_versioned_data_in_spark/,1.0,0.0,0.0,17509.0,
3176,2020-08-18 20:07:07,1597770427.0,dataengineering,[HIRING] Sr Data Engineer Chicago/Currently remote. 110 - 120k DoE.,ic51ke,boatsnbros,,https://www.reddit.com/r/dataengineering/comments/ic51ke/hiring_sr_data_engineer_chicagocurrently_remote/,1.0,21.0,0.0,17512.0,"Hi all,

Not sure if this is allowed - PM'd mods but haven't heard back... Happy to take down if violates anything.

&amp;#x200B;

I'm the hiring manager looking for a Sr D.E. to join my team at an Analytics subsidiary.

&amp;#x200B;

Some details:

\- Team of 3 DE's (inside analytics team of \~50, inside f500 company) building data pipelines using Spark + Glue to facilitate our S3/Athena data lake used by analysts for adhoc reporting, tableau, and data science.

\- Food &amp; Beverage industry for a major national provider.

\- Expanding team to 5 - 7 within the next 12 months.

\- Looking for someone with 3 - 5 years hands-on experience data engineering in AWS.

\- 110 - 120k target DoE.

\- Remote for time being, but relo to Chicago by June 2021 (based on current Covid expectations).

\- Laid back work environment, 'start up' culture but perks of being a subsidiary of a major company so financially stable.

&amp;#x200B;

PM me or comment here if you are interested and i'll send you the link/give email so you can apply. Happy to answer any questions in comments of PMs."
3177,2020-08-18 20:56:21,1597773381.0,dataengineering,Build your data platform like a product?,ic60qv,mkvor8,,https://www.reddit.com/r/dataengineering/comments/ic60qv/build_your_data_platform_like_a_product/,1.0,0.0,0.0,17513.0,"Increasingly, I've seen companies adopt a more formal (product management-like) approach to building their data platforms. 

&amp;#x200B;

https://preview.redd.it/mnl1x46wush51.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b35cb13bff8e034a10a64ed7aa88f2820451e9cd

Besides the pointers relayed here by Uber's former PM of Data, anything else comes to mind? Are people applying a PM-mindset here or is this impractical? 

[https://www.montecarlodata.com/how-to-build-your-data-platform-like-a-product/](https://www.montecarlodata.com/how-to-build-your-data-platform-like-a-product/)

&amp;#x200B;

[View Poll](https://www.reddit.com/poll/ic60qv)"
3178,2020-08-19 00:30:21,1597786221.0,dataengineering,Is Web Dev a PreReq to DE?,ica7bm,jduran9987,,https://www.reddit.com/r/dataengineering/comments/ica7bm/is_web_dev_a_prereq_to_de/,1.0,6.0,0.0,17528.0,"I feel as someone with no software development experience, I can't just learn Spark, Airflow, or Kafka, or AWS, without being rerouted to a million different concepts which seem foundational.  Does it benefit someone like myself (no software engineering experience), to take a more traditional track like web development in order to get exposed to the basics? (how the web works, creating api's, project structure, deployment) Or should i just learn these concepts on their own and apply it to my DE studies?  
I always think that the primary/most business-important source of data will come from some web/mobile app... then i feel silly trying to jump right into DE without understanding how applications are developed. thoughts?"
3179,2020-08-19 04:55:00,1597802100.0,dataengineering,Refreshing data source in Tableau (Athena Views),iceoqp,BIEIntern,,https://www.reddit.com/r/dataengineering/comments/iceoqp/refreshing_data_source_in_tableau_athena_views/,1.0,5.0,0.0,17535.0,"When I refresh my data source in Tableau (which is a View in Athena) the tables that the view is created from also get refreshed. Was wondering what the logic is behind that? 

I expected Views to just pull data from the tables that are already filled with data, and not refresh those tables as well.

Also secondary question, Is it inefficient to have a view pulling from another view?"
3180,2020-08-19 11:09:13,1597824553.0,dataengineering,"""python, ML, and Data science are the emerging words that keep echoing everywhere. Is not it? but how many really give you a challenging output when it comes to executable work. Saying so I would like to draw some of your time to a st",icjl0g,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/icjl0g/python_ml_and_data_science_are_the_emerging_words/,1.0,0.0,0.0,17548.0,
3181,2020-08-19 13:26:32,1597832792.0,dataengineering,Data Engineering Online Courses,icl3nr,MlTut,,https://www.reddit.com/r/dataengineering/comments/icl3nr/data_engineering_online_courses/,1.0,1.0,0.0,17550.0," Hi Folks,  


According to the **Dice 2020 Tech Job Report**\- data engineer is the fastest-growing job in technology in 2019, with a **50% year-over-year growth** in the number of open positions.  


So, if you are planning to start your career as a Data Engineer and looking for **Best Data Engineering Courses Online?** then you should read this article- [https://www.mltut.com/best-data-engineering-courses-online/](https://mltut.us18.list-manage.com/track/click?u=daed40bf0a68af806ccb51607&amp;id=e20412ca5e&amp;e=3d7f03b9f8)  


In this article, I have listed some **Most Popular Data Engineering Courses.** These courses will give you in-depth knowledge of Data Engineering concepts.  


I hope you will find this article helpful.  


All the Best!  


Happy Learning!"
3182,2020-08-19 13:49:22,1597834162.0,dataengineering,Air Transport Freight Ranking | TOP 10 Country from 1970 to 2017,icld9y,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/icld9y/air_transport_freight_ranking_top_10_country_from/,1.0,0.0,0.0,17550.0,
3183,2020-08-19 15:31:39,1597840299.0,dataengineering,"""what is a data warehouse ?"" - a simple explanation",icmqc5,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/icmqc5/what_is_a_data_warehouse_a_simple_explanation/,1.0,15.0,0.0,17553.0,Wrote a simple explanation of what data warehousing is and why use one [https://www.startdataengineering.com/post/what-is-a-data-warehouse/](https://www.startdataengineering.com/post/what-is-a-data-warehouse/) . Any feedback appreciated.
3184,2020-08-19 19:05:01,1597853101.0,dataengineering,How to set up a Kafka streaming project in production?,icqhra,curidpostn,,https://www.reddit.com/r/dataengineering/comments/icqhra/how_to_set_up_a_kafka_streaming_project_in/,1.0,2.0,0.0,17560.0,"Hello all,

I am looking for suggestions on how to set up one of our Kafka streaming project in production. The project is completely written Core Java (with Kafka streams), and would be one of the first Kafka stream projects in our group to get in to production. 

Here is the outline of what the project is end to end:

\- first there is a Kafka producer application that parses stock market data from an API every minute and writes in to a Kafka topic (a). Using quartz scheduler to run this every minute on week days starting from 9 to 5.

\- then there is a Kafka stream program that takes topic (a) as input and creates one minute timed windows of the data for every minute grouped by the ticker symbol. The output is written to topic(b)

\- then there is another Kafka stream that takes topic(b) as input and creates different statistical features from it (for eg: 5 minute mean, 10 minute mean etc.) and writes in to final topic(c)..

This topic(c) needs to be exposed to outer world (data scientists for model building, product managers for visualization etc.) for consumption.

I need some inputs on how this can be set up in production.. should we create a web app and host it?  should we rewrite it to different Java framework (Spring?) ? any suggestions welcome as everyone here is new to this and don't have defined standards to rely on. 

thanks in advance for your inputs."
3185,2020-08-19 21:01:16,1597860076.0,dataengineering,Data Visualization using Plotly &amp; Cufflinks | Python| Data Science,icssiv,8329417966,,https://www.reddit.com/r/dataengineering/comments/icssiv/data_visualization_using_plotly_cufflinks_python/,1.0,0.0,0.0,17564.0,https://youtu.be/7n5GzKuvPsw
3186,2020-08-19 22:15:07,1597864507.0,dataengineering,We built a public PostgreSQL proxy to 40k+ open government datasets,icu95c,mildbyte,,https://www.reddit.com/r/dataengineering/comments/icu95c/we_built_a_public_postgresql_proxy_to_40k_open/,1.0,6.0,0.0,17566.0,
3187,2020-08-19 22:28:34,1597865314.0,dataengineering,Data Teams Survey,icuirn,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/icuirn/data_teams_survey/,1.0,1.0,0.0,17568.0,I’m trying to get as much data as possible about how companies are organizing their data teams. I’m asking everyone to fill out a survey that seeks to quantify the value data teams are creating. Could you please fill out this survey for me? https://docs.google.com/forms/d/1uyusOrNRQBgnJZCTnHTQqNgUE8cqU8PLaJ1Vk29EzaM/edit
3188,2020-08-19 22:40:58,1597866058.0,dataengineering,Data modeling question,icurnr,Waveover,,https://www.reddit.com/r/dataengineering/comments/icurnr/data_modeling_question/,1.0,2.0,0.0,17568.0,"Not sure if this is the right sub or not. Please let me know if not. 

So I wanted to get an idea on best practice or if it is discouraged to push downstream process dimensional attributes upstream?

For example, I have products that are rented by customers that are shipped from different sites. In my model, I have the orders information in a sales table. And in my shipments table I have shipment facts (qty, amount) as well as dimensions like shipping location, return date, and rental duration segmentations hooked up to this table.

I would like opinions on pushing all these shipment dimensions to the sales table as well based on that order number. Now I know they won't apply if the order hasn't shipped. But I can easily put records in the dimension table for that. (rental not ended) for instance. 

The benefit of this is that it would make filtering orders data based on downstream process data easy. Such as sales dollars generated in this month lasted for this long out in the field. ""

Thanks in advance."
3189,2020-08-19 22:41:34,1597866094.0,dataengineering,Orchestration tool (like airflow) that accepts confirmation of jobs from outside sources?,icus4f,graciousgroob,,https://www.reddit.com/r/dataengineering/comments/icus4f/orchestration_tool_like_airflow_that_accepts/,1.0,2.0,0.0,17568.0,"I want to set up an orchestration tool similar to apache airflow that allows me to define and monitor DAGs of processes. However my understanding is that airflow kicks off each process in the DAG and then waits for a response. I want to integrate this system with other systems (like streamsets) that are schedule and run jobs themselves, but send confirmation to Airflow/the-orchestration tool of choice that the job is complete. Has anyone done this before?"
3190,2020-08-20 04:24:28,1597886668.0,dataengineering,4 Lessons for Data Scientists from the UK’s A-Levels Algorithm Debacle,id0wwh,dataculpa,,https://www.reddit.com/r/dataengineering/comments/id0wwh/4_lessons_for_data_scientists_from_the_uks/,1.0,1.0,0.0,17580.0,"When Ofqual, a UK government agency, changed the grading algorithm for university-entrance exams, 40% of grades for disadvantaged students dropped a full grade or more. An uproar ensued, the government reversed course, and now the government and university admissions departments are scrambling. What can data engineers and data scientists learn from this debacle?"
3191,2020-08-20 06:21:38,1597893698.0,dataengineering,Support_data_flow_GCP,id2qcy,Lord_Knight93,,https://www.reddit.com/r/dataengineering/comments/id2qcy/support_data_flow_gcp/,1.0,0.0,0.0,17583.0,
3192,2020-08-20 11:04:26,1597910666.0,dataengineering,"I come from Informatica ETL Development background, and I'm trying to shift towards Big Data Engineering. What skills should I work on and how?",id66rg,thebestnobody,,https://www.reddit.com/r/dataengineering/comments/id66rg/i_come_from_informatica_etl_development/,1.0,36.0,0.0,17595.0,"I've a total of 6+ years of experience, and until a few months ago, I was an ETL developer using the GUI tool Informatica Powercenter/BDM for building Data Warehouses. But in one of my recent projects, I observed the Data Engineering team, and their work in Hadoop seemed to be more interesting and challenging and also much more sought after in the industry. I also observed that code based development is much more flexible and customizable than tool based approach, especially in the Big Data domain. I was also feeling stuck in the same Informatica based projects as it has been 6 years of similar work, so I've decided to move away from it and do something different. And becoming a proper Big Data Engineer seemed very interesting.

Although my studies have been in Computer Science, I never actually paid attention to coding. During college, I dreaded it too much. But a couple of years ago, I decided to fight my fear of programming and took the course: *Python for Everybody* taught by Charles Severance. It was really cool. After doing that course, I wanted to do a small pet project in Python. I'm an avid book reader, and I felt like developing my own ‘vocabulary builder’ of sorts available in Kindle devices. Nothing fancy though, just a simple one. I built a small tool that takes a word from the user as the input and fetches its meaning by connecting to Oxford Dictionary web API. Finishing this personal project really boosted my confidence, which made me go ahead and pick up some programming tasks at work using Python: parsing excel sheets and extracting information from it and loading that data into Hive tables, playing around HDFS file system, making small scripts to perform reconciliation tasks between source database tables and target hive tables and etc. While coding all these things, I realized that I actually enjoyed it very much. So, now I don't dread coding as much as I used to do in the past.

Luckily, I now got a new interesting assignment recently that mostly uses Azure technologies. I’m also halfway through in the Azure Data Engineer certification (finished DP-200 and now preparing for DP-201). But I want to really improve myself much more and mould myself into a proper Data Engineer. Whenever I browse through LinkedIn job postings for Data Engineer role, I tend to get very under confident. Looking at all those requirements they ask for in the applicants, my experience feels very little. And it feels like it’s gonna take me forever to learn all the things they’re asking for: Hadoop, Spark, Kafka, compex data pipelines, Airflow, and so much more..

What should I do to really improve in those areas and become the kind of Data Engineer that the industry and the high paying companies are looking for? What kind of pet projects should I be building? It is undoubtedly true that I have much to learn. But how should I approach the learning of all these new technologies? When I try to learn too many technologies at once, I end up feeling too anxious and accomplish none of it correctly. That happened a lot in the past. So, I want to avoid being confused about learning and focus in a strategic way. 

What should I first concentrate on, what should be the priority? Please help me out with your suggestions. Thanks, all in advance."
3193,2020-08-20 14:42:14,1597923734.0,dataengineering,Data Wrangling Market Size Is Expected To Generate Huge Profits in Near Future,id8p8n,pradnya123,,https://www.reddit.com/r/dataengineering/comments/id8p8n/data_wrangling_market_size_is_expected_to/,1.0,0.0,0.0,17602.0,
3194,2020-08-20 14:47:26,1597924046.0,dataengineering,Data Wrangling Market Size Global Forecast to 2023,id8rq0,pradnya123,,https://www.reddit.com/r/dataengineering/comments/id8rq0/data_wrangling_market_size_global_forecast_to_2023/,1.0,0.0,0.0,17604.0,
3195,2020-08-20 14:52:09,1597924329.0,dataengineering,Data Wrangling Market will reach 3.18 Billion by 2023,id8tyl,pradnya123,,https://www.reddit.com/r/dataengineering/comments/id8tyl/data_wrangling_market_will_reach_318_billion_by/,1.0,0.0,0.0,17604.0,
3196,2020-08-20 16:09:11,1597928951.0,dataengineering,Server-less data ingestion using argo workflows,id9z33,apoorvqwerty,,https://www.reddit.com/r/dataengineering/comments/id9z33/serverless_data_ingestion_using_argo_workflows/,1.0,0.0,0.0,17607.0,
3197,2020-08-20 16:26:32,1597929992.0,dataengineering,Project Idea (A little different),ida924,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/ida924/project_idea_a_little_different/,1.0,6.0,0.0,17607.0,"I’m not totally new to data engineering (have about a year of experience). 

I want to setup a project to add to my resume. I’m kinda cheap and don’t want to spend money on a VPS. 

However, I’ve been playing around with Github Actions recently, and it actually seems like it can be used to orchestrate ETL pipelines. You can setup CRON like tasks and executes code. My idea would then be to take the output and put it into some sort html report and host it via Github Pages. 

Does anyone else know of similar projects? This seems like a good free way to host a data engineering project. Does anyone have ideas for what I can do?"
3198,2020-08-20 16:31:39,1597930299.0,dataengineering,Near real-time finance data warehousing using Apache Spark and Delta Lake,idac0m,Darth_Programmer,,https://www.reddit.com/r/dataengineering/comments/idac0m/near_realtime_finance_data_warehousing_using/,1.0,0.0,0.0,17606.0,
3199,2020-08-20 16:35:25,1597930525.0,dataengineering,Near real-time finance data warehousing using Apache Spark and Delta Lake,idae6x,Darth_Programmer,,https://www.reddit.com/r/dataengineering/comments/idae6x/near_realtime_finance_data_warehousing_using/,1.0,0.0,0.0,17606.0,
3200,2020-08-20 17:09:20,1597932560.0,dataengineering,Biology/Genetics DE roles,idaynh,brewmorris,,https://www.reddit.com/r/dataengineering/comments/idaynh/biologygenetics_de_roles/,1.0,6.0,0.0,17608.0,"Prior to getting a masters degree in computer science, I got a masters in Molecular, Cellular and Developmental Bio (but worked in a genetics lab —confusing, I know). I realised I liked coding much more than wet lab experiments when I found myself scripting in R, Python and Bash to make pipelines to digest data for my thesis. Of course, that was mostly scripting, but I do have front-end experience and OOP experience (took some classes and did a few projects) in Java, Python, C# and JS. 

Anyway, long story short, I’ve landed a good freelance job as a DE but it is purely as a DE. That is, I get to play, sort and cleanup a ton of data, but I am wondering if I could put my bio skills to good use. I understand it might be a niche, and I keep looking for something that could integrate both fields, but I cannot for the life of me find something that either requires a PhD or 4+ years of experience as a DE.

So all of that blabbering was to eventually ask, has anyone had luck applying to an entry-level position in the biotech world? And if so, does anyone have any pointers at things I should learn or improve upon?"
3201,2020-08-20 19:20:52,1597940452.0,dataengineering,A list of basic to advanced level interview questions related to python. You would find a different set of questions for freshers and experienced. You will also find questions based on concepts such as questions related to Object-oriented programming or coding task.,iddedj,Hussain_Mujtaba,,https://www.reddit.com/r/dataengineering/comments/iddedj/a_list_of_basic_to_advanced_level_interview/,1.0,0.0,0.0,17612.0,
3202,2020-08-20 22:33:16,1597951996.0,dataengineering,Best way to load thousands of rows into Postgres using Python,idh4kz,lord_hastings,,https://www.reddit.com/r/dataengineering/comments/idh4kz/best_way_to_load_thousands_of_rows_into_postgres/,1.0,14.0,0.0,17621.0,"As the title says. Currently using Django as an API interface but I have a stream of data that needs to be run regularly - think every 15 minutes - and this stream is returning thousands of rows that need to be loaded into Postgres. Currently, the Django ORM appears to be too slow for this - the data builds up in my cache faster than I can process it. 
I’ve considered writing the raw SQL queries to handle this but I’m not sure if psycopg2 connector will be significantly faster - or fast enough for my use case. The data I’m loading also has many-to-many relations as well as foreign key relations. 
Any advice is much appreciated. Thank you!"
3203,2020-08-21 06:00:14,1597978814.0,dataengineering,Questions about the Staging Area and Data Transformation Process,idos6d,dearmyshadow,,https://www.reddit.com/r/dataengineering/comments/idos6d/questions_about_the_staging_area_and_data/,1.0,4.0,0.0,17632.0,"I'm a college student who's recently been reading about data engineering and through some projects posted on this sub. I have next to no knowledge on DE practices, and just want some clarification on some concepts.

1. What is the purpose of the staging area? A lot of projects I see bring in raw data from web scraping / APIs / other data sources and place it inside a staging area (often see S3 buckets being used) for data transformation / processing. I understand that it's important to keep non-schematized, raw data separate from processed data that's ready to be queried for analysis, but I often see in projects people loading raw data into an S3 bucket, then transforming the data, then re-loading it into a separate 'processed' S3 bucket before finally loading it into a DW. What's the point of this intermediate loading to a processed zone? 

2. Is the data removed from the staging area completely once the transformed data has been loaded to the data warehouse and checks have been done to ensure that the transformations done on the loaded data in the DW is 'complete'? 

3. For smaller scale transformations where using Spark doesn't really make sense, what is often used for transforming data? I've seen things like dbt/sqlalchemy/etc. being mentioned on threads, but I really don't know what these are. The extent of my knowledge on data transformation techniques is using Pandas or Dask. 

4. It is to my understanding that all tasks in a pipeline are automated after the pipeline is deployed. I'm wondering how transformation scripts (either SQL or Python/etc.) are able to consistently clean/transform raw data into the desired format when the format of the data coming in from a data source may not always have consistent formatting."
3204,2020-08-21 06:44:05,1597981445.0,dataengineering,The four horsemen of the Big Data-driven enterprise | iunera,idpfsb,Timbo2020,,https://www.reddit.com/r/dataengineering/comments/idpfsb/the_four_horsemen_of_the_big_datadriven/,1.0,2.0,0.0,17633.0,
3205,2020-08-21 10:10:11,1597993811.0,dataengineering,Question on how to handle errors in ETL,ids3os,kychanbp,,https://www.reddit.com/r/dataengineering/comments/ids3os/question_on_how_to_handle_errors_in_etl/,1.0,2.0,0.0,17642.0,"I am very new to data engineering concepts.

My question is what we should do if the ETL pipeline is wrong and load the wrong data into the data warehouse?

How can we prevent and build robust ETL pipeline and data warehouse?"
3206,2020-08-21 10:45:41,1597995941.0,dataengineering,Is GraphQL a good idea for ETL? Getting data from backend,idsi4d,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/idsi4d/is_graphql_a_good_idea_for_etl_getting_data_from/,1.0,2.0,0.0,17643.0,"Hello. I work for an e-commerce web. The backend is suffering a huge refactor so that it will be structured in several microservices (users, shipments, catalog...) that are isolated from each other (different clusters in kubernetes, databases for storing the data etc)

For taking the new data we are thinking about using GraphQL. Each microservice will create a query for us (that will rely either on MySql or Elastic) and we will get the data using Airflow. For that, I have already created a GraphQLToS3Operator, that retrieves the data using request library and save it in a data frame, that is later on stored in S3. From S3 we move it to Redshift or Snowflake.

However, I have not been able to find much information about using GrahpQL for this use case and I am not sure this is the right tool. Currently (prior to the refactor) we take all the data from a MySql replica in AWS RDS. 

What do you think?"
3207,2020-08-21 12:14:30,1598001270.0,dataengineering,Choosing the Right Data Quality Tools,idtfwc,DQLabsinc,,https://www.reddit.com/r/dataengineering/comments/idtfwc/choosing_the_right_data_quality_tools/,1.0,0.0,0.0,17645.0,
3208,2020-08-21 12:24:05,1598001845.0,dataengineering,Choosing the Right Data Quality Tools,idtjjd,DQLabsinc,,https://www.reddit.com/r/dataengineering/comments/idtjjd/choosing_the_right_data_quality_tools/,1.0,0.0,0.0,17646.0, [https://www.dqlabs.ai/choosing-the-right-data-quality-tools/](https://www.dqlabs.ai/choosing-the-right-data-quality-tools/)
3209,2020-08-21 12:52:01,1598003521.0,dataengineering,Error using Geospark with Python,idtum9,Razwand,,https://www.reddit.com/r/dataengineering/comments/idtum9/error_using_geospark_with_python/,1.0,0.0,0.0,17647.0,"Hi everyone, I'm trying to use Geospark and it seems some methods are not recognize by Python. For example, when executing ""from geospark.register.java\_libs import GeoSparkLib"". More concretely, It seems not to recognize scala syntax.

I'm working on a Centos 2.7. Spark version is 2.3 (when running spark it shows Scala version 2.11), java version is 1.8, Python 3.6),

Does anyone know about this Scala - Python issue? I'm new to this and I wanted to try Geospark for the first time.

Thanks in advance"
3210,2020-08-21 13:30:45,1598005845.0,dataengineering,Facebook graph explorer to see(and extract) other users comments/posts/friends etc?,iduava,eujinski,,https://www.reddit.com/r/dataengineering/comments/iduava/facebook_graph_explorer_to_seeand_extract_other/,1.0,0.0,0.0,17647.0,Can we use Facebook graph explorer to see(and extract) other users comments/posts/friends etc? I found lot's of (older than this year) tutorials on this and it was working until about a year ago.    Is it still possible? Thanks in advance.
3211,2020-08-21 14:30:44,1598009444.0,dataengineering,Oil Energy Ranking | TOP 10 Country from 1965 to 2019,idv0no,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/idv0no/oil_energy_ranking_top_10_country_from_1965_to/,1.0,0.0,0.0,17650.0,
3212,2020-08-21 16:01:16,1598014876.0,dataengineering,What are the common data science interview questions?,idw9xo,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/idw9xo/what_are_the_common_data_science_interview/,1.0,0.0,0.0,17653.0,
3213,2020-08-21 16:23:09,1598016189.0,dataengineering,architecture suggestions for fast update for customer behavior metrics,idwmsu,drping666,,https://www.reddit.com/r/dataengineering/comments/idwmsu/architecture_suggestions_for_fast_update_for/,1.0,2.0,0.0,17655.0,"Hi guys,

i'm coming from a data scientist background, and i want to share with you a data pipeline i want to achieve :  
1/ receive realtime customer behavior tracking data : events like viewing product page, purchase.. via api

2/ one new data coming from tracking API, trigger updating some basic customer metrics : like last purchase date, cumulative spending so far...

3/ erase + update customer profile with up-to-date calculation in a nosql database for fast retrieve / expose to an API providing this as json.  


according to you what would be the easy architecture /tooling to achieve this (for on-premise)"
3214,2020-08-21 17:27:04,1598020024.0,dataengineering,Have you come across any tool that can automatically join data without us specifying a schema?,idxq3w,drwho1990,,https://www.reddit.com/r/dataengineering/comments/idxq3w/have_you_come_across_any_tool_that_can/,1.0,9.0,0.0,17657.0,"I am looking for a tool that can help me integrate varied data sources. I have data sitting in Oracle, Mysql, MongoDB and JSON files in S3. Other than just this it would be great if I could also integrate my clickstream data and some ERP systems data into all this. Is there any tool that can automatically find relationships across all my data and join them based on the relationships that it discovers?"
3215,2020-08-21 19:05:56,1598025956.0,dataengineering,How can I run an airflow worker on windows,idzjh8,FlavoredFrostedTits,,https://www.reddit.com/r/dataengineering/comments/idzjh8/how_can_i_run_an_airflow_worker_on_windows/,1.0,7.0,0.0,17662.0,I have an application on a windows server that I occasionally need to run as a part of my data pipeline. Is there a way I can create an airflow worker to run it with some parameters?
3216,2020-08-21 21:23:16,1598034196.0,dataengineering,30 days of free access to IBM Data Science and AI programs,ie280a,iphone6plususer,,https://www.reddit.com/r/dataengineering/comments/ie280a/30_days_of_free_access_to_ibm_data_science_and_ai/,1.0,4.0,0.0,17668.0,"Hey everyone, just wanted to share this with you all. IBM is offering 30 days of free access to its AI and Data Science programs until 12/31/2020. 

Do check it out here 
https://www.youtube.com/watch?v=OXr04-a6qP4"
3217,2020-08-21 23:47:10,1598042830.0,dataengineering,Standardized Public Datasets,ie4zcy,-JoeAnderson,,https://www.reddit.com/r/dataengineering/comments/ie4zcy/standardized_public_datasets/,1.0,3.0,0.0,17676.0,"Does anyone know if there is a site or platform where you can access publicly available datasets using a standardized, consistent API?  So that you don't have to learn a whole different access channel/API/library for every different set of data?"
3218,2020-08-22 00:30:05,1598045405.0,dataengineering,[Advice] Pattern for Handling Transition of Estimates to Actuals in a Dimensional Model,ie5rjq,PencilBoy99,,https://www.reddit.com/r/dataengineering/comments/ie5rjq/advice_pattern_for_handling_transition_of/,1.0,1.0,0.0,17678.0,"**TLDR**: what's the best way in a Dimensional Model (star schema) to record the transition of facts from Estimates to Actuals? For our typical report consumer, we want the outputs to automatically transition from Estimates to Actuals in reports when they become available.

Use Case:

At the beginning of the month, we have an estimate for revenue for a specific business activity. At the end of the month, we now know the actual value.

Options:

We could easily put the estimate and actual in different columns in the fact table, or have a single column in the fact table (revenue) and then a dimension that describes whether it's an actual or estimate.

However, the output designer (report designer) or end user would need to know about this, and manually code for ""if actual are available use actual else use estimates, in either case show the source""

What's the right way to model this so that the typical analyst / user doesn't have to know this? If we weren't dealing with a dimensional model and this was a traditional operational relational system we'd just make a view with this case logic."
3219,2020-08-22 00:41:03,1598046063.0,dataengineering,PUTITEM VS UPDATEITEM,ie5yn9,imaginary_reaction,,https://www.reddit.com/r/dataengineering/comments/ie5yn9/putitem_vs_updateitem/,1.0,2.0,0.0,17678.0,"Hey All,

I been digging into this all day im interested in comparing the performance of PUTITEM vs UPDATEITEM. 

&amp;#x200B;

Scenario:  You have Dynamo table where you keep operational data regarding your applications steps and progression.

&amp;#x200B;

Option 1: You have 1 record per event with multiple columns and you update each step column with a timestamp when completed and switched the boolean value from zero to 1 as the event progress over time using the update function.

&amp;#x200B;

Option 2: You create a new record each time a step is completed using the put function with status tons of records but showed you more of a timeline

Option 3:  You replace the current row using the put function with update status ?

&amp;#x200B;

I'm honestly lost on which way is ""right"" to maximize efficiency"
3220,2020-08-22 00:47:45,1598046465.0,dataengineering,Does this look like an actual plan ?,ie6334,CogLuca,,https://www.reddit.com/r/dataengineering/comments/ie6334/does_this_look_like_an_actual_plan/,1.0,1.0,0.0,17678.0,"Hi guys I know that life pulls you in different directions and you might never know what happens but does the following look like a plan that is grounded in reality or am I just too outside the ecosystem to actually understand ? (10 years plan)

I am finishing my bachelor in computer science, 1 year missing. In the next year while finishing I'm planning on picking up a back-end development job and I'm starting to develop project to showcase on my github for that reason. Next step would be acquiring a Master in Data engineering or Data science in order to become what they call a full-stack datascientist (basically from pipelines to analysis) and work at a start-up in order to learn both the skills for the role and how the start-up ecosystem works, in order to later on open up an analytics related business of my own, be that service or product based. 

In summary :

Bs Computer Science ---&gt; working as a back-end engineer and Master of datascience/engineering -----&gt; full-stack datascientist -----&gt; service or product based company"
3221,2020-08-22 05:47:35,1598064455.0,dataengineering,Recommended tools for modeling pipeline,ieasym,quizer_m,,https://www.reddit.com/r/dataengineering/comments/ieasym/recommended_tools_for_modeling_pipeline/,1.0,11.0,0.0,17691.0,"Hi, so I'm. pretty new to this and have been doing research on tools that will fit my situation and want to make sure I'm on the right direction.

My use case:

1. Get data from Hadoop (Hive)
2. Do some preprocessing (shell)
3. Train a model (need to move data from the steps above to a different machine)  (python)
4. Evaluate a model (python)
5. Visualize the result (python)

Which is pretty normal pipeline, I think.

What I want to do is to automate these step into a pipeline. I imagine something like set parameters of the experiment, run the pipeline, and get a simple report (some numbers with plots). A dashboard would be nice too.

I found several tools that may suit my needs:

1. Airflow, it seems like every one is using it, but my use case does not require scheduling, so I feel like using Airflow is overkill and might be too complicated.
2. Luigi, a more flexible version of Airflow?
3. Prefect, seems similar to Luigi, but does not offer dashboard in free version.
4. [Orchest](https://www.orchest.io/), looks like what I need, but still in early stage. So I'm not sure if it's the right tool to adopt.

Do you guys have comments on this? Anything else that I should look into?"
3222,2020-08-22 08:39:55,1598074795.0,dataengineering,How to get started in Data Engineering ?,ied2l2,rahulseetharaman,,https://www.reddit.com/r/dataengineering/comments/ied2l2/how_to_get_started_in_data_engineering/,1.0,20.0,0.0,17693.0,"I see plenty of tools being listed online for Data Engineering jobs, such as Kafka, Spark, Hadoop, Pig, Kibana and what not. In addition to this, data engineering roadmaps also list proficiency in Data Structures, Algorithms, Databases, etc. 

To tell a bit about my background, I am a CS undergrad with a somewhat good understanding of data structures and algorithms, can solve SQL problems on SQLZoo and have worked on a couple of Machine Learning projects. 

How do I get started in Data engineering ? There are plenty of courses for a specific Data engineering tool, but none that gives a good overview of how all the tools are glued together in production and in companies.

What should I start learning, and how to start learning (like books, courses, etc) ?"
3223,2020-08-22 15:35:32,1598099732.0,dataengineering,Onion Production Ranking | TOP 10 Country from 1961 to 2018,iehnz9,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/iehnz9/onion_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,17702.0,
3224,2020-08-22 17:45:03,1598107503.0,dataengineering,Cloud data engineer at Google - what to expect?,iejo23,eliza_one,,https://www.reddit.com/r/dataengineering/comments/iejo23/cloud_data_engineer_at_google_what_to_expect/,1.0,9.0,0.0,17706.0," Hey there,

I will have an interview for this position (cloud data engineer) at Google. It's a role that includes both customer-facing responsibilities and coding, but I'd love to get more information :)

First, does anyone know what's the interview process looks like? What kind of technical questions should I expect?

Second, does anyone in that role have some more detailed examples of the daily tasks?

Please feel free to share anything (resources, personal experiences...) that can be useful for preparing and/or understanding more in the detail the role.

Thank you very much!"
3225,2020-08-22 19:09:48,1598112588.0,dataengineering,"Why should you learn ""DATA SCIENCE"" in 2020??",iel5b3,frizzbuzz,,https://www.reddit.com/r/dataengineering/comments/iel5b3/why_should_you_learn_data_science_in_2020/,1.0,0.0,0.0,17714.0,https://youtu.be/L7YNQI6ZCSU
3226,2020-08-22 19:30:52,1598113852.0,dataengineering,Is there a way to write data to parquet files if I don't know the schema before hand using Apache Flink?,ielinh,drwho1990,,https://www.reddit.com/r/dataengineering/comments/ielinh/is_there_a_way_to_write_data_to_parquet_files_if/,1.0,0.0,0.0,17714.0,I have multiple event streams and I constantly keep integrating new kinds of events into my Kafka streaming layers. I currently have 3 different types of complex events coming in from my web streams and I am planning on integrating 6 more over this year. Is there a way to write all these data streams to parquet without me having to configure multiple flink datastreams for each topic with a predefined schema?
3227,2020-08-22 23:58:12,1598129892.0,dataengineering,How can I progress my data engineering career?,ieqb7m,PaddyIsBeast,,https://www.reddit.com/r/dataengineering/comments/ieqb7m/how_can_i_progress_my_data_engineering_career/,1.0,11.0,0.0,17719.0,"I have been a data engineer for a couple of years now, but the technologies we use are outdated and I have found myself in a scenario when I don't even come close to the requirements for finding a new job.

Essentially all I have to use day to day is Python and SQL to do ETL.

&amp;#x200B;

&amp;#x200B;

When I look or interview for other jobs, some are asking for all of this (and sometimes more):

Java, Scala or R (on top of SQL and python)

DevOps experience

Apache spark

Hadoop

at least 2 of: GCP, AWS and Azure

&amp;#x200B;

I have tried to bridge this gap, I have completed my own projects in AWS and got the first tier qualification.

I'm just not really sure where to go from here.

I feel Apache Spark is the next most important thing to learn, but I'm not sure.

Do you think the 'Databricks Certified Associate Developer for Apache Spark 3.0' is a realistic goal that will help me advance my career?

if so, does anyone know of a good course this prepares you for the Python version of this course?

Or any other guidance would be greatly appreciated."
3228,2020-08-23 04:11:12,1598145072.0,dataengineering,The data pipeline at my work is a total mess. How do I Un fuck it?,ieue2a,TrainquilOasis1423,,https://www.reddit.com/r/dataengineering/comments/ieue2a/the_data_pipeline_at_my_work_is_a_total_mess_how/,1.0,15.0,0.0,17725.0,"I started a data analyst position with my company 1 year ago, and I have decided I want to transition my job into more of a data engineer as my team desperately needs one, and the executives don't want to hire a real one. Looking for some beginner guidence and resources.

I work for a web hosting company that likes to buy other companies and spend $0 on dev work to integrate their systems into ours. Because of this my team reports out of 5 different billing systems, 3 different phone/chat/agent tracking systems, all our salesforce data is manually entered, and we don't have any semblance of a company wide agent directory. No systems talk to each other or automatically update with any information outside its own system. We use Google sheets, excel, snowflake, salesforce, tableau, power bi, and the slowest VPN connection in existence. It used to take 4+ hours to run the ""morning reports"" before I bogged together some VBA/python to automate some of it. 

I have a working knowledge of python, SQL, and VBA, however I'm the only one on my team with any scripting experience. Right now my biggest question is what tool do I use? No one system seems to have all the resources I need to ETL all my data without a real expert stepping in. I'm totally fine scripting everything with python, but I just feel like I'd be reinventing the wheel a lot and Id be the single point of failure since no one else wants to learn code. 

Can I just Google sheets everything? Is tableau or power bi better? I don't have full administrative access to snowflake but can I just use that?

Sorry for the rant, little outside my depth here. Any help at all would be greatly appreciated."
3229,2020-08-23 09:51:45,1598165505.0,dataengineering,Are datacamp courses any good?,ieyp9a,DnDandDryBread,,https://www.reddit.com/r/dataengineering/comments/ieyp9a/are_datacamp_courses_any_good/,1.0,14.0,0.0,17731.0,I'm a second year CS student and really want to get into data engineering. I've started doing the courses in datacamp for the data engineer career track but I'm not sure if it's going to help. Are datacamp courses recognized at all from recruiters etc or am I wasting my time?
3230,2020-08-23 14:29:00,1598182140.0,dataengineering,HIV Living People Ranking | TOP 10 Country from 1990 to 2017,if1lz1,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/if1lz1/hiv_living_people_ranking_top_10_country_from/,1.0,0.0,0.0,17737.0,
3231,2020-08-23 17:12:35,1598191955.0,dataengineering,Which Cloud Certification should I do as a Data Engineer?,if3uvg,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/if3uvg/which_cloud_certification_should_i_do_as_a_data/,1.0,20.0,0.0,17742.0,"There are certifications for Data Engineers on Cloud technologies, offered by AWS, IBM and Google. Which one should I choose? Which one will be more helpful in getting job?"
3232,2020-08-23 17:50:15,1598194215.0,dataengineering,Cannot access Zeppelin when running on WSL2,if4gr3,asaadam32,,https://www.reddit.com/r/dataengineering/comments/if4gr3/cannot_access_zeppelin_when_running_on_wsl2/,1.0,0.0,0.0,17744.0,"hey, guys, I can't access localhost:8080  when run zeppelin using wsl2.

&amp;#x200B;

[it says listen to port 8080, but can't access it](https://preview.redd.it/0vhuism4mri51.png?width=780&amp;format=png&amp;auto=webp&amp;s=e6652f20ba7bb14c92ed4bfc81ea1c9c372010dd)

it's strange, because I can run rest API or start an react app using wsl, and can access from windows browser."
3233,2020-08-23 20:58:45,1598205525.0,dataengineering,Azure Synapse in CI/CD environment,if7sz1,bonanzaguy,,https://www.reddit.com/r/dataengineering/comments/if7sz1/azure_synapse_in_cicd_environment/,1.0,0.0,0.0,17750.0,
3234,2020-08-24 01:06:52,1598220412.0,dataengineering,Data engineering vs data management,ifceuh,fannypackbringitback,,https://www.reddit.com/r/dataengineering/comments/ifceuh/data_engineering_vs_data_management/,1.0,5.0,0.0,17756.0,"What’s the difference - data management and data engineering? My company wants to rename all the data managers to data engineers.  We (DM) mostly do technical application data loading and management (some apps don’t even have APIs), data governance, change management, and ETL (non automated, data sources are often non-repeatable). I understand why we would go to data engineering 100% eventually but we are so far from that.  I understand DE is mostly data pipelines using ML or code.  Am I wrong? I’m perplexed by the company’s forward path."
3235,2020-08-24 04:01:31,1598230891.0,dataengineering,"This week's release is a new set of articles that focus on DBT, Testing, and production deployment of ML infra, ML applications from Reddit, FarFetch, Pinterest, Google Cloud, Lyft, and AI economics.",iff909,vananth22,,https://www.reddit.com/r/dataengineering/comments/iff909/this_weeks_release_is_a_new_set_of_articles_that/,1.0,0.0,0.0,17764.0,
3236,2020-08-24 06:35:43,1598240143.0,dataengineering,SimpliLearn Big Data Engineering Master's Certification,ifhl9g,DesolateAbomination,,https://www.reddit.com/r/dataengineering/comments/ifhl9g/simplilearn_big_data_engineering_masters/,1.0,0.0,0.0,17766.0,
3237,2020-08-24 08:40:16,1598247616.0,dataengineering,Job Advise on DE requirements at a large bank,ifj995,Baegar-T,,https://www.reddit.com/r/dataengineering/comments/ifj995/job_advise_on_de_requirements_at_a_large_bank/,1.0,2.0,0.0,17769.0,"I'm researching what data sources I need to familiarize myself with while applying to entry level DE roles at large banks. 

Any advice on:
-  the typical types of databases commonly used,
-  enterprise software that is relevant to the banking sector (e.g. Salesforce is less commonly used vs other industries), 
- any other pointers wrt specific types of data seen in banking / Finance (eg. SWIFT, etc.)
- general pointers, do's  and don'ts :)

Would it also make sense for me to familiarize myself with tools from Informatica, etc or should I wait and see what licenses the organization uses before investing a lot of time?"
3238,2020-08-24 12:25:55,1598261155.0,dataengineering,Redis streams vs. Kafka,iflrnx,mjwestcott,,https://www.reddit.com/r/dataengineering/comments/iflrnx/redis_streams_vs_kafka/,1.0,0.0,0.0,17778.0,
3239,2020-08-24 12:52:46,1598262766.0,dataengineering,Question: data scraping from collection of (geographical) charts?,ifm28v,tomekanco,,https://www.reddit.com/r/dataengineering/comments/ifm28v/question_data_scraping_from_collection_of/,1.0,3.0,0.0,17777.0,"Hi,

I'm working on a project where i want to scrape data from a large number of pdf-charts. I'm using [a makeshift NN](https://nbviewer.jupyter.org/github/karelvancamp/demo/blob/fcf57c520cfb9553d915942197649386b80e67ec/notebooks/Find%20image%20families%20-%20keras.ipynb) to identify the same charts across the pdfs (these are daily reports). Then extracting data using color sampling of the chart &amp; legends.

It does seem to work, but it is rather cumbersome to work with. 

Do you know any existing software which is either efficiently able to 
- detect similar graphs (ignoring the data shown, but recognize layout &amp; legends.) in a large collection
- extract data from [cartogrpahic chart overlays](https://github.com/karelvancamp/demo/blob/master/derived%20data/epistat/daily%20tests%20per%20province/20200812%20p7-58.png?raw=true) (image =&gt; tabular)"
3240,2020-08-24 14:23:39,1598268219.0,dataengineering,Hong Kong vs Singapore | Labour Productivity from 1970 to 2017,ifn4ub,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ifn4ub/hong_kong_vs_singapore_labour_productivity_from/,1.0,0.0,0.0,17782.0,
3241,2020-08-24 16:11:44,1598274704.0,dataengineering,Data load monitoring - how do you monitor and alert if new data isn't loaded for X hours?,ifoow2,Buckweb,,https://www.reddit.com/r/dataengineering/comments/ifoow2/data_load_monitoring_how_do_you_monitor_and_alert/,1.0,5.0,0.0,17785.0,"As an example, we have log data aggregated at an hourly level that we batch load in hourly increments. If the upstream data is delayed we may not load anything that hour but the scheduled job will still run without failing. In the case where the upstream data providers don't let us know there are pipeline issues, I want to alert if we haven't loaded new data to the target within X hours.

How do you guys handle monitoring data loads to ensure new data has been loaded within X hours?"
3242,2020-08-24 16:45:31,1598276731.0,dataengineering,Data and Analytics on AWS Online Event -- Registration and Details,ifp9tv,tmccormick92,,https://www.reddit.com/r/dataengineering/comments/ifp9tv/data_and_analytics_on_aws_online_event/,1.0,4.0,0.0,17787.0,"Hi r/dataengineering,

This popped into my inbox this morning, and thought you all might be able to benefit from this free AWS event. I included some details about the 3-day event below, and the link contains more descriptions on the hour-long sessions individually. I'm fairly new to data engineering (data analyst and programming background), so I'm excited for the 200-level sessions. Cheers!

[Data and Analytics on AWS Online Event Registration]([https://pages.awscloud.com/NAMER-field-OE-Data-and-Analytics-on-AWS-2020-reg-event.html?](https://pages.awscloud.com/NAMER-field-OE-Data-and-Analytics-on-AWS-2020-reg-event.html?))

#### About the Free Event

This free online event is designed to educate you about AWS services, help you to design, deploy, and modernize your data architecture. Learn how to unlock the real value from your data and create a foundation for rapid innovation while achieving agility, scalability, and cost-savings.

Hear the very latest in data and analytics, and uncover how organizations are harnessing the power of data to accelerate innovation in their businesses.

Dive deep into the steps to move your data and modernize your databases, data, and analytics architecture. Explore key concepts, use cases, and best practices to save time and costs managing data, eliminate data silos, gain accurate insights faster, and accelerate time to market. Whether you are new to the cloud, or an experienced user, you can learn something new working with AWS.

### Monday, September 21 -- Modernize your data infrastructure

* Break free and migrate your Oracle and SQL Server databases to AWS (Level 200)
* Deploy open source databases on AWS  (Level 200)
* Migrate your on-premises data warehouse to Amazon Redshift (Level 200)
* Why cloud databases like Amazon Aurora are more scalable and reliable (Level 200)

###  Tuesday, September 22 -- Build apps for the new scale of data

* Purpose-built databases: Choose the right tool for each job (Level 200)
* Build modern applications with modern databases (Level 300)
* Learn how to data model for internet scale apps with Amazon DynamoDB (Level 400)
* Supercharge your real-time apps for extreme performance with Amazon ElastiCache (Level 300)

### Wednesday, September 23 -- Get more value out of your data quickly

* Breaking the silos: Extending your data warehouse to include a data lake (Level 200)
* Processing Big Data with Hadoop, Spark, and other frameworks in Amazon EMR (Level 300)
* Building scalable, secure, log analytics with Amazon Elasticsearch Service (Level 200) 
* Real-time analytics with Amazon Kinesis and Managed Streaming for Kinesis (Level 200)"
3243,2020-08-24 16:49:13,1598276953.0,dataengineering,[Question] Do you think it's important to explain to the general public what artificial intelligence and machine learning are? Can data scientists also benefit from such explanations?,ifpbzb,Kseniase,,https://www.reddit.com/r/dataengineering/comments/ifpbzb/question_do_you_think_its_important_to_explain_to/,1.0,0.0,0.0,17787.0,"I was tired of articles about how machines are going to conquer us and with my friend, a computer scientist and CTO, we started a newsletter in which we try to explain in plain English the main concepts of ML. To show it's practical implementation. To demonstrate the variety of problems that ML tackles. 

It's been less than two months since the newsletter was launched but it's getting a great following and positive feedback from both sides: people who don't have knowledge about ML tell us they learn with us to understand how ML is integrated in their daily lives, and to data scientists and developers we help to keep up with the development of the industry and focus on the relevant research papers. 

I would love to share it with you and hope you will find it useful for you 

[https://thesequence.substack.com/p/the-introductory-post](https://thesequence.substack.com/p/the-introductory-post)"
3244,2020-08-24 16:56:57,1598277417.0,dataengineering,JSON to Pandas Dataframe to Postgres; make sense?,ifpgpg,chonbee,,https://www.reddit.com/r/dataengineering/comments/ifpgpg/json_to_pandas_dataframe_to_postgres_make_sense/,1.0,17.0,0.0,17787.0,"Hi,

I've been working on a side-project to get some experience in data engineering. Mainly to get some exposure to Airflow and Postgres. I've developed a simple data pipeline. It's basically extracting data from a JSON with parking garage data of my local city council (# of free parking spots per garage, coordinates of the garage, etc.). The JSON is refreshed every minute. Then, I load the data to a Pandas Dataframe and do my transformations (nothing too complex), and then I load it to a table in a Postgres database. I use Airflow to schedule the various steps of my pipeline.

I'm wondering if it makes sense to use Pandas for transforming data? Coming from an analyst background it's my go-to when it comes to handling data in Python, but I have a feeling that in the engineering world that's not the case. Am I right to think that?"
3245,2020-08-24 17:31:43,1598279503.0,dataengineering,Apache Flink: Accelerating your workload with GPU and other external resources,ifq2ww,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ifq2ww/apache_flink_accelerating_your_workload_with_gpu/,1.0,0.0,0.0,17788.0,
3246,2020-08-24 20:05:09,1598288709.0,dataengineering,Why do you need a data platform?,ift12g,YourDSBoy,,https://www.reddit.com/r/dataengineering/comments/ift12g/why_do_you_need_a_data_platform/,1.0,18.0,0.0,17793.0,"So our data engineering team decided a while ago to develop a custom made data platform and deploy with Terraform. It was an interesting project and I learned a lot, but frankly I didn't see a business value in it. We had a perfectly good BigQuery but our team struggled a bit developing a new one, as a result downstream users had quite a negative experience with the new platform. Also, it was super difficult to maintain it.

But it looks like it's an industry standard to create their own data platform. Is there something I am not just seeing? Why do we need to create a data platform when there are perfectly good ones available (BigQuery/Redshift) which are easier to maintain?"
3247,2020-08-24 21:24:18,1598293458.0,dataengineering,Master's program options for Data Engineering,ifumrv,dead-on-arrival-,,https://www.reddit.com/r/dataengineering/comments/ifumrv/masters_program_options_for_data_engineering/,1.0,3.0,0.0,17793.0,"I recently discovered that my company offers around ~$5k a year for college education if you choose to pursue it. I graduated with an Undergrad degree in Computer Science from a State school and have been a Data Engineer for 1.5 years now. So I am thinking of pursuing an online Master's program for both learning and credibility, and to make use of my employer's college program. 

Can anyone offer any good program suggestions (may or may not be college-specific) that will help me accelerate my career in Data Engineering? I see some options from Business Analytics to Data Science Master's programs but none of them are specifically geared towards DE. So any recommendations would be highly appreciated!"
3248,2020-08-24 21:32:52,1598293972.0,dataengineering,Metadata Management: Hive Metastore vs AWS Glue,ifusn9,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/ifusn9/metadata_management_hive_metastore_vs_aws_glue/,1.0,0.0,0.0,17793.0,
3249,2020-08-24 21:38:31,1598294311.0,dataengineering,Designing a Custom Data platform to be used within our company. Review If you can.,ifuwsd,youareafakenews,,https://www.reddit.com/r/dataengineering/comments/ifuwsd/designing_a_custom_data_platform_to_be_used/,1.0,2.0,0.0,17793.0,"Hello. Just as the title says. We are a growing company that serves national and international apparel industries. We monitor, improve, serve, etc data and IoT hardware to customers and bring next-generation industrial IoT solutions.

Our data is growing at rapid rates. I do not have an equation to give the image but we have around 12 relational and nonrelational databases (SQL Server and MongoDB) running with each database serving around 30 million records per table (2-3 tables each database of this size) alongside hundreds or basic tables give or take.

We have in place data warehousing as well for each database (implemented both OLTP and OLAP in a single database). For MongoDB, we just do not have a straight answer to it because everything is stored centralized there.

One of the selling points to our customers is that we provide near-instant or realtime reports over whole period of our product. Some products are above 5 years old and contain OLTP, OLAP mixed data and running indexes, cron jobs, reportings through custom web-based solutions, APIs whatnot. In short, It is a huge mess.

Management has decided to prepare a custom data platform with a unified technology stack. Our core data team which I lead alongside senior persons have narrowed down a Hadoop based ecosystem. The components we are looking at are:

* Apache Kafka (To stream data in realtime from sensors 24/7-356)
* Apache Spark (SQL, MLib, Streaming) for warehouse needs, realtime reports.
* Apache Kudu (For datastore)
* Apache Impala (To have SQL like interface with direct datastore)
* Apache Airflow to make pipelines (DAGs etc)
* Docker to run containers for each instance of the above services.
* Kubernetes to manage containers.
* Custom java based solution to get data from sensors to Kafka and into data platform.
* PowerBI to serve reports (Impala has direct support). Others are good too but considering we have SQL Server. Furthermore, we did not find a great plug/play reporting solutions in the Hadoop system. Reports are generated by Higher-ups so they cannot do coding via JS or programming language. They understand SQL. Impala gives a great alternative to SQL shell. If you have some, please suggest.

If I were to write down, most basics of activities;
1. Get streaming data from hardware sensors 500 requests per second at a minimum from hundreds of machines (depends upon solution) with each request of size 10kbs or more.
2. Make realtime as well as historical reports of data over long periods of time.
3. Store data into fast storage for quick streaming (high write, high read).
4. On-premise solution. Cloud is not an option; Therefore scaling should be plug and play. Therefore, Kubernetes to handle dynamic loads.

If there are questions. Ask and please guide. I myself have a good experience in Data Engineering coming from DBA background but leading project of this nature is hard for me too and the first one to handle driving seat.

In the end, the goal is to prepare a self-sufficient VM/Vagrant which is plug and play to deploy and then auto-scales itself depending upon current needs of resources. Hardware requirements for above stack are understood and available."
3250,2020-08-24 23:14:45,1598300085.0,dataengineering,Protfolio Projects for a career in DE,ifwu9r,bobasucks,,https://www.reddit.com/r/dataengineering/comments/ifwu9r/protfolio_projects_for_a_career_in_de/,1.0,9.0,0.0,17795.0,"Hi All,

I recently completed the Udacity’s Data Engineering Nano Degree program. I have hit a roadblock in terms of what do I do next? I see that a lot of Data Scientists having a Portfolio listing the projects that they have worked on. It is kind of easy for them as data sets are easily available online.

I wish to do something similar and build a portfolio showcasing my data engineering skills. Has anyone here done something similar? If yes can you please the share the details.

Thanks!"
3251,2020-08-25 02:19:41,1598311181.0,dataengineering,Analyzing Parquet Metadata and Statistics with PyArrow,ig0dp2,MrPowersAAHHH,,https://www.reddit.com/r/dataengineering/comments/ig0dp2/analyzing_parquet_metadata_and_statistics_with/,1.0,0.0,0.0,17797.0,
3252,2020-08-25 07:46:32,1598330792.0,dataengineering,"Programming is not so hard once you get started, but when starting we have these questions where to start? how to start? Here is an article that can help you get started with programming in Python. Once you know to program in Python, it is easy to move to any other language.",ig5jjy,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/ig5jjy/programming_is_not_so_hard_once_you_get_started/,1.0,0.0,0.0,17809.0,
3253,2020-08-25 08:25:09,1598333109.0,dataengineering,Metadata - The Alter Ego of Data,ig61hw,valdasm,,https://www.reddit.com/r/dataengineering/comments/ig61hw/metadata_the_alter_ego_of_data/,1.0,0.0,0.0,17810.0,
3254,2020-08-25 10:21:54,1598340114.0,dataengineering,"I don't think I'm good at CS, have a decade of experience but ironically tech is my actual area of interest.",ig7fa2,Evil333,,https://www.reddit.com/r/dataengineering/comments/ig7fa2/i_dont_think_im_good_at_cs_have_a_decade_of/,1.0,18.0,0.0,17813.0,"I have close to 10 years experience and based out of southern part of India. Wasted my first 2 years at a big tech services / consulting company on a super legacy support project and then moved to Data analytics at Big 4 firm - currently stepping into 8th year.

I don't really think I'm good at Analytics or CS in general. I can't even define my expertise skills, I'm like a mix of many things at a very basic level. I don't like Stats so ML/ AI is out of the question. I like relational databases, Python and Data engineering stuff but that too only the simpler ones. When it gets complex like CTE recursive queries or a complicated concept in Python, I kind of give up. I do Power BI but don't enjoy it much. I like web design, helped friends with developing and hosting websites for their very small scale business.

Technology is the only thing that interests me and I cant even think of an alternative career. I always tune into tech podcasts while I run, work on Udemy tutorials on my free time and try to answer things on Stack Overflow.

I'm not very bad at work place, I get promoted almost every year or two and is one of most sought out resource for tech queries - partly because I'm very easily approachable and when I don't know something I go the extra mile, research about it and help them out. I manage an team of 10 and I constantly worry about my team members looking down on me for not living up to the expectation of a 10 year veteran in the industry. Also I'm scared on thinking what would happen in the next 10 years and if I would ever be able to get another job, let alone apply for a role which would suit my very basic diverse skills."
3255,2020-08-25 12:27:33,1598347653.0,dataengineering,Income of Esra bilgic Vs Engin alton Vs Cengiz coskun.#datacrackingblade,ig8tbx,swiftins078,,https://www.reddit.com/r/dataengineering/comments/ig8tbx/income_of_esra_bilgic_vs_engin_alton_vs_cengiz/,1.0,0.0,0.0,17818.0,
3256,2020-08-25 12:35:50,1598348150.0,dataengineering,Set India Vs Zee Music Company Subscribers Count(LIVE)#datacrackingblade,ig8wsz,swiftins100,,https://www.reddit.com/r/dataengineering/comments/ig8wsz/set_india_vs_zee_music_company_subscribers/,1.0,0.0,0.0,17820.0,
3257,2020-08-25 13:36:27,1598351787.0,dataengineering,Camel Meat Production Ranking | TOP 10 Country from 1961 to 2018,ig9mij,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ig9mij/camel_meat_production_ranking_top_10_country_from/,1.0,0.0,0.0,17825.0,
3258,2020-08-25 17:16:05,1598364965.0,dataengineering,Could use help/direction on my first personal data pipeline project.,igd0i7,_work_redditor_,,https://www.reddit.com/r/dataengineering/comments/igd0i7/could_use_helpdirection_on_my_first_personal_data/,1.0,0.0,0.0,17831.0,"I'm working on a personal data pipeline project in order to expand my knowledge of industry tools and best practices. Trying to use open-source and free tools if possible. My background is specifically working with SQL and Oracle, but a while ago I did use Cognos and MicroStrategy for BI.

My goal is to take data at a source, ETL it to a postgresql database, create an optimized data-warehouse, then create a visual layer on top. I'm trying to start small and local, but hope to expand into a cloud based solution and becomes more automatic.

**Ideal project:**

* Scheduled process to extract data from source
* load postgresql database and optimize
* create visual BI layer (plan is to use Tableau)
* Ideally this would eventually be migrated to AWS (as I just got my CCP certificate) 
* have as much of the process automated as possible

**So far I have accomplished the following:**

Create SQL pipeline (ETL) to pull IMDB data from the website and upload into a local postgresql database, then parse into a usable data warehouse. This process is repeatable and will update the DW with the latest pulled data. I pull the data manually using psql with something like

    \COPY [table] FROM PROGRAM 'curve ""imdb.com"" | gunzip'...

Should I learn and use Airflow or Dagster to automate my extraction and load process of postgresql? I don't know anything but the very basics of python.

Any help and advice would be greatly appreciated."
3259,2020-08-25 20:35:39,1598376939.0,dataengineering,Anyone have a business model to commercialize or get more value out of a data warehouse. ?,iggwad,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/iggwad/anyone_have_a_business_model_to_commercialize_or/,1.0,5.0,0.0,17844.0,So business now has a centralized source of truth for data to report on performance and make decisions to aid business. But what’s next. Any cool stories of getting more value out of your warehouse. Maybe turning it into the backend of a product or building a data api out of it ?
3260,2020-08-25 21:03:11,1598378591.0,dataengineering,An interview about how LinkedIn designed their metadata management platform and how it is being used to power data discovery and integration.,ighgoq,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ighgoq/an_interview_about_how_linkedin_designed_their/,1.0,0.0,0.0,17845.0,
3261,2020-08-25 21:07:44,1598378864.0,dataengineering,What technologies/tools do you use for testing frameworks?,ighk8d,xockbou,,https://www.reddit.com/r/dataengineering/comments/ighk8d/what_technologiestools_do_you_use_for_testing/,1.0,3.0,0.0,17845.0,"New data engineer \[M23\] on my new team, and I am tasked with designing/developing a data validation testing framework to fit into most of our other Jenkins processes. We already have a testing framework for files and tables using Python, so I am building most of my implementation off of that so I can reuse prewritten logic when possible and is a good place to start (also im quite proficient with Python).

My main pipeline I will be working on will be validating data sent via Kafka, then Cassandra, and then to extract files. The Cassandra to extract file validation seems more obvious, but what are industry standards for data validation for Kafka topics/NRT streaming technologies?"
3262,2020-08-25 21:49:45,1598381385.0,dataengineering,This might lean more on ETL side but what is your opinion as a data engineer?,igiffs,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/igiffs/this_might_lean_more_on_etl_side_but_what_is_your/,1.0,10.0,0.0,17846.0,
3263,2020-08-25 23:06:32,1598385992.0,dataengineering,Share Udemy's data engineer related courses,igjxzq,datalinux,,https://www.reddit.com/r/dataengineering/comments/igjxzq/share_udemys_data_engineer_related_courses/,1.0,2.0,0.0,17848.0,Share Udemy's data engineer related courses that you had used for your career.
3264,2020-08-26 01:44:05,1598395445.0,dataengineering,We are on Product Hunt and we need your feedback TODAY. Announcing Kanaree.io: We want to build a better product and need honest feedback.,igmw9p,Kanaree_io,,https://www.reddit.com/r/dataengineering/comments/igmw9p/we_are_on_product_hunt_and_we_need_your_feedback/,1.0,4.0,0.0,17852.0,"We are on Product Hunt and we need your feedback TODAY. Announcing Kanaree.io: We want to build a better clickstream data product and need honest feedback.

Today is super exciting for the Data in the Raw team. We have taken the leap to open the platform to anyone in hopes of giving everyone the ability to protect their users while being able to remove the stronghold that third-party data platforms have had for years on your data.

We would like to introduce you to Kanaree.io. We have rebranded, rebuilt and made data more accessible. Today we are on Product Hunt and need your help on Product Hunt giving us feed back, up-voting, leaving comments, and sharing the link. We are up against some big names in tech today and trying to do the right thing by liberating and making data accessible for all organization. BUT WE NEED YOUR HELP BY GIVING FEEDBACK, UP VOTING, COMMENTING AND SHARING!!! We hope to continue to share more updates as we get past the first big hurdle of Product Hunt. Thank you for your support.

How cool would it be if the reddit data community could surpass the big guys in tech up-voting and sharing.

[https://www.producthunt.com/posts/kanaree-io](https://www.producthunt.com/posts/kanaree-io)"
3265,2020-08-26 01:53:23,1598396003.0,dataengineering,Good way to configure airflow dag's running on kubernetes?,ign1ve,edmguru,,https://www.reddit.com/r/dataengineering/comments/ign1ve/good_way_to_configure_airflow_dags_running_on/,1.0,3.0,0.0,17852.0,"I am in the process of setting up an Airflow running in my orgs Kubernetes cluster. So far the installation has gone alright and I can run some simpe dags however the productionizing part is definitely a challenge. I plan to run mainly python scripts that need to connect to databases/warehouse/s3 etc.. and need to supply the credentials for this. Should I store conf files in a persistent volume and have the worker pods read those? What about trying to get everything in a k8's Secret is that possible? Could you use multiple secrets?

Looking for any insight into anyone who might be doing this kinda thing already and could give some insight into what works well or what would be a good practice."
3266,2020-08-26 02:22:01,1598397721.0,dataengineering,Confluent Cloud vs Amazon MSK,ignjaz,Electric_pokemon,,https://www.reddit.com/r/dataengineering/comments/ignjaz/confluent_cloud_vs_amazon_msk/,1.0,4.0,0.0,17853.0,"Guys and Gals,
Has anyone here tried out Confluent's managed Kafka service and see how it compares to AWS?

I know there were some issues with Confluent, Redis and Mongo changing license terms to stop AWS stripping their code, and was worried about functionality of AWS managed offerings."
3267,2020-08-26 10:55:09,1598428509.0,dataengineering,[USE CASE]: Monitoring and Controlling Networks of IoT Devices with Flink Stateful Functions,igurmr,Marksfik,,https://www.reddit.com/r/dataengineering/comments/igurmr/use_case_monitoring_and_controlling_networks_of/,1.0,1.0,0.0,17867.0,
3268,2020-08-26 13:43:48,1598438628.0,dataengineering,Eating Disorder Death Rate Ranking | TOP 10 Country from 1990 to 2017,igwl60,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/igwl60/eating_disorder_death_rate_ranking_top_10_country/,1.0,0.0,0.0,17872.0,
3269,2020-08-26 13:58:05,1598439485.0,dataengineering,Elon Musk promises to present working brain-computer chip,igwqw6,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/igwqw6/elon_musk_promises_to_present_working/,1.0,0.0,0.0,17872.0,
3270,2020-08-26 14:08:08,1598440088.0,dataengineering,Senior Big Data Engineer Certification - Because Big Data Doesn’t Build Itself,igwvdd,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/igwvdd/senior_big_data_engineer_certification_because/,1.0,0.0,0.0,17872.0,
3271,2020-08-26 15:24:50,1598444690.0,dataengineering,"Diabetes prediction using ""Machine Learning"" | Kaggle |Python",igxup1,frizzbuzz,,https://www.reddit.com/r/dataengineering/comments/igxup1/diabetes_prediction_using_machine_learning_kaggle/,1.0,0.0,0.0,17876.0,https://youtu.be/RCLfD2qrlEM
3272,2020-08-26 15:49:49,1598446189.0,dataengineering,"Transforming JSONs with Google Cloud Platform before BigQuery, best practice?",igy7n5,rysnotnice,,https://www.reddit.com/r/dataengineering/comments/igy7n5/transforming_jsons_with_google_cloud_platform/,1.0,12.0,0.0,17876.0,"I have a deeply nested JSON document that is variable length and has variable arrays respective to the document, I am looking to unnest certain sections and write them to BigQuery, and disregard others.

I was excited about Dataprep by Trifacta but as they will be accessing the data, this will not work for my company. We work with healthcare data and only have authorized Google. 

Has anyone worked with other solutions in GCP to transform JSONs? The nature of the document is so long and nested that writing a custom Regex and running it on a pod before ingestion is taking significant compute."
3273,2020-08-26 17:08:18,1598450898.0,dataengineering,"Here is an article about generative models, specifically Boltzmann Machine (BM), its popular variant Restricted Boltzmann Machine (RBM), working of RBM and some of its applications.",igzibx,instigator-001,,https://www.reddit.com/r/dataengineering/comments/igzibx/here_is_an_article_about_generative_models/,1.0,0.0,0.0,17880.0,
3274,2020-08-26 17:40:48,1598452848.0,dataengineering,Question about PostgreSQL,ih01ym,dexdagr8,,https://www.reddit.com/r/dataengineering/comments/ih01ym/question_about_postgresql/,1.0,6.0,0.0,17880.0,"Hello folks, i just started on my job as Data Engineer for 4 months,

currently im tasked to create a pipeline to batch insert new data from foreign table on slave to analytics schema, it works wonderfully, but suddenly i have some problem, how do i know if a table in slave gets updated ex : new column? so i can ""automate"" the alteration table on analytics schema?"
3275,2020-08-26 17:44:11,1598453051.0,dataengineering,Need suggestions,ih043v,Miserable-Bake-8695,,https://www.reddit.com/r/dataengineering/comments/ih043v/need_suggestions/,1.0,2.0,0.0,17880.0,"I  need your suggestions. I am holding about 8 years of experience in BFSI domain mostly from the business side. Among tech I know Python, sqls, PowerShell, mdm &amp; I am trying to switch to the data engineering field. I was wondering if I can learn Pyspark &amp; if learnt will there be any opportunity for me as fresher in Pyspark(spark with python) in the current job market? I am not looking for any generic technology to learn since competition is much high there &amp; I am sure many more deserving people are looking for that positions. Considering my 8 years of experience &amp; some moderate understanding of python, what technology do you suggest to learn if not spark with python?"
3276,2020-08-26 17:55:59,1598453759.0,dataengineering,Multiple inserts into MySQL Db,ih0bea,virajrathod,,https://www.reddit.com/r/dataengineering/comments/ih0bea/multiple_inserts_into_mysql_db/,1.0,2.0,0.0,17881.0,"Hello wonderful people,

I have just gotten into Data Engineering and been assigned small tasks lately. 
The recent one is actually a complexed one and I could use a little nudge in a right direction from anyone who has been through the a similar issue. 

So I have an insert query which is dynamically build from scratch starting from a Select query. The select query is then broken into parts and assigned to the insert query (host, port, table name, schema name etc).
*** There are duplicates in the table! ***
The insert query picks up 1000 records at a time using cursor.fetchmany(1000). 
Now when the inserts are done, there is an update query which returns the number of records inserted. That number is always different after the script is finished running. 

After investigating the issue, the issue is because the select query always picks up records in a different order and so the duplicate records are always going to be in a different 1000 chunk. 

My question is -
How can make sure the records are always selected in an orderly fashion so that the inserts are uniform?"
3277,2020-08-26 18:32:29,1598455949.0,dataengineering,[Help required] How to present AI projects?,ih0zhd,saady96,,https://www.reddit.com/r/dataengineering/comments/ih0zhd/help_required_how_to_present_ai_projects/,1.0,0.0,0.0,17882.0,"Hello everyone,  
Looking  to get some help &amp; suggestions from all the experts out there. My  team is providing Computer Vision based services to different  clients/startups. We have done some exciting projects in this domain and  now we are looking to reach out to more clients/partners.  
One  problem I'm facing is that we have signed NDA's with all of the previous  clients for not displaying their names and their datasets. So now I  don't know how to present our previous work in a catchy way, if you can  share some tips or even a link to any existing cool portfolio in  computer vision(any DL) domain, I would really appreciate this.

Btw most of our work is in Retail, Real Estate and academics."
3278,2020-08-26 18:55:17,1598457317.0,dataengineering,Data reconciliation vs. Data Remediation,ih1fbk,farting_samurai,,https://www.reddit.com/r/dataengineering/comments/ih1fbk/data_reconciliation_vs_data_remediation/,1.0,0.0,0.0,17883.0," Hi everyone.

I am doing a ux research project on different data environments and I have three questions for you:  
**1) what is the difference between Data Reconciliation and Data Remediation?**

\- these seem like synonyms but I don't want to make a rookie mistake and just go along with it before I asked people who know more than I do about the subject.

**2) What would be some of the methods and/or environments that you guys are using to reconcile/remedy the data**

\- if you can give me some feedback on why and when you're using which software/service/app that would be great so I can really get a grasp of the phenomenon in it's human form :)

**3) what are the common issues you're facing during reconciliation/remediation process and what do you do to fix it?**

I know I'm asking a lot in terms of answers, but any help is greatly appreciated as I am looking to expand my knowledge on this as best I can.

Thank you!"
3279,2020-08-26 19:16:11,1598458571.0,dataengineering,Has anybody taken Jesse Anderson's Data Engineering course,ih1ua2,t2nanoflex,,https://www.reddit.com/r/dataengineering/comments/ih1ua2/has_anybody_taken_jesse_andersons_data/,1.0,20.0,0.0,17885.0,"I'm looking to transition into Data Engineering, and came across a course from Jesse Anderson:  


[https://www.jesse-anderson.com/get-your-data-engineering-dream-job/](https://www.jesse-anderson.com/get-your-data-engineering-dream-job/)  


It's extremely expensive, but I am considering investing in it if I can streamline my learning to get into the field. However, you can always use the course outline he lays out and find online resources / books to learn about them. Does anybody have any advice? Anybody taken this course?"
3280,2020-08-26 20:53:50,1598464430.0,dataengineering,How do you detect data anomalies in your pipeline?,ih3piq,bhargavn07,,https://www.reddit.com/r/dataengineering/comments/ih3piq/how_do_you_detect_data_anomalies_in_your_pipeline/,1.0,22.0,0.0,17885.0,Are there any open source tools? How do you recognize a anomaly like the number expected rows for a table have dropped?
3281,2020-08-27 02:51:50,1598485910.0,dataengineering,transitioning to data engineering/ data scientist,ihadsm,Fantastic_Scholar,,https://www.reddit.com/r/dataengineering/comments/ihadsm/transitioning_to_data_engineering_data_scientist/,1.0,7.0,0.0,17899.0," I have a bachelors in life science and took basic courses in social science statistics as electives covering Z test, the paired, independent samples and one samples t test, one way ANOVA , ANCOVA , Repeated Measures ANOVA, Randomized block design Anova, Kruskal-Wallis Test &amp; Friedman’s Test , Multiple Comparisons – A Priori Tests, Multiple Comparisons – Post Hoc Tests , Factorial ANOVA and Mixed-Design ANOVA with simple effects. I also took an elective class covering how to run some of these test in r and sas, visualize the data, read the data and clean the data for testing purposes. I have also finished some online moocs covering how to clean and visualize data in python using pandas, matplotlib and numpy. Currently, I have enrolled in a private institution to take a course in Azure since I am unable to find jobs with a bachelor in life science following graduating this summer in 2020 and unable to get into masters. In the course at the private institute, I learned the following Azure Data Factory with knowledge of how to implement triggers, pipelines, run times, various transformations, copy data flows, data sets in the UI and some knowledge of how to build info cubes in SSAS, as well as how to build simple dashboards in Power BI . I don't have knowledge of DAX however. I also have some basic knowledge of SQL. I was wondering what type of jobs I can apply or whether I have to gain more knowledge before applying to jobs. I am based in Canada."
3282,2020-08-27 04:23:47,1598491427.0,dataengineering,Question about sharing Twitter stream data,ihbt7x,spiceycookie1,,https://www.reddit.com/r/dataengineering/comments/ihbt7x/question_about_sharing_twitter_stream_data/,1.0,1.0,0.0,17901.0,"As part of a side project, I've collected a few million tweets from Twitter's API and have parsed the jsons into a tabular format. I was thinking about making the data public (Kaggle dataset, for example) but am not sure what the policy is on sharing user specific attributes (such as username and, if available, location). Granted, all of this info is publicly available if you go to Twitter and search on the tweet id... Is this something that would be frowned upon? Does this present a problem with data privacy?"
3283,2020-08-27 08:28:07,1598506087.0,dataengineering,AWS Steps &amp; Airflow,ihfade,x246ab,,https://www.reddit.com/r/dataengineering/comments/ihfade/aws_steps_airflow/,1.0,5.0,0.0,17906.0,"I am curious what you guys think about using AWS steps as part of an airflow pipeline that needs to scale across multiple datasets. Have you ever done it? What about AWS steps where the steps are spark-submit statements?

As far as I see it, you can basically use AWS steps to do your consecutive spark-submits or you can use an SSHOperator. I'm opting for the SSHOperator currently and am having pretty decent results.

I feel like the SSHOperator is superior to AWS Steps with regard to restart logic and handling complex dependencies. But I feel like AWS Steps may have an advantage with regard to startup/teardown of the cluster and might be more predictable. 

I'd love to hear what any of you think about this."
3284,2020-08-27 10:29:34,1598513374.0,dataengineering,IBM Data Science and AI Programs on Coursera Free for 30 Days,ihgp68,awsconsultant,,https://www.reddit.com/r/dataengineering/comments/ihgp68/ibm_data_science_and_ai_programs_on_coursera_free/,1.0,0.0,0.0,17914.0,
3285,2020-08-27 13:20:40,1598523640.0,dataengineering,Energy Imports Ranking | TOP 10 Country from 1971 to 2014,ihii9j,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ihii9j/energy_imports_ranking_top_10_country_from_1971/,1.0,0.0,0.0,17923.0,
3286,2020-08-27 15:30:58,1598531458.0,dataengineering,11 Must-Know Machine Learning Algorithms for AI Professionals,ihk5q2,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/ihk5q2/11_mustknow_machine_learning_algorithms_for_ai/,1.0,0.0,0.0,17925.0,
3287,2020-08-27 15:31:00,1598531460.0,dataengineering,Meet Silq- The First Intuitive High-Level Language for Quantum Computers,ihk5qn,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/ihk5qn/meet_silq_the_first_intuitive_highlevel_language/,1.0,0.0,0.0,17925.0,
3288,2020-08-27 15:54:49,1598532889.0,dataengineering,Boost your Data Science Portfolio and contribute to open-source Automated Machine Learning project,ihkid7,pp314159,,https://www.reddit.com/r/dataengineering/comments/ihkid7/boost_your_data_science_portfolio_and_contribute/,1.0,0.0,0.0,17925.0,
3289,2020-08-27 19:18:49,1598545129.0,dataengineering,Incremental updates with Spark - introducing Streaming into batch load architecture,iho1d4,TKTheJew,,https://www.reddit.com/r/dataengineering/comments/iho1d4/incremental_updates_with_spark_introducing/,1.0,1.0,0.0,17933.0,
3290,2020-08-27 20:52:09,1598550729.0,dataengineering,Opinion on Snowflake?,ihpt0f,TrainquilOasis1423,,https://www.reddit.com/r/dataengineering/comments/ihpt0f/opinion_on_snowflake/,1.0,44.0,0.0,17936.0,"I am trying to transition my from data analyst to data engineer within my team. My company recently started moving a lot of our data onto Snowflake and I was hoping to get some feedback on how useful this tool could be for my current role as well as prospects. 

&amp;#x200B;

I have to do a lot of transformation in my position as none of the data I use is clean at all. can Snowflake handle transformations on multiple streams of semi-structured data + the chaos of manually entered data? would the computing costs outweigh the benefits of computing everything in one place? With Snowflake operating on top of the main 3 cloud platforms would I get any exposure to those platforms that I could mention on a resume or would everything happen in a snowflake environment meaning I can only say I've worked with Snowflake?"
3291,2020-08-27 21:29:14,1598552954.0,dataengineering,Data engineering position OR Software Engineering (possible ML) position?,ihqiy5,theSimpleTheorem,,https://www.reddit.com/r/dataengineering/comments/ihqiy5/data_engineering_position_or_software_engineering/,1.0,5.0,0.0,17935.0,"I have two remote offers. A little about me, currently SE / ML engineer with about 1 year of experience.

One is Data engineering with AWS purely in python. Sounds like repetitive pipeline work, etl, leveraging our database. From what I have seen, not a classical tech but pseudo wannabe tech company. Only 5 Data engineers on complete tech team. $90k base.

Description:

* Monitor all data pipelines for accuracy and availability. Troubleshoot and resolve any problems that occur.
* Respond to ticket requests from the Data Team for data corrections and pipeline updates.
* * Cloud infrastructure monitoring and maintenance with a focus on reliability and performance.
* Create reusable scripts and programs for data Extraction, Transformation and Loading (ETL).
* Work closely with other departments to understand their business needs and proactively engineer data solutions
* Serve as subject matter expert on our pipelining solutions and cloud infrastructure
* AWS Certification and/or extensive cloud infrastructure experience
* Familiarity with AWS or similar services: Redshift, DynamoDB, SQS, SES, Lambda, Cloudwatch, S3, IAM, EC2
* Familiarity with High Availability infrastructure
* NoSQL and Graph database experience

The other is more of a classical software engineer. Product exists, needs to maintain and implement new features. * Working with new customers to integrate the SAAS. Future potential ML work. Tech company / Late startup. Great reviews, $115k base

Description:

* Maintain a web application written primarily in Node.js/Typescript, composed of a REST API and a React.js UI.
* Postgresql database
* Our inspectors which are responsible for collecting data from systems and reporting it back to our platform
* Work as a software engineer in a fast paced environment with a growing engineering team
* Contribute readable, maintainable, and performant code to our existing codebases
* Participate in code reviews with other members of the development team
* Follow standard Software Development Lifecycle practices to ensure both quality and efficient delivery of code: requirements, design, and testing
* Work with the Product team to solidify and validate code requirements
* Work with the Engineering team to solidify and validate software designs as well as to solidify and validate software test plans
* Debug and determine root causes for customer issues in the field, clearly communicating resolutions and workarounds to both customers and the internal team
* Contribute to internal and external knowledge base articles and documentation
* Develop blameless postmortems to identify points of failure and prevent future issues

I really believe the SE company has a much better culture and will allow me to learn good practices and grow. The DE really peeks my interest more but the company doesnt seem to value the tech side too much and its mayday frequently. The SE will start ML soon but very slowly which is definitely in my interest.

What would you do?"
3292,2020-08-27 23:44:22,1598561062.0,dataengineering,Fast insert to AZURE SQL SERVER database with pyodbc,iht6dv,Usurper__,,https://www.reddit.com/r/dataengineering/comments/iht6dv/fast_insert_to_azure_sql_server_database_with/,1.0,5.0,0.0,17940.0,"Hi!

I'm looking for the best way to fast-insert data to Azure SQL Server Database using python pyodbc module. The data is a list of tuples (0.5 - 1M tuples is a list). So far I've been using this, but it's not as fast as I would hope. I would appreciate if you can help me find a faster solution.

connection = create_connection() # returns a connection  
cursor = connection.cursor()
cursor.fast_executemany = True 
cursor.executemany(query, data)
connection.commit() 
connection.close()"
3293,2020-08-28 01:28:54,1598567334.0,dataengineering,Job Prospects: Data Engineering vs Data Scientist,ihv2cv,DesolateAbomination,,https://www.reddit.com/r/dataengineering/comments/ihv2cv/job_prospects_data_engineering_vs_data_scientist/,1.0,39.0,0.0,17945.0,"The number of Data Engineering jobs posted online in the midwestern US is 5-6 times greater than the Data Science (ML related jobs in general). Another thing that I have observed is that the number of applicants for the Data Science jobs are normally +100, while many Data Engineering roles barely get double digit number of applicants in Chicago, even less than that in places like St. Louis and Minneapolis. I see DE jobs in Wisconsin that get ZERO applicants on LinkedIn during one month. More senior DE roles, such as Big Data Engineers are having a hard time finding any applicants, let alone qualified applicants. 

1) Why is data engineering in much higher demand than DS? I took a quick look at the Bay Area market and it appears that there are way more DE jobs there as well.

2) Why is there so little supply for DE opportunities while DS is clearly over saturated? I mean I get it that the average salary for DS is higher but I don't understand why people pass a vast number of DE jobs to compete with lots of advanced degree CS graduates over a smaller DS job market."
3294,2020-08-28 04:03:46,1598576626.0,dataengineering,"do you use dbt to model your data? re-use your dbt models and send that data back to your CRM, Marketing tools, etc. AKA Reverse Fivetran",ihxkuy,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/ihxkuy/do_you_use_dbt_to_model_your_data_reuse_your_dbt/,1.0,0.0,0.0,17949.0,
3295,2020-08-28 04:40:56,1598578856.0,dataengineering,Career path after data engineering?,ihy4xk,tmg863,,https://www.reddit.com/r/dataengineering/comments/ihy4xk/career_path_after_data_engineering/,1.0,0.0,0.0,17950.0,"Yo. Googled this but wasn't convinced by the search results lol. 

Been a data engineer for over 1 year, already wondering what the next step is. Have Master's in analytics related field and 4 years experience. I want to work on new portfolio projects but before I do that, I need to think about the direction I want to take. 

So, what are some options post-data-engineering? I have both DE and DS skills and am in the business intelligence industry. I'd like to stay in the industry but as far as job function, I'm thinking of something more analytical...which also incorporates aspects of DE. Thoughts?

No trolls plz. You know nothing about me as a person; don't try me ;)"
3296,2020-08-28 05:41:41,1598582501.0,dataengineering,"I have 4 years in business/data analytics, 1 year report automation(macros , python ) , and 1 year experience monitoring daily etl (aws,snowflake) and building out a datawarehouse and a new data pipeline. I am located an 2 hrs outside SF.",ihz1oi,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/ihz1oi/i_have_4_years_in_businessdata_analytics_1_year/,1.0,8.0,0.0,17954.0,What should my salary be ? Having trouble gauging what my role is.
3297,2020-08-28 08:38:19,1598593099.0,dataengineering,What's in your GitHub for resume?,ii1fo9,pbj800100,,https://www.reddit.com/r/dataengineering/comments/ii1fo9/whats_in_your_github_for_resume/,1.0,4.0,0.0,17956.0,"I use GitHub daily for work but because my code contains sensitive information I keep everything private. I'm going to start applying for jobs and figured I should put my account on my resume (this is my second job since getting my BS so first time thinking about adding it to resume). I'm not sure what I should make public in my account though. I don't really work on personal stuff in my own time to be honest. I can upload some code to new repositories that I've written for work projects and just remove the sensitive info so it's more generic, but is that weird? Will a potential employer also realise if I've only committed everything to a bunch of repositories once (because I'm not actively working in them, im just uploading samples)? Thanks"
3298,2020-08-28 10:51:00,1598601060.0,dataengineering,Become a Data Engineer,ii2ywc,lclhr,,https://www.reddit.com/r/dataengineering/comments/ii2ywc/become_a_data_engineer/,1.0,6.0,0.0,17962.0,"Hello Everyone,

&amp;#x200B;

Recently found this career path and I fell in love immediately. Currently,  I'm an IT engineer. Working with windows server, some cloud both mostly just exchange online, virtualization some network stuff so I have a pretty technical background also i was programming in College and recently doing some PowerShell scripts as well.

I joined the [datacamp.com](https://datacamp.com) and started to refresh my SQL knowledge as I completely forget and found the data engineer path which could take up around 200 hours of hard learning. (I know possible after that even more practice and doing some own project to be more comfortable  with everything).

&amp;#x200B;

My question is that is enough to start a career? I really planning to put a loads of hours to finish the datacamp's program but would like to know what it will give me in terms of opportunity and how can I start to work.

&amp;#x200B;

My plan is to be ready to find a junior job around February. Do you think is that doable? 

&amp;#x200B;

Thanks"
3299,2020-08-28 13:22:25,1598610145.0,dataengineering,Google Cloud Platform newbie resources?,ii4kgw,KingCouchBouncer,,https://www.reddit.com/r/dataengineering/comments/ii4kgw/google_cloud_platform_newbie_resources/,1.0,3.0,0.0,17967.0,"Junior digital analyst here. We have some legacy BigQuery stuff that no one in our team can understand. Can anyone recommend any resources for learning BQ/GCP from scratch? I’ve heard Coursera might be good?

I have basic SQL/JavaScript knowledge for context if that helps.

Thanks!"
3300,2020-08-28 13:22:51,1598610171.0,dataengineering,Rabbit Meat Production Ranking | TOP 10 Country from 1961 to 2018,ii4kn6,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ii4kn6/rabbit_meat_production_ranking_top_10_country/,1.0,0.0,0.0,17967.0,
3301,2020-08-28 15:08:24,1598616504.0,dataengineering,"Data Wrangling Market Growing at a CAGR 19.7% | Key Player IBM, Oracle, SAS Institute, Trifacta, Datawatch",ii5v8n,pradnya123,,https://www.reddit.com/r/dataengineering/comments/ii5v8n/data_wrangling_market_growing_at_a_cagr_197_key/,1.0,0.0,0.0,17970.0,
3302,2020-08-28 15:58:57,1598619537.0,dataengineering,Technical Mentor Support and Career Coaching for Udacity's 'Become a Data Engineer' Nanodegree Program,ii6k4t,scatterbrained_mugen,,https://www.reddit.com/r/dataengineering/comments/ii6k4t/technical_mentor_support_and_career_coaching_for/,1.0,6.0,0.0,17971.0,"Does anybody have any experience with this program, specifically the 'personalized' support of a mentor and career coach?

I'm currently working in a data analyst / technical support role for a team building out our company's data infrastructure. I'm relatively new to the team but I want to talk with our data engineers (I think our company just refers to them as developers) building out our ETL pipeline and see if I can get involved on my own time.  Granted, I wouldn't call myself a particularly strong 'self-directed learner' at the moment, but I'm hoping that once I get off the ground and have some initial support I can get there.

But in the case that I can't get involved in DE projects within my company, I'm looking for some sort of mentorship and structure as I work towards becoming a DE in my own time. I definitely am willing to put in the work, but I have a past history of getting overwhelmed and distracted by the plethora of things I feel I need to learn. So having structure (whether that be through a learning program or through speaking with a technical mentor who can help keep me in check) would be really beneficial to me."
3303,2020-08-28 18:19:24,1598627964.0,dataengineering,Why Kanaree.io built what they did,ii8y0x,Kanaree_io,,https://www.reddit.com/r/dataengineering/comments/ii8y0x/why_kanareeio_built_what_they_did/,1.0,0.0,0.0,17976.0,"Exciting news after our Kanaree.io launch on Product Hunt Tuesday. The Portland Business Journal has a feature on why we built what we did. [**#kanaree**](https://www.linkedin.com/feed/hashtag/?keywords=kanaree&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6704912053620088832) [**#kanareelife**](https://www.linkedin.com/feed/hashtag/?keywords=kanareelife&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6704912053620088832) [**#kanareeio**](https://www.linkedin.com/feed/hashtag/?keywords=kanareeio&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6704912053620088832) 

[https://www.bizjournals.com/portland/news/2020/08/27/kanareeio-launches-to-help-retailers-control-data.html](https://www.bizjournals.com/portland/news/2020/08/27/kanareeio-launches-to-help-retailers-control-data.html)"
3304,2020-08-28 18:25:06,1598628306.0,dataengineering,How to Actually start your Career as a Data Engineer,ii91vp,ByrdandChen101,,https://www.reddit.com/r/dataengineering/comments/ii91vp/how_to_actually_start_your_career_as_a_data/,1.0,26.0,0.0,17976.0,"That is the question that I have.

&amp;#x200B;

I'm a recent grad ('19) and I have a Bachelor in Accounting. After school, I worked as a Jr. Business Systems Analyst for a multinational company, with which I used my accounting skills to support the accounting department's ERP system. Through that role, I got my first blood taste of data management and engineering while working/shadowing a solutions architect. I have since been laid off because of COVID/department restructuring. I have dedicated my time since then to research how I can enter the data engineering field and be a valuable candidate.

For the hiring managers in this sub:  Does experience matter more than education or Is education an absolute pre-requisite? What is the Most absolute soft-skill to have?

For the new entrants: How was your journey into this field, what did you wish you knew, and what some mistakes you have encountered?

I will appreciate anyone who will take their time to help me. Thanks in advance!"
3305,2020-08-28 20:00:10,1598634010.0,dataengineering,"Predict ""Wine Quality"" using ""Machine Learning"" |Real world problem solve| python| Data Science",iiav7m,8329417966,,https://www.reddit.com/r/dataengineering/comments/iiav7m/predict_wine_quality_using_machine_learning_real/,1.0,0.0,0.0,17978.0,https://youtu.be/qakGunkQ9AU
3306,2020-08-28 20:39:51,1598636391.0,dataengineering,Tool for Automating SQL Transforms,iibmpz,punknight,,https://www.reddit.com/r/dataengineering/comments/iibmpz/tool_for_automating_sql_transforms/,1.0,0.0,0.0,17978.0,"Hey everyone this is Michael and Daniel from the structure.rest team. We built structure as an alternative to the command line based tools that currently exist for building DAGs for your data warehouse.

With command line based tools, you have to edit and explore in a sql editor, paste that into a code editor, use the command line tool and use a web browser to view your data catalog. And then you have to go back and forth constantly between all these tools and do this over and over again for the hundreds of models in your DAG.

Instead, we’ve built an open source editor + command line utility that integrates all of this into a single integrated experience. We feel that better tools lead to better data analysis which  helps organizations make better data driven decisions

Here’s a video that shows how intuitive the structure editor is : https://www.youtube.com/watch?v=hskhBTyg258

Come check us out at www.structure.rest and join our [slack] (https://join.slack.com/t/structuresupport/shared_invite/zt-ddx04ho4-_q43i5o3zQ9jv00qx~dx8A). Both the editor and command line utility are open source and  the editor downloads as an app for Windows Linux, and Mac. Our command line tool makes it easy to run your DAG as part of CI/CD.

We currently support [Snowflake](snowflakecomputing.com), but we are looking forward to supporting other platforms. Let us know if there is a platform you would like us to support next."
3307,2020-08-28 23:27:53,1598646473.0,dataengineering,Please suggest,iietx2,popcee,,https://www.reddit.com/r/dataengineering/comments/iietx2/please_suggest/,1.0,6.0,0.0,17983.0,"I have like 8 months. Please help me with what I have to study so that I can be ready to get Job in Data Sciences.

I am looking for a Job that involves less coding.

&amp;#x200B;

Thanks."
3308,2020-08-29 03:11:12,1598659872.0,dataengineering,Need opinions/options on a pipeline solution,iiio3r,TeleTummies,,https://www.reddit.com/r/dataengineering/comments/iiio3r/need_opinionsoptions_on_a_pipeline_solution/,1.0,0.0,0.0,17988.0,"Hi! 

A quick context about myself. I am new to data engineering. I have experience in SSIS and feel very comfortable in SQL. I also have an understanding of dimensional modeling and relational modeling. 

That said, I am working on a small internal project for a company where we are trying to create a data warehouse sourced by off the shelf systems(like a payroll system, Microsoft dynamics, other systems that provide REST API access. 

At this point, we have an azure data lake that is synced to Dynamics CRM through common data services. Those files are in containers now. 

I have a few questions for this group about my situation: 

- Should I alao store my REST API data in azure data lake as well, and then move all via a pipeline/set of jobs? 

- I feel like when I google this ETL solutions, there are SO many options to move data and schedule those moves. Azure data factory, data bricks, custom scripts and azure functions. Can anyone weigh in on what you'd recommend?  All I want to do is dump raw data into tables and have stored procs clean it and translate to my warehouse schemas. 

I apologize for asking such a specific question. I am happy to read any resources/literature on this if you're not willing to help me prescribe a solution."
3309,2020-08-29 09:04:48,1598681088.0,dataengineering,Worth to spend time on Hadoop ?,iingoq,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/iingoq/worth_to_spend_time_on_hadoop/,1.0,11.0,0.0,17995.0,"Heya Folks ! 
Wanted to know if it is worth spending time and learning in-detail on Hadoop hdfs , YARN architecture and it's configuration , Pig Latin, Hive,HBase ,Oozie ?

I'm from a spark databricks and azure data engineering framework background and have also worked on airflow.

Does data engineering interviews expect you to know in detail about the Hadoop ecosystem ?"
3310,2020-08-29 09:16:46,1598681806.0,dataengineering,Something equivalent to Kaggle for practicing Data Engineering..?,iinlis,vaibhi2502,,https://www.reddit.com/r/dataengineering/comments/iinlis/something_equivalent_to_kaggle_for_practicing/,1.0,17.0,0.0,17995.0,
3311,2020-08-29 13:38:45,1598697525.0,dataengineering,Agricultural Area Ranking | TOP 10 Country from 1961 to 2013,iiqc9q,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/iiqc9q/agricultural_area_ranking_top_10_country_from/,1.0,0.0,0.0,18000.0,
3312,2020-08-29 13:52:15,1598698335.0,dataengineering,Rate this course,iiqh91,powok,,https://www.reddit.com/r/dataengineering/comments/iiqh91/rate_this_course/,1.0,0.0,0.0,18001.0,"Hey guys ,

How would you rate this course , is it worth taking this course for building Hadoop knowledge.

https://practice.geeksforgeeks.org/courses/big-data"
3313,2020-08-29 19:47:32,1598719652.0,dataengineering,Airflow not seeing my directory,iivo4u,Iffexibility1,,https://www.reddit.com/r/dataengineering/comments/iivo4u/airflow_not_seeing_my_directory/,1.0,4.0,0.0,18012.0,"I am new to airflow and I am trying to load a CSV file into PostgreSQL. I have used file\_hook to work on the CSV data using PythonOperator to see if I can load the data which worked perfectly.

The major problem I am having is using the filepath in SQL query, when I pass the filepath, it says 'there is no such directory'

Below is the python callable code

&amp;#x200B;

 

def load\_data\_to\_covid\_donors(\*args, \*\*kwargs):  
fs\_hook = FSHook('system\_files')        #connect to file  
base\_path = fs\_hook.get\_path()

\# get file path  
file\_path = os.path.join(base\_path, ""/home/dit/airflow/dags/covid19\_donors\_data\_cleaned.csv"")  


sql = f""""""  
COPY covid\_donors  
FROM '{file\_path}'  
HEADER  
CSV  
""""""  
pg\_hook = PostgresHook(""postgres\_id"")  
pg\_hook.run(sql)   #this give me 'no such directory error'

&amp;#x200B;

PLEASE HELP OUT"
3314,2020-08-29 20:38:32,1598722712.0,dataengineering,How to handle EU format csv files in Azure data factory or databricks?,iiwljb,shashikr_a,,https://www.reddit.com/r/dataengineering/comments/iiwljb/how_to_handle_eu_format_csv_files_in_azure_data/,1.0,3.0,0.0,18016.0,"Hello data engineering community!

I have thousands of, varied schema, csv files in European format (with semicolon ; delimiter and “ . “ as thousand separator &amp; “,” as decimal.

We wanted to keep the numbers in European format in our synapse DWH.

Any suggestions or blog post reference on how to handle this efficiently in ADF data pipeline (may be with help of databricks too)?"
3315,2020-08-30 00:24:57,1598736297.0,dataengineering,What is the best resource to learn Hadoop and Spark?,ij0j5v,PM_ME_YOUR_DONUT_PLS,,https://www.reddit.com/r/dataengineering/comments/ij0j5v/what_is_the_best_resource_to_learn_hadoop_and/,1.0,24.0,0.0,18024.0,For those who already have a decent background in python and sql.
3316,2020-08-30 04:16:25,1598750185.0,dataengineering,Resources on Apache Beam,ij45ij,cofonlafaefe,,https://www.reddit.com/r/dataengineering/comments/ij45ij/resources_on_apache_beam/,1.0,2.0,0.0,18023.0,"I've been learning Apache Beam (Python SDK) for a couple of weeks reading the docs and few examples I've found in blog posts, but feel like I'm now in need of a more hands-on course and examples of how to run both streaming and batch pipelines in production (preferably in AWS, I know GCP has Dataflow). Does anyone know any good resources on deploying Beam pipelines for [Lambda architectures](https://en.wikipedia.org/wiki/Lambda_architecture)?"
3317,2020-08-30 05:56:13,1598756173.0,dataengineering,Advice on breaking into data specific roles?,ij5jdm,greywaterbottle44,,https://www.reddit.com/r/dataengineering/comments/ij5jdm/advice_on_breaking_into_data_specific_roles/,1.0,0.0,0.0,18025.0,"Hi All,

Wanted to get everyone's thoughts on how best to improve my resume/profile for data specific tech roles given my semi-relevant background. Also had some other questions about the field in general (see below).

So a bit of background:

I have an engineering background (non-cs) from college and have had some experience coding through coursework (C,R,SQL,Python etc.) but ended up joining a big 4 consultancy after college and have been working here for the last two years. My work has primarily been in the Cyber and Analytics practices working to implement technical tools as well as more business type consulting.

Earlier this year in February a headhunter reached out to me for a role as a Data engineer at a startup, I was curious and ended up interviewing but did not making it to the final round due to not passing the technical interview. I've since had interviews for 2 similar pseudo-data engineer/consultant roles at different companies that have been on pause due to Covid-19.

I have a few questions about the space was wondering if you could all help me out since new to the technology industry. Thank you in advance!

Questions:

**1.** **I want a role that is more technical and hands on (use coding) but also ideally has some sort of client facing or organizational/stakeholder management facet as well - what specific roles should I be looking for? (Name/Type etc.)**

* For example the first data engineer role (Forward Deployed Engineer) that I had interviewed for seemed to have a combination of both these elements - I had to use Python and SQL to analyze datasets and automate various processes but still had to interface with the clients of the product and manage their expectations, ensuring they were satisfied. From my understanding data science/engineer roles are not usually client facing as they are expected to comb through existing data sets or maintain existing company databases and kind of act in as support without much customer interaction.

**2. What can I do to bolster my resume to make it attractive to these tech positions when cold applying.**

* Are there specific key words/phrases/experience/certifications the recruiters like to see? I've been lucky to get interviews through 3rd party head hunters and referrals but am wondering if there are certain things I can put on my resume to become a more attractive candidate on paper. So far I have 1 interview out of 300+ online apps. The issue seems to be my lack professional projects, I've mainly used my technical skills in an ad-hoc fashion (e.g. writing code to aggregate accounts/approval processes) and maybe had one instance where we created a proof of concept for an ""AI"" analytics solution for a client. **How can I show I have the aptitude to preform in the role without directly being in a very similar role?** I have applied for a MS in computational analytics at Georgia Tech but that won't start until next year and I ideally would want to have switched jobs before then (also tbh it's a bit overkill if I had do a masters to make this job switch possible).

**3. If my current goal is to eventually be a product lead/product manager of a data/analytics type of product am I taking the right path?**

* My theory was if I got some hands experience working with data in some sort of hybrid software engineering capacity it would set me up well to eventually be a PM for similar type of data tool/platform. Let me know if this is completely off base or if there is a way to make the jump without first working in a technical role (from what I've seen there isn't)

Tl;dr I know there is some demand for someone with my past experience and am looking for recommendations on how to fully solidify my profile so I can secure a job.

Also if anyone has any other resources they can recommend that would help, let me know!"
3318,2020-08-30 06:11:24,1598757084.0,dataengineering,Do cloud datawarehouses spell doom for datawarehouse appliances,ij5rfv,Roof_Remote,,https://www.reddit.com/r/dataengineering/comments/ij5rfv/do_cloud_datawarehouses_spell_doom_for/,1.0,11.0,0.0,18025.0,"With Snowflake filing its S1, cloud datawarehouses is a hot topic. In this new world order what is the future of Teradata and Netezza like offerings? 

Do you think all data warehouse appliances (whether on-prem or in the cloud) would be replaced by cloud datawarehouses like Snowflake and Bigquery? Is there any need for datawarehouse behind the firewall or in virtual private cloud?"
3319,2020-08-30 13:25:25,1598783125.0,dataengineering,Energy Exports Ranking | TOP 10 Country from 1971 to 2014,ijamzr,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ijamzr/energy_exports_ranking_top_10_country_from_1971/,1.0,0.0,0.0,18036.0,
3320,2020-08-30 17:43:35,1598798615.0,dataengineering,"I want to become a data engineer. Can someone please recommend me books or step to go forward? (I have working experience on SSIS , SSAS and SSRS).",ijdr6k,codeinsync,,https://www.reddit.com/r/dataengineering/comments/ijdr6k/i_want_to_become_a_data_engineer_can_someone/,1.0,37.0,0.0,18043.0,
3321,2020-08-30 20:57:23,1598810243.0,dataengineering,Data engineering data structures,ijh4nq,redder_ph,,https://www.reddit.com/r/dataengineering/comments/ijh4nq/data_engineering_data_structures/,1.0,4.0,0.0,18049.0,"I am preparing for a data engineering interview. The job description includes building ETL integration of large scale, real-time data. Are there specific data structures I should focus on in my preparation, like Lists, Stacks, Queues or should I just study all data structures?"
3322,2020-08-30 22:29:59,1598815799.0,dataengineering,Spark for Data Warehouse?,ijit53,jonscrypto,,https://www.reddit.com/r/dataengineering/comments/ijit53/spark_for_data_warehouse/,1.0,16.0,0.0,18050.0,"I'm new to Spark and planning to learn to use it to consume a big data stream for machine learning alongside a BI warehouse for customer and sales analytics. I'd like to be able to run everything locally as well.

I've been looking at traditional structured database for the warehouse (SQL Server, Postgres, MySQL) but am wondering whether it makes sense to do it all with Spark.

Is it common to use Spark for BI with structured data (if already being used for big data)? If so would I store the tables on a data lake (Hive?), hdfs, or just as native text files in my file system?"
3323,2020-08-30 23:52:34,1598820754.0,dataengineering,how do the pieces fit together?,ijk9zu,dilf314,,https://www.reddit.com/r/dataengineering/comments/ijk9zu/how_do_the_pieces_fit_together/,1.0,3.0,0.0,18055.0,"I’ve heard of tons of different data engineering technologies but I can’t picture how they all work together in the end. Are there any examples of data engineering projects from start to finish using different software? Like I know there’s Hadoop, Hive, Pyspark, etc but I can’t visualize in my head how all of those technologies work together to form a pipeline."
3324,2020-08-31 03:50:11,1598835011.0,dataengineering,"The data engineering weekly #6 is out. This week's release is a new set of articles that focus on Kafka summit recap, Type 2 dimension modeling, securing Presto, handling bios in AI from Shopify, DoorDash, Linkedin &amp; Confluent.",ijoaxe,vananth22,,https://www.reddit.com/r/dataengineering/comments/ijoaxe/the_data_engineering_weekly_6_is_out_this_weeks/,1.0,0.0,0.0,18071.0,
3325,2020-08-31 13:16:44,1598869004.0,dataengineering,Male High Blood Pressure Ranking | TOP 10 Country from 1975 to 2015,ijvefo,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ijvefo/male_high_blood_pressure_ranking_top_10_country/,1.0,0.0,0.0,18096.0,
3326,2020-08-31 14:59:48,1598875188.0,dataengineering,Advanced data structures in DE interviews,ijwmv3,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/ijwmv3/advanced_data_structures_in_de_interviews/,1.0,2.0,0.0,18100.0,"Hey guys,

I would like to take your opinion on some interview questions that people ask in DE interviews. Does it make sense to ask questions on advaced DS like graphs, trees, dynamic programming etc. There are times I have seen such questions whose optimised solution can only come up if you have practised before hand"
3327,2020-08-31 15:00:01,1598875201.0,dataengineering,Advanced data structures in DE interviews,ijwmyh,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/ijwmyh/advanced_data_structures_in_de_interviews/,1.0,19.0,0.0,18100.0,"Hey guys,

I would like to take your opinion on some interview questions that people ask in DE interviews. Does it make sense to ask questions on advaced DS like graphs, trees, dynamic programming etc. There are times I have seen such questions whose optimised solution can only come up if you have practised before hand"
3328,2020-08-31 17:06:50,1598882810.0,dataengineering,Effective testing for machine learning systems.,ijylmp,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/ijylmp/effective_testing_for_machine_learning_systems/,1.0,0.0,0.0,18102.0,
3329,2020-08-31 17:47:44,1598885264.0,dataengineering,The State of Flink on Docker,ijzbw4,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ijzbw4/the_state_of_flink_on_docker/,1.0,0.0,0.0,18102.0,
3330,2020-08-31 19:08:23,1598890103.0,dataengineering,Hunting for an internship,ik0twa,CodeFather007,,https://www.reddit.com/r/dataengineering/comments/ik0twa/hunting_for_an_internship/,1.0,2.0,0.0,18102.0,"Hi, a quick intro. I am a data science graduate student graduating in May 2021. I am actively looking for an internship but till now the effort has only gone in vain. 

Initially I was just applying through LinkedIn, Indeed, etc. and all I was getting was rejections. Even though I haven't got an interview till now, I have started connecting with people who are already working in my field, bit it never got to a point where they can a referral for me. Summer has gone. Now I am looking for internships in winter 2020 and Spring 2021. A little more info about my background, I don't have a proper work experience other than a 2 month internship which I did in my UG. 

I don't know where I am going wrong. I know having no work experience is a problem but I have utilized this summer by being a Research Assistant and doing a pet project along with that. I would like if someone could guide me in this"
3331,2020-08-31 19:19:56,1598890796.0,dataengineering,Corona-virus data visualization using Geographical Plotting | Plotly| Choropleth plot,ik11sx,8329417966,,https://www.reddit.com/r/dataengineering/comments/ik11sx/coronavirus_data_visualization_using_geographical/,1.0,0.0,0.0,18102.0,https://youtu.be/M5gAIAZ00Ic
3332,2020-08-31 21:50:53,1598899853.0,dataengineering,Tracing Data Issues in Data Pipelines,ik40gz,Roof_Remote,,https://www.reddit.com/r/dataengineering/comments/ik40gz/tracing_data_issues_in_data_pipelines/,1.0,4.0,0.0,18105.0,"Hello experts,

I was wondering if folks can point me to the best practices or tools that can help trace and debug data issues in data pipelines. I am trying to find out if there are some robust and easy solutions to debug problems like erroneous null values appearing in a table or duplicates being inserted that lead to wrong results after the data has run through multiple stages of a data pipeline."
3333,2020-08-31 21:57:39,1598900259.0,dataengineering,How to implement delayed tasks in airflow,ik453g,FlavoredFrostedTits,,https://www.reddit.com/r/dataengineering/comments/ik453g/how_to_implement_delayed_tasks_in_airflow/,1.0,4.0,0.0,18105.0,"I want to create a DAG that runs multiple ETLs but one of the ETL will run a few hours later. After all the ETLs finish running there will be an additional task to fire off a task to Rabbitmq. 

Now I'm a total Airflow noob so please advise me. I've read a little on sensors and that looks promising but not too many examples of their usage."
3334,2020-08-31 22:57:38,1598903858.0,dataengineering,"""Continuous Analysis, Continuous Insights, and Data Quality"" - An interview with Gordon Wong, the data scientist who built FitBit's first analytics platform.",ik5av8,dataculpa,,https://www.reddit.com/r/dataengineering/comments/ik5av8/continuous_analysis_continuous_insights_and_data/,1.0,0.0,0.0,18104.0,
3335,2020-09-01 02:22:55,1598916175.0,dataengineering,Migration from NLP to Data Engineering,ik91x4,nlpguy2,,https://www.reddit.com/r/dataengineering/comments/ik91x4/migration_from_nlp_to_data_engineering/,1.0,0.0,0.0,18111.0,"I have been doing Natural Language Processing for the past few years, mostly learning how to train heavy deep learning models and publishing research papers.  I'm familiar with PyTorch and the Python data science stack.      

However the field as a whole is very much tied to academia and highly biased towards advanced degree holders. I have just a CS degree.   

As for various reasons I cannot afford pursuing a PhD in this lifetime, I have been looking at adjacent fields that seem to have lower academic requirements.    

I have thought about becoming a run-of-the-mill full-stack software developer ( Spring Boot,  React, .NET ), as the demand is so high everywhere. But I believe my NLP background would not be helpful at all, a novelty item at best. I feel like I would be starting from scratch.    

However, I have been looking around on LinkedIn and it seems to me that someone who knows data engineering and also has a background in NLP is more likely to stand out.    

Is this a wise decision? I have never had any experience with Spark and Hadoop and similar tools but I see them quite often as job requirements to attractive positions and it is making me consider becoming a data engineer over the usual route of becoming a full-stack software developer."
3336,2020-09-01 05:01:10,1598925670.0,dataengineering,Data of amazon and amazon prime volumes by zip code,ikbkub,sanga,,https://www.reddit.com/r/dataengineering/comments/ikbkub/data_of_amazon_and_amazon_prime_volumes_by_zip/,1.0,0.0,0.0,18114.0,"Any data sources that will give me historical ( couple of years) amazon purchase trends by zip code?  
Thanks"
3337,2020-09-01 07:20:53,1598934053.0,dataengineering,Guiding principles for a data engineering team,ikdnbu,rahulj51,,https://www.reddit.com/r/dataengineering/comments/ikdnbu/guiding_principles_for_a_data_engineering_team/,1.0,3.0,0.0,18118.0,
3338,2020-09-01 10:39:45,1598945985.0,dataengineering,An Apache Airflow MVP: Complete Guide for a Basic Production Installation Using LocalExecutor,ikg1p7,j0selit0342,,https://www.reddit.com/r/dataengineering/comments/ikg1p7/an_apache_airflow_mvp_complete_guide_for_a_basic/,2.0,0.0,0.0,18124.0,
3339,2020-09-01 11:22:06,1598948526.0,dataengineering,Considering to get into Data Engineering,ikghk3,nlpguy2,,https://www.reddit.com/r/dataengineering/comments/ikghk3/considering_to_get_into_data_engineering/,2.0,8.0,0.0,18124.0,"I have been doing Natural Language Processing for the past few years, mostly learning how to train heavy deep learning models and publishing research papers. I'm familiar with PyTorch and the Python data science stack.

However the field as a whole is very much tied to academia and highly biased towards advanced degree holders. I have just a CS degree.

As for various reasons I cannot afford pursuing a PhD in this lifetime, I have been looking at adjacent fields that seem to have lower academic requirements.

I have thought about becoming a run-of-the-mill full-stack software developer ( Spring Boot, React, .NET ), as the demand is so high everywhere. But I believe my NLP background would not be helpful at all, a novelty item at best. I feel like I would be starting from scratch.

However, I have been looking around on LinkedIn and it seems to me that someone who knows data engineering and also has a background in NLP is more likely to stand out.

Is this a wise decision? I have never had any experience with Spark and Hadoop and similar tools but I see them quite often as job requirements to attractive positions and it is making me consider becoming a data engineer over the usual route of becoming a full-stack software developer."
3340,2020-09-01 13:28:29,1598956109.0,dataengineering,Tuberculosis Death Rate Ranking | TOP 10 Country from 2000 to 2015,ikhtmw,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ikhtmw/tuberculosis_death_rate_ranking_top_10_country/,0.0,0.0,0.0,18128.0,
3341,2020-09-01 22:10:09,1598987409.0,dataengineering,Help with Data Transformation,ikqo96,UD48,,https://www.reddit.com/r/dataengineering/comments/ikqo96/help_with_data_transformation/,1.0,7.0,0.0,18149.0,"Data Transformation Question. Hi! Full Disclaimer, I'm not a Data Engineer so I apologize for any feelings that the following question might cause. :)

I'm working on a product where as part of a business process we're systematically collecting a bunch of information about individuals (think Privacy). The collection requires transformation (some content must be converted from machine codes to human-readable content, other information needs to be obscured and all of the data should be reformatted into something that this is easy to read and consume). We're doing this today using a bunch of scripts (through an eDiscovery vendor).

I'm looking for a set of libraries or a tool that could ingest or process all of this content (usually from a JSON file) and allow us to run a standard set of business rules against the data and then output the data into a nice HTML or JSON format. Each collection could contain dozens of files from different data repositories.

Again, my apologies, if this is the wrong forum, but I'm a little lost trying to find a solution."
3342,2020-09-01 23:02:55,1598990575.0,dataengineering,"Are there any advantages of using pipe (|) instead of comma (,) as file delimiter?",ikroc9,andresg3,,https://www.reddit.com/r/dataengineering/comments/ikroc9/are_there_any_advantages_of_using_pipe_instead_of/,1.0,5.0,0.0,18150.0,"Hi, I'm working on a small Pyspark project. I noticed some people prefer using pipe over comma to write a df into ""csv"" files. Are there any advantages by doing that?"
3343,2020-09-02 02:03:24,1599001404.0,dataengineering,How can I find a free or extremely cheap way for someone to help me learn Data Engineering?,ikv2fr,freebird348,,https://www.reddit.com/r/dataengineering/comments/ikv2fr/how_can_i_find_a_free_or_extremely_cheap_way_for/,1.0,13.0,0.0,18159.0,"I know this is an unconventional question, but essentially I am a Data Analyst trying to transition to Data Engineering (which I know is common on this sub). 

So I have spent a ton of time teaching myself DE technologies (such as Python, Spark, AWS EMR, etc) and I'm at the point where I'm really stagnating in my self learning and need some help propelling by someone walking me through sample projects or answering my very specific questions. I don't have the money to pay for these subpar bootcamps that are basically all online now and definitely don't have the money to pay for a masters. 

Now this sub has been awesome, but there is only so much that I can get answered in a block of text instead of someone over my shoulder helping me out. So I come to you all and see if you have any ideas on how I can find ways to get specific help on my issues. Maybe go to a college office hours and see if a professor would help me (out of the goodness of their heart)? My company has a $5k per year in training budget that I may be able to use -- is there a way I can put that to good use? 

Basically, I've exhausted all my online options and want to get in front of someone to ask them very specific questions instead of taking a virtual course."
3344,2020-09-02 05:46:36,1599014796.0,dataengineering,Question: Role of Amazon Data Engineer,ikynp5,civilsaspirant13,,https://www.reddit.com/r/dataengineering/comments/ikynp5/question_role_of_amazon_data_engineer/,1.0,21.0,0.0,18162.0,"Hi guys, 

Any Amazon DE here? Can you share your experience at Amazon, in the lines of, what you do, what tools you use, what kind &amp; volume of data you deal with, what are the expectations from a DE etc.

Thank you so much, in advance."
3345,2020-09-02 07:11:06,1599019866.0,dataengineering,Does anyone else here handle ETL/ELT with Domo?,ikzwbr,b_st,,https://www.reddit.com/r/dataengineering/comments/ikzwbr/does_anyone_else_here_handle_etlelt_with_domo/,1.0,1.0,0.0,18164.0,"I am relatively new to the field, but my expertise is centered mainly around a Domo data warehouse. Our team utilizes SQL, Redshift, and a couple of Domo's proprietary ETL solutions.  

I never see anyone talk about Domo, although I know it is used by quite a few big players and counting, such as Coca Cola, Zillow, eBay, Ford, Cisco, etc.  

Is being an advanced Domo DE considered valuable in our industry? Thanks in advance, looking forward to any conversation."
3346,2020-09-02 09:12:40,1599027160.0,dataengineering,Regarding the best laptops 20202,il1hkp,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/il1hkp/regarding_the_best_laptops_20202/,1.0,0.0,0.0,18169.0,"[removed]

[View Poll](https://www.reddit.com/poll/il1hkp)"
3347,2020-09-02 10:32:00,1599031920.0,dataengineering,[VIDEO]: Apache Flink Dataflow &amp; Snapshots,il2e0e,Marksfik,,https://www.reddit.com/r/dataengineering/comments/il2e0e/video_apache_flink_dataflow_snapshots/,1.0,0.0,0.0,18175.0,
3348,2020-09-02 12:33:11,1599039191.0,dataengineering,Ideas for data engineering hackathon,il3ojf,dev178,,https://www.reddit.com/r/dataengineering/comments/il3ojf/ideas_for_data_engineering_hackathon/,1.0,2.0,0.0,18179.0,"I am a senior data engineer. I thought about metadata and lineage management, simplifying data pipelines. Looking for more ideas for a data engineering hackathon."
3349,2020-09-02 13:03:56,1599041036.0,dataengineering,DataCamp has one week unlimited free access!,il40ih,RonFo,,https://www.reddit.com/r/dataengineering/comments/il40ih/datacamp_has_one_week_unlimited_free_access/,1.0,5.0,0.0,18180.0,"I am learning on DataCamp right now (to get Data Engineering basics) and am more than happy with it!  
Today, I wanted to upgrade to premium and as I logged in, I had quite a nice surprise.  
So I am sharing that with you, guys!

[https://datacamp.com/freeweek](https://datacamp.com/freeweek)

Enjoy!"
3350,2020-09-02 14:01:26,1599044486.0,dataengineering,Japan vs South Korea | GDP per capita from 1960 to 2017,il4odw,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/il4odw/japan_vs_south_korea_gdp_per_capita_from_1960_to/,1.0,0.0,0.0,18182.0,
3351,2020-09-02 17:19:37,1599056377.0,dataengineering,Data Council Berlin Picnic #1: Data Stacks Volume I,il7na8,soobrosa,,https://www.reddit.com/r/dataengineering/comments/il7na8/data_council_berlin_picnic_1_data_stacks_volume_i/,1.0,0.0,0.0,18190.0,"remote meetups don’t really work for us. So we want to try something else: A PICNIC IN THE PARK. With proper distancing, your own drinks &amp; food.  
[https://www.meetup.com/DataCouncil-ai-Berlin-Data-Engineering-Science/events/272930965/](https://www.meetup.com/DataCouncil-ai-Berlin-Data-Engineering-Science/events/272930965/)  
If you are in or near Berlin, join us!"
3352,2020-09-02 19:32:03,1599064323.0,dataengineering,Actuarial Team - Need help on interview questions for Data Engineer,ila50h,werteen1,,https://www.reddit.com/r/dataengineering/comments/ila50h/actuarial_team_need_help_on_interview_questions/,1.0,2.0,0.0,18197.0,"I'm looking for some help on interview questions for a data engineer to be on our actuarial team. I don't quite have the technical knowledge and I'm looking for some guidance. We are looking for someone who can:

* Be the sole actuarial data steward within the wider organization, and strive to develop and promote sound and consistent data management practices.
* Create data models to support actuarial analysis and end-user analytics and reporting.
* Validate and integrate data models produced elsewhere in the organization on which actuarial relies.
* Own and improve upon the actuarial data production, storage, validating and cataloging.

We have a SQL test that we use for hiring actuaries but I feel the questions might be too simple for a Data Engineer:
    
    1.	Find a character(s) within a string field, and replace with different character(s)
     a.	Replace “test” with “final”
     b.	Replace tab and carriage return with semi-colon (;)
    2.	Find duplicate records in a table
    3.	Join two tables together and find records that are in one but not another using ID
     a.	Combine table A and B, with only records in both
     b.	Append records from table B to A, keeping all records in table A, and dropping those in B that don’t match
     c.	Find IDs in table B, that don’t exist in table A 
    4.	Use formula, to create Sales_Group
    5.	Modify existing table
     a.	Add new column “NetSales”
     b.	Make new column be equal to 70% of Sales
    6.	Fill in missing measurements in table B
     a.	Use previous measurement(s) from last recorded time, when current measurement doesn’t exist
        
Is it common to give a technical take home challenge and then have the candidate walk-though their work during the interview? We were thinking since a ton of analysis is done in Excel to give a take-home challenge that would require the candidate to pull data from a few tables into an Excel template which then runs some analysis which would get output to a SQL table.  Is this too hard or too easy? Should we add more requirements? How much time would you need to complete something like this?"
3353,2020-09-02 19:32:31,1599064351.0,dataengineering,Large up to date Flights dataset for Spark,ila5cw,Fnmokh,,https://www.reddit.com/r/dataengineering/comments/ila5cw/large_up_to_date_flights_dataset_for_spark/,1.0,3.0,0.0,18197.0,"Hello everyone,

I need a large up to date dataset for a Spark application, containing data about flights, delays, destinations, origins timestamps, etc 

There is à flights dataset on the spark learning github repo, but it dates to 2015.

Any idea where I can find an updated version of it ? Or a similar dataset that’s more recent please ?

Thanks in advance !"
3354,2020-09-02 20:57:20,1599069440.0,dataengineering,MultiProcessingBenchmark,ilbtc7,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/ilbtc7/multiprocessingbenchmark/,1.0,0.0,0.0,18200.0,"Hi Folks

I have created a library that benchmarks your system for single vs milti core performance on various pandas functions. It can be found at [pypi](https://pypi.org/project/MultiProcessingBenchmark/0.1.9/#description) and the github link is [here](https://github.com/srivassid/MultiProcessingBenchmark).

Please let me know what do you think.

Cheers"
3355,2020-09-02 21:34:27,1599071667.0,dataengineering,Best practise on logging pipelines in Python,ilcjwr,Luukv93,,https://www.reddit.com/r/dataengineering/comments/ilcjwr/best_practise_on_logging_pipelines_in_python/,1.0,12.0,0.0,18201.0,"Hello,

What are your best practices regarding logging data pipelines in Python? 

What library do you use and where do you store logs?

&amp;#x200B;

Regards,

Luuk"
3356,2020-09-02 22:13:43,1599074023.0,dataengineering,Data Lake organization,ildcno,drping666,,https://www.reddit.com/r/dataengineering/comments/ildcno/data_lake_organization/,1.0,9.0,0.0,18201.0,"Hi guys,  
i'm coming from RDBMS world and transitioning to big data universe. searched a lot in internet about data lake 'concept' but couldn't find a real world implementation : how ingested data is organized ? as a folders for each data source / day? what is recommended way to architect the 'zoning' for data lake : landing area for raw data / transformed data / governed data / ?  
how data scientists should have access and use data stored in data lake : should they create their own 'data mart' / analytical tables or should they ask data engineers to prepare some 'common' data sets for common needs ?"
3357,2020-09-02 22:58:42,1599076722.0,dataengineering,Software Recommendations for Managing Metadata / Catalog for Microsoft-Based DW Solution,ileabb,PencilBoy99,,https://www.reddit.com/r/dataengineering/comments/ileabb/software_recommendations_for_managing_metadata/,1.0,6.0,0.0,18200.0,"Looking for recommendations for a metatdata / catalog tool that would support deep text documentation of our data warehouse (what this field means, where it comes from, etc)."
3358,2020-09-03 03:25:01,1599092701.0,dataengineering,Career path as a new grad data engineer?,ilj2vx,mohomie,,https://www.reddit.com/r/dataengineering/comments/ilj2vx/career_path_as_a_new_grad_data_engineer/,1.0,2.0,0.0,18210.0,"Hi everyone, I finished my bachelor’s in CS recently and just started my career as a new grad data engineer for a large finance company. Really excited to learn, but I’m trying to figure out where I want to be as an engineer over time. I am on a data engineering team for one year, and will then move to another team of my preference for the next year.

Part of me is interested in going into ML engineering once my skills develop further, but I’m not sure how much overlap exists between the two roles. I don’t plan on going for my master’s at the moment so I understand there may be some limitations with this.

For the data engineers here, can you tell me about your journey in the role, and where it might go from here? Also, what are other roles to consider for my next rotation that might complement or overlap with the skillset I will develop as a junior data engineer?

Thank you in advance!"
3359,2020-09-03 04:02:12,1599094932.0,dataengineering,Updating facts and dimensions in DW,iljo9m,Silicon-Data,,https://www.reddit.com/r/dataengineering/comments/iljo9m/updating_facts_and_dimensions_in_dw/,1.0,1.0,0.0,18211.0,"Hi all,  


I have streaming data coming into my data pipeline non-stop. Obviously at some point dimensions and facts should occur, hopefully not causing any downtime since data analysts and BI people prefer viewing the latest snapshot of data.  


Here are my questions:

1) What are your typical strategies to avoid downtime in OLAP system due to data load?

2) How would you back fill records that arrive a day or two later?"
3360,2020-09-03 09:51:56,1599115916.0,dataengineering,Software Developer to Data Engineer,ilojjs,Mr_Metakinon,,https://www.reddit.com/r/dataengineering/comments/ilojjs/software_developer_to_data_engineer/,1.0,2.0,0.0,18218.0,"Hi guys,

I have a Bachelor's  in CS and currently working in a startup for about 18 months now. I was employed as Software Developer but after a couple of months, I was given various task of creating ETL pipelines, etc. using Airflow during which i realized i was way more interested in data engineering domain than in being a software developer.   
As i mentioned before, i am working in a startup and we didn't had anyone working in this domain before me, so i was responsible from research for the right tool to creation and deployment of the pipeline and services. Hence, I don't know any industry standards and processes. I went in this domain out of curiosity and like working in it.

Now, I am looking for a new job, as the startup is failing due to COVID but I am not able to find a entry level job due to less experience. What should I do, should I get a Masters, or should I work as a Software developer to gain experience and try again?

I guess, the biggest issue is that I don't know where i stand with my skill set."
3361,2020-09-03 09:55:49,1599116149.0,dataengineering,Kafka and Airflow tutorials?,ilol6n,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/ilol6n/kafka_and_airflow_tutorials/,1.0,12.0,0.0,18218.0,Is there any recommendations on Udemy courses of Kafka and Airflow? I'm a beginner so a shirt course that has projects would be more suitable for me.
3362,2020-09-03 10:07:27,1599116847.0,dataengineering,SQL In 20 Minutes,iloqb6,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/iloqb6/sql_in_20_minutes/,1.0,0.0,0.0,18220.0,
3363,2020-09-03 10:24:41,1599117881.0,dataengineering,"Kafka, Druid and Superset - worth pursuing learning for career/ skills?",ilox8h,throway_2020,,https://www.reddit.com/r/dataengineering/comments/ilox8h/kafka_druid_and_superset_worth_pursuing_learning/,1.0,1.0,0.0,18220.0,"I understand how this fits in conceptually, and I would like to learn these. How are these skill sets in terms of job opportunities? What is the outlook? Is it worth pursuing this? Thanks."
3364,2020-09-03 12:40:09,1599126009.0,dataengineering,A Look into Uber’s Futuristic Self-Driving Cars Technology,ilqcpu,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/ilqcpu/a_look_into_ubers_futuristic_selfdriving_cars/,1.0,0.0,0.0,18222.0,
3365,2020-09-03 13:52:50,1599130370.0,dataengineering,Modern Data Engineer Roadmap 2020,ilr6er,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/ilr6er/modern_data_engineer_roadmap_2020/,1.0,68.0,0.0,18222.0,"Hey everyone — In the last couple of weeks I've put a lot of effort into creating a high quality, comprehensive roadmap for data engineers. Hope you'll find it useful.

Here is the Github repo with the roadmap: [https://github.com/datastacktv/data-engineer-roadmap](https://github.com/datastacktv/data-engineer-roadmap)

Let me know what you think!"
3366,2020-09-03 14:13:35,1599131615.0,dataengineering,Age Dependency Ranking | TOP 10 Country from 1960 to 2017,ilrfk5,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ilrfk5/age_dependency_ranking_top_10_country_from_1960/,1.0,0.0,0.0,18222.0,
3367,2020-09-03 17:40:19,1599144019.0,dataengineering,Running python jobs on Databricks,ilunda,de1pher,,https://www.reddit.com/r/dataengineering/comments/ilunda/running_python_jobs_on_databricks/,1.0,5.0,0.0,18227.0,"Hi all,

I've recently joined a new company that uses Databricks for most ETLs. Note that I'm reasonably new to Databricks. Sadly, I feel that our practices are far from ideal as we have a lot of notebooks in production together with some jars.

Setting up our own spark cluster on k8s would have optimal, but I don't see it happening in the near future. With that in mind, I've been looking for effective ways to run jobs on Databricks without relying on notebooks.

I've identified 2 methods so far:

1. Start the cluster with your own docker image which contains the job (Python module) that you need to execute. This seems sensible to me, however, based on the documentation, it seems that custom docker images are intended to give users control over the environment, not running jobs
2. Clone your application repo to DBFS and as part of your CI/CD pipeline run git pull, and install the module on the cluster then execute it -- this can all be done inside the cluster init script.

Neither of these options looks particularly clean to me, so I wanted to reach out to you all for advice.

Let me know what you think.

Thanks!"
3368,2020-09-03 22:26:55,1599161215.0,dataengineering,A Tour of End-to-End Machine Learning Platforms,im09wi,geneorama,,https://www.reddit.com/r/dataengineering/comments/im09wi/a_tour_of_endtoend_machine_learning_platforms/,1.0,2.0,0.0,18236.0,
3369,2020-09-03 22:45:29,1599162329.0,dataengineering,What should I be doing as an undergrad?,im0mtf,pmp1321,,https://www.reddit.com/r/dataengineering/comments/im0mtf/what_should_i_be_doing_as_an_undergrad/,1.0,2.0,0.0,18236.0,"I am currently a sophomore majoring in Data Science, looking to learn Data Engineering skills as well. Should I just go through this resource [https://github.com/andkret/Cookbook](https://github.com/andkret/Cookbook) ?"
3370,2020-09-04 01:32:14,1599172334.0,dataengineering,How do you send a large JSON response to frontend?,im3syh,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/im3syh/how_do_you_send_a_large_json_response_to_frontend/,1.0,5.0,0.0,18244.0,"I thought this is more of a engineering problem. I'm doing some computation at the backend which results in a veey large json response, the size starts at 50 MB atleast, could go higher as well.

My usecase is such that it needs all the data for frontend rendering in real time. What are your thoughts on how can I pass this huge json to frontend without breaking HTTP requests or crashing the user's browser."
3371,2020-09-04 04:32:20,1599183140.0,dataengineering,How To Call APIs/Web Services Without Hitting Rate Limit Using Pyspark?,im6q16,rirhun,,https://www.reddit.com/r/dataengineering/comments/im6q16/how_to_call_apisweb_services_without_hitting_rate/,1.0,4.0,0.0,18248.0,"I have a Spark DataFrame with 4 columns: \`location\_string\`, \`locality\`, \`region\`, and \`country\`. I am using Google Map's Geocode API to parse each \`location\_string\` and then use the results to fill in the NULL \`locality\`, \`region\` and \`country\` fields. 

I have made the function that calls the geocoding library a udf, but the problem I'm facing is that I eventually get an 'OVERLIMIT' response status when I exceed the rate limit of Google's API policy. 

To navigate around this issue, I tried a function like below, but didn't appear to be successful. Any guidance would be much appreciated!  

def geocoder\_decompose\_location(location\_string):

if not location\_string:

return Row('nation', 'state', 'city')(None, None, None)

GOOGLE\_GEOCODE\_API\_KEYS = \[key1, key2, key3\]        

GOOGLE\_GEOCODE\_API\_KEY = random.choice(GOOGLE\_GEOCODE\_API\_KEYS)

attempts = 0

success = False

while status != True and attempts &lt; 5:

result = [geocoder.google](https://geocoder.google)(location\_string, key=GOOGLE\_GEOCODE\_API\_KEY)

attempts += 1

status = result.status

if status == 'OVER\_QUERY\_LIMIT':

time.sleep(2)

\# retry

continue

success = True

if attempts == 5:

print('Daily Limit Reached')

return Row('nation', 'state', 'city')([result.country](https://result.country), result.state, [result.city](https://result.city))"
3372,2020-09-04 12:55:17,1599213317.0,dataengineering,Build Machine Learning Web-Service with Python and Django,imd5qz,pp314159,,https://www.reddit.com/r/dataengineering/comments/imd5qz/build_machine_learning_webservice_with_python_and/,1.0,3.0,0.0,18264.0,
3373,2020-09-04 13:36:06,1599215766.0,dataengineering,Maize Production Ranking | TOP 10 Country from 1961 to 2018,imdm2h,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/imdm2h/maize_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,18265.0,
3374,2020-09-04 16:31:10,1599226270.0,dataengineering,Activation functions and their purpose in neural networks|know why ReLU is used mostly as default activation functions|We shall also know how to tackle some of the limitations of ReLu with another activation function with Leaky ReLU|You ll also see how to plot these activation functions using python,imfyow,instigator-001,,https://www.reddit.com/r/dataengineering/comments/imfyow/activation_functions_and_their_purpose_in_neural/,1.0,0.0,0.0,18269.0,
3375,2020-09-04 17:10:54,1599228654.0,dataengineering,"DataCamp is sponsoring another free week of access to anyone who wants to improve their data skills. It ends on September 9—don’t miss this opportunity to check out our courses, projects, and assessments—no credit card required!",imgmjm,DataCamp,,https://www.reddit.com/r/dataengineering/comments/imgmjm/datacamp_is_sponsoring_another_free_week_of/,1.0,0.0,0.0,18271.0,
3376,2020-09-04 17:50:16,1599231016.0,dataengineering,5 “baby” steps to develop a Flink application,imhb8b,Marksfik,,https://www.reddit.com/r/dataengineering/comments/imhb8b/5_baby_steps_to_develop_a_flink_application/,1.0,0.0,0.0,18272.0,
3377,2020-09-04 20:51:14,1599241874.0,dataengineering,Beginner Project : Kafka + Spark Structured Streaming for Real-Time Updates,imkqmb,theAviCaster,,https://www.reddit.com/r/dataengineering/comments/imkqmb/beginner_project_kafka_spark_structured_streaming/,1.0,29.0,0.0,18276.0,"I'm about to start my first data engineering project and I'd appreciate any suggestions, recommendations for other technologies or advice on shortcomings.

I'm basically trying to set up a pipeline to source, transform and persist real time cryptocurrency price updates.

* Fetch the data as JSON from an HTTP endpoint. There will be multiple API calls per second. Serialize it to Avro format, and send it to a Kafka topic via a producer.
* Use Spark Structured Streaming to get data from this Kafka topic. Run some user defined functions and aggregates to calculate metrics.
* Persist these metrics to some sort of data store. I'm not sure what to use but Cassandra sounds promising. I'd love to get exposure to cloud storage or cloud technologies though.
* (Possibly) Load from this data store to a front end dashboard and show real-time updating graphs etc. I'm open to learn React.
* Use Docker for the whole project."
3378,2020-09-04 23:58:36,1599253116.0,dataengineering,How to crack Data Product Manager interviews,imo6na,Snoo35471,,https://www.reddit.com/r/dataengineering/comments/imo6na/how_to_crack_data_product_manager_interviews/,1.0,0.0,0.0,18281.0,"I have been working as a mid-level product owner at a healthcare IT org. I lead the development of data lake, data warehouse, database and APIs to store/process/publish big data on-prem . I have been trying to understand the ideal path to hone my skill set. I have hands on experience in SQL (6+ years), SAS Enterprise Miner for predictive modeling (6 months) Tableau (1 year), AWS Redshift (1 year) and a beginner level in Python. What  are the courses/skill sets I can build upon to be a better candidate for Data Product Manager opportunities?"
3379,2020-09-05 00:41:03,1599255663.0,dataengineering,Help me convince my CTO that continuing to use a small fry developed ETL and automation tool in the foreseeable future is a bad idea...,imoyck,wtfismyjob,,https://www.reddit.com/r/dataengineering/comments/imoyck/help_me_convince_my_cto_that_continuing_to_use_a/,1.0,4.0,0.0,18282.0,"Our shop has some particular dependencies, the major of which is integrating an extremely closed source hierarchical transaction database running on AIX. The company is basically in the technology dark ages and we’re trying to hoist it out. They never sprung for traditional RDBMS systems for analytics and historical data storage like the rest of the world did 20 years ago. Instead, they’ve been limping along approximating the behaviors of main stream analytics systems with a Rube Goldberg arrangement of ancient fintech software. Think putty wrappers that navigate this system with user credentials to launch jobs on schedules and to trigger ftp downloads of old line printer report files that get parsed out and converted to xlsx to be emailed kind of thing. 

We have a scheduled as described and it’s hot garbage. The GUI is super buggy, everything is list based, single chain dependencies only, no error recovery, it’s generally just slow. If a job fails you have to dig through cryptic text logs to determine which task in a job failed, then go out and manually remedy the reason, then go in and edit the job to disable the tasks that succeeded, restart the job, wait for it to finish, disable it, enable the disabled tasks, then enable the job and hope it doesn’t fail again when it’s supposed to run again. 

Alerting for the system is just an email, but only so long as you set up an email as a dependency on failure at the end of the job. 

Adding new data sources requires a 4 hour support call with the company to have them add it. Of course, there’s only one guy there that knows this system. They bought it a decade ago or something and are just in perpetual maintenance mode until they lose their last client (probably us). 

This one AIX system does have API access to do stuff and doesn’t really require the whole putty wrapper SSH macro parse line printer file kind of interface anymore. So I’m thinking a more normal tool like Stitch or Aiflow might be a better option with greater flexibility and support, plus generally just providing transferable experience to employees and we’d be able to hire out of the standard market for people to use it. 

We’re still fighting the, “if it ain’t broke don’t fix it,” mentality that got us stuck in 1995 level tech in the first place. So the admin and some of the hold outs don’t want to even think about changing this small fry tool out for something current. I dread even logging in to it to manually check if anything failed all day and then spending half an hour per job bringing them back up. And yeah, it fails on an almost hours basis, the whole software, not just the jobs. So imagine having to spend half your day dealing with this stuff daily. Actually it takes three of us logging in in shifts to deal with it all day. But that inefficiency is so obfuscated by other problems that no one sees it as a problem except me."
3380,2020-09-05 01:53:59,1599260039.0,dataengineering,"Recommendation on courses for Pyspark, Spark streaming, end to end pipelines including examples of consuming data from kafka/kinesis, data transformations, performance tuning, prod deployment etc. Is there any course that covers pyspark on AWS.",imq6q5,WithorAgainstMM,,https://www.reddit.com/r/dataengineering/comments/imq6q5/recommendation_on_courses_for_pyspark_spark/,1.0,12.0,0.0,18282.0,
3381,2020-09-05 09:35:59,1599287759.0,dataengineering,Table Mapping between two tables,imwgs8,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/imwgs8/table_mapping_between_two_tables/,1.0,7.0,0.0,18295.0,"Hello everyone,

We are doing the cloud migration. Migrating everything from Oracle to Snowflake. However, there is the key which is an auto-increment in both Oracle and Snowflake. 

For example:-

Oracle data : Store ID is auto increment  

Store  ID	Store No	Store Zip Code

1	                  1000	               XYA

2	                  1001	               XYB

3	                   1000	               XYC

Snowflake Data	Store ID is auto increment  		

Store ID 	Store No	Store Zip Code

1	               1000	          XYC

2	                 1000	         XYA

3	              1001	                XYB

&amp;#x200B;

When you query with Store ID in Oracle and Snowflake both provide different results. How to efficiently handle the situation . We cannot use combination(store id + store zip) due to the limitation in the application. Hence we have to use a single column. How to create a column which should be same on both oracle and snowflake."
3382,2020-09-05 13:27:22,1599301642.0,dataengineering,Learning and Education yourself on Data Engineering - which platform do you use?,imywee,AdiPolak,,https://www.reddit.com/r/dataengineering/comments/imywee/learning_and_education_yourself_on_data/,1.0,3.0,0.0,18299.0,"Hi Everyone,

From a learning perspective, where do you usually go to read about Data Engineering concepts, use cases, and best practices?

[View Poll](https://www.reddit.com/poll/imywee)"
3383,2020-09-05 13:53:25,1599303205.0,dataengineering,Asthma Prevalence Ranking | TOP 10 Country from 1990 to 2017,imz6ob,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/imz6ob/asthma_prevalence_ranking_top_10_country_from/,1.0,0.0,0.0,18300.0,
3384,2020-09-05 16:27:21,1599312441.0,dataengineering,Advice setting up a rogue data team in a big company,in139b,CantGoogleMe,,https://www.reddit.com/r/dataengineering/comments/in139b/advice_setting_up_a_rogue_data_team_in_a_big/,1.0,45.0,0.0,18306.0,"My company is very dysfunctional.

We have a central data warehouse team in IT, but their world is an utter mess and they have little to no capacity to meet my department's needs.

We're standing up our own small data team to build out a reporting / analytics system focused on our objectives.

I'm hoping to get some advice on setting this up.

The central IT team's warehouse has all of their stuff in oracle databases. They move data around with informatica. This is read only for us. 

There are also numerous source systems that we need to report out of that have a mix of oracle and sql server back ends. Most of these are not integrated into the central IT data warehouse.

We have been given our own oracle instance with plenty of space (our DBA team is very supportive and helpful).

Our initial plan is to move data over database links from the IT warehouse or source system databases into our own space, and then build up our layer of business logic in sql scripts creating reporting views/tables. The transformation code will all be sql version controlled in git. 

All of this would run on a weekly refresh cycle over weekends.

We also have Qlik as a BI tool, and could just pull data from sources into QVD files, but we would prefer to keep our business logic in SQL scripts, and would like many of the tables/views we build to be query-able by the statistician team directly from oracle rather than locked up in qlik's data extracts.

Any advice on how we could set this up better?

some additional questions / thoughts

1) we're often moving big files out of the IT warehouse, and the database links can be slow. 

Our queries are basically run from our department reporting db and look like this:
    select * from table@source_db where change_timestamp &gt; last_run 
 with the results inserted into a table in our department db. 

Are there faster better ways to do this, like using an etl tool?

Someday maybe the central warehouse will be changed to where we can work in it and avoid moving data like this, but it's not happening for now. 

2) I've been talking to our information security team about possibly standing up and using a cloud data warehouse for this project. They are working on enabling use of cloud tools for our company (something we have not done yet) and thought that perhaps this project/team could serve as a guinea pig. I need to educate myself but this looks potentially appealing given some of the tools it would open up for us to use. I need to make sure the expense makes sense for us."
3385,2020-09-05 19:10:23,1599322223.0,dataengineering,Guidance needed choosing data source for Beginner Data Engineering Project,in3qxm,TheWannabeDumbledore,,https://www.reddit.com/r/dataengineering/comments/in3qxm/guidance_needed_choosing_data_source_for_beginner/,1.0,6.0,0.0,18310.0,"I am a Software Engineer (2 years experience) and currently a beginner in Data Engineering wanting to start a personal project involving a traditional Data Pipeline: **Collection --&gt; Message Queue --&gt; Stream Processing --&gt; Data Store --&gt; Visualization**

But I am having trouble selecting an appropriate data source for data collection. I want the data to be ***streaming*** in nature. At first, I thought of doing analysis on my home network data like which devices in my home network use how much of bandwidth and downloads, which domains are accessed at what times, what are the peak times for people using Facebook, how many devices are connected at a single time, etcetera. But on searching a bit, (correct me if I am wrong here) I came to know that as most of the sites use HTTPS and TLS now, I won't be able to decode the network packets and get the information I want out of them.

Can anyone let me know either how can I collect the above data from my home network or in case if it is not possible what could be other public and free streaming data sources I can use and what would be the analytics use-cases for those data sources?

P.S. As I am a beginner in this field, any advice on how I can get the most out of this project are welcome even if they are not related to the question at hand."
3386,2020-09-05 21:12:57,1599329577.0,dataengineering,Great Jupyter notebook workflow using Poetry and Pandas,in5x42,MrPowersAAHHH,,https://www.reddit.com/r/dataengineering/comments/in5x42/great_jupyter_notebook_workflow_using_poetry_and/,1.0,0.0,0.0,18310.0,
3387,2020-09-06 03:31:26,1599352286.0,dataengineering,Switching to DE from FE. Want some recs for courses/reads or insight to get ahead of work.,ince82,hellpark,,https://www.reddit.com/r/dataengineering/comments/ince82/switching_to_de_from_fe_want_some_recs_for/,1.0,3.0,0.0,18319.0,"I have been doing FE for my company for about 9 months now. Working with React/Typescript/RxJS. 

A need for DE came up so I jumped on it. However I have my experience from a bootcamp so I don’t have the traditional CS training. 

The stack is Spark with Scala. I have read Scala for the Impatient and while the language makes sense, some of the actual DE concept are hard to grasp, such as when to operate on rows and what grouping by and such means in a concrete way. It all is a bit to abstract for me currently and that’s why I try to visualize the datasets while I work on them.

Is there anything you guys recommend on what I can do now to improve, other than just learning on the job. I appreciate it."
3388,2020-09-06 05:26:59,1599359219.0,dataengineering,How does a DE use a non relational database?,ine49e,jduran9987,,https://www.reddit.com/r/dataengineering/comments/ine49e/how_does_a_de_use_a_non_relational_database/,1.0,10.0,0.0,18321.0,"I write some etl/elt pipelines at work and it always involves some relational database.  Whether I need it for a data lake or I’ve design some ER diagram and load into it.. it’s still always a relational dB.  
Also, when I read what tools other data engineers are using, I only see techs like big query, Postgres, redshift, etc.

How do DE’s exactly use things like redis, mongo, dynamo, or graph databases for their work? This seems like it would only be useful for backend developers working on an web application."
3389,2020-09-06 08:23:01,1599369781.0,dataengineering,British Columbia split into 3 areas of equal population,inghfi,WildFireca,,https://www.reddit.com/r/dataengineering/comments/inghfi/british_columbia_split_into_3_areas_of_equal/,1.0,2.0,0.0,18326.0,
3390,2020-09-06 13:25:47,1599387947.0,dataengineering,Cancer Living People Ranking | TOP 10 Country from 1990 to 2017,injst1,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/injst1/cancer_living_people_ranking_top_10_country_from/,1.0,2.0,0.0,18334.0,
3391,2020-09-06 14:08:58,1599390538.0,dataengineering,scala's role in data engineering,inka2i,just_another_dude_69,,https://www.reddit.com/r/dataengineering/comments/inka2i/scalas_role_in_data_engineering/,1.0,16.0,0.0,18334.0,"hi guys, 
How important do you think the knowledge of scala is in the role of DE? I am familiar with python both in  functional and OOPs paradigm and SQL. I started scala because I found it interesting. As a language i felt its more refined and to the point although bit difficult to understand on first try. Do you think scala would help in DE role?"
3392,2020-09-06 16:00:35,1599397235.0,dataengineering,Spark Reading from JDBC Sources,inllq0,yoquierodata,,https://www.reddit.com/r/dataengineering/comments/inllq0/spark_reading_from_jdbc_sources/,1.0,1.0,0.0,18336.0,Curious to get any first hand experience reading data from a JDBC source like Oracle or SQL Server into Spark for processing and writing to a Data Lake. Has performance been an issue? Are there better options for identifying and integrating changes into your Lake?
3393,2020-09-06 16:09:27,1599397767.0,dataengineering,Upgrading from windows task scheduler,inlq5t,Luukv93,,https://www.reddit.com/r/dataengineering/comments/inlq5t/upgrading_from_windows_task_scheduler/,1.0,7.0,0.0,18336.0,"Hello,

Some months ago I started running my Python scripts as .bat files from windows task scheduler. Now that the amount of scripts is growing rapidly as well as dependencies among them I need a way to keep track of them. 

Right now scripts are scheduled on a windows VM using windows task scheduler and I have no way to be notified when scripts didn't run, apart from checking their status on the VM.

&amp;#x200B;

1. What is a better tool to schedule 15 + python scripts containing dependencies?
2. From what OS should the tool be hosted? Is a VM the best choice?
3. Is there a way to be notified when scripts didn't run?

&amp;#x200B;

Thank you."
3394,2020-09-06 21:40:00,1599417600.0,dataengineering,"The seventh edition of data engineering weekly is out. This week's release is a new set of articles that focus on Natural language query processing, Data Ethics, MLOps, Data Orchestration comparison, and A/B testing platform from NVIDIA, Criteo, Netflix, Zendesk, Salesforce, Airbnb, and Facebook.",inrdqa,vananth22,,https://www.reddit.com/r/dataengineering/comments/inrdqa/the_seventh_edition_of_data_engineering_weekly_is/,1.0,0.0,0.0,18350.0,
3395,2020-09-07 06:48:59,1599450539.0,dataengineering,"Trying to break into the data analytics field, please advise [26/M]",io0c40,Sanchez94,,https://www.reddit.com/r/dataengineering/comments/io0c40/trying_to_break_into_the_data_analytics_field/,1.0,8.0,0.0,18362.0," **Background**

I am a 26 year old college undergrad with a bachelor’s in Psychology and I currently work in a customer service job in South Florida. I lost my interest in pursuing a master’s in neuropsych after coming across the subjects of machine learning and data science. I was enamored by the fact that the data science field took what I loved about my undergrad research (data collection, and analyzing tests and trials) and the possibility of training a computer to perform and automate tasks, as well as understand and process large amounts of data to make informed decisions.

**My Goal**

My goal is to break into this field to obtain an entry level position in a data science related field, such as analytics.

**Steps I Have Taken**

Here are the steps I’ve currently taken:

· Starting Back in 2017 I researched the field and learned about MOOCs.

· I took a handful of classes, and learned as much as I could using Udemy, YouTube etc.

· I learned some Python with no prior knowledge about programming.

· Took a class about machine learning, it was a bit too hard to understand but I took what I could from it and enjoyed it.

· Its 2018, I learn more about the field and I also I start looking at job postings and looking at the skills required for data analytics.

· I develop a handful of projects which are reports on things I find on Kaggle and built a portfolio website which can be viewed here: [http://asketez.com/](http://asketez.com/)

· After developing a couple of small projects, I feel confident and start applying to jobs while keeping track of them in a Google Sheet.

After applying to around 150 jobs, I get 0 calls back or responses. I understand I have no experience, or a related degree. But what can I do, to show employer’s that I could be a good entry level employee? I have a portfolio with some projects. Should I create more? Should I intern instead of looking for something entry level? Should I go back to school?

Please advise. Thanks for any assistance."
3396,2020-09-07 09:26:49,1599460009.0,dataengineering,How to prepare for Cloudera Certifified Associate Spark and Hadoop Developer (CCA175) exam?,io2e2f,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/io2e2f/how_to_prepare_for_cloudera_certifified_associate/,1.0,0.0,0.0,18367.0,"Can you please guide me on how to start preparing for CCA175? How many months may be needed for preparation? What are the good resources?

I have 3 years of experience working in Hadoop and I have hands-on experience with Spark using Scala."
3397,2020-09-07 09:48:18,1599461298.0,dataengineering,"How up-to-date is the Coursera ""Data Engineering with Google Cloud Professional Certificate"" course?",io2n9b,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/io2n9b/how_uptodate_is_the_coursera_data_engineering/,1.0,9.0,0.0,18367.0,"How up-to-date is the Coursera ""Data Engineering with Google Cloud Professional Certificate"" course?

[https://www.coursera.org/professional-certificates/gcp-data-engineering](https://www.coursera.org/professional-certificates/gcp-data-engineering)

Anyone taken it recently? Did it help? Would you recommend?

Thanks"
3398,2020-09-07 13:29:32,1599474572.0,dataengineering,Germany vs France | GDP from 1970 to 2018,io5395,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/io5395/germany_vs_france_gdp_from_1970_to_2018/,1.0,0.0,0.0,18376.0,
3399,2020-09-07 16:43:30,1599486210.0,dataengineering,What is Dimensionality reduction ? and why is it important for our Machine Learning Projects. Click here to know more,io7nrf,instigator-001,,https://www.reddit.com/r/dataengineering/comments/io7nrf/what_is_dimensionality_reduction_and_why_is_it/,1.0,0.0,0.0,18383.0,
3400,2020-09-07 18:01:14,1599490874.0,dataengineering,Have you ever thought about creating an online course?,io8yva,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/io8yva/have_you_ever_thought_about_creating_an_online/,1.0,2.0,0.0,18384.0,"Have you thought about creating an online course? If yes, why / why not? Which platform would you prefer to upload your course to? I guess the options are [Udemy](https://www.udemy.com/), [DatCamp](https://www.datacamp.com/) and [datastack.tv](https://datastack.tv)"
3401,2020-09-07 18:50:20,1599493820.0,dataengineering,Needing Some Guidance w/ Old School/Archaic Systems,io9w6y,Solopreneurial,,https://www.reddit.com/r/dataengineering/comments/io9w6y/needing_some_guidance_w_old_schoolarchaic_systems/,1.0,0.0,0.0,18387.0,"Hey All,

First off, this may not be the correct place for my question, so if not, please advise. And thanks in advance for any and all help.

I currently work for a relatively small company in an obscure industry. We are the Coca-Cola of our industry, but the industry is relatively small. Because of our size, we don't have a ""data team"". We have a ""team"" of analysts that consist of me and one other person. We're more operations/sales/BI/data analysts than data engineers. I'm more or less learning as I go with everything, and while I've gained a ton of knowledge on how we currently do things in our business, communities like this and others have made me realize that we likely could be doing things much more efficiently. I am hoping, even with my current constraints, that some guidance and expertise from this community can help move us along.

I currently build dashboards and sales reporting for anywhere between 8-12 small to large retailers. I build the reporting in both Excel and Power BI. The data sources are 852 EDI documents that feed into an old-school (2009 -- senior management has made it clear that we are not upgrading anytime soon, even though 2009 is no longer supported) Microsoft Dynamics Navision ERP system (with no developer licenses). When I came on board, the SOP (though not formally called that) was to use Access to create queries to merge related data (e.g. the 852 would contain a retailer, store number, date, and SKU and the other tables in Access would contain SKU details such as customer margin percentage, date details such as fiscal week, and store details such as division or segmentation). While this works, it's tedious, remarkably slow, and is prone to crashing. On top of that, I have to run my Access queries and refresh my sales reports. I want this all to be faster and automated.

I'm getting more and more familiar with Python every day (both on the data analysis side and the ""traditional"" programming side), I use SQL in Excel, Power BI, and Access (though not anything wildly complex) and have a relatively good understanding of database architecture, and I've written scripts/batch files before to automate certain processes. So I am not afraid of more technical work. I just don't have any formal training in data engineering or anyone in my company more advanced than I am, so I am unsure where to go with this.

Given that information, do you have any tips, videos, tutorials, or general information on how I can use the tools currently at my disposal to make this process faster or more seamless?"
3402,2020-09-07 20:23:47,1599499427.0,dataengineering,Modeling Relationships in MongoDB.,iobqxi,United_Special9754,,https://www.reddit.com/r/dataengineering/comments/iobqxi/modeling_relationships_in_mongodb/,1.0,0.0,0.0,18390.0,
3403,2020-09-07 21:06:38,1599501998.0,dataengineering,Hive Tutorial,ioclby,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/ioclby/hive_tutorial/,1.0,0.0,0.0,18391.0,
3404,2020-09-07 21:27:05,1599503225.0,dataengineering,Will Databricks be coming out with two new Data Engineering certifications?,iod059,DataD23,,https://www.reddit.com/r/dataengineering/comments/iod059/will_databricks_be_coming_out_with_two_new_data/,1.0,2.0,0.0,18391.0,"No idea if this has been asked yet but if anyone has some information it would be much appreciated! 

I came across this learning path in my Databricks learning account. I know that they have an Associate Apache Spark Developer certification (Which is the 3rd step in the learning path below). Does this mean that they will be coming out with two new data engineering certifications (4th and 5th step in the learning path below)? Or are these just courses that they will be producing?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/lw7h7ihvprl51.png?width=1936&amp;format=png&amp;auto=webp&amp;s=892d16a7bdc2b0b2abbae4ca07fcc62345598abf"
3405,2020-09-07 22:58:50,1599508730.0,dataengineering,How exactly is Hadoop &amp; Spark implemented,ioesw8,brandonrf94,,https://www.reddit.com/r/dataengineering/comments/ioesw8/how_exactly_is_hadoop_spark_implemented/,1.0,3.0,0.0,18395.0,"Hi,

I've been doing a lot of research on the use of technologies such as Hadoop and Spark and am trying to understand the real world implementations of these tools.

Hadoop:
How often is the map reduce function typically run? Do you typically run it on a schedule via a cron job and then use another tool to examine the output file?

I.e. if I have a dataset of customer information and I map reduce the data. I want to track who my most profitable customers are. If a company were to look up this data, would I typically recalculate the map reduce output at real time, or would I look at the most recently executed version of output. I imagine the output file is more valuable because it can then be loaded onto a rdbms to then build reports?

How does a tool like Kafka tie into this?

Spark:
I suppose spark serves a very similar process as Hadoop except it encompasses all components of an ETL as well? I ran the demo on databricks and created a cluster which did some manipulation on the data and saved an output file. I realised after two hours of not using it my cluster was removed. What is the purpose of removing my cluster, wouldn't I want to keep it so that I can process a continuous stream of incoming data?

Sorry I'm very new to data engineering and I'm attempting to get into the field as I'm extremely interested in these concepts.

If anyone has any books or courses to suggest, I'm very open to these suggestions! 

Thank you!"
3406,2020-09-07 23:50:23,1599511823.0,dataengineering,What data persona are you?,ioftfh,mkvor8,,https://www.reddit.com/r/dataengineering/comments/ioftfh/what_data_persona_are_you/,1.0,0.0,0.0,18396.0,"BuzzFeed meets data ops. :)  Where does data reliability fall for your organization? AKA who is paged at 3 a.m. to fix pipeline disasters? DEs? DAs?

[https://towardsdatascience.com/which-of-the-six-major-data-personas-are-you-8dbf434b7c9e?source=friends\_link&amp;sk=b4e6479aa4ba886324a0b4383e9b0cb7](https://towardsdatascience.com/which-of-the-six-major-data-personas-are-you-8dbf434b7c9e?source=friends_link&amp;sk=b4e6479aa4ba886324a0b4383e9b0cb7)"
3407,2020-09-08 01:07:30,1599516450.0,dataengineering,An interview about how Firebolt is building a cloud data warehouse optimized for speed and cost effectiveness to unlock your data's full potential.,ioh9tl,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/ioh9tl/an_interview_about_how_firebolt_is_building_a/,1.0,0.0,0.0,18397.0,
3408,2020-09-08 07:17:12,1599538632.0,dataengineering,Top Most Reason to do Data Science in Python,iomz5g,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/iomz5g/top_most_reason_to_do_data_science_in_python/,1.0,0.0,0.0,18404.0,
3409,2020-09-08 07:26:52,1599539212.0,dataengineering,Namespacing,ion3y4,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/ion3y4/namespacing/,1.0,2.0,0.0,18404.0,"Namespacing in not just related to one stream but across multiple stream. I believe it is very important to properly name you resources, tables, files, folders to identify them just by looking at names. I generally tend to follow some practise when naming resources on cloud or creating tables or files.

I would like to know if you also do that. Is there a convention that is followed for naming resources on cloud? Pep8 is something in python that is followed when writing and creating classes functions modules and libraries"
3410,2020-09-08 11:47:16,1599554836.0,dataengineering,An overview of Flink's Table API &amp; SQL for unified stream and batch processing,ioq48j,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ioq48j/an_overview_of_flinks_table_api_sql_for_unified/,1.0,2.0,0.0,18411.0,
3411,2020-09-08 12:33:03,1599557583.0,dataengineering,Deequ implementation,ioqkxk,NakkiGN,,https://www.reddit.com/r/dataengineering/comments/ioqkxk/deequ_implementation/,1.0,1.0,0.0,18411.0,"Hi there,

I wanted to implement amazon's deequ , data quality framework in my org. Was wondering if anyone here has implemented it and what the impact on data quality was ?[deequ](https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/)

Thanks in advance."
3412,2020-09-08 13:15:43,1599560143.0,dataengineering,Butter (Cow) Production Ranking | TOP 10 Country from 1961 to 2014,ior16k,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ior16k/butter_cow_production_ranking_top_10_country_from/,1.0,0.0,0.0,18413.0,
3413,2020-09-08 14:32:35,1599564755.0,dataengineering,An interview with Martin Traverso about how the Presto distributed SQL engine reduces complexity and increases flexibility for analytical workloads across heterogeneous data sources at scale.,iorwz9,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/iorwz9/an_interview_with_martin_traverso_about_how_the/,1.0,0.0,0.0,18415.0,
3414,2020-09-08 16:53:19,1599573199.0,dataengineering,"As we all know that a resume is an important tool for your job search because it offers a page or two where you can display your top skills and qualities. That is why I am sharing this article on Python Developer Resume, that will help you to build your resume and make it attractive",ioty9h,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/ioty9h/as_we_all_know_that_a_resume_is_an_important_tool/,1.0,0.0,0.0,18419.0,
3415,2020-09-08 17:43:13,1599576193.0,dataengineering,"Learn how to build a Data Union to crowdsell real-time data, either as a dedicated offering or integrated into an existing end-user application.",iout2h,thamilton5,,https://www.reddit.com/r/dataengineering/comments/iout2h/learn_how_to_build_a_data_union_to_crowdsell/,1.0,0.0,0.0,18423.0,
3416,2020-09-08 17:43:24,1599576204.0,dataengineering,ELT tool not requiring bin/log access?,iout6u,inlovewithabackpack,,https://www.reddit.com/r/dataengineering/comments/iout6u/elt_tool_not_requiring_binlog_access/,1.0,0.0,0.0,18423.0,"Hello my fellow data nerds. I've got a couple of offsite databases my team doesn't administer (Oracle, MySQL) where we don't have bin access to replicate data. Anyone have recommendations on good key based data replication tools, or tools using other methodologies?"
3417,2020-09-08 17:55:50,1599576950.0,dataengineering,Data catalogue solution?,iov12s,mouldy_potate_toe,,https://www.reddit.com/r/dataengineering/comments/iov12s/data_catalogue_solution/,1.0,4.0,0.0,18423.0,"Hi all, 

I’m looking for an internal data catalogue solution. Something that shows the current state of the data warehouse (available tables, columns, last load time, data quality checks). My current thinking is a custom frontend, but maybe there’s something better out there? Any thoughts?"
3418,2020-09-08 18:57:13,1599580633.0,dataengineering,"Is there some MIT, Stanford good courses for big data ? Like workshops where we can learn the application of technologies like Hadoop spark hive ..?",iow6kg,Any-Ad-3888,,https://www.reddit.com/r/dataengineering/comments/iow6kg/is_there_some_mit_stanford_good_courses_for_big/,1.0,2.0,0.0,18426.0,
3419,2020-09-08 19:30:03,1599582603.0,dataengineering,.NET ETL library - cross-platform actionETL released!,iowt3c,envobi,,https://www.reddit.com/r/dataengineering/comments/iowt3c/net_etl_library_crossplatform_actionetl_released/,1.0,0.0,0.0,18429.0,
3420,2020-09-08 21:43:48,1599590628.0,dataengineering,Advise for interview,iozf68,Big_Wet_Foot,,https://www.reddit.com/r/dataengineering/comments/iozf68/advise_for_interview/,1.0,2.0,0.0,18434.0,"Hello, 

I need some advice on what to do for a potential interview for an entry level data engineering job.

Here’s a little background-
I’m Dyslexic and ADD so I’ve always had to work harder than others except for when it comes to programming. I can read and understand code better than books (sounds more impressive then it actually is). I decided I wanted to change my career path to something software related because it comes easier to me than most things. My first step was getting accepted into a master in software engineering program. I started the program last spring. I originally planned to finish the program and then apply for a job related to software but recently I was given the opportunity to apply for an entry level data engineering job. I was recommended by a friend who told the hiring manager about my lack of experience. They still wanted me to apply. 

The main language they use is SQL. I do not have any experience with SQL or data engineering. Most of my experience is limited to creating applications with Java and C++. During my undergrad years I used MATLAB in a few of my classes. One of the classes had us dealing with large amounts of data to produce accurate analysis of real world problems.

I’m sure that I’m fully capable of leaning SQL and what is required of a Data engineer but I want to do well on the interview. Even if they decide not to give me the job based on my lack of experience I still want to leave a good impression. 

Would my below experience below be related to Data engineering?-
I created scripts in MATLAB that where similar to what you would see in a CAD analysis. I would use Young’s Module to determine stiffness at a point along a beam. Then The program would gather multiple points to create a stiffness matrix. Then the script would use that data from the stiffness matrix to determine stress and strain when different forces where applied at different locations. The script would display a GIF showing the displacement of the beam and highlight where the most stress/strain was located by changing the color of the beam at those locations. The script would also output a report that contained where the stress was applied and where the maximum stress was located and if the beam would yield.

Is data engineering just taking large amount of data and arranging it to get a useful/accurate analysis?

Is there any examples that someone could share with me?

What questions might be asked during the interview?

Any advice would be greatly appreciated!"
3421,2020-09-09 01:19:10,1599603550.0,dataengineering,Data Catalogs - AWS Glue,ip3ij4,ranj_sriv,,https://www.reddit.com/r/dataengineering/comments/ip3ij4/data_catalogs_aws_glue/,1.0,3.0,0.0,18445.0,What database does Glue use for its data catalog?
3422,2020-09-09 05:20:25,1599618025.0,dataengineering,Interviews,ip7fig,Few-Savings-6124,,https://www.reddit.com/r/dataengineering/comments/ip7fig/interviews/,1.0,7.0,0.0,18452.0,Hey guys !!!! I presently looking for new opportunities in field of data engineering. I was wondering if anyone is interested in taking the mock interviews. I can do the same thing for other person and it will be beneficial for both!!!
3423,2020-09-09 09:20:52,1599632452.0,dataengineering,The Best Ever Data Analytics Platforms For The Beginners,ipaq7d,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/ipaq7d/the_best_ever_data_analytics_platforms_for_the/,1.0,1.0,0.0,18456.0,
3424,2020-09-09 09:44:29,1599633869.0,dataengineering,Enroll today for free online certificate course of Python for Machine,ipazpg,greatlearningsandip,,https://www.reddit.com/r/dataengineering/comments/ipazpg/enroll_today_for_free_online_certificate_course/,1.0,0.0,0.0,18456.0,
3425,2020-09-09 10:37:58,1599637078.0,dataengineering,Data warehouse schema design - dimensional modeling and star schema,ipbkcl,SnirD,,https://www.reddit.com/r/dataengineering/comments/ipbkcl/data_warehouse_schema_design_dimensional_modeling/,1.0,1.0,0.0,18457.0,
3426,2020-09-09 11:32:24,1599640344.0,dataengineering,"Coding Bootcamp Market Analysis 2020 - I wrote a summary about the coding bootcamp market covering its most important dynamics and included a lot of useful links. Whether you are an industry professional or a future bootcamp participant, I think you'll find some relevant content here.",ipc549,soobrosa,,https://www.reddit.com/r/dataengineering/comments/ipc549/coding_bootcamp_market_analysis_2020_i_wrote_a/,1.0,0.0,0.0,18457.0,[https://www.reddit.com/r/codingbootcamp/comments/imbozt/coding\_bootcamp\_market\_analysis\_2020\_i\_wrote\_a/](https://www.reddit.com/r/codingbootcamp/comments/imbozt/coding_bootcamp_market_analysis_2020_i_wrote_a/)
3427,2020-09-09 13:55:32,1599648932.0,dataengineering,Merchandise Exports Ranking | TOP 10 Country from 1971 to 2014,ipdowd,beautiescollect,,https://www.reddit.com/r/dataengineering/comments/ipdowd/merchandise_exports_ranking_top_10_country_from/,1.0,0.0,0.0,18460.0,
3428,2020-09-09 16:09:26,1599656966.0,dataengineering,Easier Integrations with Cloud Services,ipfkqi,BlakeBurch,,https://www.reddit.com/r/dataengineering/comments/ipfkqi/easier_integrations_with_cloud_services/,1.0,1.0,0.0,18465.0,"I’ve spent a lot of time in the past building out mundane code to integrate data between different platforms. While it’s never difficult, it’s always a time-consuming task to learn the new packages, read through the documentation, and ultimately go through trial and error. I figured there had to be an easier way to help teams get integrations up and running quickly.

To try and combat this issue, our team at Shipyard built out over 50 CLIs into what we call “Blueprints” that we've open-sourced. Every Blueprint is designed to do exactly _one_ action (ex. download data, upload data, execute query, send message), work directly with CSVs, and have minimal dependencies. To run the Blueprints locally, all you need to do is:

1. clone the repo

2. create a virtual environment

3. install packages in requirements.txt

4. In the main folder of the repo, run `python3 script_name.py` with any command line arguments.

These commands can be executed manually or scheduled to run in succession. In our platform, we have a native integration with these blueprints that allows you to quickly use them with just a few inputs.

Here’s a list of the Blueprints for easy access:

- [Salesforce](https://github.com/shipyardapp/salesforce-blueprints)
- [Email](https://github.com/shipyardapp/email-blueprints)
- [PostgreSQL](https://github.com/shipyardapp/postgresql-blueprints)
- [Amazon Athena](https://github.com/shipyardapp/amazonathena-blueprints)
- [SQL Server](https://github.com/shipyardapp/microsoftsqlserver-blueprints)
- [MySQL](https://github.com/shipyardapp/mysql-blueprints)
- [Snowflake](https://github.com/shipyardapp/snowflake-blueprints)
- [Amazon Redshift](https://github.com/shipyardapp/amazonredshift-blueprints)
- [Bigquery](https://github.com/shipyardapp/googlebigquery-blueprints)
- [Box](https://github.com/shipyardapp/box-blueprints)
- [Dropbox](https://github.com/shipyardapp/dropbox-blueprints)
- [Google Sheets](https://github.com/shipyardapp/googlesheets-blueprints)
- [Amazon S3](https://github.com/shipyardapp/amazons3-blueprints)
- [Azure Blob Storage](https://github.com/shipyardapp/azurestorage-blueprints)
- [Google Cloud Storage](https://github.com/shipyardapp/googlecloudstorage-blueprints)
- [Google Drive](https://github.com/shipyardapp/googledrive-blueprints)
- [Slack](https://github.com/shipyardapp/slack-blueprints)
- [SFTP](https://github.com/shipyardapp/sftp-blueprints)
- [FTP](https://github.com/shipyardapp/ftp-blueprints)

Hope these are helpful to the community!"
3429,2020-09-09 18:54:07,1599666847.0,dataengineering,Critique My Data Engineering Resume,ipikpd,aquaroad1,,https://www.reddit.com/r/dataengineering/comments/ipikpd/critique_my_data_engineering_resume/,1.0,12.0,0.0,18470.0,"Trying to get my 2nd data engineering job as I feel like I'm not growing anymore in my 1st gig listed on my resume below. But I'm not sure if I should include other professional experience that are unrelated to DE field. 

The DE job listed on my resume is my first 'real' as I am 25 and graduated college in 2017. Is it bad that I only have one thing under professional experience or should I list the other random jobs that I've had? Also, what level data engineer would you 'hire' me on as based on my resume? 

Any criticism or advice appreciated.

&amp;#x200B;

https://preview.redd.it/gn2z7ndq75m51.jpg?width=732&amp;format=pjpg&amp;auto=webp&amp;s=f84d7f0293935367e2b58fed52489f84614cc1ad"
3430,2020-09-09 19:41:24,1599669684.0,dataengineering,I'm excited to share a video tour of our Serverless Spark platform that our team at Data Mechanics (YC S19) has been working on in the past few months. We'd love your feedback!,ipjjot,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/ipjjot/im_excited_to_share_a_video_tour_of_our/,1.0,2.0,0.0,18472.0,
3431,2020-09-09 23:21:45,1599682905.0,dataengineering,Amazon Career Day,ipoatj,civilsaspirant13,,https://www.reddit.com/r/dataengineering/comments/ipoatj/amazon_career_day/,1.0,5.0,0.0,18481.0,"Amazon announced virtual career day this year. 

You can register here: [https://www.amazoncareerday.com/](https://www.amazoncareerday.com/)

This is the 1st time I registered, if anybody attended previously, please share your thoughts."
3432,2020-09-10 02:05:24,1599692724.0,dataengineering,Looking for advice on my Backend/Data Engineer Resume,iprd5c,Tangodelta004,,https://www.reddit.com/r/dataengineering/comments/iprd5c/looking_for_advice_on_my_backenddata_engineer/,1.0,19.0,0.0,18483.0,
3433,2020-09-10 03:06:32,1599696392.0,dataengineering,"ETL and ELT, definition and use cases",ipse0d,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/ipse0d/etl_and_elt_definition_and_use_cases/,1.0,10.0,0.0,18483.0,"Hi all, 

Wrote a post about ETL and ELT paradigms and when to use them at [https://www.startdataengineering.com/post/elt-vs-etl/](https://www.startdataengineering.com/post/elt-vs-etl/) , hope it helps someone. Any feedback is appreciated. 

Thank you."
3434,2020-09-10 03:23:30,1599697410.0,dataengineering,Presto Community Broadcast Episode 1,ipsnsr,bitsondatadev,,https://www.reddit.com/r/dataengineering/comments/ipsnsr/presto_community_broadcast_episode_1/,1.0,0.0,0.0,18482.0,"Hey all!

Tune in to the inaugural episode of the Presto Community Broadcast on https://www.twitch.tv/prestosql at 5pm GMT that Manfred Moser and myself will be hosting.

If you can't make it, we will eventually be hosting the recordings on YouTube, Podcast, and possibly other formats as needed.

In tomorrow's show, we will be interviewing Karol Sobczak and Raunaq Morarka over the https://github.com/prestosql/presto/pull/1072 pull request which added dynamic partition pruning. As well we will be covering some foundational knowledge of presto and introducing folks who aren't as well acquainted with getting started with presto.

Here's a little teaser I put together, hope to see you there!

https://www.youtube.com/watch?v=WYeh_Z_wVdU"
3435,2020-09-10 05:58:19,1599706699.0,dataengineering,What's your use case with spark before DW. Here is mine .,ipv4w8,powok,,https://www.reddit.com/r/dataengineering/comments/ipv4w8/whats_your_use_case_with_spark_before_dw_here_is/,1.0,2.0,0.0,18484.0,"Hi guys , 
We are using azure  .

We are currently applying transformations , taxonomy classifications with spark in databricks  and also we have a ML guy who gives us a dimension table with his predictions before we load the data into the Datwarehouse .
Is this how you guys are using spark in ETL ? 
We are doing the heavy transformations and calculation in spark databricks so that there is more compute available in DW for reads. There are joins which happen in data warehouse though.

Wanted to check if anyone is following a similar process and or anyone using it for something else too."
3436,2020-09-10 09:31:16,1599719476.0,dataengineering,IT start-up from Switzerland needs beta-testers #dataplatform #apachekafka,ipy2nm,cecispoud,,https://www.reddit.com/r/dataengineering/comments/ipy2nm/it_startup_from_switzerland_needs_betatesters/,1.0,0.0,0.0,18496.0,"Hey, we are a swiss IT startup and we recently came out with the beta-version of product, [SPOUD Agoora](https://www.agoora.com), that is a data-platform for Apache Kafka. We would need some help from beta-testers to help shape the future of data transparency in Apache Kafka. Are any of you would be up for checking it out and giving us some feedback if we are on the right way? Or does anyone of you know someone that would help us? You can check out our product here: [www.agoora.com](https://www.agoora.com)

We would be glad for any kind of help Thanks and cheers from Bern :)"
3437,2020-09-10 09:54:56,1599720896.0,dataengineering,why is there no separate 'web scraping/parsing engineer' title?,ipycty,stolpodakta,,https://www.reddit.com/r/dataengineering/comments/ipycty/why_is_there_no_separate_web_scrapingparsing/,1.0,3.0,0.0,18499.0,"it seems to me web scraping/parsing is a pretty big field that needs its own job title.

**why i did not see any job titles? is it not that important?**

Web scraping is listed as just 'a skill' that any data engineer, analyst needs to have"
3438,2020-09-10 10:19:39,1599722379.0,dataengineering,Relevant podcasts?,ipynht,adgjl12,,https://www.reddit.com/r/dataengineering/comments/ipynht/relevant_podcasts/,1.0,3.0,0.0,18500.0,Are there any good podcasts for data engineering or something more specifically relevant than general software engineering/tech? I love podcasts and haven't been able to find a good one
3439,2020-09-10 11:24:01,1599726241.0,dataengineering,Key Points on the Importance of Data Science for Business,ipzd9i,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/ipzd9i/key_points_on_the_importance_of_data_science_for/,1.0,0.0,0.0,18501.0,
3440,2020-09-10 15:06:49,1599739609.0,dataengineering,Big Data Engineer Certification | DASCA,iq21gq,Luciaadams22,,https://www.reddit.com/r/dataengineering/comments/iq21gq/big_data_engineer_certification_dasca/,1.0,0.0,0.0,18509.0,
3441,2020-09-10 15:39:10,1599741550.0,dataengineering,Israel vs United Arab Emirates | GDP from 1975 to 2018,iq2i09,videoconcertt,,https://www.reddit.com/r/dataengineering/comments/iq2i09/israel_vs_united_arab_emirates_gdp_from_1975_to/,1.0,0.0,0.0,18512.0,
3442,2020-09-10 19:18:30,1599754710.0,dataengineering,What did you do to improve your company’s data infrastructure?,iq6i3f,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/iq6i3f/what_did_you_do_to_improve_your_companys_data/,1.0,73.0,0.0,18520.0,
3443,2020-09-11 01:10:55,1599775855.0,dataengineering,Looker,iqdjvb,tpedar50,,https://www.reddit.com/r/dataengineering/comments/iqdjvb/looker/,1.0,9.0,0.0,18536.0,I have a client that is interested in Looker. What has your experience been with Looker? What are the pros and cons and any gotchas to be aware of.
3444,2020-09-11 06:58:28,1599796708.0,dataengineering,How important is knowledge of algorithms in data engineering?,iqjcac,djurisic_luka,,https://www.reddit.com/r/dataengineering/comments/iqjcac/how_important_is_knowledge_of_algorithms_in_data/,1.0,24.0,0.0,18541.0,"Hi everyone! As the title says... how important is it to have deep knowledge of algorithms for data engineers? I always read how software engineers need to have deep knowledge of algorithms and data structures and how it’s the most difficult part of the interview process... so, how much should a decent data engineer know when it comes to that topic? Thanks!"
3445,2020-09-11 09:50:40,1599807040.0,dataengineering,A deep dive on Change Data Capture with Flink SQL during Flink Forward,iqljvp,Marksfik,,https://www.reddit.com/r/dataengineering/comments/iqljvp/a_deep_dive_on_change_data_capture_with_flink_sql/,1.0,0.0,0.0,18547.0,
3446,2020-09-11 10:39:44,1599809984.0,dataengineering,ODBC vs REST API integration,iqm47m,Luukv93,,https://www.reddit.com/r/dataengineering/comments/iqm47m/odbc_vs_rest_api_integration/,1.0,9.0,0.0,18548.0,"Hello,

We have a vendor of a system that doesn't support REST API integration, they offer an ODBC interface. We need data of the new system to connect to many other of our systems. 

What arguments would you use to convince the vendor that REST API integration is the new standard over ODBC interfacing?"
3447,2020-09-11 14:57:13,1599825433.0,dataengineering,Covid-19 Infected Ranking | TOP 10 Country (updated on 10 Sep 2020),iqp1sr,videosharring,,https://www.reddit.com/r/dataengineering/comments/iqp1sr/covid19_infected_ranking_top_10_country_updated/,1.0,0.0,0.0,18554.0,
3448,2020-09-11 16:13:11,1599829991.0,dataengineering,What is my best option here?,iqq79g,bichoprequica20,,https://www.reddit.com/r/dataengineering/comments/iqq79g/what_is_my_best_option_here/,1.0,2.0,0.0,18553.0,"I'm a computer systems engineer, and I want to apply for a Masters in Data Science and Advanced Analytics with a major in business analytics. The reason is to learn more about the business side of data analysis. 

I'm familiar with Machine Learning and Data Analytics.

My questions are :

1) After earning the Masters's degree, is it worth it to pursue a PhD in this field?

2) What are the careers/jobs I can follow/take after taking this master? 

3) Is it worth it to even apply for this Masters knowing that my goal is becoming a professor and working in a company or on a project?"
3449,2020-09-11 18:10:17,1599837017.0,dataengineering,How to calculate the distance between Geo-locations (latitude&amp; longitude) using pyspark?,iqsb8q,impraveenchand,,https://www.reddit.com/r/dataengineering/comments/iqsb8q/how_to_calculate_the_distance_between/,1.0,8.0,0.0,18558.0,"Hi guys, I'm working on a problem, where I have to find the distance(Kms) between the latitudes &amp; longitudes using pyspark. For example, I have table two tables as below

**Table 1:**

|Brand|latitude|longitude|
|:-|:-|:-|
|Shoppers stop|12.991929|77.571132|
|H &amp; M|12.992157|77.570128|

**Table 2:**

|Name|lat|long|
|:-|:-|:-|
|Office1|12.951861|77.640499|
|Office2|12.951760|77.641113|

So, from the above two tables, I want the output should be as the below table.

**Output :**

|Brand|latitude|longitude|Name|lat|long|distance (Kms)|
|:-|:-|:-|:-|:-|:-|:-|
|Shoppers stop|12.991929|77.571132|Office1|12.951861|77.640499|8.737|
|H &amp; M|12.992157|77.570128|Office2|12.951760|77.641113|8.907|
|Shoppers stop|12.991929|77.571132|Office2|12.951760|77.641113|8.800|
|H &amp; M|12.992157|77.570128|Office1|12.951861|77.640499|8.844|

The above example is just a sample I have billions of records like this, so anybody can please help me to solve this problem using pyspark. Thank you!!"
3450,2020-09-11 18:21:05,1599837665.0,dataengineering,🎥 Kafka Connect in 60 seconds,iqsirw,rmoff,,https://www.reddit.com/r/dataengineering/comments/iqsirw/kafka_connect_in_60_seconds/,1.0,3.0,0.0,18558.0,"I recorded a short video to explain the very basics of Kafka Connect, check it out here: [https://rmoff.dev/what-is-kafka-connect](https://rmoff.dev/what-is-kafka-connect)

&amp;#x200B;

https://preview.redd.it/a3p3egv5djm51.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=e9f76d553d8a5a4a5a64b2f919abc98ddb66b1c4"
3451,2020-09-11 20:19:31,1599844771.0,dataengineering,Undergrad Senior With Data Science Internship Experience Wanting to Switch to Data Engineering,iquwok,spoopypoptartz,,https://www.reddit.com/r/dataengineering/comments/iquwok/undergrad_senior_with_data_science_internship/,1.0,9.0,0.0,18564.0,"i’m an undergraduate senior graduating in the spring. I’ve done three different data science internships during my time at college but I want to shift to Data Engineering because I feel that the goals/success metrics are more clearly defined than in DS. Also I'm finding myself liking the programming side of things more and more and growing less fond of the statistics and math sides of things.
Does it make sense to pivot, especially if I want a full-time position right after graduation? I want to commit to DE but I see that there aren't that many junior roles normally open for this type of position and the pandemic is making things worse. (If I search on LinkedIn I can find 2021 new grad DS positions... But only a handful ""junior"" data engineer positions show up and many of them require 3-5 years of experience)"
3452,2020-09-12 00:29:36,1599859776.0,dataengineering,What is benchmarketing and why is it bad? And my Justin Bieber Parody I wrote to discuss it.,iqzq4e,bitsondatadev,,https://www.reddit.com/r/dataengineering/comments/iqzq4e/what_is_benchmarketing_and_why_is_it_bad_and_my/,1.0,5.0,0.0,18570.0,"There's something I have to get off my chest. If you really need to, just read the TLDR and listen to the Justin Bieber parody posted below. If you’re confused by the lingo, the rest of the post will fill in any gaps.

TL;DR: Benchmarketing, the practice of using benchmarks for marketing, is bad. Consumers should run their own benchmarks and ideally open-source them instead of relying on an internal and biased report.

Enjoy the Justin Bieber Parody I wrote about this silly practice.

[https://www.youtube.com/watch?v=FSy8V-R0\_Zw](https://www.youtube.com/watch?v=FSy8V-R0_Zw)  
For the longest time, I have wondered what is the point of corporations, specifically in the database sectors, running their own benchmarks. Would a company ever have any incentive to post results from a benchmark that didn't show its own system winning in at least the majority of cases? I understand that these benchmarks have become part of the furniture we come to expect to see when visiting any hot new database's website. I doubt anybody in the public domain gains much insight out of these results, to begin with, at least nothing they weren't expecting to see.

Now to be clear, I am in no way indicating that companies running their own internal benchmarks to analyze their own performance in comparison to their competitors is a bad thing. It’s when they take those results and intentionally skew the methods or data from these benchmarks for sales or marketing purposes that is the problem we’re discussing here. Vendors that take part in the practice, not only use these benchmarks to show their systems succeeding a little but rather perversely taint their methodology with settings, caching, and other performance enhancements, while leaving their competition’s settings untouched.

This should be obvious that this is NOT what benchmarking is about! If you read about the history of the [Transaction Processing Performance Council (TPC)](http://www.tpc.org/information/about/history5.asp) you come to understand that this is the very wrongdoing that the council was created to address. But like with any proxy involving measurements, the measurements are inherently pliable.  


&gt;By the spring of 1991, the TPC was clearly a success. Dozens of companies were running multiple TPC-A and TPC-B results. Not surprisingly, these companies wanted to capitalize on the TPC's cachet and leverage the investment they had made in TPC benchmarking. Several companies launched aggressive advertising and public relations campaigns based around their TPC results. In many ways, this was exactly why the TPC was created: to provide objective measures of performance. What was wrong, therefore, with companies wanting to brag about their good results? What was wrong is that there was often a large gap between the objective benchmark results and their benchmark marketing claims--this gap, over the years, has been dubbed ""benchmarketing."" So the TPC was faced with an ironic situation. It had poured an enormous amount of time and energy into creating a good benchmark and even a good benchmark review process. However, the TPC had no means to control how those results were used once they were approved. The resulting problems generated intense debates within the TPC.

This benchmarketing ultimately fails the clients that these companies are marketing to. It demonstrates not only a lack of care for addressing the users' actual pain but a lack of respect by intentionally pulling the wool over their eyes simply in an attempt to mask that their performance isn't up to par with their competitors. **This leads to consumers not being able to make informed decisions as most of our decisions are made from gut instincts and human emotion which these benchmarks aim to manipulate.**

If you’re not sure exactly how a company would pull this off, an example of might be that database A enables using a cost-based optimizer that requires precomputing statistics about different tables involved in the computation, while database B is running a query against this table without any type of stats based optimization made available to it. Database A will clearly dominate as now it can reorder joins and apply better execution plans while database B is going to go with the simplest plan and run much slower in most scenarios. The company whose product depends on database A will then hone in on the numerical outcomes of this report. Even if they're decent enough to report the methods they skewed to get these results, they bury it within their report and focus on advertising the outcome of what would otherwise be considered an absurd comparison. Companies will even go as far as to say that their competition's database wasn't straight forward to configure when they were setting up optimizations. If you're not capable of understanding how to make equivalent changes to both systems, well then I guess you don't get to run that comparison until you figure it out.  

Many think that consumers are not susceptible to such attacks and would be able to see right through this scheme, but these reports appeal to any of us when we don't have the necessity or resources to thoroughly examine all the data. Many times we have to take cues from our gut when a decision needs to be made and the time to make it is constrained by our time and other business needs. We see this type of phenomenon described in the book, *Thinking Fast and Slow* by Daniel Kahneman. To briefly summarize the model they use, there are two modes that humans use when they reason about their decisions, System 1 and System 2.

&gt;Systems 1 and 2 are both active whenever we are awake. System 1 runs automatically and System 2 is normally in comfortable low-effort mode, in which only a fraction of its capacity is engaged. System 1 continuously generates suggestions for System 2: impressions, intuitions, intentions, and feelings. If endorsed by System 2, impressions and intuitions turn into beliefs, and impulses turn into voluntary actions. When all goes smoothly, which is most of the time, System 2 adopts the suggestions of System 1 with little or no modification. You generally believe your impressions and act on your desires, and that is fine — usually.

No surprise, that’s usually the part where we get into trouble. While we like to think that we are generally thinking in the logical System 2 mode, we don't have time or energy to live in this space for long periods throughout the day and we find ourselves very reliant on System 1 for much of our decision making.

&gt;The measure of success for System 1 is the coherence of the story it manages to create. The amount and quality of the data on which the story is based are largely irrelevant. When information is scarce, which is a common occurrence, System 1 operates as a machine for jumping to conclusions.

This is why benchmarketing can be so dangerous because it is so effective at manipulating our belief in claims that simply aren't true. These decisions affect how your architecture will unfold, your time-to-value, and lost hours for your team and customers. It makes having these systems that fairly compare the performance and merits of two systems all-the-more paramount.  


So why am I talking about this now?

I have become a pretty big fanboy of a [Presto](https://prestosql.io/), a distributed query engine that runs interactive queries from many sources. I have witnessed firsthand how fast a cluster of Presto nodes are able to process through a huge amount of data at blindingly fast speeds. When you dive into how these speeds are achieved you find that this project is an incredible modern feat of solid engineering that makes interactive analysis over petabytes of data a reality. Going into all the reasons I like this project would be too tangential but it fuels the fire for why I believe this message needs to be heard.

Recently there was a ""benchmark"" that came out comparing the performance of a commercial competitor and Presto open-source and enterprise versions, touting performance improvements over Presto by an amount that would have been called out as too high in a CSI episode [&lt;insert canonical csi clip here&gt;](https://www.youtube.com/watch?v=hkDD03yeLnU). If you need to find out what I’m talking about, simply google ""presto benchmark 3000"" and you will find the benchmark along with plenty of other hype they've generated around these ""findings"". [Presto isn't the only system in the data space to come under similar types of attacks.](https://blog.yugabyte.com/yugabytedb-vs-cockroachdb-bringing-truth-to-performance-benchmark-claims-part-1/) It makes sense too, as this type of technical peacocking is common as it successfully gains attention.

Luckily, as more companies strive to become transparent and associate themselves with open-source efforts, we are starting to see a relatively new pattern of open-source efforts emerge. Typically, you're used to hearing about open-source within the context of software projects maintained by open-source communities. We are now arriving at the age of any noun being able to be used in an open-source framework. There is open-source music, open-source education, and even open-source data. So why not reach a point where open-source benchmarking through consumer collaboration is a thing. This is not just for the sake of the consumers of these technologies who simply want to have more data to inform their design choices to better serve their clients, it's also unfortunate that this affects developer communities that are putting in a lot of hard work on these projects, only to have that hard work get berated unintelligibly by the likes of some corporate status competition.

Now I'm clearly a little biased when I tell you that I think Presto is currently the best analytics engine on the market today. When I say this, you really should be skeptical too. Really, I encourage it. You should verify in some way beyond a shadow of a doubt that:

 1. Any TPC or other benchmarks are validated and no ""magic"" was used to improve their performance.

 2. Using your own use cases to make sure the system you choose is going to meet the needs of your particular use case.  


While this may seem like a lot of work, with cloud infrastructure and simplicity of deploying different systems into the cloud, it's now more possible to do this today than it ever was even 10 years ago to run a benchmark of competing systems internally and at scale. Not only can this benchmark be run by your own unbiased data engineers who have more stake to find out which system best fits the companies' needs, but you don't have to rely on generic benchmarking data to analyze this if you don't want. You can spin up these systems and let them query your system, using your use cases, and do it any way you want it.

In summary, if consumers can work together, we can work to get rid of this specific type of misinformation while providing a richer more insightful analysis that will aid both companies and consumers. As I mention in the song above, go run the test yourselves."
3453,2020-09-12 00:30:48,1599859848.0,dataengineering,Justin Bieber Parody about benchmarketing,iqzqwe,bitsondatadev,,https://www.reddit.com/r/dataengineering/comments/iqzqwe/justin_bieber_parody_about_benchmarketing/,1.0,0.0,0.0,18570.0,
3454,2020-09-12 15:04:41,1599912281.0,dataengineering,Covid-19 Death Ranking | TOP 10 Country (updated on 10 Sep 2020),irbdng,videopromottion,,https://www.reddit.com/r/dataengineering/comments/irbdng/covid19_death_ranking_top_10_country_updated_on/,1.0,0.0,0.0,18597.0,
3455,2020-09-12 17:27:32,1599920852.0,dataengineering,Tips for complete beginner to ace a DE Interview.,irdje5,neon_moonlite,,https://www.reddit.com/r/dataengineering/comments/irdje5/tips_for_complete_beginner_to_ace_a_de_interview/,1.0,7.0,0.0,18601.0,"Hi! I'm a final year undergrad from a non-CS stream but having experience in Machine learning and deep learning. (Python - Intermediate, SQL - beginner)

 I have a DE interview coming up in a few weeks, but I have literally no knowledge in this field. The company did slip-in the fact that they're not expecting much domain knowledge from fresh graduates, but how can I be prepared to do well in the interview? What resources/ knowledge can I acquire in this very little time to make a good impression in the interview?

&amp;#x200B;

Reading about it and checking some roadmaps, I understand that acquiring DE skills in this little time seems like a horrendous task. I just want to go to the interview prepared, that's all (""failing to prepare is preparing to fail"").  I understand that DE roles are not easy to score as a fresh graduate, but since I'm getting the opportunity, I want to give it my best shot.

Moreover, if some of you nice people here have been in a similar situation (being the Interviewee or the Interviewer), I'd love to know What I should expect in the Interview?"
3456,2020-09-12 18:16:39,1599923799.0,dataengineering,Airflow operators,iree0u,shettyhitesh,,https://www.reddit.com/r/dataengineering/comments/iree0u/airflow_operators/,1.0,7.0,0.0,18603.0,Which are the most important/intresting operators in Apache Airflow that you have come across and why?
3457,2020-09-12 19:24:32,1599927872.0,dataengineering,How Useful Would the Databricks Associate Developer for Apache Spark certification be?,irflld,DataD23,,https://www.reddit.com/r/dataengineering/comments/irflld/how_useful_would_the_databricks_associate/,1.0,1.0,0.0,18605.0,Just like the title says. I would like to know how useful this certification would be given that I have no experience with Spark.
3458,2020-09-12 20:31:19,1599931879.0,dataengineering,"From Data Engineering to Software Engineering or Cybersecurity, possible?",irguks,aym4ne,,https://www.reddit.com/r/dataengineering/comments/irguks/from_data_engineering_to_software_engineering_or/,2.0,13.0,0.0,18625.0,"Hello everyone, I am a 20yo male student, so my school offers two courses, either software engineering, or data engineering which is basically what software engineers study plus a lot of maths and some small other differences.  
I was passionate my whole life about IT stuff like programming, web dev. and of course hacking because you know, it's ""cool"". 

I decided to choose Data Engineering at my school because it's somehow sounds better, but I'm not sure if this is really what I want to do for the rest of my life, I actually have no idea about the field, I just know things about software engineering.

My question is, if I ever regret this choice in the future, will it be easy to switch to either software or security? or I'll be stuck with data engineering for life?"
3459,2020-09-12 23:12:49,1599941569.0,dataengineering,Need advice for preparation,irju09,ashay_t,,https://www.reddit.com/r/dataengineering/comments/irju09/need_advice_for_preparation/,2.0,2.0,0.0,18626.0,"Hi guys, 
I am currently working in a service provider software company for almost a year. I am working on Datastage ETL tool and some bigdata technologies also. I want to join some product based company as a data engineer or bigdata engineer. Can you guys give me some tips about how my roadmap should be. I'll leaving this company in the early months of 2021."
3460,2020-09-13 00:50:45,1599947445.0,dataengineering,What do you use for ingesting data into your data warehouse?,irlk8i,tedfahrvergnugent,,https://www.reddit.com/r/dataengineering/comments/irlk8i/what_do_you_use_for_ingesting_data_into_your_data/,23.0,128.0,1.0,18627.0,"Im hoping to leverage the wisdom of the crowd to help me choose a data ingestion tool. 

I’ve used Fivetran, pipelinewise, and Stitch but haven’t loved them and infosec hates the cloud storage cache they all keep. I’ve heard of Hevo but I’m not sure how it stacks up. What tools exist to fill this niche and what is everyone using? If you’ve done this all in-house what made you decide to avoid one of these tools?"
3461,2020-09-13 08:27:18,1599974838.0,dataengineering,Need help understading table strucutre for data warehouse,irshwy,sitaram_,,https://www.reddit.com/r/dataengineering/comments/irshwy/need_help_understading_table_strucutre_for_data/,1.0,4.0,0.0,18634.0,"I am building an analytic app which will, periodically , consume data from various social media ( fb, twitter, insta) and  present those data in a dashboard for  the user.  


I may be wrong so please correct me if I am.  


Since, I am collecting data from various sources and storing them in my database in a particular format(separate columns), this  is related with data warehousing.  


I have written code for fetching those data but now I want to know how  I should structure the table to store the data.

If I am getting json like this from one of the data source(facebook)  


    {
        name: ""xavier"",
        age: 15,
        usless_info: ""useless info not gona store in table""
      }

Should I just store in the table(facebook) like this  


&amp;#x200B;

|id|name|age|
|:-|:-|:-|
|1|xavier|15|

does it need any other structure?"
3462,2020-09-13 14:39:26,1599997166.0,dataengineering,Covid-19 Infected per capita Ranking | TOP 10 Country (updated on 10 Sep 2020),irwref,videoconcertt,,https://www.reddit.com/r/dataengineering/comments/irwref/covid19_infected_per_capita_ranking_top_10/,0.0,0.0,0.0,18645.0,
3463,2020-09-13 16:19:27,1600003167.0,dataengineering,Airflow Executor Purpose.,iry4s1,shettyhitesh,,https://www.reddit.com/r/dataengineering/comments/iry4s1/airflow_executor_purpose/,0.0,6.0,0.0,18645.0,"I'm not quite able grash what an executor does in the airflow architecture.

As far as I can understand it is responsible for allocation of resources and also the type of executor decides how tasks will be executed.


The usual definition says it is the mechanism that gets the tasks to run.


And I am not sure about the relation between the scheduler, executor and queue either.

If anyone can elaborate that will be great."
3464,2020-09-13 18:50:06,1600012206.0,dataengineering,Generating Weekly Funnel Charts from GA Data,is0myu,punknight,,https://www.reddit.com/r/dataengineering/comments/is0myu/generating_weekly_funnel_charts_from_ga_data/,6.0,0.0,1.0,18647.0,"Hi all, I added to Raspby Projects this week by creating an open source python project to generate funnel charts from Google Analytics Data. I thought you guys might be interested.

Here's a link to a demo video: https://youtu.be/FY2BjUY5IVg
Link to an associated blog article: https://raspbyprojects.com/?utm-source=r/dataengineering
Link to open source github project: https://github.com/punknight16/raspby-projects"
3465,2020-09-13 20:30:03,1600018203.0,dataengineering,Reflect changes made to database without executing SQL each time,is2i2f,wedeval,,https://www.reddit.com/r/dataengineering/comments/is2i2f/reflect_changes_made_to_database_without/,1.0,0.0,0.0,18650.0,
3466,2020-09-13 21:07:52,1600020472.0,dataengineering,Snowflake cost Analysis,is39id,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/is39id/snowflake_cost_analysis/,17.0,43.0,0.0,18650.0,"Very much confused on Snowflake's cost.

The total run time of Snowflake Medium warehouse for 7 days is 21000 Minutes(warehouse used min) and charged for $3160 approx

We have removed most of the jobs from the pipeline which is not needed and runtime has been reduced to 1700 Minutes for 7 days. Cost is still $2660 approx

Confused, We have removed 75% of the runtime, and conversion of the cost is only $500 savings per week. 

How can I calculate the cost in Snowflake accurately.

2. Any reference to calculate the cost or any tools available to accurately tell the values ."
3467,2020-09-14 01:17:01,1600035421.0,dataengineering,"Live demos of batch, streaming and change data capture workloads",is81y9,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/is81y9/live_demos_of_batch_streaming_and_change_data/,1.0,0.0,0.0,18655.0,
3468,2020-09-14 01:59:39,1600037979.0,dataengineering,"Apache Spark 3.0.1 is now released and ready to be deployed in production! Few more issues to be resolved by 3.0.2, more information on notable changes in the link below.",is8rux,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/is8rux/apache_spark_301_is_now_released_and_ready_to_be/,1.0,0.0,0.0,18655.0,
3469,2020-09-14 04:12:46,1600045966.0,dataengineering,How about getting hired by a staffing agency for a contract job?,isayqx,DesolateAbomination,,https://www.reddit.com/r/dataengineering/comments/isayqx/how_about_getting_hired_by_a_staffing_agency_for/,1.0,7.0,0.0,18657.0,"Has anyone tried any temporary staffing agencies, such as Rober Half and Aerotek, to be placed at a 6 months contract position within a company?

The pay is not that great. Usually less than $20/hour. I was thinking this will be great way to get your foot in the door though, especially since most data engineering jobs require years of experience."
3470,2020-09-14 12:32:24,1600075944.0,dataengineering,How to do Data Visualization in Python for Data Science,ishq87,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/ishq87/how_to_do_data_visualization_in_python_for_data/,0.0,0.0,0.0,18668.0,
3471,2020-09-14 13:01:33,1600077693.0,dataengineering,PySpark ETL from MySQL and MongoDB to Cassandra,isi1po,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/isi1po/pyspark_etl_from_mysql_and_mongodb_to_cassandra/,4.0,3.0,0.0,18667.0,
3472,2020-09-14 13:11:38,1600078298.0,dataengineering,"Interviewing data engineers working with BigQuery, Snowflake, or using Spark for analytics",isi5xk,MageGen,,https://www.reddit.com/r/dataengineering/comments/isi5xk/interviewing_data_engineers_working_with_bigquery/,8.0,3.0,0.0,18667.0,"Hello data engineers!

I'm an engineer working at [Dataform](https://dataform.co/). We're interviewing folks to get a broad understanding of analytics workloads, and how you folks use, or interact with, various technologies: BigQuery, Snowflake, or Spark (specifically using it for analytics).

If you use one of these technologies, and you'd be up for talking about it with us, please let us know [here](https://docs.google.com/forms/d/e/1FAIpQLSf2jqKSsQSRKW-0LJbNhMuP5UPb6R3zrK_SHT0AdJVy-Tkj5g/viewform). We're offering a $60 Amazon gift voucher as a thank you gift for anyone who participates in the interviews."
3473,2020-09-14 13:50:08,1600080608.0,dataengineering,Covid-19 Death Rate Ranking | TOP 10 Country (updated on 10 Sep 2020),isilwm,Weak-Side5573,,https://www.reddit.com/r/dataengineering/comments/isilwm/covid19_death_rate_ranking_top_10_country_updated/,1.0,0.0,0.0,18667.0,
3474,2020-09-14 15:52:49,1600087969.0,dataengineering,[VIDEO] - Building an Event-Driven Application with Flink,isk9hx,Marksfik,,https://www.reddit.com/r/dataengineering/comments/isk9hx/video_building_an_eventdriven_application_with/,1.0,0.0,0.0,18672.0,
3475,2020-09-14 16:35:03,1600090503.0,dataengineering,Need your advise on extracting data as granular as possible to S3 or Azure Storage,iskyeg,Luukv93,,https://www.reddit.com/r/dataengineering/comments/iskyeg/need_your_advise_on_extracting_data_as_granular/,1.0,3.0,0.0,18673.0,"Hi,

If you were the data engineer responsible for extracting On-Prem and SaaS sources, what would be your approach in order to extract data **as granular (detailed) as possible** ?

Also what is your preferred mechanism of extracting SaaS data? Would that be ODBC, REST or different?"
3476,2020-09-14 16:43:10,1600090990.0,dataengineering,Diary of a Data Engineer,isl3a6,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/isl3a6/diary_of_a_data_engineer/,1.0,10.0,0.0,18673.0,
3477,2020-09-14 17:20:22,1600093222.0,dataengineering,"ELT , CDC, DDL",islrax,Oct04,,https://www.reddit.com/r/dataengineering/comments/islrax/elt_cdc_ddl/,1.0,13.0,0.0,18674.0,"Hey guys,

What tools or processes do you use to ingest data from a SQL database with CDC enabled into a data warehouse? - We're pushing data from such a source into Snowflake and looking at a variety of ELT partners. DDL changes at the source seem, in most cases, to need a complete re-sync of said table and broken pipelines. With a source DB which is having weekly or more frequent releases with said DDL changes this seems quite painful.

Just curious what your strategy is with this really. 

Thanks in advance :)"
3478,2020-09-14 18:23:21,1600097001.0,dataengineering,New free course: Learn Scala in 2 hours,ismyje,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/ismyje/new_free_course_learn_scala_in_2_hours/,1.0,0.0,0.0,18676.0,"Hey all,

I'd like to share with you our new course on [datastack.tv](https://datastack.tv) by Daniel Ciocîrlan. It's a short 2 hour long course that aims to help you get started with Scala. It covers the following topics:

* How to set up a Scala project
* Basics like defining variables, if-else expressions, code blocks, functions and recursive functions
* Scala as an object oriented language
* Scala as a functional language
* Pattern matching
* Advanced concepts like lazy evaluation, Option &amp; Try types, asynchronous programming and implicits

Hope you will find this course useful! Let me know if you have any feedback in the comments.

[You can enroll here for free](https://datastacktv.teachable.com/p/scala-now)

We're a new learning site for the modern data stack. We produce concise screencast tutorials for data engineers. [Browse our courses here!](https://datastack.tv/courses.html)"
3479,2020-09-14 18:40:49,1600098049.0,dataengineering,Webinar | Modern Data Architecture,isnb6e,Sprinkle_Data,,https://www.reddit.com/r/dataengineering/comments/isnb6e/webinar_modern_data_architecture/,1.0,0.0,0.0,18676.0,"When we think of Data Integration, we think of ETL. ETL was designed more than 2 decades back. Now with the rise of Cloud platforms and Warehouse, there are alternative modern architectures.  


**What You'll Learn?**

* Why ETL based architecture makes data pipelines slow and brittle
* Modern approaches to data pipelines
* How to move to modern architecture in weeks rather than in months

[Click here](https://www.sprinkledata.com/modern-data-architecture.html?utm_source=Reddit_140920&amp;utm_medium=Forum%20Post) to Register."
3480,2020-09-14 20:43:24,1600105404.0,dataengineering,Solving the Apache NiFi IOPS issue,ispvcs,TomerG98,,https://www.reddit.com/r/dataengineering/comments/ispvcs/solving_the_apache_nifi_iops_issue/,1.0,0.0,0.0,18678.0,
3481,2020-09-14 21:26:21,1600107981.0,dataengineering,Open Source Python/Kubernetes/Helm package,isqrrd,Ryzacha,,https://www.reddit.com/r/dataengineering/comments/isqrrd/open_source_pythonkuberneteshelm_package/,1.0,0.0,0.0,18679.0,"Hi all, I've just released a new open source Python package for writing and deploying helm charts to Kubernetes in an object oriented fashion. It's called avionix and you can find source code [here](https://github.com/zbrookle/avionix) and docs [here](https://avionix.readthedocs.io/en/latest/)"
3482,2020-09-14 22:21:16,1600111276.0,dataengineering,"I want to move into data engineering, can you help me plan it?",isrwz8,danbcooper,,https://www.reddit.com/r/dataengineering/comments/isrwz8/i_want_to_move_into_data_engineering_can_you_help/,1.0,6.0,0.0,18682.0,"Hi,

I'm a petroleum engineer. I'm a self taught programmer and I've been working as an analyst for an oil company.

In said company, I manage all the production, sales, sensors, etc data from our oil platforms and treatment plants. We receive most of that data through hundreds of emails per day, those emails get processed by program I wrote and then the data gets loaded into a MS SQL Server DB by an Oil &amp; Gas specific database manager.

Then the data is pulled by Power BI, Excel, etc to generate reports.

Doing this project I realized that I like it a lot, and I would like to make a career change into it. But of course I don't have experience working as a programmer or data engineer for any company. In fact, I've only programmed as a hobby, until this project.

I'm thinking on researching about the most used technologies and start building small projects to create a portfolio... Do you guys think that would be enough to get a job in the future? What do you recommend?

Thanks!"
3483,2020-09-15 00:14:13,1600118053.0,dataengineering,A short guide on how to get a job as a data engineer and where to start to get into the field,isub4m,dziewczynaaa,,https://www.reddit.com/r/dataengineering/comments/isub4m/a_short_guide_on_how_to_get_a_job_as_a_data/,1.0,1.0,0.0,18681.0,"I saw the question on how to get a job as a data engineer being asked several times in this subreddit so I created a blog post about it. I hope it may be helpful to somebody who wants to get into the field and is confused about where to start and what skills are really important:
https://towardsdatascience.com/how-to-get-a-job-as-a-data-engineer-990e1cbbe192"
3484,2020-09-15 00:39:13,1600119553.0,dataengineering,Python - virtualenv in production?,isusoj,Buckweb,,https://www.reddit.com/r/dataengineering/comments/isusoj/python_virtualenv_in_production/,1.0,18.0,0.0,18682.0,"I've only ever used virtual environments for local development and side projects. I'm not sure a virtual environment should be set up for production environments and how the environment is sourced in a job scheduler.

Do you use virtual environments in production? If so, do you have any special setup advice?"
3485,2020-09-15 10:17:16,1600154236.0,dataengineering,Python and azure data factory,it3xyc,Luukv93,,https://www.reddit.com/r/dataengineering/comments/it3xyc/python_and_azure_data_factory/,1.0,13.0,0.0,18700.0,"Hello,

My company uses azure data factory for ELT and orchestration. Is it possible to schedule Python scripts with data factory? Or is it better practise to write Python scripts in Azure Databricks and orchestrate them using Azure Data Factory?"
3486,2020-09-15 11:03:16,1600156996.0,dataengineering,Google IT Support Professional Certificate - 100% Online Learning,it4hb1,awsconsultant,,https://www.reddit.com/r/dataengineering/comments/it4hb1/google_it_support_professional_certificate_100/,1.0,0.0,0.0,18701.0,
3487,2020-09-15 14:20:33,1600168833.0,dataengineering,Data Lake book recommendations,it6rzy,francamacdowell,,https://www.reddit.com/r/dataengineering/comments/it6rzy/data_lake_book_recommendations/,1.0,12.0,0.0,18705.0,"Hey!

&amp;#x200B;

I'm trying to expand my theorical knowledge on **Data Lakes** and would like to ask to you:  


Do you know good resources? It can be book, papers or specific blog about data lakes. The unique book I know is: *Architecting Data Lake, Data Management Architectures for Advanced Business Use Cases*"
3488,2020-09-15 14:36:17,1600169777.0,dataengineering,Hong Kong vs Macau | GDP per capita from 1982 to 2017,it6zrb,WeekendMelodic185,,https://www.reddit.com/r/dataengineering/comments/it6zrb/hong_kong_vs_macau_gdp_per_capita_from_1982_to/,1.0,0.0,0.0,18707.0,
3489,2020-09-15 16:05:09,1600175109.0,dataengineering,Moncc deploys your data stacks so you don’t have to,it8cwq,greyjumps,,https://www.reddit.com/r/dataengineering/comments/it8cwq/moncc_deploys_your_data_stacks_so_you_dont_have_to/,1.0,2.0,0.0,18712.0,"Hi all. We’ve just opened a private beta access to the tool we have been building - Moncc. It’s a brand new orchestration platform - and K8s/Terraform alternative.

As a data engineer myself, I’m guessing that many of you are frustrated with the chores of setting up your tooling and pipelines or going back &amp; forth with your DevOps buddies just to start crunching the numbers. We’ve created Moncc because we’ve felt that the industry standard tools for deployment aren’t exactly made to make our lives easy enough.

Check out the [video](https://youtu.be/RiXON2FmgyA) to learn how to plop a Databricks style stack into your cloud project with just couple lines of yaml. And by the way, remixing it to suit your needs is also super simple with Moncc.

We are releasing new videos every day with example use cases and features, so please subscribe and [request access](https://moncc.io) if you’d like to know more!"
3490,2020-09-15 16:34:20,1600176860.0,dataengineering,"Data Science and Business Analytics, often used interchangeably, are very different domains.Both of these are unique fields, with the biggest difference being the scope of the problems addressed.Here is an article further describing the differences between the two.",it8uxg,instigator-001,,https://www.reddit.com/r/dataengineering/comments/it8uxg/data_science_and_business_analytics_often_used/,1.0,0.0,0.0,18715.0,
3491,2020-09-15 17:04:13,1600178653.0,dataengineering,Themes from the Subsurface Data Lake Conference,it9ekt,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/it9ekt/themes_from_the_subsurface_data_lake_conference/,1.0,0.0,0.0,18716.0,
3492,2020-09-15 18:21:17,1600183277.0,dataengineering,Should I keep or remove my ‘personal projects’ section with coding projects (unrelated to data engineering; iOS app and python automation scripts) when I’m applying for mid level data engineering positions? I have 2 years of experience if that matters.,itavlj,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/itavlj/should_i_keep_or_remove_my_personal_projects/,1.0,2.0,0.0,18718.0,"I have 2 years experience at a company for data engineering straight out of college and am looking to apply for mid level positions and was wondering if I should remove my personal just for fun projects section of my resume. Not trying to boast but my resume looks good in my opinion without the personal projects section. It just contains iOS apps and python automation scripts that I host on an ec2 instance. Nothing really related to DE. if I remove it my resume becomes single page but with it, it’s 2 pages. I was thinking of removing it because I thought for a mid level DE position it may look unprofessional and too “fresh-out-of-college” vibey. Thoughts?

[View Poll](https://www.reddit.com/poll/itavlj)"
3493,2020-09-15 18:25:04,1600183504.0,dataengineering,How to build an effective data catalog for your organization,itayal,treeschema,,https://www.reddit.com/r/dataengineering/comments/itayal/how_to_build_an_effective_data_catalog_for_your/,1.0,0.0,0.0,18718.0,
3494,2020-09-15 18:40:29,1600184429.0,dataengineering,Code independence considerations when using Airflow as an ETL orchestrator,itb934,youngmysterious1,,https://www.reddit.com/r/dataengineering/comments/itb934/code_independence_considerations_when_using/,1.0,22.0,0.0,18718.0,"I am working on a PoC project using Airflow for ETL orchestration. One of the goals of this PoC is to develop a solution which is independent of any workflow engine, so that we can change the workflow engine/scheduler later if we want to, with minimal changes to the ETL code.

Currently, I perform ETL tasks using custom operators that I wrote for moving the data and it works well. However the concern is that this solution is not independent of Airflow since it relies on underlying Hooks and Operators written in the Airflow project. The workaround I see for this is to write our own Python scripts for ETL and use Python operators to orchestrate them. We have 200+ ETL jobs in production, which means our team will have to write and maintain a large code base if we take the python scripts route. Taking the custom operators approach seems like a easier and more elegant route.

Another idea our team has is to write our own independent solution for ETL and run it using a scheduler such as Jenkins. I am personally not a fan of this idea since we lose a lot of the features of Airflow and have to maintain a huge code base. Does anyone think this could be an idea worth exploring?

People who have used Airflow in production, I want to know if code independence was a concern in your teams when choosing Airflow over other solutions. How did you address it? Are there other downsides you see of using Airflow in the short/long term?"
3495,2020-09-15 19:25:32,1600187132.0,dataengineering,Webinar | Modern Data Architecture,itc5w6,[deleted],,https://www.reddit.com/r/dataengineering/comments/itc5w6/webinar_modern_data_architecture/,1.0,0.0,0.0,18718.0,
3496,2020-09-15 19:27:08,1600187228.0,dataengineering,Webinar | Modern Data Architecture,itc70m,Sprinkle_Data,,https://www.reddit.com/r/dataengineering/comments/itc70m/webinar_modern_data_architecture/,1.0,0.0,0.0,18718.0,"When we think of Data Integration, we think of ETL. ETL was designed more than 2 decades back. Now with the rise of Cloud platforms and Warehouse, there are alternative modern architectures.  


**What You'll Learn?**

* Why ETL based architecture makes data pipelines slow and brittle
* Modern approaches to data pipelines
* How to move to modern architecture in weeks rather than in months

[Click Here](https://www.sprinkledata.com/modern-data-architecture.html?utm_source=Reddit_150920&amp;utm_medium=Webinar) to Register"
3497,2020-09-15 22:36:50,1600198610.0,dataengineering,Team lead asked me what kind of big project I'd like to do for the company. Not sure what to answer...,itfxht,intfloatbikechain,,https://www.reddit.com/r/dataengineering/comments/itfxht/team_lead_asked_me_what_kind_of_big_project_id/,1.0,9.0,0.0,18724.0,"I work for a company that sells niche data.   


So far my responsibilities have been to write spark jobs that feed the data science models and spark jobs that publish the results.   


The jobs have been organized with Kubernetes + airflow. 

We don't use a lot of databases, all the data we interact with is inside of an azure data lake.   


Since our projects are pretty much in maintenance mode, they asked the team what big projects we wanted to work on next.   


Not really sure how to answer since for the last 2 years, I've just been trying to turn this bash scripts running on VM's into orchestrated pipelines in kubernetes.   


Any ideas of something I can implement? maybe help my resume?"
3498,2020-09-16 00:32:20,1600205540.0,dataengineering,turned on LinkedIn setting to alert recruiters I’m looking for new opportunities and got 4 inmails with only 2 years of DE experience at Fortune 500 company,iti6nu,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/iti6nu/turned_on_linkedin_setting_to_alert_recruiters_im/,1.0,23.0,0.0,18731.0,"Is it normal to have gotten 4 in mails by recruiters with only 2 years experience in DE right after college (it was my first real job). I literally just turned on the setting to let recruiters know I’m looking for a new DE job. The day I turned it on, I got 4 inmails. I know it’s not a lot but I just want to know do recruiters just spam inmails?"
3499,2020-09-16 00:49:44,1600206584.0,dataengineering,Python questions | Read and operate a file,itiif8,iwillgetintofaang,,https://www.reddit.com/r/dataengineering/comments/itiif8/python_questions_read_and_operate_a_file/,1.0,4.0,0.0,18731.0,"Hi All,

I am prepping for Data Engineer interviews. Currently i am solving LC easy- medium under String and Array topics. I looked up but couldn't find much info/materials on questions regarding fetching specific data from the file using Python. I see information on reading a file but only to limited extent. 

Appreciate if anyone can share resource/videos on the topic of handling files."
3500,2020-09-16 13:52:30,1600253550.0,dataengineering,Cheese Production Ranking | TOP 10 Country from 1961 to 2014,itt5uh,Adorable-Ice-9056,,https://www.reddit.com/r/dataengineering/comments/itt5uh/cheese_production_ranking_top_10_country_from/,1.0,0.0,0.0,18752.0,
3501,2020-09-16 15:13:27,1600258407.0,dataengineering,Azure Automation vs other options?,itu91u,Black_Magic100,,https://www.reddit.com/r/dataengineering/comments/itu91u/azure_automation_vs_other_options/,1.0,7.0,0.0,18752.0,"Hey guys I am a SQL DBA that has written a variety of collection ETLs in powershell and hosted them in Azure Runbooks. They use hybrid workers to connect to our on prem servers and store data in a centralized database. The biggest issue I've seen with this approach is that you can only install one hybrid worker per machine so if I have 2 scripts that each take 45 minutes, they will have to wait on one another.

I'm curious if anybody has exposure to azure Automation and whether or not I am using it correctly. Should I not be using this as a scheduling software for my ETLs? Was it designed for doing basic tasks like booting up VMs and other server admin work?

I'm not a data engineer, but I am planning on moving into our brand new DE team within our company and have started learning about some of the other technologies out their like Airflow. Also, our DE team is interested in what I am doing, but I'm not sure if this is the right use case."
3502,2020-09-16 15:49:36,1600260576.0,dataengineering,Need advice on preparing for interviews for 'Senior Big Data Engineer' role.,itusv5,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/itusv5/need_advice_on_preparing_for_interviews_for/,1.0,1.0,0.0,18753.0,"This is going to be my first interview for a Senior position. I have worked on Java, Hadoop and I have hands-on experience on Spark with Scala. Currently I am working as Data Engineer and have not appeared for interviews extensively since 2 years. 

What may be the expectations for a senior role? I heard that there will be 5-6 rounds."
3503,2020-09-16 15:57:24,1600261044.0,dataengineering,Data Engineering from the Ground Up - Part 1 - Baby's First Data Pipeline,ituxbu,petedannemann,,https://www.reddit.com/r/dataengineering/comments/ituxbu/data_engineering_from_the_ground_up_part_1_babys/,1.0,20.0,0.0,18753.0,
3504,2020-09-16 19:07:23,1600272443.0,dataengineering,Monte Carlo Raises $16M to Build the World’s First Data Reliability Platform,ityjkt,mkvor8,,https://www.reddit.com/r/dataengineering/comments/ityjkt/monte_carlo_raises_16m_to_build_the_worlds_first/,1.0,0.0,0.0,18762.0,
3505,2020-09-16 19:55:27,1600275327.0,dataengineering,Do mid level data engineering interviews still ask candidates data structures/algo questions?,itzied,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/itzied/do_mid_level_data_engineering_interviews_still/,1.0,4.0,0.0,18765.0,"The type of data engineer position that I am applying to is more of the (hadoop/hive/spark/python/big data/etl pipeline development) and less strong on the (SSIS/SQL/RDBMS) side.  


I have 2 years experience at a fortune 100 company designing and developing production big data etl data pipelines.   


My question is, for candidates like me who are trying to get their 2nd big data engineering job with 2 years enterprise level experience with a fortune 100 company what will the distribution of technical questions be sided more towards? More data structures/algos? More big data understanding quetions (hdfs/hadoop/hive/spark/etc)? Or more SQL questions?"
3506,2020-09-16 20:42:46,1600278166.0,dataengineering,Modern Data Architecture,iu0ggq,Sprinkle_Data,,https://www.reddit.com/r/dataengineering/comments/iu0ggq/modern_data_architecture/,1.0,0.0,0.0,18767.0,"When we think of Data Integration, we think of ETL. ETL was designed more than 2 decades back. Now with the rise of Cloud platforms and Warehouse, there are alternative modern architectures.

**What You'll Learn?**

* Why ETL based architecture makes data pipelines slow and brittle
* Modern approaches to data pipelines
* How to move to modern architecture in weeks rather than in months

[Click here](https://www.sprinkledata.com/modern-data-architecture.html?utm_source=Reddit_160920&amp;utm_medium=Webinar) to register."
3507,2020-09-17 10:20:40,1600327240.0,dataengineering,Am I creating a data pipeline using ETL? (See description),iuelq6,palendrome298,,https://www.reddit.com/r/dataengineering/comments/iuelq6/am_i_creating_a_data_pipeline_using_etl_see/,1.0,0.0,0.0,18804.0,"Data is from share point. I use a python library that extracts that data and brings it into a pandas dataframe. I then make some changes to it and create an unique ID. I then use a function created by a colleague that takes the pandas dataframe and inserts/updates the table that is specified in Oracle. I then set this to run either daily, weekly or monthly using windows tasks manager and a bash script. 

Am I building a data pipeline here using the ETL process ?"
3508,2020-09-17 10:49:16,1600328956.0,dataengineering,Predicting potential sales?,iueyqs,becks0815,,https://www.reddit.com/r/dataengineering/comments/iueyqs/predicting_potential_sales/,1.0,0.0,0.0,18804.0,"We got a nice topic here within the company, and I am searching for ideas how to start.

I am working in Supply Chain, and we calculate which item should be stocked in a warehouse or not. This is done by looking and demand/forecast and using the standard approach (ABC grouping, policies,...).

We are now looking into ways to find items which aren't stocked (because they generated no sales in the past), but would have been sold if we would have stocked it in a region. Our idea is to analyze the log files of the web server where all products are offered and find the ones which caught the attraction of customers, but didn't result in being sold (because they weren't physically available in the region).

Any ideas or pages/articles I could read and get some ideas how this could be achieved?"
3509,2020-09-17 14:17:40,1600341460.0,dataengineering,Europe GDP Ranking | TOP 10 Country from 1970 to 2018,iuhgj8,Intelligent-Name-404,,https://www.reddit.com/r/dataengineering/comments/iuhgj8/europe_gdp_ranking_top_10_country_from_1970_to/,1.0,0.0,0.0,18808.0,
3510,2020-09-17 18:01:36,1600354896.0,dataengineering,Suggestions for warm storage?,iul66z,HowlingForYou,,https://www.reddit.com/r/dataengineering/comments/iul66z/suggestions_for_warm_storage/,1.0,6.0,0.0,18815.0,I'm looking for what others might have leveraged for a warm storage database?   This would be serving raw IoT data and aggregated data for certain intervals/TTL.  Preferred to have a managed service that would be cloud portable and obviously keeping cost in mind.   Does anyone have suggestions or could point me to some references that would support these requirements?
3511,2020-09-17 18:16:01,1600355761.0,dataengineering,How Do I Import a Python Script In Airflow Dag file?,iulgbl,DatKalvin,,https://www.reddit.com/r/dataengineering/comments/iulgbl/how_do_i_import_a_python_script_in_airflow_dag/,1.0,21.0,0.0,18815.0,"Hi everyone,  
I've been trying to import a Python Script as a module in my airflow dag file with No success.   
Here is how my project directory look like:  


\- LogDataProject

\- Dags

\- log\_etl\_dag.py

\- log\_etl.py

\- docker-compose.yml

\- \_\_init\_\_.py  
So, I am trying to import the log\_etl.py script into log\_etl\_dag so I can use all the functions in the log\_etl.py script in my dag. All effort to get this done has proved abortive with a ""No module log\_etl found  Error when I run airflow via Docker.  


I have used: import log\_etl, from LogDataProject import log\_etl, import LogDataProject .log\_etl, also used sys.path etc. and yet nothing worked.  


I am running Airflow via docker using the docker-compose file.  


All my search on stackoverflow has not yielded any result. Can somebody show me what I'm missing and how to resolve this?  


Thanks"
3512,2020-09-17 18:54:18,1600358058.0,dataengineering,Does anyone use PyScaffold?,ium7jb,de1pher,,https://www.reddit.com/r/dataengineering/comments/ium7jb/does_anyone_use_pyscaffold/,1.0,0.0,0.0,18816.0,"Hi all,

I have recently joined a new team that is experimenting with [PyScaffold](https://github.com/pyscaffold/pyscaffold). I have to admit that my first impression hasn't been a good one, but since the project has just under 1K stars, there are must be a lot of people who would disagree with me.

To those of you who are not familiar with this project, PyScaffold is a utility that allows you to generate a template repository for a new project. We have developed an internal PyScaffold extension that allows us to introduce a few modifications to the template. Having said this, there are a lot of things that are just baked into PyScaffold and cannot be easily changed with an extension, this is because PyScaffold has its own philosophy about how a project should be structured. Just to give you an example of where I disagree with PyScaffold: I don't want to have a docs/ repository and a sphinx dependency in every one of my projects.

My problem with it is this: if I want to add or remove stuff from my template, I have to (1) hope that the change I'd like to introduce is consistent with PyScaffold and (2) update my extension code (a substantial amount of Python code), rebuild my library and run \`putup --myextension myproject\`. It'll probably take a couple of tries before I get it right too. The alternative here feels pretty simple: if you want your developers to follow a certain standard, why not just create a template repository and allow people to clone it? 

Perhaps we are just not using it right or perhaps I'm missing the point somehow. I'd love to hear what you guys think."
3513,2020-09-17 21:34:30,1600367670.0,dataengineering,Python script connecting to Teradata so slow,iupgn2,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/iupgn2/python_script_connecting_to_teradata_so_slow/,1.0,9.0,0.0,18822.0,"Hi. I work on a data team was asked for a Python script to read a file, load into a dataframe, and write to a table in Teradata. 

I wrote a script that works but it takes 3-4 minutes to write a table that's 300 rows. Now that's a tiny job for a data warehouse, and our Teradata works fine when handling massive data sets, but I find it's such a drag waiting 4 minutes for this script to run. So I don't believe it's a system issue. 

Is there a better way to load small to medium size tables into Teradata? If we did this in SQL Server it would take seconds but some other data we need to access is already there in Teradata. 

If you're interested, the main code looks like this:

&amp;#x200B;

`import pandas as pd`

`from sqlalchemy import create_engine`

`from sqlalchemy.types import Integer, String, DateTime, Float`

`conn_string = 'teradata://' + user + ':' + passw + '@' + host + '/?authentication=LDAP'`

`eng = create_engine(conn_string)`

`# insert into NDW table`

`new_df.to_sql(table_name, con = eng, if_exists = 'replace',` 

`schema='some_schema', index=False, dtype=df_datatypes)`"
3514,2020-09-17 22:32:03,1600371123.0,dataengineering,Narrator.ai: A new standard for data modeling,iuqliw,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/iuqliw/narratorai_a_new_standard_for_data_modeling/,1.0,30.0,0.0,18820.0,
3515,2020-09-17 22:34:46,1600371286.0,dataengineering,Salary questions for moving into Management,iuqnds,x1084,,https://www.reddit.com/r/dataengineering/comments/iuqnds/salary_questions_for_moving_into_management/,1.0,4.0,0.0,18820.0,"I may have an opportunity to pivot from IC -&gt; management and I'm curious about salary expectations if I decide to change gears. Assuming ""typical"" career paths that look something like this:

\&gt; DE -&gt; Sr DE -&gt; Lead/Principal DE -&gt; Sr. Principal -&gt; Architect/Fellow/ETC

\&gt; DE -&gt; Sr DE -&gt; Lead/Principal DE  -&gt; Manager -&gt; Sr. Manager -&gt; Director -&gt; VP

It will vary from company to company, but I would imagine that in general higher level ICs make about the same as managers until the Director level, after which salary ramps up, but I don't have any hard numbers to prove that. What should my salary expectations be in the management track versus IC track? Any/all data points welcomed."
3516,2020-09-18 14:03:49,1600427029.0,dataengineering,Beef Consumption Ranking | TOP 10 Country from 1961 to 2013,iv3zxu,QuirkyYesterday9992,,https://www.reddit.com/r/dataengineering/comments/iv3zxu/beef_consumption_ranking_top_10_country_from_1961/,0.0,0.0,0.0,18858.0,
3517,2020-09-18 14:35:39,1600428939.0,dataengineering,where else do you all talk shop? other dataengineering forums?,iv4dzu,five4three2,,https://www.reddit.com/r/dataengineering/comments/iv4dzu/where_else_do_you_all_talk_shop_other/,1.0,6.0,0.0,18858.0,"Hi all,

I love this forum, I'm always learning about new resources and methods.  Where else do you all go to post and read about DE?  This could include other subreddits or, even better, other forums.

Thanks!"
3518,2020-09-18 15:07:44,1600430864.0,dataengineering,PostgreSQL and Machine Learning,iv4thw,pp314159,,https://www.reddit.com/r/dataengineering/comments/iv4thw/postgresql_and_machine_learning/,1.0,0.0,0.0,18859.0,
3519,2020-09-18 16:05:13,1600434313.0,dataengineering,Community for Data Engineers - Yay or Nay?,iv5pex,Party_Farm,,https://www.reddit.com/r/dataengineering/comments/iv5pex/community_for_data_engineers_yay_or_nay/,2.0,9.0,0.0,18860.0,"[Preview](https://preview.redd.it/ab5tntq1jwn51.png?width=2066&amp;format=png&amp;auto=webp&amp;s=b101149597042ad2d43e93be81424f2336ae6379)

Hi all,

Given the ongoing pandemic, I figured that now is when engagement and community involvement is pretty crucial for people for many different reasons (job search, online networking, etc.). I started pulling together an online community platform of sorts by using a pre-built tool, and so far I seem to be really pleased with where this could possibly go. But before I dig too much into this, I wanted to sense this subreddit's interest in being a part of this community. This is not meant to be a replacement for Reddit, StackOverflow, or Slack, as I envision this community serving completely different purposes that don't really fit into any of those online platforms. 

For instance, as seen in the preview image, we'll have organization of topics into broad categories, and some topics (such as Syllabus) will serve as a ""Wiki"", whereas other topics will serve as a forum where people can have discussions.

So far, here's my proposed outline of the community:

\- **Welcome**: Topics for on-boarding into the community

\- **Knowledge Base**: Topics for current data engineerings working in the industry

\- **Getting Started**: Topics for getting into data engineering

\- **Building Projects**: Topics for sharing resources for building resume-worthy projects; also an area where you can show off your project(s)

\- **Interview Prep**: Topics for the job search, how to frame your resume for DE roles, interview checklist, negotiation tips, and a discussion forum

\- **Community Resources**: Topics for submitting requests to the community mods for adding a new category/topic and a guide for navigating the community

\- **Networking &amp; Events**: Obviously not really active during the pandemic, but perhaps this is where we can share virtual conference, meetup, and other online event details

\- **Languages, Databases, Tools**: Each one of these words is a category in itself, with topics dedicated to each. These will primarily service as discussion forums where people can ask questions about best practices, different approaches, etc.

\- **Lounge**: This is where members can create topics of their own without any need of approval from mods

Thoughts? Any categories and/or topics that should be added, modified, and/or discarded?

\------------------------------------------------------------------------

*Additional notes*: This tool (and my time) aren't free, but I'd be willing to subsidize for the first 1000 members that want to join. I'm also looking for members who want to be actively involved in this community and help provide context wherever it's missing, as I'm working a full-time job and won't have time to fill out all of the Wikis myself in the next few days 😅"
3520,2020-09-18 16:24:43,1600435483.0,dataengineering,Is this cool? Made these for my coworkers in BI. Now I have a lot.,iv60hg,PracticallyUncommon,,https://www.reddit.com/r/dataengineering/comments/iv60hg/is_this_cool_made_these_for_my_coworkers_in_bi/,18.0,43.0,0.0,18862.0,
3521,2020-09-18 17:09:58,1600438198.0,dataengineering,What would be your preferred warehouse/data lake solution?,iv6rj8,warclaw133,,https://www.reddit.com/r/dataengineering/comments/iv6rj8/what_would_be_your_preferred_warehousedata_lake/,2.0,16.0,0.0,18862.0,"My work is approaching a crossroads where we need to choose a future path for where to store the majority of our analytics data. Currently we are utilizing SQL Server Standard edition hosted on AWS, but that server is under-powered for the requests for data the business needs. We have some data in Athena as well. Our team is having to make highly specialized data marts in order to keep queries from taking forever. But this will not scale well. 

As I'm researching newer warehouse solutions, I'm very interested in products that separate compute and storage for easy scalability and efficiency. Things like Snowflake, Firebolt, and Databricks Delta Lake. Open source would be fantastic (somewhat limited budget). But I'm having trouble picking the best of the huge number of options out there.

So I'm curious what the current community thoughts are - what would you pick for a near-greenfield warehouse/data lake solution, and why? Bonus points if it can easily be hosted on AWS and queried from Tableau."
3522,2020-09-18 17:40:19,1600440019.0,dataengineering,Using the Debezium MS SQL connector with ksqlDB embedded Kafka Connect,iv7b7v,rmoff,,https://www.reddit.com/r/dataengineering/comments/iv7b7v/using_the_debezium_ms_sql_connector_with_ksqldb/,1.0,0.0,0.0,18863.0,
3523,2020-09-18 18:28:35,1600442915.0,dataengineering,Top 3 AI Engineer Certifications You Should Not Miss In 2020,iv87k7,saik2363,,https://www.reddit.com/r/dataengineering/comments/iv87k7/top_3_ai_engineer_certifications_you_should_not/,1.0,0.0,0.0,18864.0,
3524,2020-09-18 21:30:37,1600453837.0,dataengineering,Developing new products: How would you prepare?,ivbomq,abunavsa,,https://www.reddit.com/r/dataengineering/comments/ivbomq/developing_new_products_how_would_you_prepare/,1.0,0.0,0.0,18873.0,"Hi,

I'm a graduate computer scientist facing this question ""Developing new products: How would you prepare?"" Any tips? or some direction? The question requests example and I've never faced it so I'm not sure where to start.

Skills required for the role: R, Python, Excel and SQL.

Appreciate any help whatsoever to get me started!"
3525,2020-09-18 23:50:48,1600462248.0,dataengineering,Industry challenge - project recommendation request,iveb81,SnooTangerines1201,,https://www.reddit.com/r/dataengineering/comments/iveb81/industry_challenge_project_recommendation_request/,1.0,4.0,0.0,18879.0,"Hi all!

I am transitioning into Data Engineering and taking up a challenge to work on a solid data engineering project. I understand there are too many tools in the Data Engineering tech stack Spark, Airflow, Cassandra, Kafka, Flume etc.

&amp;#x200B;

I am taking up a challenge to work on an industry level data engineering project. The project should focus mainly on the data engineering side, can maybe try to touch data science aspect. A data engineer professional told me that while working on the project it should answer or have:

&amp;#x200B;

Data Challenges:

\- Source of data?

\- How to get it ready into your pipeline?

\- What is the format?

Good Engineering challenge:

\- Do engineers have good takeaways here?

\- What did you learn by doing this?

\- How does this prove your engineering skills?

&amp;#x200B;

I am looking for ideas, leads or some resource that might help me finalize a project without wasting much time ahead. Yes, although tools are not important for me now, I do hope to include 4-5 technologies from the tech stack.

&amp;#x200B;

Maybe someone can recommend some real-time or graph based data engineering projects?

&amp;#x200B;

PS: I am hoping to work on datasets that are at least 500 GB+ in size, would be great if there are multiple datasets"
3526,2020-09-19 00:00:44,1600462844.0,dataengineering,Modeling Date Dimension w/ Different Grains,ivehke,PencilBoy99,,https://www.reddit.com/r/dataengineering/comments/ivehke/modeling_date_dimension_w_different_grains/,1.0,7.0,0.0,18881.0,"We have some use cases for our DW where we have fact tables at different grains - e.g., sales by store by day (fact 1) and sales budget targets by month (fact 2). They both involve Date as a grain, but in one case the grain is day and the other the grain is period.

Assuming we can't in the near term change the grain, what's the right way to model this?

A Date and a Month dimension, which will have conformed attributes?

1 Date dimension, with nulls or flags or something when it's representing a higher value (e.g., month)

Something else?"
3527,2020-09-19 00:37:18,1600465038.0,dataengineering,"Which cloud platform would you recommend learning, to increase employability in 2020/21?",ivf5zz,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/ivf5zz/which_cloud_platform_would_you_recommend_learning/,1.0,5.0,0.0,18884.0,"

[View Poll](https://www.reddit.com/poll/ivf5zz)"
3528,2020-09-19 12:22:34,1600507354.0,dataengineering,CentOS or Red Hat RHEL or Ubuntu/Mint for Data Engineering / Analytics and Web Development,ivpctu,RogerSmithII,,https://www.reddit.com/r/dataengineering/comments/ivpctu/centos_or_red_hat_rhel_or_ubuntumint_for_data/,1.0,10.0,0.0,18910.0,"I'm trying to decide between getting either a Macbook Pro and a Thinkpad X1E or P1. If I buy a Thinkpad, I will have to use Linux because I will likely starting a data science MS next year (or year after that). I'm currently learning data engineering and my main uses are Jupyter/Python, Tableau, R, Airflow and connecting 2 external 4K monitors (essential for me). I may learn web development later on. 

I don't have experience with Linux. I got an intro to the command line on the Mac but that's about it. I've watched a lot of videos on YouTube on different distros and I narrowed my pick down to CentOS or RHEL or Ubuntu/Mint. I'm leaning on either CentOS or RHEL because I want to have stability and not upgrade every few months and because of driver support for things like external monitors. 

Of the aforementioned distros, which do you recommend? Which has the community for me to get help as I'm a newbie? And which has the best drivers (I'm guessing RHEL.)?"
3529,2020-09-19 14:17:33,1600514253.0,dataengineering,Road Traffic Death Rate Ranking | TOP 10 Country from 2000 to 2013,ivqq5d,DreamOk678,,https://www.reddit.com/r/dataengineering/comments/ivqq5d/road_traffic_death_rate_ranking_top_10_country/,1.0,0.0,0.0,18913.0,
3530,2020-09-19 17:41:14,1600526474.0,dataengineering,"The Data Janitor Letters - August 2020 (dbt, Clickhouse, GCP)",ivtqgz,soobrosa,,https://www.reddit.com/r/dataengineering/comments/ivtqgz/the_data_janitor_letters_august_2020_dbt/,1.0,0.0,0.0,18924.0,[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-august-2020](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-august-2020)
3531,2020-09-20 08:37:20,1600580240.0,dataengineering,What's the fastest way to move data from a Scala to a Python process?,iw85r3,iamiamwhoami,,https://www.reddit.com/r/dataengineering/comments/iw85r3/whats_the_fastest_way_to_move_data_from_a_scala/,1.0,13.0,0.0,18947.0,"I had a bunch of Python code that wasn't very performant, so I rewrote a big chunk of it in Scala. The parts that are still in Python now communicate with the Scala code via GRPC + Protobuf. These parts have to stay in Python because of their use of Python only ML libraries. The slow parts are now much more performant, but now I'm running into bottlenecks with serialization/deserialization when sending data from the Scala process to the Python process. I'm thinking this is because the Python proto library is pretty slow. What in your opinion is the fastest way to move data from the Scala process to the Python process? 

Right now I'm looking into Apache Arrow. Its description of 
```
Apache Arrow is a cross-language development platform for in-memory data.
```
makes me think it's promising for this purposes, but I haven't found any examples of Java/Python interoperability. Does anyone have knowledge of this?"
3532,2020-09-20 09:04:01,1600581841.0,dataengineering,Recommended DE Certification for UnderGrads,iw8ggi,V4G4X,,https://www.reddit.com/r/dataengineering/comments/iw8ggi/recommended_de_certification_for_undergrads/,1.0,1.0,0.0,18947.0,"College Dude, Currently in my final year.

I bagged a job I'm not really that interested in,

But I have been learning Data Engineering bit-by-bit for the past year.

I want to pass a certification to be able to enter the hiring game again.

Which Data Engineering Certification would you recommend to me, who has no work experience?

I have looked some up:

1. Google Cloud Platform Professional Data Engineer: I was initially interested in this exam because its recommended Coursera training is covered by my college for free. But the issue here is that they have recommend around 3+ years of industry experience, and 1+ year of system design. This I don't have. On top that the whole exam seems to be centered around using Google Cloud's services and not the pure Data Engineering ""art"".
2. Databricks Certified Associate Developer for Apache Spark 3.0: This seems to the easier option so far. From what I can tell I can already do a third of the ""portion"". The problem here is that their training costs over a a 1000$ dollars and I can't afford that. Nor have I found any good Practice Test Material for this exam (yet).
3. CCA Spark and Hadoop Developer Exam (CCA175): Okay so I found this one while writing this question. This one seems to be focused on Hadoop a little too, not that I mind. This exam costs only a hundred bucks more than the rest, I can take it. Plus there seems to be a lot of affordable exam material on Udemy. They haven't mentioned any experience so far.

Any other certifications or materials you guys can point me out to? 

Also, I haven't given any other Certification yet, so I would also be grateful for General Exam Advice for  these types."
3533,2020-09-20 14:15:44,1600600544.0,dataengineering,Africa GDP Ranking | TOP 10 Country from 1960 to 2018,iwc27p,video_1234234,,https://www.reddit.com/r/dataengineering/comments/iwc27p/africa_gdp_ranking_top_10_country_from_1960_to/,1.0,0.0,0.0,18952.0,
3534,2020-09-20 16:23:53,1600608233.0,dataengineering,An interview about the Hazelcast in memory processing grid and the Jet streaming engine that was built on top of it and the use cases that they unlock.,iwds3e,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/iwds3e/an_interview_about_the_hazelcast_in_memory/,1.0,0.0,0.0,18960.0,
3535,2020-09-20 21:06:13,1600625173.0,dataengineering,Questions about Virtual Machines vs Cloud Computing,iwiu2m,neobanana8,,https://www.reddit.com/r/dataengineering/comments/iwiu2m/questions_about_virtual_machines_vs_cloud/,1.0,0.0,0.0,18965.0,
3536,2020-09-20 22:59:03,1600631943.0,dataengineering,I have a possibly stupid question. What platform should I learn and focus on? AWS/GCP/AZURE/Snowflake?,iwkzmx,TrainquilOasis1423,,https://www.reddit.com/r/dataengineering/comments/iwkzmx/i_have_a_possibly_stupid_question_what_platform/,1.0,0.0,0.0,18973.0,
3537,2020-09-20 23:15:18,1600632918.0,dataengineering,5 Pitfalls of NoSQL Databases,iwlb71,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/iwlb71/5_pitfalls_of_nosql_databases/,1.0,6.0,0.0,18974.0,
3538,2020-09-20 23:18:31,1600633111.0,dataengineering,Getting company wise ad data Facebook,iwldf8,SnooTangerines1201,,https://www.reddit.com/r/dataengineering/comments/iwldf8/getting_company_wise_ad_data_facebook/,1.0,5.0,0.0,18974.0,"I am working on a project and am interested in advertisement data from Facebook. I understand Facebook Marketing API is the possible source to work on.

However, as much as I have explored, I don't suppose I can get advertisement and campaign information, including Click Rate, Impressions etc filtering by company.

I don't want to manually put campaign ID, so would like to know if there is a way to get advertisement data and check performance by filtering for company.

Maybe, I can also extract campaign IDs from another source?"
3539,2020-09-21 02:02:52,1600642972.0,dataengineering,"The 9th edition of the data engineering newsletter is out. This week's release is a new set of articles that focus on the COVID-19 effect on ML models, combat disinformation, Analytics @ Netflix, data quality libraries, and Apache Pinot from Microsoft, Netflix, Doordash, CapitalOne &amp; Databricks.",iwoeot,vananth22,,https://www.reddit.com/r/dataengineering/comments/iwoeot/the_9th_edition_of_the_data_engineering/,1.0,0.0,0.0,18978.0,
3540,2020-09-21 04:32:55,1600651975.0,dataengineering,The Path to Becoming a Data Engineer in 2021,iwqx91,chase2learn,,https://www.reddit.com/r/dataengineering/comments/iwqx91/the_path_to_becoming_a_data_engineer_in_2021/,1.0,0.0,0.0,18980.0,
3541,2020-09-21 05:55:21,1600656921.0,dataengineering,How to prepare for data engineering roles?,iws6ch,testingwaters000,,https://www.reddit.com/r/dataengineering/comments/iws6ch/how_to_prepare_for_data_engineering_roles/,1.0,3.0,0.0,18982.0,I am looking for some pointers to prepare for data engineering roles. What would be the format for the interviews ? Any suggestions that i should start with ? Thank you !
3542,2020-09-21 06:24:11,1600658651.0,dataengineering,How to Become a Data Scientist for a Business Analyst.,iwslpw,popcee,,https://www.reddit.com/r/dataengineering/comments/iwslpw/how_to_become_a_data_scientist_for_a_business/,1.0,0.0,0.0,18982.0,
3543,2020-09-21 08:02:08,1600664528.0,dataengineering,Any tips on asking for a raise and how to quantify the value of etl and dwh.?,iwtzea,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/iwtzea/any_tips_on_asking_for_a_raise_and_how_to/,1.0,0.0,0.0,18984.0,
3544,2020-09-21 11:24:37,1600676677.0,dataengineering,Intel’s distributed Model Inference platform presented at Flink Forward,iwwft9,Marksfik,,https://www.reddit.com/r/dataengineering/comments/iwwft9/intels_distributed_model_inference_platform/,1.0,0.0,0.0,18989.0,
3545,2020-09-21 13:57:08,1600685828.0,dataengineering,Milk Consumption Ranking | TOP 10 Country from 1961 to 2017,iwy6h1,Brief_Snow_2852,,https://www.reddit.com/r/dataengineering/comments/iwy6h1/milk_consumption_ranking_top_10_country_from_1961/,1.0,0.0,0.0,18991.0,
3546,2020-09-21 14:36:36,1600688196.0,dataengineering,Writing Airflow 1.10 Logs To S3,iwyor1,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/iwyor1/writing_airflow_110_logs_to_s3/,1.0,0.0,0.0,18991.0,
3547,2020-09-21 15:11:43,1600690303.0,dataengineering,Data Engineering Story - The Beginning | feedback,iwz6xd,abcbuntha,,https://www.reddit.com/r/dataengineering/comments/iwz6xd/data_engineering_story_the_beginning_feedback/,1.0,0.0,0.0,18993.0,
3548,2020-09-21 15:13:42,1600690422.0,dataengineering,Data Engineering Story - The Beginning | Feedback,iwz7zb,abcbuntha,,https://www.reddit.com/r/dataengineering/comments/iwz7zb/data_engineering_story_the_beginning_feedback/,1.0,0.0,0.0,18993.0,
3549,2020-09-21 15:35:07,1600691707.0,dataengineering,Data Engineering from the Ground Up - Part 2: Better Pipelines with Python and Idempotency,iwzjm8,petedannemann,,https://www.reddit.com/r/dataengineering/comments/iwzjm8/data_engineering_from_the_ground_up_part_2_better/,1.0,5.0,0.0,18996.0,
3550,2020-09-21 17:24:28,1600698268.0,dataengineering,"The Streamr p2p Network can be expected to deliver messages in roughly 150–350 milliseconds worldwide, even at a large scale. This is on par with centralized message brokers today.",ix1ebg,thamilton5,,https://www.reddit.com/r/dataengineering/comments/ix1ebg/the_streamr_p2p_network_can_be_expected_to/,1.0,0.0,0.0,18998.0,
3551,2020-09-21 18:29:00,1600702140.0,dataengineering,Azure Data Catalogue,ix2lqu,dataeng_queries,,https://www.reddit.com/r/dataengineering/comments/ix2lqu/azure_data_catalogue/,1.0,1.0,0.0,19000.0,"Hi there, I'm looking for alternatives to the azure data catalogue, both open source, and paid. 

We are currently running azure infrastructure but due to a security flaw and the limit of 1 admin per organisation, we are looking at other options.

Your help is appreciated on this matter - thanks!"
3552,2020-09-21 19:27:58,1600705678.0,dataengineering,Why use docker (for data engineering)?,ix3qmj,graciousgroob,,https://www.reddit.com/r/dataengineering/comments/ix3qmj/why_use_docker_for_data_engineering/,1.0,1.0,0.0,19000.0,I'm new to data engineering and every tutorial I see for airflow seems to use docker. I've never used docker but I understand that it is a way to create an OS image between the hardware and your application to move the application from system to system more easily. Can someone ELI5 why docker is necessary with airflow?
3553,2020-09-21 21:33:10,1600713190.0,dataengineering,Mock Database Activity,ix698x,Fazza192,,https://www.reddit.com/r/dataengineering/comments/ix698x/mock_database_activity/,1.0,0.0,0.0,19002.0,"So I've just come into a little project at work, and I'm not 100% on how to approach it so I'm hoping I can use the collective wisdom in this sub.

So as background I work at a small Saas company who wants to transition to be more 'data driven' (yeh I know - who doesn't!), and I've been building out a datalake on S3 with EMR to support ML &amp; Analytics. So far so good, been a steep learning curve but I've now come across the need to host a separate instance of the core database with synthetic activity. 

I already have the schema &amp; associated structures in an RDS instance that I've been using for some proof of concepts, and am comfortable modelling the actual activity, my question is has anyone approached an issue like this before in AWS? What sort of tools could be helpful here? The only thing I can think is to write a custom spark job, which is certainly plausible, but I can't help but feel like I'm reinventing the wheel if I went and did that?"
3554,2020-09-22 00:18:01,1600723081.0,dataengineering,DataQuest is free for everyone for a week Sep 21 - Sep 28,ix9lms,vendetta33,,https://www.reddit.com/r/dataengineering/comments/ix9lms/dataquest_is_free_for_everyone_for_a_week_sep_21/,1.0,0.0,0.0,19011.0,
3555,2020-09-22 03:49:28,1600735768.0,dataengineering,Advertisement data use cases,ixd9ru,civilsaspirant13,,https://www.reddit.com/r/dataengineering/comments/ixd9ru/advertisement_data_use_cases/,1.0,0.0,0.0,19019.0,
3556,2020-09-22 03:57:58,1600736278.0,dataengineering,Airflow email on Scrapy failure,ixdeln,digichap28,,https://www.reddit.com/r/dataengineering/comments/ixdeln/airflow_email_on_scrapy_failure/,1.0,0.0,0.0,19019.0,
3557,2020-09-22 04:23:35,1600737815.0,dataengineering,What personal projects would you guys recommend for somebody who wants to do more data engineering?,ixdtl2,Black_Magic100,,https://www.reddit.com/r/dataengineering/comments/ixdtl2/what_personal_projects_would_you_guys_recommend/,1.0,9.0,0.0,19019.0,"I will leave this open ended so that others can benefit too. Let's assume I have a website/GitHub.

Tell me some personal projects that can be done with data that is available which would impress you! Also be sure to mention all the technologies you would use to accomplish said project."
3558,2020-09-22 08:04:58,1600751098.0,dataengineering,Solution for Connecting HANA (on-prem) with GCP (cloud)?,ixh4eb,rirhun,,https://www.reddit.com/r/dataengineering/comments/ixh4eb/solution_for_connecting_hana_onprem_with_gcp_cloud/,1.0,1.0,0.0,19027.0,
3559,2020-09-22 10:08:26,1600758506.0,dataengineering,Data Engineering Courses listicles,ixim7z,AdiPolak,,https://www.reddit.com/r/dataengineering/comments/ixim7z/data_engineering_courses_listicles/,1.0,0.0,0.0,19031.0,
3560,2020-09-22 21:09:22,1600798162.0,dataengineering,Anybody have experience with Deploying Recommendation Systems on Airflow?,ixsta3,m2rik,,https://www.reddit.com/r/dataengineering/comments/ixsta3/anybody_have_experience_with_deploying/,1.0,5.0,0.0,19048.0, Any References are appreciated. I am building a content based Recommendation System.
3561,2020-09-23 02:11:14,1600816274.0,dataengineering,Share your experience please,ixyjnb,Alert_Purpose3444,,https://www.reddit.com/r/dataengineering/comments/ixyjnb/share_your_experience_please/,1.0,2.0,0.0,19057.0,"Can anyone share what projects using Big Data in office they are working on?

I wanna know how Big data technologies are used in production environment."
3562,2020-09-23 06:00:43,1600830043.0,dataengineering,Introducing Metrics Advisor - A new Cognitive Service,iy28en,AysSomething,,https://www.reddit.com/r/dataengineering/comments/iy28en/introducing_metrics_advisor_a_new_cognitive/,1.0,0.0,0.0,19061.0,
3563,2020-09-23 13:18:55,1600856335.0,dataengineering,Blogpost - Help your organization to appreciate Kafka!⁠,iy7h8r,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/iy7h8r/blogpost_help_your_organization_to_appreciate/,1.0,0.0,0.0,19068.0,"We have just released our latest blog post on how to enable an organization to leverage the full value of event-driven architectures. Our starting point was, that it's definitely not enough to just add Kafka to the existing enterprise technology mix and then wait for a miracle until everybody is realizing the real value of it. We explain our ideas here: [https://medium.com/@spoud.io/ready-steady-connect-help-your-organization-to-appreciate-kafka-8ed6cfcfd6d8](https://medium.com/@spoud.io/ready-steady-connect-help-your-organization-to-appreciate-kafka-8ed6cfcfd6d8) What do you think? Any feedback?"
3564,2020-09-23 13:30:55,1600857055.0,dataengineering,Apple Consumption Ranking | TOP 10 Country from 1961 to 2013,iy7mb2,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/iy7mb2/apple_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,19068.0,
3565,2020-09-23 16:14:39,1600866879.0,dataengineering,What is a Cost Function?,iy9v17,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/iy9v17/what_is_a_cost_function/,1.0,0.0,0.0,19069.0,
3566,2020-09-23 18:19:38,1600874378.0,dataengineering,🎁 A collection of Kafka-related talks 💝,iyc49y,rmoff,,https://www.reddit.com/r/dataengineering/comments/iyc49y/a_collection_of_kafkarelated_talks/,1.0,0.0,0.0,19070.0,
3567,2020-09-23 19:54:25,1600880065.0,dataengineering,"A Guide on Setting up, Managing &amp; Monitoring Spark on Kubernetes (&amp; some points about the future of Spark on k8s)",iydzee,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/iydzee/a_guide_on_setting_up_managing_monitoring_spark/,1.0,0.0,0.0,19073.0,
3568,2020-09-23 20:04:42,1600880682.0,dataengineering,Raspberry Pi for Personal Project,iye705,Beertarian,,https://www.reddit.com/r/dataengineering/comments/iye705/raspberry_pi_for_personal_project/,1.0,0.0,0.0,19073.0,
3569,2020-09-23 22:04:17,1600887857.0,dataengineering,Data Engineer career trajectories,iygmww,ByrdandChen101,,https://www.reddit.com/r/dataengineering/comments/iygmww/data_engineer_career_trajectories/,1.0,0.0,0.0,19080.0,
3570,2020-09-24 01:28:22,1600900102.0,dataengineering,Data Engineering Projects - End to End,iykqgs,leccysound,,https://www.reddit.com/r/dataengineering/comments/iykqgs/data_engineering_projects_end_to_end/,1.0,12.0,0.0,19086.0,"Hi there, I want to do an end-to-end data engineering project and I'm looking for some places to start. 

I've done the GCP data engineering course on coursera but tbh, it's really a  course for experienced data engineers (it's almost a sales pitch for GCP products). 

Correct me if I'm wrong, but the general premise of data engineering is you just get JSON data, or log data from your website/app and translate that with python or SQL into normalised tables (via database modelling etc.)

Anything that is an end-to-end would be much appreciated. Thanks!"
3571,2020-09-24 04:37:06,1600911426.0,dataengineering,DataQuest's Data Engineering Path Review?,iynmli,vendetta33,,https://www.reddit.com/r/dataengineering/comments/iynmli/dataquests_data_engineering_path_review/,1.0,0.0,0.0,19090.0,
3572,2020-09-24 05:25:01,1600914301.0,dataengineering,Best alternative to a merge statement for large fact and dimension tables?,iyofvg,WatchingTheGrassGrow,,https://www.reddit.com/r/dataengineering/comments/iyofvg/best_alternative_to_a_merge_statement_for_large/,1.0,0.0,0.0,19093.0,
3573,2020-09-24 06:45:50,1600919150.0,dataengineering,"Questions about a simple system with Airflow, Kafka and Vertica",iypr04,levelworm,,https://www.reddit.com/r/dataengineering/comments/iypr04/questions_about_a_simple_system_with_airflow/,1.0,5.0,0.0,19095.0,"Hi experts, here is my situation:

- I have a Kafka topic ""marketing"" which receives data 5~7 times a day;

- Each time it receives data, it takes all install data of latest 3 days, e.g. let's say it's now GMT 2020-09-03 09:00:00 and it will give me all install data of 2020-09-01, 2020-09-02 and everything available for 2020-09-03 up to 09:00:00;

- I need to consume this topic with Airflow and dump all data into Vertica (In this problem the type of database is not important, we can assume it's anything that you are comfortable with, say, PostgreSQL).

Here is my plan:

- At each GMT day, around 02:00:00, I'll take all data from that topic using Airflow;

- However once the data is in a staging table I'll only take install days of previous day (so I'm sure that it's complete) and truncate the rest;

- I'll deduplicate the data and put it into a production table. For Vertica I'll just swap the partition (partitioned by Airflow's `execution_date`).

Now here is my issue. Because I'm not familiar with Airflow, I'm wondering:

1. I want to use the `execution_date` macro. Do I need to pass the `execution_date` to the part that consumes Kafka? So for example the `execution_date` is 2020-09-24, and I pass it into the function that consumes Kafka data, and use it to manipulate the offset.

2. From my understanding Airflow is able to re-run the scripts if needed (say an error). My question is how exactly it decides which date ranges it's going to re-run? I think it has something to do with `execution_date` but I'm really confused."
3574,2020-09-24 13:55:18,1600944918.0,dataengineering,Banana Consumption Ranking | TOP 10 Country from 1961 to 2013,iyv0vf,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/iyv0vf/banana_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,19106.0,
3575,2020-09-24 15:27:50,1600950470.0,dataengineering,How you can use Dataform's run caching feature to save ELT-related data warehouse costs,iywatm,MageGen,,https://www.reddit.com/r/dataengineering/comments/iywatm/how_you_can_use_dataforms_run_caching_feature_to/,1.0,2.0,0.0,19110.0,
3576,2020-09-24 16:06:48,1600952808.0,dataengineering,Apache hudi in production,iywwwd,moardatamoar,,https://www.reddit.com/r/dataengineering/comments/iywwwd/apache_hudi_in_production/,1.0,1.0,0.0,19111.0,"Hi All, my team is evaluating using hudi in our data pipeline to manage ingestion and profit from its main features. We are a bit concerned with the maturity of the project and the status of the community around it. I wanted to know if there's people out there using it and how the experience has been so far, would you recommend it? Any main pitfalls that you have identified?"
3577,2020-09-24 17:01:32,1600956092.0,dataengineering,New Free Mini-Course: Introduction to Docker for Data Engineers,iyxsyw,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/iyxsyw/new_free_minicourse_introduction_to_docker_for/,1.0,0.0,0.0,19113.0,
3578,2020-09-24 17:15:15,1600956915.0,dataengineering,"As a Data Analyst entering Data Engineering, how do I phrase my ""tell me about yourself"" story?",iyy1k5,incognitoRed23,,https://www.reddit.com/r/dataengineering/comments/iyy1k5/as_a_data_analyst_entering_data_engineering_how/,1.0,2.0,0.0,19115.0,"I'm a Data Analyst with 2 years of work ex. And I want to showcase my passion for coding and data (analytics, machine learning) both ... And how that transitioned into me wanting to work as a Data Engineer. Something like a best of development and data world's thing. But I need help putting it together in a cohesive manner.

I would love to know what your story of Data Engineering is/has been - how &amp; have you transitioned into the field of Data Engineering?

(It's obviously not okay for me to mention that Data Engg is a step in the career path of ML Engineering!)"
3579,2020-09-24 19:25:49,1600964749.0,dataengineering,Data Engineer interview - lack of hands-on big data experience,iz0fe8,incognitoRed23,,https://www.reddit.com/r/dataengineering/comments/iz0fe8/data_engineer_interview_lack_of_handson_big_data/,1.0,6.0,0.0,19117.0,"How do I handle big data related questions - I don't have a hands on experience with Data Warehousing or with Big Data. Only the theory about the concepts, ideal infrastructure, best practices, etc..
How do I answer the interviewer and tell them that while I lack hands on experience, I am familiar with the concepts and I'm a quick learner?"
3580,2020-09-24 20:20:44,1600968044.0,dataengineering,AWS SDE II Interview Process (Online Assssment)?,iz1hg3,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/iz1hg3/aws_sde_ii_interview_process_online_assssment/,1.0,2.0,0.0,19120.0," I submitted an application for AWS SDE II position (not necessarily a data engineering position but I have a big data engineering background 2 yoe) and just heard back from an internal Amazon recruiter that they would like to move forward with my application. He emailed me an 'online assessment' in which i need to complete within 30 days.

I'm not sure if this online assessment is standard of the interview process for Amazon SDE II position or if it's a new thing during COVID times. Anyhow, it comprises of 2 sections:

1. Section 1: 2 coding challenges
2. Section 2: Leadership principles

Could anyone give me insight on the % of importance on the following categories so I can have a general sense of how to allocate my time appropriately for prepping. Preferably from someone familiar with Amazon's interview process for AWS SDE II:

1. Data Structures &amp; Run times (most important data structures?)
2. Algorithms &amp; Run times (most important algos?)
3. SQL
4. Data Modelling
5. System Design
6. Leadership Principles

Note: I have 2 YOE in big data engineering but this not necessarily a data engineering role so not sure how much focus I should put on SQL/Data Modelling. Basically want to know: if you had to rate each of the categories above with a number of importance out of 10 (1 being not important and 10 being extremely important), what would you rate each category?

Thanks"
3581,2020-09-24 21:01:49,1600970509.0,dataengineering,Best database solution for ML project,iz2aa4,clone290595,,https://www.reddit.com/r/dataengineering/comments/iz2aa4/best_database_solution_for_ml_project/,1.0,0.0,0.0,19123.0,
3582,2020-09-24 21:29:42,1600972182.0,dataengineering,What tools will be a must-have for the data engineering stack in 5 years?,iz2ucy,mkvor8,,https://www.reddit.com/r/dataengineering/comments/iz2ucy/what_tools_will_be_a_musthave_for_the_data/,1.0,32.0,0.0,19123.0,"Snowflake's IPO is validation of the increasing importance of the data warehouse/data engineering as a value add for any company serious about data. What tools will be integral to the DS/data engineering stack in five years? What technologies will be the ""cloud warehouse"" of 2025? 

This article has some thoughts on how data observability will play into this future: [https://towardsdatascience.com/data-observability-the-next-frontier-of-data-engineering-f780feb874b?source=friends\_link&amp;sk=c6f791910928c8d0d59000884a91917e](https://towardsdatascience.com/data-observability-the-next-frontier-of-data-engineering-f780feb874b?source=friends_link&amp;sk=c6f791910928c8d0d59000884a91917e)"
3583,2020-09-24 21:30:16,1600972216.0,dataengineering,OOP Python Code,iz2ure,mdghouse1986,,https://www.reddit.com/r/dataengineering/comments/iz2ure/oop_python_code/,1.0,0.0,0.0,19123.0,
3584,2020-09-24 22:15:01,1600974901.0,dataengineering,Free practice test for Azure Data Engineer DP-200,iz3pyy,spartan7610,,https://www.reddit.com/r/dataengineering/comments/iz3pyy/free_practice_test_for_azure_data_engineer_dp200/,1.0,1.0,0.0,19124.0,"Hi!

I've put together a practice test for the Azure Data Engineer (DP-200) exam, that I wanted to share.
The test is based on the skills measured from the official Microsoft DP-200 page, https://docs.microsoft.com/en-us/learn/certifications/exams/dp-200
The test is built in ClassMarker and the link is https://www.classmarker.com/online-test/start/?quiz=gej5f25af96a67ca.
Let me know if you have any feedback or questions.

Kind regards,
Tobias"
3585,2020-09-25 13:48:57,1601030937.0,dataengineering,Pineapple Consumption Ranking | TOP 10 Country from 1961 to 2013,izhas3,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/izhas3/pineapple_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,19153.0,
3586,2020-09-25 17:55:06,1601045706.0,dataengineering,Tackling Fragmentation in Serverless Data Pipelines,izkwl1,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/izkwl1/tackling_fragmentation_in_serverless_data/,1.0,0.0,0.0,19157.0,
3587,2020-09-25 21:06:46,1601057206.0,dataengineering,Please critique my resume for the Data Engineer posts,izok9v,youareafakenews,,https://www.reddit.com/r/dataengineering/comments/izok9v/please_critique_my_resume_for_the_data_engineer/,1.0,0.0,0.0,19169.0,"Hello, 

I am actively looking for a better position as a data engineer. I have relevant experience and willing to see critiques of experienced data engineers on my resume. Please have a look and let me know which area I lack or can improve upon.

Thank you!

[here](https://smallpdf.com/shared#st=91d6635f-a9c3-43ec-9f40-961df6c32a2b&amp;fn=online-critique-resume.pdf&amp;ct=1601057019219&amp;tl=share-document&amp;rf=link) is the online shared PDF document."
3588,2020-09-26 01:07:37,1601071657.0,dataengineering,Is it realistic to expect to break in with just certs and projects?,izt6gh,SteezeNYC,,https://www.reddit.com/r/dataengineering/comments/izt6gh/is_it_realistic_to_expect_to_break_in_with_just/,1.0,0.0,0.0,19173.0,
3589,2020-09-26 02:36:24,1601076984.0,dataengineering,Career path advice Software Engineering vs Data Engineering,izuq1b,yoonislimbo,,https://www.reddit.com/r/dataengineering/comments/izuq1b/career_path_advice_software_engineering_vs_data/,1.0,0.0,0.0,19176.0,
3590,2020-09-26 06:00:18,1601089218.0,dataengineering,Intro to data collection,izxx64,frettedwithfire,,https://www.reddit.com/r/dataengineering/comments/izxx64/intro_to_data_collection/,1.0,0.0,0.0,19179.0,
3591,2020-09-26 11:23:39,1601108619.0,dataengineering,Versioning ML Models &amp; Automating ML Pipelines Effectively using DVC &amp; C...,j02c5j,ashutosh1919,,https://www.reddit.com/r/dataengineering/comments/j02c5j/versioning_ml_models_automating_ml_pipelines/,1.0,2.0,0.0,19188.0,
3592,2020-09-26 13:57:07,1601117827.0,dataengineering,Grape Consumption Ranking | TOP 10 Country from 1961 to 2013,j042et,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/j042et/grape_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,19190.0,
3593,2020-09-26 16:40:47,1601127647.0,dataengineering,Big Query vs Snowflake,j067ld,Alert_Dragonfly,,https://www.reddit.com/r/dataengineering/comments/j067ld/big_query_vs_snowflake/,1.0,0.0,0.0,19197.0,
3594,2020-09-26 18:06:14,1601132774.0,dataengineering,What are some relevant Data Engineering projects I can do in Java?,j07mfn,Ubermensch001,,https://www.reddit.com/r/dataengineering/comments/j07mfn/what_are_some_relevant_data_engineering_projects/,1.0,4.0,0.0,19203.0,"Hello,

So I'm quite rusty when it comes to Java because I took a break off it for a long time to focus on other languages that I had to use (Scala, Python, Cassandra, etc). I wanted to get back to Java and even get better at it than I was before (which was pretty average tbh). I learn best when I just dive head first in a project. I wanted a project that had to do with Data Engineering so I can learn something new. I was thinking about building a simple data pipeline from scratch. Is that doable in Java, and how should I go about it? Or are there better projects I should undertake?

Thanks."
3595,2020-09-26 23:05:24,1601150724.0,dataengineering,Data engineering 101 to data engineer,j0ctdq,inquisitvestarfish,,https://www.reddit.com/r/dataengineering/comments/j0ctdq/data_engineering_101_to_data_engineer/,1.0,0.0,0.0,19215.0,
3596,2020-09-27 13:54:09,1601204049.0,dataengineering,Malaysia vs Philippines | GDP from 1961 to 2018,j0p6vj,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/j0p6vj/malaysia_vs_philippines_gdp_from_1961_to_2018/,1.0,0.0,0.0,19232.0,
3597,2020-09-27 17:59:21,1601218761.0,dataengineering,What are the most popular DBMSs' for data engineering?,j0shr5,Black_Magic100,,https://www.reddit.com/r/dataengineering/comments/j0shr5/what_are_the_most_popular_dbmss_for_data/,1.0,33.0,0.0,19243.0,"I'm trying to choose a DBMS for my personal portfolio website and wondering if I should stick with what I am comfortable with (SQL Server and or Azure SQL DB for $5/month) or choose another DBMS like snowflake. Has anybody actually scraped linkedin for example to see what DBMS appears most often in job listings? If not, I may try to attempt something with Python so I can figure it out."
3598,2020-09-28 00:11:46,1601241106.0,dataengineering,why are there so many similar apache analytics platforms?,j0yz4o,stolpodakta,,https://www.reddit.com/r/dataengineering/comments/j0yz4o/why_are_there_so_many_similar_apache_analytics/,1.0,0.0,0.0,19256.0,
3599,2020-09-28 02:09:58,1601248198.0,dataengineering,How helpful/ necessary are these classes for Data Engineering?,j10whl,pmp1321,,https://www.reddit.com/r/dataengineering/comments/j10whl/how_helpful_necessary_are_these_classes_for_data/,1.0,0.0,0.0,19257.0,
3600,2020-09-28 02:20:46,1601248846.0,dataengineering,Linux Systems admin transition to Data Engineering,j112ho,jgengr,,https://www.reddit.com/r/dataengineering/comments/j112ho/linux_systems_admin_transition_to_data_engineering/,1.0,6.0,0.0,19257.0,"Hey Guys,  I have more than a decade of linux, windows, network administration.   I worked at a university where they hosted everything in-house, KVM based VMs, so I have only a little experience with AWS or any cloud providers. 

I would like to get into data engineering and eventually data science/ml.  What type of personal projects can I do to get employers' attention?  What can I do to round out my skills to get employed as a data engineer?  

Thanks."
3601,2020-09-28 04:39:56,1601257196.0,dataengineering,How to build an ETL to load a file to GCP Big Query,j135pu,iwillgetintofaang,,https://www.reddit.com/r/dataengineering/comments/j135pu/how_to_build_an_etl_to_load_a_file_to_gcp_big/,1.0,0.0,0.0,19265.0,
3602,2020-09-28 06:15:07,1601262907.0,dataengineering,Informatica Data Catalog,j14jel,axorrb,,https://www.reddit.com/r/dataengineering/comments/j14jel/informatica_data_catalog/,1.0,0.0,0.0,19265.0,
3603,2020-09-28 08:33:35,1601271215.0,dataengineering,Starting Your Career in Machine Learning: What Are Your Options?,j16d41,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j16d41/starting_your_career_in_machine_learning_what_are/,1.0,0.0,0.0,19268.0,
3604,2020-09-28 13:48:28,1601290108.0,dataengineering,Orange Consumption Ranking | TOP 10 Country from 1961 to 2013,j19s3y,United_Item8539,,https://www.reddit.com/r/dataengineering/comments/j19s3y/orange_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,19282.0,
3605,2020-09-28 14:35:58,1601292958.0,dataengineering,Data Engineering from the Ground Up - Part 3: Moving Data from Databases to Data Warehouses,j1acv1,petedannemann,,https://www.reddit.com/r/dataengineering/comments/j1acv1/data_engineering_from_the_ground_up_part_3_moving/,1.0,17.0,0.0,19285.0,
3606,2020-09-28 17:19:58,1601302798.0,dataengineering,Why data quality is key to successful ML Ops,j1cuvt,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/j1cuvt/why_data_quality_is_key_to_successful_ml_ops/,1.0,0.0,0.0,19291.0,
3607,2020-09-28 19:49:27,1601311767.0,dataengineering,Wrote an open source data discovery tool,j1fofv,atticus45,,https://www.reddit.com/r/dataengineering/comments/j1fofv/wrote_an_open_source_data_discovery_tool/,1.0,0.0,0.0,19301.0,
3608,2020-09-28 20:03:36,1601312616.0,dataengineering,Webinar | How to Select Right Data Warehouse,j1fyuo,Sprinkle_Data,,https://www.reddit.com/r/dataengineering/comments/j1fyuo/webinar_how_to_select_right_data_warehouse/,1.0,0.0,0.0,19301.0,
3609,2020-09-28 20:21:56,1601313716.0,dataengineering,End-to-End data project - Need your feedback (scraping/Kafka Connect/API),j1gc4q,dklis,,https://www.reddit.com/r/dataengineering/comments/j1gc4q/endtoend_data_project_need_your_feedback/,1.0,5.0,0.0,19302.0,"After reading an inspiring article `""How we launched a data product in 60 days with AWS""`,  I decided to create infrastructure for a similar pipeline using only open-source projects.

Today, I finished my first end-to-end data project.

***Any feedback is more than welcome!***

https://preview.redd.it/sy7ty9y2axp51.png?width=1135&amp;format=png&amp;auto=webp&amp;s=6ffeb4b71c2f4d114a8529b9344c4200ec86a069

**Link:** 

[https://github.com/damklis/DataEngineeringProject](https://github.com/damklis/DataEngineeringProject)

**Questions:**

* Is this an e2e pipeline? If not, what I am missing?
* Which part would you handle differently?
* What should be the next step? Kubernetes, Logging?
* What other tools can I use to level-up my DE skills?

**Collaboration:**

Currently, I am thinking about another project, this time using Cloud technologies (GCP free tier?).

If you are interested in collaboration, feel free to dm me or on [https://github.com/damklis](https://github.com/damklis)"
3610,2020-09-28 22:20:30,1601320830.0,dataengineering,Can you people have a look at my resume for a mature Data Engineer position?,j1irty,youareafakenews,,https://www.reddit.com/r/dataengineering/comments/j1irty/can_you_people_have_a_look_at_my_resume_for_a/,1.0,5.0,0.0,19307.0,"Hello,

I have some experience as a data engineer; been working as a Data Engineer for around 4 years with my current company. I am looking to switch to a better position in a better firm and would you like to see the resume and let me know where I should add or remove things. What is clear and what is ambiguous.

PDF is [here](https://docdro.id/pTUXmBK).

Thank you."
3611,2020-09-28 22:29:00,1601321340.0,dataengineering,Online Case Studies for MMO's/Online Videogames.,j1iy5q,CactusOnFire,,https://www.reddit.com/r/dataengineering/comments/j1iy5q/online_case_studies_for_mmosonline_videogames/,1.0,0.0,0.0,19308.0,
3612,2020-09-28 22:44:13,1601322253.0,dataengineering,"Trying to go from data analyst to data scientist, I could use some advice.",j1j9l3,someguy_000,,https://www.reddit.com/r/dataengineering/comments/j1j9l3/trying_to_go_from_data_analyst_to_data_scientist/,1.0,3.0,0.0,19308.0,"Hi, I hope this is the correct sub to post in.

Anyway, I've been a data analyst for a couple years now and have always dabbled with data science/engineering. I feel pretty solid with my python knowledge and I'm especially happy with how I've been able to pickup Pandas, Numpy, Scikit-learn, Matplotlib, Plotly, etc.. 

Recently, I've taken it upon myself to move my work from IDEs and Jupyter notebooks, into a production-ish environment. I know this is the next step for me if I want to become a proper data scientist. The way I went about this, was to create dashboards from scratch. I am using a web app framework called ""Streamlit"". I can't believe how easy its been to go from notebook to a full on web app dashboard with all my visualizations, insights, tables, and business rules laid out for the end user. It has me really excited!

My question to this sub is, what is the best way to truly push my work into production?  I need to host these dashboards somehow. I've seen people suggest AWS Lambda, so I'm going to look into that. Beyond hosting, I also need a way to password protect my web app. Since it has company data, this is a must before anyone sees my work.

Before anyone tells me not to do this and its not worth it... I've built a lot of trust and respect at the company and have the freedom to do stuff like this. I want (actually NEED) to prove myself here, that I'm not an ""analyst"" anymore, but actually a full on python developer who can take data projects end to end on my own. Thanks to this community, I browse this sub all the time and love it here."
3613,2020-09-29 00:34:55,1601328895.0,dataengineering,BI tool to plug on mongoDB ? (on premises),j1lhc6,steak_hallal,,https://www.reddit.com/r/dataengineering/comments/j1lhc6/bi_tool_to_plug_on_mongodb_on_premises/,1.0,1.0,0.0,19312.0,"Hi there.

Anyone knows of a BI tool that can be plugged on a mongoDB ? I've heard of Metabase, has anyone used it, or heard of it ? Do you guys know what are the mainstream choices for this ?

I could write a REST API that would send the right mongo requests, but I'm wondering if there is a better solution, especially if the BI needs increase a lot. I'm not sure if solutions exist for this, but I would also like to be able to integrate the statistical/BI tool dashboards with an existing web application.

Thanks a lot in advance for your inputs !"
3614,2020-09-29 02:21:15,1601335275.0,dataengineering,Where do I start? Social justice data engineering.,j1nefn,frettedwithfire,,https://www.reddit.com/r/dataengineering/comments/j1nefn/where_do_i_start_social_justice_data_engineering/,1.0,0.0,0.0,19313.0,
3615,2020-09-29 03:10:09,1601338209.0,dataengineering,"In this episode Daniel Molnar shares his experiences as a data janitor and the foundational elements of data engineering, as well as his work to build a practical bootcamp for data engineers.",j1o8t5,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/j1o8t5/in_this_episode_daniel_molnar_shares_his/,1.0,2.0,0.0,19314.0,
3616,2020-09-29 07:06:44,1601352404.0,dataengineering,Build API for your DWH data ? Anyone ever do this ?,j1s2jl,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/j1s2jl/build_api_for_your_dwh_data_anyone_ever_do_this/,1.0,0.0,0.0,19319.0,
3617,2020-09-29 10:44:43,1601365483.0,dataengineering,10 Things to know for a python developer,j1us0u,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j1us0u/10_things_to_know_for_a_python_developer/,1.0,0.0,0.0,19322.0,
3618,2020-09-29 10:46:14,1601365574.0,dataengineering,Best programming language to learn in 2021,j1usm1,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j1usm1/best_programming_language_to_learn_in_2021/,1.0,0.0,0.0,19322.0,
3619,2020-09-29 10:54:35,1601366075.0,dataengineering,Flink Stateful Functions 2.2.0 Release Announcement --&gt; Adding support for Async functions in the Python SDK,j1uw0a,Marksfik,,https://www.reddit.com/r/dataengineering/comments/j1uw0a/flink_stateful_functions_220_release_announcement/,1.0,0.0,0.0,19322.0,
3620,2020-09-29 14:14:08,1601378048.0,dataengineering,Trump vs Biden | 2020 U.S. Presidential Election,j1x887,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/j1x887/trump_vs_biden_2020_us_presidential_election/,1.0,0.0,0.0,19327.0,
3621,2020-09-29 15:03:34,1601381014.0,dataengineering,Question to ALL DATA ENGINEERS.,j1xw5b,Dave_TheOneAndOnly,,https://www.reddit.com/r/dataengineering/comments/j1xw5b/question_to_all_data_engineers/,1.0,1.0,0.0,19329.0," Hey, IT Professional! Sorry if this is slightly off-topic. I'm trying to build my first online business thus I'm researching on IT Professionals.

So, I'm currently trying to find out how one could make the lives of Data Engineers a lot easier by helping them lose weight with online coaching.

I know for a fact that a sedentary lifestyle, stressful deadlines, and busy lifestyles won't make it easy for someone in IT to lose weight. And it may even result in you gaining some weight over the years...

Which is why I have two very simple questions:

1. As a Data Engineer, what are 2 of the biggest issues you're dealing with?
2. Regarding losing weight, what would you wish for more than anything else?

Thank you in advance - looking forward to reading your answers!"
3622,2020-09-29 15:19:23,1601381963.0,dataengineering,Scalable Task Execution with Hazelcast and Spring Boot,j1y483,PeoplelogicAI,,https://www.reddit.com/r/dataengineering/comments/j1y483/scalable_task_execution_with_hazelcast_and_spring/,1.0,0.0,0.0,19330.0,
3623,2020-09-29 15:37:09,1601383029.0,dataengineering,"I've wrote a (DE biased) article about data jobs, highlighting how most companies will not need a DS or at least shouldn't start their data project with one",j1ydju,sib_n,,https://www.reddit.com/r/dataengineering/comments/j1ydju/ive_wrote_a_de_biased_article_about_data_jobs/,1.0,0.0,0.0,19330.0,
3624,2020-09-29 15:40:59,1601383259.0,dataengineering,"I've written a (DE biased) article about data jobs, highlighting how most companies will not need a DS or at least shouldn't start their data project with one",j1yfkg,sib_n,,https://www.reddit.com/r/dataengineering/comments/j1yfkg/ive_written_a_de_biased_article_about_data_jobs/,1.0,26.0,0.0,19330.0,
3625,2020-09-29 16:37:18,1601386638.0,dataengineering,A simple Hive table compactor utility,j1zcdh,abhilater,,https://www.reddit.com/r/dataengineering/comments/j1zcdh/a_simple_hive_table_compactor_utility/,1.0,0.0,0.0,19335.0,
3626,2020-09-29 17:07:41,1601388461.0,dataengineering,Data engineer salaries in Germany 2020,j1zvp5,soobrosa,,https://www.reddit.com/r/dataengineering/comments/j1zvp5/data_engineer_salaries_in_germany_2020/,1.0,0.0,0.0,19337.0,
3627,2020-09-29 17:29:44,1601389784.0,dataengineering,"ClickHouse, Redshift and 2.5 Billion Rows of Time Series Data",j20a4u,nameBrandon,,https://www.reddit.com/r/dataengineering/comments/j20a4u/clickhouse_redshift_and_25_billion_rows_of_time/,1.0,0.0,0.0,19338.0,"Just wanted to share my experience with ClickHouse for those of you interested in large scale, OLAP style solutions. It's is my site, so a bit of a shameless plug. 

[http://brandonharris.io/redshift-clickhouse-time-series/](http://brandonharris.io/redshift-clickhouse-time-series/)  


Also x-posted to r/bigdata"
3628,2020-09-29 17:36:25,1601390185.0,dataengineering,Best PostgreSQL BI &amp; Reporting tools,j20erz,duyenla257,,https://www.reddit.com/r/dataengineering/comments/j20erz/best_postgresql_bi_reporting_tools/,1.0,0.0,0.0,19339.0,
3629,2020-09-29 18:26:40,1601393200.0,dataengineering,Which data structures and algorithms are most important for data engineer interview?,j21ds3,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/j21ds3/which_data_structures_and_algorithms_are_most/,1.0,0.0,0.0,19342.0,
3630,2020-09-29 21:43:33,1601405013.0,dataengineering,Best Industry recognized Certifications for Data engineering,j25g0e,JasonNC86,,https://www.reddit.com/r/dataengineering/comments/j25g0e/best_industry_recognized_certifications_for_data/,1.0,0.0,0.0,19353.0,
3631,2020-09-30 04:14:24,1601428464.0,dataengineering,What is Career path of Machine Learning after Pandemic Crisis,j2cgtj,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j2cgtj/what_is_career_path_of_machine_learning_after/,1.0,0.0,0.0,19371.0,
3632,2020-09-30 04:16:35,1601428595.0,dataengineering,10 Things to know for a python developer,j2chww,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j2chww/10_things_to_know_for_a_python_developer/,1.0,0.0,0.0,19371.0,
3633,2020-09-30 04:18:23,1601428703.0,dataengineering,Starting Your Career in Machine Learning: What Are Your Options?,j2cirt,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j2cirt/starting_your_career_in_machine_learning_what_are/,1.0,0.0,0.0,19371.0,
3634,2020-09-30 04:19:54,1601428794.0,dataengineering,Learn Machine Learning,j2cjia,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j2cjia/learn_machine_learning/,1.0,0.0,0.0,19372.0,
3635,2020-09-30 05:28:22,1601432902.0,dataengineering,AWS Data Analytics Specialty vs Associate Solutions Architect ?,j2dk7s,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/j2dk7s/aws_data_analytics_specialty_vs_associate/,1.0,0.0,0.0,19373.0,
3636,2020-09-30 06:05:11,1601435111.0,dataengineering,What should I do?,j2e536,namrataaaaa,,https://www.reddit.com/r/dataengineering/comments/j2e536/what_should_i_do/,1.0,0.0,0.0,19377.0,
3637,2020-09-30 07:50:03,1601441403.0,dataengineering,Luigi for data pipelines - things I like.,j2fp3g,tonnamb,,https://www.reddit.com/r/dataengineering/comments/j2fp3g/luigi_for_data_pipelines_things_i_like/,1.0,0.0,0.0,19381.0,
3638,2020-09-30 13:55:34,1601463334.0,dataengineering,United Kingdom vs France | GDP from 1961 to 2018,j2jyaw,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/j2jyaw/united_kingdom_vs_france_gdp_from_1961_to_2018/,1.0,0.0,0.0,19385.0,
3639,2020-09-30 16:44:54,1601473494.0,dataengineering,"Currently a college freshman, should I study Cloud Computing?",j2mdr4,Pervert_Spongebob,,https://www.reddit.com/r/dataengineering/comments/j2mdr4/currently_a_college_freshman_should_i_study_cloud/,1.0,9.0,0.0,19399.0,"Currently, I am trying to learn about data warehousing (design, creation, usage, etc.). 

Should I also learn cloud computing? If so, what resource do you recommend?

Also, what else should students like me learn?"
3640,2020-09-30 17:01:13,1601474473.0,dataengineering,Are there any certifications for data engineering? If so which are the best ones to get?,j2mnuh,owter12,,https://www.reddit.com/r/dataengineering/comments/j2mnuh/are_there_any_certifications_for_data_engineering/,1.0,0.0,0.0,19401.0,
3641,2020-09-30 17:27:29,1601476049.0,dataengineering,"Is it possible to create a data warehouse inside VMs in a homelab server for learning, that more or less mimics real world implementations?",j2n4vq,num8lock,,https://www.reddit.com/r/dataengineering/comments/j2n4vq/is_it_possible_to_create_a_data_warehouse_inside/,1.0,25.0,0.0,19402.0,"If yes, what kind of route (open source toolings, architecture, etc) would you suggest for implementation? Please share your experience if you have built one before?"
3642,2020-09-30 21:20:44,1601490044.0,dataengineering,Can anyone please share Udacity Data Engineering Nanodegree templates?,j2rm8p,dondraper36,,https://www.reddit.com/r/dataengineering/comments/j2rm8p/can_anyone_please_share_udacity_data_engineering/,1.0,0.0,0.0,19416.0,
3643,2020-09-30 21:24:47,1601490287.0,dataengineering,Anyone here knows what tool was used to create this diagram?,j2rp1n,andresg3,,https://www.reddit.com/r/dataengineering/comments/j2rp1n/anyone_here_knows_what_tool_was_used_to_create/,1.0,2.0,0.0,19416.0,
3644,2020-09-30 22:43:03,1601494983.0,dataengineering,What Digital Transformation Is (and What It Isn't),j2t6l2,robertinoc,,https://www.reddit.com/r/dataengineering/comments/j2t6l2/what_digital_transformation_is_and_what_it_isnt/,1.0,1.0,0.0,19421.0,
3645,2020-09-30 23:36:54,1601498214.0,dataengineering,Is Redshift really that bad?,j2u6pw,tedfahrvergnugent,,https://www.reddit.com/r/dataengineering/comments/j2u6pw/is_redshift_really_that_bad/,1.0,34.0,0.0,19426.0,"I’ve used Azure SQL Data Warehouse, BigQuery, and Snowflake in production but never Redshift. I’ve caught many data scientists, analysts, and data engineers whinging at the thought of working on Redshift but have never really drilled into why. 

I may have to use it in a new role and want to understand what I’m getting myself into. I’d love to hear any stories good or bad."
3646,2020-10-01 01:21:25,1601504485.0,dataengineering,What options are there besides kumu that are simple and easiest to use?,j2w3k4,happypuppy100,,https://www.reddit.com/r/dataengineering/comments/j2w3k4/what_options_are_there_besides_kumu_that_are/,1.0,0.0,0.0,19432.0,
3647,2020-10-01 01:45:16,1601505916.0,dataengineering,Job Opening US - Remote,j2wipt,smccaffrey,,https://www.reddit.com/r/dataengineering/comments/j2wipt/job_opening_us_remote/,1.0,0.0,0.0,19433.0,
3648,2020-10-01 09:09:55,1601532595.0,dataengineering,How you can start working on Kaggle that will increase the chances of getting a job.,j332xi,kartikaya12,,https://www.reddit.com/r/dataengineering/comments/j332xi/how_you_can_start_working_on_kaggle_that_will/,1.0,0.0,0.0,19439.0,
3649,2020-10-01 14:18:02,1601551082.0,dataengineering,Grapefruit Consumption Ranking | TOP 10 Country from 1961 to 2013,j36mni,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/j36mni/grapefruit_consumption_ranking_top_10_country/,1.0,0.0,0.0,19443.0,
3650,2020-10-01 18:15:19,1601565319.0,dataengineering,How Simulations Work - How to Predict the Presidential Election Using Simulations,j3ai1l,buktotruth,,https://www.reddit.com/r/dataengineering/comments/j3ai1l/how_simulations_work_how_to_predict_the/,1.0,0.0,0.0,19453.0,
3651,2020-10-01 18:17:12,1601565432.0,dataengineering,The Data Council Berlin Picnic is back!,j3ajd3,soobrosa,,https://www.reddit.com/r/dataengineering/comments/j3ajd3/the_data_council_berlin_picnic_is_back/,1.0,0.0,0.0,19453.0,
3652,2020-10-01 18:41:38,1601566898.0,dataengineering,Your Company Can't Afford Data Debt,j3azz1,kelseyfecho,,https://www.reddit.com/r/dataengineering/comments/j3azz1/your_company_cant_afford_data_debt/,1.0,0.0,0.0,19455.0,
3653,2020-10-01 19:25:34,1601569534.0,dataengineering,Azure Data Factory - Parameterizing Sources for Copy Activities,j3bubz,XenoTypeXX121,,https://www.reddit.com/r/dataengineering/comments/j3bubz/azure_data_factory_parameterizing_sources_for/,1.0,0.0,0.0,19457.0,
3654,2020-10-01 20:01:13,1601571673.0,dataengineering,Best in Breed vs All in One Suite,j3cjoi,BakerElegant8439,,https://www.reddit.com/r/dataengineering/comments/j3cjoi/best_in_breed_vs_all_in_one_suite/,1.0,6.0,0.0,19458.0,"I am building out an updated data stack for my company. I am struggling to decide between using a single provider for our ELT/ETL (Matillion) versus best in breeds for each component (FiveTran/Fishtown/DBT) ? 

Wonder if anyone else had to make this decision - what did they choose and how did they decide."
3655,2020-10-01 20:01:28,1601571688.0,dataengineering,Hub-and-Spoke vs. Point-to-Point Data Synchronization: Which one to pick?,j3cjv7,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/j3cjv7/hubandspoke_vs_pointtopoint_data_synchronization/,3.0,2.0,0.0,19458.0,
3656,2020-10-01 20:07:36,1601572056.0,dataengineering,Use cloud database (like snowflake) as a backend for airflow?,j3cohx,graciousgroob,,https://www.reddit.com/r/dataengineering/comments/j3cohx/use_cloud_database_like_snowflake_as_a_backend/,1.0,2.0,0.0,19458.0,"I am setting up an airflow instance. By default running 

    airflow initdb

initializes an sqllite database. However most tutorials [suggest](https://airflow.apache.org/docs/stable/howto/initialize-database.html) using postgres or something more heavy duty.

Would there be any benefit to using a cloud-based database like snowflake for this? Is that even possible?

I can see in airflow.cfg that by default, the sqlalchemy engine points (on a EC2 linux instance) to:

    # The SqlAlchemy connection string to the metadata database.   
    # SqlAlchemy supports many different database engine, more information
    # their website
    sql_alchemy_conn = sqlite:////home/ubuntu/airflow/airflow.db

I know snowflake has sqlalchemy connectors, but I don't know how to configure airflow.cfg to interact with it. Can anyone point me in the right direction?"
3657,2020-10-01 20:11:35,1601572295.0,dataengineering,ETL Excel Template to Share?,j3crgx,PencilBoy99,,https://www.reddit.com/r/dataengineering/comments/j3crgx/etl_excel_template_to_share/,1.0,0.0,0.0,19458.0,
3658,2020-10-01 20:21:30,1601572890.0,dataengineering,How do you keep tabs on your critical tables/reports?,j3cys5,mkvor8,,https://www.reddit.com/r/dataengineering/comments/j3cys5/how_do_you_keep_tabs_on_your_critical/,1.0,0.0,0.0,19459.0,
3659,2020-10-01 21:19:12,1601576352.0,dataengineering,Why Auth0 is 'Shifting-Left' on Security,j3e3ke,robertinoc,,https://www.reddit.com/r/dataengineering/comments/j3e3ke/why_auth0_is_shiftingleft_on_security/,1.0,1.0,0.0,19460.0,
3660,2020-10-01 22:11:01,1601579461.0,dataengineering,The first CI workflow (that I know of) for Data Pipelines available directly from PRs on GitHub using Great Expectations in Github Actions,j3f3ak,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/j3f3ak/the_first_ci_workflow_that_i_know_of_for_data/,1.0,6.0,0.0,19460.0,
3661,2020-10-02 02:55:51,1601596551.0,dataengineering,Query regarding Data Balancing,j3k6j0,groversarthak29,,https://www.reddit.com/r/dataengineering/comments/j3k6j0/query_regarding_data_balancing/,1.0,0.0,0.0,19464.0,
3662,2020-10-02 08:13:14,1601615594.0,dataengineering,Understanding Basics Of SVM With Example And Python Implementation,j3ono0,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j3ono0/understanding_basics_of_svm_with_example_and/,1.0,0.0,0.0,19471.0,
3663,2020-10-02 13:56:38,1601636198.0,dataengineering,China vs Japan | GDP from 1961 to 2018,j3sfku,Shine-Major,,https://www.reddit.com/r/dataengineering/comments/j3sfku/china_vs_japan_gdp_from_1961_to_2018/,1.0,0.0,0.0,19475.0,
3664,2020-10-03 06:19:58,1601695198.0,dataengineering,Request: Amazon DE On-site interview tips,j48mbg,floydhead11,,https://www.reddit.com/r/dataengineering/comments/j48mbg/request_amazon_de_onsite_interview_tips/,1.0,0.0,0.0,19491.0,
3665,2020-10-03 07:52:43,1601700763.0,dataengineering,What is dbt and why should we use it in DE context?,j49x3v,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/j49x3v/what_is_dbt_and_why_should_we_use_it_in_de_context/,1.0,0.0,0.0,19493.0,
3666,2020-10-03 12:40:40,1601718040.0,dataengineering,Difficulties understanding docker/airflow,j4d1pz,Luukv93,,https://www.reddit.com/r/dataengineering/comments/j4d1pz/difficulties_understanding_dockerairflow/,1.0,0.0,0.0,19500.0,
3667,2020-10-03 12:49:02,1601718542.0,dataengineering,Configuring Apache Airflow with AWS RDS,j4d4q9,marclamberti,,https://www.reddit.com/r/dataengineering/comments/j4d4q9/configuring_apache_airflow_with_aws_rds/,1.0,0.0,0.0,19500.0,
3668,2020-10-03 14:03:28,1601723008.0,dataengineering,Eating Disorder Ranking | TOP 10 Country from 1990 to 2017,j4dxxx,Quirky_Classroom,,https://www.reddit.com/r/dataengineering/comments/j4dxxx/eating_disorder_ranking_top_10_country_from_1990/,1.0,0.0,0.0,19503.0,
3669,2020-10-03 18:50:18,1601740218.0,dataengineering,Is there any data engineer roadmap plus guide?,j4hasg,aakhri_paasta,,https://www.reddit.com/r/dataengineering/comments/j4hasg/is_there_any_data_engineer_roadmap_plus_guide/,1.0,0.0,0.0,19508.0,
3670,2020-10-03 22:12:51,1601752371.0,dataengineering,Why is it so difficult to get a job as Data Engineer in Healthcare domain?,j4kvyo,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/j4kvyo/why_is_it_so_difficult_to_get_a_job_as_data/,1.0,0.0,0.0,19512.0,
3671,2020-10-04 04:16:00,1601774160.0,dataengineering,Help! What does basic clustering of items?,j4qjk8,happypuppy100,,https://www.reddit.com/r/dataengineering/comments/j4qjk8/help_what_does_basic_clustering_of_items/,1.0,1.0,0.0,19518.0,"Items are words, not numbers.

Need good Viz tool that does this basic thing

1. Say you have a set of 5-7 associated/related  items
2. There's multiple sets
3. The viz tool automatically creates clusters / a network graph of these items
   1. and associated moreso with each other are closer in the clusters

I know there are viz sites/tools out there that make making certain kinds of visualizations simple with a click of a button

Know of any sites like that ??"
3672,2020-10-04 05:09:20,1601777360.0,dataengineering,Very basic clustering question,j4r9w7,happypuppy100,,https://www.reddit.com/r/dataengineering/comments/j4r9w7/very_basic_clustering_question/,1.0,0.0,0.0,19520.0,"Gonna do this on google sheets or w/e else is simple

Say you have a set of 5 items

* These items are words, not numbers

You have many sets of items

How do you make it so that items that were more frequent / common within each set are shown as being closer together / more related?"
3673,2020-10-04 05:09:47,1601777387.0,dataengineering,An interview with Vectorized founder Alexander Gallego about the Red Panda streaming engine and building a drop-in replacement for Kafka with better performance and throughput.,j4ra4g,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/j4ra4g/an_interview_with_vectorized_founder_alexander/,1.0,0.0,0.0,19520.0,
3674,2020-10-04 13:35:00,1601807700.0,dataengineering,When to use AWS/GCP/Azure solution and when create own with Kubernetes,j4wwgm,Sihal,,https://www.reddit.com/r/dataengineering/comments/j4wwgm/when_to_use_awsgcpazure_solution_and_when_create/,1.0,0.0,0.0,19537.0,
3675,2020-10-04 14:15:08,1601810108.0,dataengineering,Anxiety Disorder Ranking | TOP 10 Country from 1990 to 2017,j4xbwp,AdLevel1044,,https://www.reddit.com/r/dataengineering/comments/j4xbwp/anxiety_disorder_ranking_top_10_country_from_1990/,1.0,0.0,0.0,19537.0,
3676,2020-10-04 14:29:25,1601810965.0,dataengineering,Processing big json file 30GB,j4xhfk,WarningStrong,,https://www.reddit.com/r/dataengineering/comments/j4xhfk/processing_big_json_file_30gb/,1.0,47.0,0.0,19538.0,"I have a big json file \~30 GB (JSON Lines),which needs to be processed (some transformations and loading into database). The file is compressed with gzip (\~18GB) and it is in URL. I'm thinking about downloading it in chunks using python, then processing relatively smaller files but feel like there are better ways dealing with large files."
3677,2020-10-04 19:02:17,1601827337.0,dataengineering,Bootcamps for Data Engineers?,j51b8o,wtfismyjob,,https://www.reddit.com/r/dataengineering/comments/j51b8o/bootcamps_for_data_engineers/,1.0,0.0,0.0,19543.0,
3678,2020-10-04 21:28:28,1601836108.0,dataengineering,End-to-end Beginner Project,j53tmm,theAviCaster,,https://www.reddit.com/r/dataengineering/comments/j53tmm/endtoend_beginner_project/,1.0,0.0,0.0,19543.0,
3679,2020-10-04 21:37:47,1601836667.0,dataengineering,End-to-end Beginner Project,j53zg2,theAviCaster,,https://www.reddit.com/r/dataengineering/comments/j53zg2/endtoend_beginner_project/,1.0,0.0,0.0,19543.0,
3680,2020-10-04 21:39:36,1601836776.0,dataengineering,End-to-end Beginner Project,j540jj,theAviCaster,,https://www.reddit.com/r/dataengineering/comments/j540jj/endtoend_beginner_project/,1.0,1.0,0.0,19543.0,
3681,2020-10-04 21:42:22,1601836942.0,dataengineering,What is dbt and why should we use it in DE context?,j5426p,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/j5426p/what_is_dbt_and_why_should_we_use_it_in_de_context/,1.0,0.0,0.0,19543.0,
3682,2020-10-05 00:20:59,1601846459.0,dataengineering,Handling unexpected source data schema changes?,j56uco,Lsaone,,https://www.reddit.com/r/dataengineering/comments/j56uco/handling_unexpected_source_data_schema_changes/,1.0,0.0,0.0,19547.0,
3683,2020-10-05 11:53:24,1601888004.0,dataengineering,Why Your Company Needs a Data Engineer,j5fzqi,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/j5fzqi/why_your_company_needs_a_data_engineer/,1.0,0.0,0.0,19561.0,
3684,2020-10-05 13:16:57,1601893017.0,dataengineering,Batch Processing with Apache Beam in Python Mini-Course,j5gusy,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/j5gusy/batch_processing_with_apache_beam_in_python/,1.0,0.0,0.0,19565.0,
3685,2020-10-05 13:33:09,1601893989.0,dataengineering,Review of a great book on the analytics stack and architecture,j5h1go,today_is_tuesday,,https://www.reddit.com/r/dataengineering/comments/j5h1go/review_of_a_great_book_on_the_analytics_stack_and/,1.0,0.0,0.0,19566.0,
3686,2020-10-05 13:44:20,1601894660.0,dataengineering,5 Trends In Data &amp; What Software Buyers Are Looking For In New Tools,j5h65z,femstreet,,https://www.reddit.com/r/dataengineering/comments/j5h65z/5_trends_in_data_what_software_buyers_are_looking/,1.0,0.0,0.0,19566.0,
3687,2020-10-05 13:52:38,1601895158.0,dataengineering,Looking for advice on my Baackend / Data Engineer Resume,j5h9gu,Bianca_di_Angelo,,https://www.reddit.com/r/dataengineering/comments/j5h9gu/looking_for_advice_on_my_baackend_data_engineer/,1.0,0.0,0.0,19567.0,"Hi everyone, any help will be appreciated."
3688,2020-10-05 14:30:59,1601897459.0,dataengineering,Looking for advice on my Backend/Data Engineer Resume,j5hq4j,Bianca_di_Angelo,,https://www.reddit.com/r/dataengineering/comments/j5hq4j/looking_for_advice_on_my_backenddata_engineer/,1.0,17.0,0.0,19567.0,
3689,2020-10-05 15:27:02,1601900822.0,dataengineering,The 2020 Data Landscape Chart Is Out! How Should You Feel About It?,j5igr7,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/j5igr7/the_2020_data_landscape_chart_is_out_how_should/,1.0,8.0,0.0,19569.0,
3690,2020-10-05 17:00:02,1601906402.0,dataengineering,How to use python to perform a complex data mapping &amp; transformation in Azure platform ?,j5jw5y,rasviz,,https://www.reddit.com/r/dataengineering/comments/j5jw5y/how_to_use_python_to_perform_a_complex_data/,1.0,0.0,0.0,19571.0,
3691,2020-10-05 17:21:06,1601907666.0,dataengineering,Optimal approach to maintain a changing relationship between 2 entities?,j5k91o,Testher75,,https://www.reddit.com/r/dataengineering/comments/j5k91o/optimal_approach_to_maintain_a_changing/,1.0,0.0,0.0,19572.0,
3692,2020-10-05 17:53:06,1601909586.0,dataengineering,Introduction to KubernetesExecutor and KubernetesPodOperator,j5ksrg,iamspoilt,,https://www.reddit.com/r/dataengineering/comments/j5ksrg/introduction_to_kubernetesexecutor_and/,1.0,0.0,0.0,19573.0,
3693,2020-10-05 18:06:16,1601910376.0,dataengineering,Streaming XML messages from IBM MQ into Kafka into MongoDB,j5l1fd,rmoff,,https://www.reddit.com/r/dataengineering/comments/j5l1fd/streaming_xml_messages_from_ibm_mq_into_kafka/,1.0,0.0,0.0,19574.0,
3694,2020-10-05 18:46:47,1601912807.0,dataengineering,"Kafka and XML - what, why, how?",j5lscf,rmoff,,https://www.reddit.com/r/dataengineering/comments/j5lscf/kafka_and_xml_what_why_how/,1.0,0.0,0.0,19575.0,
3695,2020-10-05 18:59:23,1601913563.0,dataengineering,How to Manage Your Data the Way You Manage Your Code,j5m0w8,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/j5m0w8/how_to_manage_your_data_the_way_you_manage_your/,1.0,0.0,0.0,19574.0,
3696,2020-10-05 19:21:07,1601914867.0,dataengineering,"[help] ETL from two RDBMS with foreign keys, Data Mapping",j5mg17,Majestic-Jump,,https://www.reddit.com/r/dataengineering/comments/j5mg17/help_etl_from_two_rdbms_with_foreign_keys_data/,1.0,0.0,0.0,19576.0,
3697,2020-10-05 19:39:19,1601915959.0,dataengineering,Should I do Backend Engineering first?,j5msn3,NoctisUchiha,,https://www.reddit.com/r/dataengineering/comments/j5msn3/should_i_do_backend_engineering_first/,1.0,0.0,0.0,19576.0,
3698,2020-10-05 21:40:52,1601923252.0,dataengineering,Recommendations for bootcamps?,j5p5f5,Specialist-Gur,,https://www.reddit.com/r/dataengineering/comments/j5p5f5/recommendations_for_bootcamps/,1.0,7.0,0.0,19583.0,Does anyone have any data engineering bootcamps they recommend? Or advice on how to research those? I’m looking to make a career switch. Thanks!
3699,2020-10-05 23:35:46,1601930146.0,dataengineering,How can Delta Lake be used with JDBC/ODBC to connect to Azure SQL but not on Databricks?,j5rbyj,overtaker123,,https://www.reddit.com/r/dataengineering/comments/j5rbyj/how_can_delta_lake_be_used_with_jdbcodbc_to/,1.0,0.0,0.0,19586.0,
3700,2020-10-06 01:59:09,1601938749.0,dataengineering,RedShift vs Snowflake?,j5tw0d,Electric_pokemon,,https://www.reddit.com/r/dataengineering/comments/j5tw0d/redshift_vs_snowflake/,1.0,0.0,0.0,19590.0,
3701,2020-10-06 09:24:58,1601965498.0,dataengineering,CCA 175 Vs databricks Spark Developer Certification,j6058h,powok,,https://www.reddit.com/r/dataengineering/comments/j6058h/cca_175_vs_databricks_spark_developer/,1.0,0.0,0.0,19604.0,
3702,2020-10-06 10:06:37,1601967997.0,dataengineering,Understanding Basics Of SVM With Example And Python Implementation,j60llg,chase2learn,,https://www.reddit.com/r/dataengineering/comments/j60llg/understanding_basics_of_svm_with_example_and/,1.0,0.0,0.0,19606.0,
3703,2020-10-06 13:43:07,1601980987.0,dataengineering,"Advice needed, getting started with a project",j62u28,vlahunter,,https://www.reddit.com/r/dataengineering/comments/j62u28/advice_needed_getting_started_with_a_project/,1.0,10.0,0.0,19609.0,"Hello there,

i am a fullstack developer (Mostly Node.js and React with Mongo || Postgres) and i have been researching around the field of big data and data engineering. Since most of the books out there are not project based and they mostly cover the High level of each stack, i have been struggling a bit to understand some parts of the Big Data Stack.

As a stack, i am trying to focus on the Hadoop ecosystem and start with a project there. My question would be the following. 

Assuming i would like to build a cluster (using VMs) and that each node would have HDFS installed, what would be the process and the next steps? I would find some sample CSV files to begin with but what would be the procedures and where should i focus in order to build an end to end project? (both to understand and also to have it in my Github portfolio in the future).

In more details regarding the project/scenario i am trying to solve is the following. i am thinking of applying a Big Data solution using a set of Nodes in a cluster that would be able to receive/ingest data (in CSV probably) and then i suppose after some processing/cleaning to have them available and stored in order to be read by a potential analyst.

&amp;#x200B;

Keep in mind that i have already checked a lot of resources online and in the most part i understand the logic and the problems this set of technologies tries to solve (Hadoop Ecosystem). Also, i have seen the hype and importance of the Cloud providers and their ecosystem but i feel that in order to grasp the basics it would be better for me to start building something locally using Hadoop.

Also, apart from your advice regarding the project, i would be happy if someone would have a personal recommendation on a book/course or whatever that could help me move faster in this project and domain.

&amp;#x200B;

Thanks in advance.

Regards"
3704,2020-10-06 14:37:57,1601984277.0,dataengineering,United Kingdom vs Germany | GDP from 1970 to 2018,j63h2f,OutlandishnessOk7324,,https://www.reddit.com/r/dataengineering/comments/j63h2f/united_kingdom_vs_germany_gdp_from_1970_to_2018/,1.0,0.0,0.0,19612.0,
3705,2020-10-06 17:44:13,1601995453.0,dataengineering,resumes: do recruiters want to see all flavors of SQL?,j66a9z,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/j66a9z/resumes_do_recruiters_want_to_see_all_flavors_of/,1.0,0.0,0.0,19618.0,
3706,2020-10-06 19:48:30,1602002910.0,dataengineering,The Benefits Of Using Python And T-SQL Over SSIS For ETL,j68l82,bi_expert,,https://www.reddit.com/r/dataengineering/comments/j68l82/the_benefits_of_using_python_and_tsql_over_ssis/,1.0,30.0,0.0,19619.0,"“Why use SQL instead of SSIS for ETL?” 

I got asked this question in an interview. Since I’m not fast on my feet, I stammered out a passible but highly unimpressive answer.

The link to the article below represents what I WOULD have said if we lived in the Ghost in the Shell universe and my brain was wirelessly connected to a cloud drive.

It’s a long read, but if you’re interested in building ETL for data warehouses, it’s incredibly valuable.

[The Benefits Of Using Python And T-SQL Over SSIS For ETL](https://datadrivenperspectives.com/the-benefits-of-using-python-and-t-sql-over-ssis-for-etl-ca50c6e11819)"
3707,2020-10-06 21:11:58,1602007918.0,dataengineering,Does Entity Resolution Belong to Data Engineering?,j6a84b,Silicon-Data,,https://www.reddit.com/r/dataengineering/comments/j6a84b/does_entity_resolution_belong_to_data_engineering/,1.0,3.0,0.0,19620.0,"I was tasked to help DS and ML team with entity resolution to rectify their inferences.

Now I wonder if this is in scope of DE at all..."
3708,2020-10-06 22:28:09,1602012489.0,dataengineering,Wanting a masters - which one?,j6bpvh,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/j6bpvh/wanting_a_masters_which_one/,1.0,0.0,0.0,19622.0,
3709,2020-10-06 22:36:55,1602013015.0,dataengineering,Webinar | How to Select the Right Data Lake,j6bw2w,Sprinkle_Data,,https://www.reddit.com/r/dataengineering/comments/j6bw2w/webinar_how_to_select_the_right_data_lake/,1.0,0.0,0.0,19622.0,
3710,2020-10-06 23:31:11,1602016271.0,dataengineering,All things Data - Job board,j6cycp,shad-rocks,,https://www.reddit.com/r/dataengineering/comments/j6cycp/all_things_data_job_board/,1.0,0.0,0.0,19626.0,
3711,2020-10-07 00:05:01,1602018301.0,dataengineering,Kafka Connect in a nutshell,j6dlg9,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/j6dlg9/kafka_connect_in_a_nutshell/,1.0,2.0,0.0,19628.0,
3712,2020-10-07 02:05:37,1602025537.0,dataengineering,Data Modeling in DE vs DBA vs BI,j6fp05,CesQ89,,https://www.reddit.com/r/dataengineering/comments/j6fp05/data_modeling_in_de_vs_dba_vs_bi/,1.0,3.0,0.0,19634.0,"Do you all think that the meaning of Data Modelling is different between Data Engineering, Database Administration, and Business Intelligence. 

I learned Data Modeling in College with respect to traditional RDBMS and later in my professional career in BI using Dimensional Data Models which has carried over to my existing Data Engineering role along with general software engineering best practices but it seems like the definition of Data Modeling has changed since becoming a DE.

It could very well be that I think it's different because I have come to discover that not a lot of Data Engineers have the same training in terms of Computer Science or even Information Systems so they don't quite understand the jargon or the background, not that there is anything wrong with that.

Just curious as to what this sub thinks of in terms of Data Modeling."
3713,2020-10-07 04:51:31,1602035491.0,dataengineering,Data Engineering project for beginners - Streaming edition,j6i9wr,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/j6i9wr/data_engineering_project_for_beginners_streaming/,1.0,0.0,0.0,19636.0,
3714,2020-10-07 06:47:02,1602042422.0,dataengineering,How to ensure stability of processes?,j6jxva,pbj800100,,https://www.reddit.com/r/dataengineering/comments/j6jxva/how_to_ensure_stability_of_processes/,1.0,5.0,0.0,19640.0,"Generally speaking, what do you do to ensure stability of automated data processes? I'm new to engineering and working on my own for a small company. I've set up a lot of automated processes using task scheduler to run simple python scripts on a virtual machine. However I've found that a lot of times these scripts fail for various reasons and I haven't really been able to implement a fix as the failures seem to be inconsistent. For example, sometimes my task will fail as scheduled at 8am, but if I trigger it manually 20 mins later, it runs fine. Besides obviously just figuring out what the error is, what sort of things do engineers typically do to prevent this stuff from happening?

I always hear that ensuring the ongoing stability of processes is a key role of a data engineer but I don't know where to go. Is it just constantly awaiting issues and then finding fixes, or implementing ways to say like ""if this fails, rerun X many times"", or something else?

Sometimes I'll get failures inside of my python scripts instead. In some cases, I'm connecting to an API to pull data and sometimes the API will be unavailable for whatever reason. How would you fix something like this - write another script to check whether the original script has run properly and if not, trigger a rerun?

Sorry if these are stupid questions. Thanks!!"
3715,2020-10-07 07:11:23,1602043883.0,dataengineering,How do sensors send data to Kafka / any pub sub,j6k9ye,powok,,https://www.reddit.com/r/dataengineering/comments/j6k9ye/how_do_sensors_send_data_to_kafka_any_pub_sub/,1.0,1.0,0.0,19639.0,
3716,2020-10-07 11:29:41,1602059381.0,dataengineering,Data = Knowledge = Power | How Power is Harvested from Data,j6n8fa,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/j6n8fa/data_knowledge_power_how_power_is_harvested_from/,1.0,0.0,0.0,19643.0,
3717,2020-10-07 13:51:20,1602067880.0,dataengineering,Bipolar Disorder Ranking | TOP 10 Country from 1990 to 2017,j6opbo,Warm-Accident-4463,,https://www.reddit.com/r/dataengineering/comments/j6opbo/bipolar_disorder_ranking_top_10_country_from_1990/,1.0,0.0,0.0,19647.0,
3718,2020-10-07 14:53:02,1602071582.0,dataengineering,How to prepare for Cloudera Certifified Associate Spark and Hadoop Developer Certification Exam CCA175?,j6pfh3,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/j6pfh3/how_to_prepare_for_cloudera_certifified_associate/,1.0,0.0,0.0,19647.0,
3719,2020-10-07 22:25:32,1602098732.0,dataengineering,What can data teams do to prepare for IPO?,j6xi53,mkvor8,,https://www.reddit.com/r/dataengineering/comments/j6xi53/what_can_data_teams_do_to_prepare_for_ipo/,1.0,0.0,0.0,19660.0,
3720,2020-10-08 00:58:20,1602107900.0,dataengineering,Push API Data Sources,j70djo,lampostpark,,https://www.reddit.com/r/dataengineering/comments/j70djo/push_api_data_sources/,1.0,0.0,0.0,19661.0,
3721,2020-10-08 05:25:44,1602123944.0,dataengineering,YouTube channels,j74rga,sankalpthakur2610,,https://www.reddit.com/r/dataengineering/comments/j74rga/youtube_channels/,1.0,13.0,0.0,19669.0,Do we have some YouTube channels which may either be doing projects or teaching or sharing experience?
3722,2020-10-08 06:17:27,1602127047.0,dataengineering,Anyone here work as a data engineer in the art industry or somehow combine building data pipeline and have it bring you closer to artist endeavors,j75lra,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/j75lra/anyone_here_work_as_a_data_engineer_in_the_art/,1.0,0.0,0.0,19668.0,
3723,2020-10-08 13:57:31,1602154651.0,dataengineering,[Hiring] Data Engineer - Argentina,j7az8c,FMArgentina,,https://www.reddit.com/r/dataengineering/comments/j7az8c/hiring_data_engineer_argentina/,1.0,0.0,0.0,19674.0,
3724,2020-10-08 15:00:25,1602158425.0,dataengineering,Europe GDP per capita Ranking | TOP 10 Country from 1970 to 2017,j7br82,OutlandishnessOk7324,,https://www.reddit.com/r/dataengineering/comments/j7br82/europe_gdp_per_capita_ranking_top_10_country_from/,1.0,0.0,0.0,19675.0,
3725,2020-10-08 17:04:34,1602165874.0,dataengineering,An interview with the co-founders of Meroxa about their work to build a self service data integration platform that is easy to implement and scale for end users.,j7dm7l,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/j7dm7l/an_interview_with_the_cofounders_of_meroxa_about/,1.0,3.0,0.0,19680.0,
3726,2020-10-08 18:59:59,1602172799.0,dataengineering,How do I read DeltaLake files from spark-sql?,j7fou9,siddharths067,,https://www.reddit.com/r/dataengineering/comments/j7fou9/how_do_i_read_deltalake_files_from_sparksql/,1.0,0.0,0.0,19684.0,
3727,2020-10-08 22:27:33,1602185253.0,dataengineering,Livestream with Q&amp;A: Python Package Management on a Fluid-Slurm-GCP Cluster,j7jnzq,fluid_numerics,,https://www.reddit.com/r/dataengineering/comments/j7jnzq/livestream_with_qa_python_package_management_on_a/,1.0,0.0,0.0,19691.0,
3728,2020-10-08 23:05:17,1602187517.0,dataengineering,Is Airflow the correct tool for this simple job?,j7kedz,OtherwiseUniversity7,,https://www.reddit.com/r/dataengineering/comments/j7kedz/is_airflow_the_correct_tool_for_this_simple_job/,1.0,0.0,0.0,19692.0,
3729,2020-10-09 02:00:08,1602198008.0,dataengineering,Python Job Interview Flash Cards,j7nk99,bi_expert,,https://www.reddit.com/r/dataengineering/comments/j7nk99/python_job_interview_flash_cards/,1.0,0.0,0.0,19694.0,
3730,2020-10-09 02:28:46,1602199726.0,dataengineering,Is it possible to become hybrid Devops and Data engineer ?,j7o1un,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/j7o1un/is_it_possible_to_become_hybrid_devops_and_data/,1.0,0.0,0.0,19695.0,
3731,2020-10-09 10:38:41,1602229121.0,dataengineering,A short story about QUEL: SQL's biggest rival,j7ur3x,nvqh,,https://www.reddit.com/r/dataengineering/comments/j7ur3x/a_short_story_about_quel_sqls_biggest_rival/,1.0,5.0,0.0,19702.0,
3732,2020-10-09 12:56:06,1602237366.0,dataengineering,Is it possible to search for specific words in Excel in multiple sheets using Databricks?,j7w61e,1010101100111,,https://www.reddit.com/r/dataengineering/comments/j7w61e/is_it_possible_to_search_for_specific_words_in/,1.0,2.0,0.0,19703.0,"I'm using Databricks for a data processing task.

I have an Excel workbook with over 40 sheets.

I have to search for specific words in the entire workbook, like a customer reference number that is spread across sheets?

Is it possible to use Databricks for this?"
3733,2020-10-09 14:04:12,1602241452.0,dataengineering,Depression Ranking | TOP 10 Country from 1990 to 2017,j7wx4r,Warm-Accident-4463,,https://www.reddit.com/r/dataengineering/comments/j7wx4r/depression_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,19703.0,
3734,2020-10-09 16:26:27,1602249987.0,dataengineering,"Python Driven ETL systems VS ""10 Clicks Data Sync"" Cloud ETL Platforms",j7yw08,fnaxou,,https://www.reddit.com/r/dataengineering/comments/j7yw08/python_driven_etl_systems_vs_10_clicks_data_sync/,1.0,19.0,0.0,19707.0,"I am really bugged by the the data digestion solutions offered by different platforms like spitch or begment. I mean, I do understand their utility in terms of getting a pure SaaS solution when it comes to ETL. At the end, conceptually, writing code for something that already exists is not the best practice. However, I do find it troubling to rely on these solutions as the master ETL/ ELT systems for organizations.   
They act as a black box. They have control over DDL in the target DWH (WTF). Transformations are quite limited. In addition, they do not allow full extensibility or treating the unusual cases or source systems-which is quite normal for every company. You can't even have a proper monitoring system. To some extent, their behavior is quite stochastic. I have seen a spitch etl trying to alter a table in the dwh by adding over 1600 columns just because the source system changed its JSON structure. 

I am fascinated by Apache Airflow- or even similar frameworks. It is literally a revolution in my opinion in code-driven data pipeline design and scheduling. Seasoning it  with Dockers, Kubernetes and microservices, ETL systems will be so powerful and extensible, scalable, versionable etc... I have read threads about this where some have thought that using Airflow is like reinventing the wheel. How? Airflow is a framework, hundreds of contributions were made to it. You are using Airflow not rewriting it. 

My problem: I couldn't find a nice scientific or empirical article/ book that confirms my bias. I always read blogs and try to keep myself up to date with this. However, these companies are just bombing search engines with their ""I can solve all of your problems"" and ""Mine is better than anyone else"" articles."
3735,2020-10-09 22:38:56,1602272336.0,dataengineering,A Data Warehouse Implementation on AWS,j85r48,ajhenaor,,https://www.reddit.com/r/dataengineering/comments/j85r48/a_data_warehouse_implementation_on_aws/,1.0,11.0,0.0,19712.0,
3736,2020-10-10 00:28:27,1602278907.0,dataengineering,Getting started with XComs in Apache Airflow,j87ros,marclamberti,,https://www.reddit.com/r/dataengineering/comments/j87ros/getting_started_with_xcoms_in_apache_airflow/,1.0,0.0,0.0,19715.0,
3737,2020-10-10 05:30:17,1602297017.0,dataengineering,Mobile App for Databricks?,j8ckeo,mrdeere123,,https://www.reddit.com/r/dataengineering/comments/j8ckeo/mobile_app_for_databricks/,1.0,1.0,0.0,19728.0,"Hey all, is anyone familiar with a way I can query Databricks on mobile? I'd like this ability since my queries sometime can last a long time and it'd be nice to be able to make small modifications to them on the go..."
3738,2020-10-10 06:08:34,1602299314.0,dataengineering,Any good resources for learning SSIS?,j8d3y7,K0NGO,,https://www.reddit.com/r/dataengineering/comments/j8d3y7/any_good_resources_for_learning_ssis/,1.0,0.0,0.0,19729.0,
3739,2020-10-10 14:03:03,1602327783.0,dataengineering,Nuclear Electricity Ranking | TOP 10 Country from 1965 to 2019,j8ilrq,OutlandishnessOk7324,,https://www.reddit.com/r/dataengineering/comments/j8ilrq/nuclear_electricity_ranking_top_10_country_from/,1.0,0.0,0.0,19741.0,
3740,2020-10-10 14:23:39,1602329019.0,dataengineering,Delta Lake in Production,j8iu8t,AndreSionek,,https://www.reddit.com/r/dataengineering/comments/j8iu8t/delta_lake_in_production/,1.0,15.0,0.0,19741.0,
3741,2020-10-10 19:03:46,1602345826.0,dataengineering,Looking for CV feedback,j8n05o,BrowsasaurusRex,,https://www.reddit.com/r/dataengineering/comments/j8n05o/looking_for_cv_feedback/,1.0,2.0,0.0,19746.0,"Would really appreciate some feedback on my CV. I spent the first few years of my career doing data analyst type work, and since moving to my current job I've worked really hard to develop my data eng. skills and have been able to do more cloud / data engineering work. I really want to double down on this for my next career move :)

Please forgive some of the stuff I've redacted to not tie me to my current employer."
3742,2020-10-10 19:38:24,1602347904.0,dataengineering,Building a Data Warehouse: Basic Architectural principles,j8nlmp,ajhenaor,,https://www.reddit.com/r/dataengineering/comments/j8nlmp/building_a_data_warehouse_basic_architectural/,1.0,0.0,0.0,19747.0,
3743,2020-10-10 21:19:41,1602353981.0,dataengineering,My project lets you edit a spreadsheet to generate Python,j8pet8,Jake_Stack808,,https://www.reddit.com/r/dataengineering/comments/j8pet8/my_project_lets_you_edit_a_spreadsheet_to/,1.0,0.0,0.0,19750.0,
3744,2020-10-10 22:02:50,1602356570.0,dataengineering,Interview prep for promotion to a senior data engineering role,j8q6n7,ElectricalFilm2,,https://www.reddit.com/r/dataengineering/comments/j8q6n7/interview_prep_for_promotion_to_a_senior_data/,1.0,5.0,0.0,19752.0,"I have been working at a junior data engineering position for a year, and I will be interviewing for promotion to a senior role soon. I will have multiple interviews - with managers, senior folk and other people I think I'd interact with for projects in this senior role. 

Brief description of the job:

* Technology stack: AWS, Snowflake, a few ETL tools (plus Python and SQL, of course)
* Duties: ETL (and enhancements like CDC), data warehouse support (I guess that includes anything other than ""core DBA stuff""), other projects involving cloud based applications 

My profile, as of now:

* I have done a bit of work in AWS, mostly involving building Lambda functions that also use services like SNS, SQS
* In my current role, I've been heavily involved in a fewer number of projects than what is expected in the senior role I'm interviewing for - mostly CDC implementation and monitoring
* They want someone who understands the relationships between servers, applications, databases and networks

Since I am already known to everyone that is interviewing me, how could I prepare for these interviews?"
3745,2020-10-11 03:09:30,1602374970.0,dataengineering,"Cross post from CSCareers - I know masters are necessary, but how does this one look?",j8vbxp,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/j8vbxp/cross_post_from_cscareers_i_know_masters_are/,1.0,0.0,0.0,19755.0,
3746,2020-10-11 14:11:55,1602414715.0,dataengineering,Asia GDP per capita Ranking | TOP 10 Country from 1960 to 2017,j93mi9,OutlandishnessOk7324,,https://www.reddit.com/r/dataengineering/comments/j93mi9/asia_gdp_per_capita_ranking_top_10_country_from/,1.0,0.0,0.0,19769.0,
3747,2020-10-11 16:56:56,1602424616.0,dataengineering,Building a DIY geospatial data management service for $5 a month,j95tbb,bereg0stNPC,,https://www.reddit.com/r/dataengineering/comments/j95tbb/building_a_diy_geospatial_data_management_service/,1.0,10.0,0.0,19769.0,
3748,2020-10-11 17:19:17,1602425957.0,dataengineering,Streaming Data Engineering project for beginners,j966af,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/j966af/streaming_data_engineering_project_for_beginners/,1.0,4.0,0.0,19771.0,
3749,2020-10-11 22:41:26,1602445286.0,dataengineering,"The 12th edition of the data engineering newsletter is out. This week's release is a new set of articles that focus on Facebook's data discovery engine, a look back on Amundsen, Neilsen's AWS lambda design, building AI assistance, best practices on data engineering from Facebook, Airbnb, Linkedin.",j9byxe,vananth22,,https://www.reddit.com/r/dataengineering/comments/j9byxe/the_12th_edition_of_the_data_engineering/,1.0,0.0,0.0,19777.0,
3750,2020-10-11 23:13:31,1602447211.0,dataengineering,TDD (Trust Driven Development) in Data Engineering,j9ckic,ronbarab,,https://www.reddit.com/r/dataengineering/comments/j9ckic/tdd_trust_driven_development_in_data_engineering/,1.0,0.0,0.0,19778.0,"Hey all,
Sharing a blog post on how we integrated Deequ in our open source ETL engine..
Hope ull like it:
https://link.medium.com/590GwKCcvab"
3751,2020-10-11 23:51:21,1602449481.0,dataengineering,Spark data engineers - do you really ever use the snowflake pushdowns feature with scala\python language Dataframe API?,j9d9ff,OmegaConstant,,https://www.reddit.com/r/dataengineering/comments/j9d9ff/spark_data_engineers_do_you_really_ever_use_the/,1.0,0.0,0.0,19779.0,
3752,2020-10-12 00:19:47,1602451187.0,dataengineering,What features would you like to see with PDF extractions or creation?,j9drpj,pdfDOCTOR,,https://www.reddit.com/r/dataengineering/comments/j9drpj/what_features_would_you_like_to_see_with_pdf/,1.0,0.0,0.0,19781.0,
3753,2020-10-12 03:35:09,1602462909.0,dataengineering,"Using PowerBI with Azure Synapse Serverless, First Look",j9gxlv,mim722,,https://www.reddit.com/r/dataengineering/comments/j9gxlv/using_powerbi_with_azure_synapse_serverless_first/,1.0,0.0,0.0,19786.0,
3754,2020-10-12 03:48:18,1602463698.0,dataengineering,Features you would like with PDF Extraction and Creation,j9h4kk,tukasouth,,https://www.reddit.com/r/dataengineering/comments/j9h4kk/features_you_would_like_with_pdf_extraction_and/,1.0,5.0,0.0,19786.0,"  
We are building a new product that lets you extract data from PDF using a simple API in wide range of output formats. You can also create PDF documents at scale within you applications.

I am wondering what features you are missing from PDF extraction or creation functionality? Please comment below.

I will really appreciate if you can take this survey to share you thoughts - 

[https://docs.google.com/forms/d/1v2Hqo99SvBk8wwsLvwTLlSCD07q5RedfuF524tg6hPw/viewform?gxids=7628 ](https://docs.google.com/forms/d/1v2Hqo99SvBk8wwsLvwTLlSCD07q5RedfuF524tg6hPw/viewform?gxids=7628&amp;edit_requested=true)"
3755,2020-10-12 05:50:00,1602471000.0,dataengineering,"10 Key tech skills you need, to become a competent Data Engineer",j9iwv4,techgadgetsguy,,https://www.reddit.com/r/dataengineering/comments/j9iwv4/10_key_tech_skills_you_need_to_become_a_competent/,1.0,1.0,0.0,19786.0,
3756,2020-10-12 06:47:17,1602474437.0,dataengineering,Python Operators,j9jpy0,islamandyourduty,,https://www.reddit.com/r/dataengineering/comments/j9jpy0/python_operators/,1.0,0.0,0.0,19786.0,
3757,2020-10-12 13:00:58,1602496858.0,dataengineering,Fastest way to insert 1.000.000 rows into SQL server table,j9o5i9,Luukv93,,https://www.reddit.com/r/dataengineering/comments/j9o5i9/fastest_way_to_insert_1000000_rows_into_sql/,1.0,1.0,0.0,19792.0,
3758,2020-10-12 13:31:23,1602498683.0,dataengineering,Domain Age younger than 1 year data collect API,j9oifx,glassAlloy,,https://www.reddit.com/r/dataengineering/comments/j9oifx/domain_age_younger_than_1_year_data_collect_api/,1.0,3.0,0.0,19793.0,I am looking for an API that can give me all the less than 1 year old domains.
3759,2020-10-12 14:54:19,1602503659.0,dataengineering,Intellectual Disability Ranking | TOP 10 Country from 1990 to 2017,j9phxl,OutlandishnessOk7324,,https://www.reddit.com/r/dataengineering/comments/j9phxl/intellectual_disability_ranking_top_10_country/,1.0,0.0,0.0,19794.0,
3760,2020-10-12 15:41:50,1602506510.0,dataengineering,Introducing lakeview: A Visibility Tool for AWS S3 Based Data Lakes,j9q5xh,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/j9q5xh/introducing_lakeview_a_visibility_tool_for_aws_s3/,1.0,0.0,0.0,19793.0,
3761,2020-10-12 17:54:22,1602514462.0,dataengineering,Spark decode - pyspark,j9sbhf,thesnail97,,https://www.reddit.com/r/dataengineering/comments/j9sbhf/spark_decode_pyspark/,1.0,3.0,0.0,19796.0,"I'm new to programming and have tried in the past failed miserably multiple times. However, I'm in a company where I need to understand spark code , technically pyspark code with logic. Decode end to end . 

1.How would I do it from scratch. I don't have programming knowledge . 
2. How to become pyspark expert from nothing. Please guide me or mentor me. Its personally frustrating me when I started to learn programming at the age of 35.but it's need and I need to write code else I'm out of the company. How can I master this.

Sorry guys . I'm new to this . Is this the right forum to answer question or not . I don't know . Can anyone please guide me on this with right learning path or mentor me daily ."
3762,2020-10-12 18:46:54,1602517614.0,dataengineering,Mini-project using dbt and Metabase,j9tb9o,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/j9tb9o/miniproject_using_dbt_and_metabase/,1.0,0.0,0.0,19796.0,
3763,2020-10-12 19:10:14,1602519014.0,dataengineering,Mini-project using dbt and Metabase,j9trt7,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/j9trt7/miniproject_using_dbt_and_metabase/,1.0,0.0,0.0,19796.0,
3764,2020-10-12 19:23:37,1602519817.0,dataengineering,Mini-project using dbt and Metabase,j9u0za,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/j9u0za/miniproject_using_dbt_and_metabase/,1.0,0.0,0.0,19797.0,
3765,2020-10-12 20:02:21,1602522141.0,dataengineering,Spark,j9us0f,youthrowthisaway121,,https://www.reddit.com/r/dataengineering/comments/j9us0f/spark/,1.0,0.0,0.0,19797.0,What is it?
3766,2020-10-12 20:35:04,1602524104.0,dataengineering,Where can I learn Apache NiFi?,j9vf9q,The_Mask_Girl,,https://www.reddit.com/r/dataengineering/comments/j9vf9q/where_can_i_learn_apache_nifi/,1.0,0.0,0.0,19798.0,
3767,2020-10-12 22:13:21,1602530001.0,dataengineering,Profiling Python Code In Jupyter Cheat Sheet,j9xdtr,eyaltrabelsi,,https://www.reddit.com/r/dataengineering/comments/j9xdtr/profiling_python_code_in_jupyter_cheat_sheet/,1.0,0.0,0.0,19804.0,
3768,2020-10-12 22:20:18,1602530418.0,dataengineering,What are some of the most important programming languages or system concepts to learn for a sustained future in the data world?,j9xish,TheDataGentleman,,https://www.reddit.com/r/dataengineering/comments/j9xish/what_are_some_of_the_most_important_programming/,1.0,0.0,0.0,19804.0,
3769,2020-10-12 22:50:01,1602532201.0,dataengineering,HPC in the Cloud - Python Package Management - Thursday Evening Livestream,j9y3mr,fluid_numerics,,https://www.reddit.com/r/dataengineering/comments/j9y3mr/hpc_in_the_cloud_python_package_management/,1.0,0.0,0.0,19805.0,
3770,2020-10-12 23:13:49,1602533629.0,dataengineering,"What do I need to learn to become a data engineer? I have some experience with python, SQL, and webscraping.",j9ykop,Fun2badult,,https://www.reddit.com/r/dataengineering/comments/j9ykop/what_do_i_need_to_learn_to_become_a_data_engineer/,1.0,20.0,0.0,19808.0,"I have worked for a company as doing some data analytics and some database admin where I oversaw the data warehouse and validated stuff. I used sql and python to create views and scripts for automating some stuff. I prefer more of the technical stuff so I’m looking into becoming a data engineer. I also have some experience webscraping and creating my own dataset, cleaning using pandas, and doing some visuals using seaborn. 

What are the minimum things I need to know in order to become a data engineer? I realized I’m not too fond of doing data analytics stuff but enjoy building stuff and data cleaning. 

I’m planning on learning some pyspark, have used amazon S3 for Data storage when I used flask to create a site, and have experience with PostgreSQL, MySQL, Maria DB, and NoSql.

Not sure if this is the right place to post but I wanted to get some advice and feedback from Data engineers. I also have a stem degree from a university in USA. Thanks for your input"
3771,2020-10-13 04:39:27,1602553167.0,dataengineering,Have you had an easy time getting hired in this field because it is comparatively untapped and less competitive?,ja42hn,DesolateAbomination,,https://www.reddit.com/r/dataengineering/comments/ja42hn/have_you_had_an_easy_time_getting_hired_in_this/,1.0,0.0,0.0,19808.0,
3772,2020-10-13 05:54:53,1602557693.0,dataengineering,Mini-project using dbt and Metabase,ja57gi,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/ja57gi/miniproject_using_dbt_and_metabase/,1.0,0.0,0.0,19807.0,
3773,2020-10-13 06:00:03,1602558003.0,dataengineering,Mini-project using dbt and Metabase,ja5a2v,youthrowthisaway121,,https://www.reddit.com/r/dataengineering/comments/ja5a2v/miniproject_using_dbt_and_metabase/,1.0,14.0,0.0,19807.0,"Hi everyone,

I hope you are all well. Just wanted to give back and share this mini-project of mine. It's an end-to-end analytics workflow for processing Spotify data - listening history (limit to only last 50, Spotify 😢), top tracks and artists. I used Python for scraping, [dbt](https://www.getdbt.com/) for transformations and loading, Postgres for database, and [Metabase](https://www.metabase.com/) for dashboard. I had some interesting results for my profile, I hope you will discover some good insights as well. I'll try to add more functionalities, feel free to give some feedback. 

[github repo](https://github.com/ftupas/dbt-spotify-analytics/tree/master)

Cheers"
3774,2020-10-13 06:46:52,1602560812.0,dataengineering,Who Gets To Claim The Title Of Data Engineer?,ja5y5q,bi_expert,,https://www.reddit.com/r/dataengineering/comments/ja5y5q/who_gets_to_claim_the_title_of_data_engineer/,1.0,0.0,0.0,19808.0,
3775,2020-10-13 11:17:19,1602577039.0,dataengineering,How BIG is Big Data?,ja992c,gavlaaaaaaaa,,https://www.reddit.com/r/dataengineering/comments/ja992c/how_big_is_big_data/,1.0,0.0,0.0,19809.0,
3776,2020-10-13 14:02:24,1602586944.0,dataengineering,Lemon and Lime Consumption Ranking | TOP 10 Country from 1961 to 2013,jab607,Worth-Cod762,,https://www.reddit.com/r/dataengineering/comments/jab607/lemon_and_lime_consumption_ranking_top_10_country/,1.0,0.0,0.0,19813.0,
3777,2020-10-13 15:16:54,1602591414.0,dataengineering,The 3 Most Interesting Ideas From the Future Data Conference,jac6yt,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/jac6yt/the_3_most_interesting_ideas_from_the_future_data/,1.0,6.0,0.0,19815.0,
3778,2020-10-13 17:34:27,1602599667.0,dataengineering,Extracting data from Excel spreadsheets,jaemvk,danbcooper,,https://www.reddit.com/r/dataengineering/comments/jaemvk/extracting_data_from_excel_spreadsheets/,1.0,0.0,0.0,19820.0,
3779,2020-10-13 17:36:10,1602599770.0,dataengineering,Manage your data lineage as code and give your data partners the traceability they want for how data moves - with examples for how to integrate into your CICD pipeline and even your Faust app!,jaeo2r,treeschema,,https://www.reddit.com/r/dataengineering/comments/jaeo2r/manage_your_data_lineage_as_code_and_give_your/,1.0,0.0,0.0,19820.0,
3780,2020-10-13 18:18:13,1602602293.0,dataengineering,[help] handling foreign keys in ETL Process,jafhh7,Majestic-Jump,,https://www.reddit.com/r/dataengineering/comments/jafhh7/help_handling_foreign_keys_in_etl_process/,1.0,0.0,0.0,19820.0,
3781,2020-10-13 18:46:11,1602603971.0,dataengineering,How to improve your Spark job performace? Deep Study.,jag1t7,panispanizo,,https://www.reddit.com/r/dataengineering/comments/jag1t7/how_to_improve_your_spark_job_performace_deep/,1.0,1.0,0.0,19821.0,"Humble contribution, studying the documentation, articles and information from different sources to extract the key points of performance improvement with spark.

[https://medium.com/empathyco/improving-performance-in-spark-jobs-8a60a56327da](https://medium.com/empathyco/improving-performance-in-spark-jobs-8a60a56327da)"
3782,2020-10-13 18:46:55,1602604015.0,dataengineering,How to improve your Spark job performace? Deep Study.,jag2cz,panispanizo,,https://www.reddit.com/r/dataengineering/comments/jag2cz/how_to_improve_your_spark_job_performace_deep/,1.0,0.0,0.0,19821.0,"Humble contribution, studying the documentation, articles and information from different sources to extract the key points of performance improvement with spark.

[https://medium.com/empathyco/improving-performance-in-spark-jobs-8a60a56327da](https://medium.com/empathyco/improving-performance-in-spark-jobs-8a60a56327da)"
3783,2020-10-13 20:27:54,1602610074.0,dataengineering,"An ML EdTech platform for professionals, anyone interested?",jai32k,mrnerdy59,,https://www.reddit.com/r/dataengineering/comments/jai32k/an_ml_edtech_platform_for_professionals_anyone/,1.0,0.0,0.0,19824.0,
3784,2020-10-13 20:41:56,1602610916.0,dataengineering,How to build Data Pipeline on AWS?,jaid3s,Techtter,,https://www.reddit.com/r/dataengineering/comments/jaid3s/how_to_build_data_pipeline_on_aws/,1.0,1.0,0.0,19824.0,
3785,2020-10-13 21:54:42,1602615282.0,dataengineering,What is a Data Warehouse: Basic Architecture,jajubx,ajhenaor,,https://www.reddit.com/r/dataengineering/comments/jajubx/what_is_a_data_warehouse_basic_architecture/,1.0,0.0,0.0,19826.0,
3786,2020-10-13 22:38:04,1602617884.0,dataengineering,Spark data engineers - do you really ever use the snowflake pushdowns feature with scala\python Dataframe API?,jakpxh,OmegaConstant,,https://www.reddit.com/r/dataengineering/comments/jakpxh/spark_data_engineers_do_you_really_ever_use_the/,1.0,0.0,0.0,19826.0,
3787,2020-10-13 22:39:37,1602617977.0,dataengineering,What is a decent inserting rate when moving CSV data into SQL ?,jakr2c,Luukv93,,https://www.reddit.com/r/dataengineering/comments/jakr2c/what_is_a_decent_inserting_rate_when_moving_csv/,1.0,0.0,0.0,19827.0,
3788,2020-10-14 00:02:20,1602622940.0,dataengineering,Intellipatt recommendation,jame1u,Tamiyo22,,https://www.reddit.com/r/dataengineering/comments/jame1u/intellipatt_recommendation/,1.0,0.0,0.0,19833.0,
3789,2020-10-14 00:15:05,1602623705.0,dataengineering,ETL slowly changing dimensions,jamn98,AliGMaye,,https://www.reddit.com/r/dataengineering/comments/jamn98/etl_slowly_changing_dimensions/,1.0,9.0,0.0,19835.0,"Hello awesome data engineers, I need your help with implementing slowly changing dimensions type 2 using Matillion for Snowflake.I have been able to follow the Matillion guide for Redshift and now the main challenge is how to retire changed rows.Any help will be highly appreciated. 
Link to matillion scd type 2 for redshift: youtube.com/watch?=4LQKkYEI44"
3790,2020-10-14 08:13:59,1602652439.0,dataengineering,FB DE interview,jauhz6,Exchange_Neat,,https://www.reddit.com/r/dataengineering/comments/jauhz6/fb_de_interview/,1.0,17.0,0.0,19855.0,"I am prepping for my DE interview, my recruiter mentioned there will be two rounds

1) SQL + scripting language 
15 min gap and 
2) data modeling and ETL - round

Can some one shed some light on the second part - data modeling and ETL. What sort of questions should I expect ? I don't see much on glassdoor regarding this second part."
3791,2020-10-14 09:27:13,1602656833.0,dataengineering,dbt free course,javfhn,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/javfhn/dbt_free_course/,1.0,0.0,0.0,19855.0,
3792,2020-10-14 14:35:01,1602675301.0,dataengineering,Spain vs Portugal | GDP per capita from 1960 to 2017,jayzdf,Worth-Cod762,,https://www.reddit.com/r/dataengineering/comments/jayzdf/spain_vs_portugal_gdp_per_capita_from_1960_to_2017/,1.0,0.0,0.0,19863.0,
3793,2020-10-14 15:28:26,1602678506.0,dataengineering,What next for a Data engineer,jazq1f,goveyy,,https://www.reddit.com/r/dataengineering/comments/jazq1f/what_next_for_a_data_engineer/,1.0,0.0,0.0,19865.0,
3794,2020-10-14 17:41:56,1602686516.0,dataengineering,Amazon De oa,jb1xvy,sankalpthakur2610,,https://www.reddit.com/r/dataengineering/comments/jb1xvy/amazon_de_oa/,1.0,0.0,0.0,19874.0,
3795,2020-10-14 18:24:15,1602689055.0,dataengineering,How do I prepare for positions both positions Data Engineer as well as Software Engineer - Data Infrastructure/Big Data/ML Platform?,jb2qjn,swapripper,,https://www.reddit.com/r/dataengineering/comments/jb2qjn/how_do_i_prepare_for_positions_both_positions/,1.0,12.0,0.0,19874.0,"I have good familiarity with SQL and Python. Have made custom web applications using that combo, although most of these catered to smaller audience(&lt;100,000 dau). Nothing that qualifies as web-scale. Also, I have limited deployment experience since our team has specialized cloud devops folks - hence I possess very limited knowledge of various cloud solutions and their offerings.

With that in mind, I  want guidance as to how can I prepare for both aforementioned positions. Ideally, I need assistance with preparation material and personal portfolio projects that will help me position myself as a good candidate for these job roles. Have already begun leetcoding last week. 

I'm willing to put in daily 2-3 hours spanning over the next 2 months. Right now, I feel lost and quite overwhelmed. Would really appreciate if the sub can help me create a preparation plan that maximizes bang for buck. Forgive the grammatical errors as I'm not a native English speaker.

Feel free to ask any additional information/clarification that might help."
3796,2020-10-14 18:39:29,1602689969.0,dataengineering,20 Sorting Algorithms Visualized,jb3199,compilerstuck,,https://www.reddit.com/r/dataengineering/comments/jb3199/20_sorting_algorithms_visualized/,1.0,1.0,0.0,19874.0,
3797,2020-10-14 18:57:36,1602691056.0,dataengineering,Nob question re: AWS S3 Bucket technique,jb3e55,andresg3,,https://www.reddit.com/r/dataengineering/comments/jb3e55/nob_question_re_aws_s3_bucket_technique/,1.0,5.0,0.0,19875.0,"I often come across projects (mostly academic projects) where data is uploaded from a source to a ""landing bucket"" in S3, then data is copied to a ""stage"" or ""working"" bucket and from there data is sent for processing. 

I'm trying to understand why this approach. Instead of just uploading directly to a ""working"" or ""staging"" bucket and then ship data to be processed. I'm not sure why the extra step of the landing zone.

Any insight would be most greatly appreciated."
3798,2020-10-14 21:39:52,1602700792.0,dataengineering,How Airbnb trained 5K+ Employees to use data in their day to day job,jb6ody,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/jb6ody/how_airbnb_trained_5k_employees_to_use_data_in/,1.0,6.0,0.0,19874.0,
3799,2020-10-14 23:04:25,1602705865.0,dataengineering,Spark and Docker: Your Spark development cycle just got 10x faster !,jb8cun,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/jb8cun/spark_and_docker_your_spark_development_cycle/,1.0,2.0,0.0,19876.0,
3800,2020-10-14 23:52:35,1602708755.0,dataengineering,Adapting SQL queries to Airflow for proper backfilling,jb9ccm,levelworm,,https://www.reddit.com/r/dataengineering/comments/jb9ccm/adapting_sql_queries_to_airflow_for_proper/,1.0,10.0,0.0,19878.0,"Hi experts,

I have an aggregate Vertica SQL query that initially I only ran once:

```
-- Original query without taking into consideration of the frequency (4 hour)
-- So essentially this is a big load into the empty table
INSERT INTO table_name
SELECT UserId, AttempNumber, SUM(Gold) AS GoldObtained
WHERE AttempNumber IS NOT NULL
```

Now that I need to run it incrementally every hour, let's say everytime I ran it I want to fetch data of previous full hour, as database is near real-time:

```
INSERT INTO table_name
SELECT UserId, AttempNumber, SUM(Gold) AS GoldObtained
WHERE AttempNumber IS NOT NULL 
AND EventTime BETWEEN DATE_TRUNC('HOUR', NOW()) AND TIMESTAMPADD('HOUR', -1, DATE_TRUNC('HOUR', NOW()))
```

My question is: If I want to schedule it with Airflow, how should I modify the query so that Airflow can properly backfill missing hours (say somehow previous 3 full hours were missed)?"
3801,2020-10-15 11:25:37,1602750337.0,dataengineering,Airflow tasks are hanging when running ML inference,jbjnsl,RoughWorking,,https://www.reddit.com/r/dataengineering/comments/jbjnsl/airflow_tasks_are_hanging_when_running_ml/,1.0,0.0,0.0,19898.0,
3802,2020-10-15 13:44:24,1602758664.0,dataengineering,Microsoft Azure Data Fundamentals [DP-900]: All You Need To Know,jbl7kl,Sahil2113,,https://www.reddit.com/r/dataengineering/comments/jbl7kl/microsoft_azure_data_fundamentals_dp900_all_you/,1.0,5.0,0.0,19903.0,"**DP-900 Microsoft Azure Data Fundamentals Certification** is intended for those candidates who want to start working with data, get basic skills in cloud data services, and also build foundational knowledge of cloud data services in Microsoft Azure.

This Certification is for **Data professionals, Data architects, and Business intelligence professionals** who want to learn about the data platform technologies existing on Microsoft Azure. To learn more, check out this video on - [Microsoft Azure Data Fundamentals DP-900](https://www.youtube.com/watch?v=Fi9qa1U8c9A&amp;feature=youtu.be&amp;utm_source=reddit&amp;utm_medium=referral&amp;utm_campaign=dp90011_oct20_rdataengineering), and refresh your understanding."
3803,2020-10-15 15:00:43,1602763243.0,dataengineering,Barley Production Ranking | TOP 10 Country from 1961 to 2014,jbm7b3,IsopodLumpy6508,,https://www.reddit.com/r/dataengineering/comments/jbm7b3/barley_production_ranking_top_10_country_from/,1.0,0.0,0.0,19905.0,
3804,2020-10-15 16:30:03,1602768603.0,dataengineering,DataStage Certification exam,jbnk5x,ashay_t,,https://www.reddit.com/r/dataengineering/comments/jbnk5x/datastage_certification_exam/,1.0,0.0,0.0,19908.0,
3805,2020-10-15 17:33:10,1602772390.0,dataengineering,What do you(interviewee) ask during DE interviews?,jbonj5,ExternalPanda,,https://www.reddit.com/r/dataengineering/comments/jbonj5/what_do_youinterviewee_ask_during_de_interviews/,1.0,0.0,0.0,19909.0,
3806,2020-10-15 22:57:20,1602791840.0,dataengineering,Snowflake Cost,jbv08l,joshtree41,,https://www.reddit.com/r/dataengineering/comments/jbv08l/snowflake_cost/,1.0,0.0,0.0,19918.0,
3807,2020-10-15 23:44:46,1602794686.0,dataengineering,Data observability - what works/what doesn't,jbvxbz,mkvor8,,https://www.reddit.com/r/dataengineering/comments/jbvxbz/data_observability_what_workswhat_doesnt/,1.0,0.0,0.0,19920.0,
3808,2020-10-16 10:57:31,1602835051.0,dataengineering,"Learn Machine Learning Algorithms from Scratch in Python like Neural Network, Decision Tree, Logistic Regression",jc5tu0,dhiraj8899,,https://www.reddit.com/r/dataengineering/comments/jc5tu0/learn_machine_learning_algorithms_from_scratch_in/,1.0,2.0,0.0,19941.0,
3809,2020-10-16 11:47:20,1602838040.0,dataengineering,Big Data Engineering: Live Coding Round,jc6dqi,nilbro,,https://www.reddit.com/r/dataengineering/comments/jc6dqi/big_data_engineering_live_coding_round/,1.0,0.0,0.0,19941.0,
3810,2020-10-16 13:55:07,1602845707.0,dataengineering,How do you store external data sources into cloud,jc7qk7,powok,,https://www.reddit.com/r/dataengineering/comments/jc7qk7/how_do_you_store_external_data_sources_into_cloud/,1.0,0.0,0.0,19944.0,
3811,2020-10-16 14:01:20,1602846080.0,dataengineering,Yam Production Ranking | TOP 10 Country from 1961 to 2018,jc7t3a,Aggressive_Ad_2989,,https://www.reddit.com/r/dataengineering/comments/jc7t3a/yam_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,19944.0,
3812,2020-10-16 15:13:34,1602850414.0,dataengineering,Awesome Data Engineering learning path,jc8p2p,SnirD,,https://www.reddit.com/r/dataengineering/comments/jc8p2p/awesome_data_engineering_learning_path/,1.0,0.0,0.0,19945.0,
3813,2020-10-16 17:07:14,1602857234.0,dataengineering,Awesome data engineering learning path,jcaflo,SnirD,,https://www.reddit.com/r/dataengineering/comments/jcaflo/awesome_data_engineering_learning_path/,1.0,36.0,0.0,19948.0,
3814,2020-10-16 18:35:06,1602862506.0,dataengineering,Optimal streaming data workflow,jcc00k,jsd2358,,https://www.reddit.com/r/dataengineering/comments/jcc00k/optimal_streaming_data_workflow/,1.0,7.0,0.0,19951.0,"Hey all! Running through a workflow issue. Currently my workflow looks like this:

    API stream -&gt; S3 bucket -&gt; SQS event -&gt; snowflake staging table -&gt; snowpipe auto ingest -&gt; target table

There is just too much latency between staging and target. Any suggestions how I can optimize (happy to scrap/bring in new technologies)?"
3815,2020-10-16 19:14:58,1602864898.0,dataengineering,Python comments,jccs2a,islamandyourduty,,https://www.reddit.com/r/dataengineering/comments/jccs2a/python_comments/,1.0,0.0,0.0,19953.0,
3816,2020-10-16 20:02:18,1602867738.0,dataengineering,Advice on moving to Data Engineering,jcdooj,The-Loaded-Goat,,https://www.reddit.com/r/dataengineering/comments/jcdooj/advice_on_moving_to_data_engineering/,1.0,0.0,0.0,19954.0,
3817,2020-10-16 20:08:06,1602868086.0,dataengineering,Ingesting data from spreadsheets into a database,jcdsmt,danbcooper,,https://www.reddit.com/r/dataengineering/comments/jcdsmt/ingesting_data_from_spreadsheets_into_a_database/,1.0,0.0,0.0,19954.0,
3818,2020-10-16 22:35:30,1602876930.0,dataengineering,What does a system design interview look like for a senior/mid level Data Engineer?,jcgmwq,0xkn,,https://www.reddit.com/r/dataengineering/comments/jcgmwq/what_does_a_system_design_interview_look_like_for/,1.0,0.0,0.0,19959.0,
3819,2020-10-17 01:37:55,1602887875.0,dataengineering,Any preference of OS for data engineering?,jcjtk6,vizbird,,https://www.reddit.com/r/dataengineering/comments/jcjtk6/any_preference_of_os_for_data_engineering/,1.0,7.0,0.0,19967.0,"I have been working on Windows for a while and Linux on some personal projects, but my new job gives me the option of selecting Mac. I haven't used Mac in a while and wanted to hear what other data engineers are using and how it works out. With so much being cloud based and tools being platform agnostic I assume there isn't much difference between them."
3820,2020-10-17 01:55:28,1602888928.0,dataengineering,Apache Parquet column keyvalue pair metadata,jck3lg,bhontz,,https://www.reddit.com/r/dataengineering/comments/jck3lg/apache_parquet_column_keyvalue_pair_metadata/,1.0,0.0,0.0,19968.0,
3821,2020-10-17 03:02:24,1602892944.0,dataengineering,[D] Open source project for life learning learning,jcl58q,awannaphasch2016,,https://www.reddit.com/r/dataengineering/comments/jcl58q/d_open_source_project_for_life_learning_learning/,1.0,0.0,0.0,19968.0,
3822,2020-10-17 12:06:47,1602925607.0,dataengineering,How to Clean Text Data at the Command Line by EzzEddin Abdullah,jcrw8p,ezzeddinabdallah,,https://www.reddit.com/r/dataengineering/comments/jcrw8p/how_to_clean_text_data_at_the_command_line_by/,1.0,0.0,0.0,19986.0,
3823,2020-10-17 12:09:14,1602925754.0,dataengineering,How to Clean Text Data at the Command Line,jcrx81,ezzeddinabdallah,,https://www.reddit.com/r/dataengineering/comments/jcrx81/how_to_clean_text_data_at_the_command_line/,1.0,8.0,0.0,19986.0,
3824,2020-10-17 12:46:57,1602928017.0,dataengineering,What next for a Data engineer,jcscom,goveyy,,https://www.reddit.com/r/dataengineering/comments/jcscom/what_next_for_a_data_engineer/,1.0,0.0,0.0,19986.0,
3825,2020-10-17 13:45:24,1602931524.0,dataengineering,World Cup Champions Ranking from 1930 - 2018,jct0ol,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jct0ol/world_cup_champions_ranking_from_1930_2018/,1.0,0.0,0.0,19986.0,
3826,2020-10-17 20:47:50,1602956870.0,dataengineering,Data Engineering - Beginner's Guide.,jcznjr,jitendergosain,,https://www.reddit.com/r/dataengineering/comments/jcznjr/data_engineering_beginners_guide/,1.0,0.0,0.0,20000.0,
3827,2020-10-18 09:59:55,1603004395.0,dataengineering,Twitter data analysis for the lazy in Elastic Stack (Xbox VS PlayStation),jdbq7w,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/jdbq7w/twitter_data_analysis_for_the_lazy_in_elastic/,1.0,0.0,0.0,20018.0,
3828,2020-10-18 13:39:49,1603017589.0,dataengineering,BRICS GDP Ranking from 1960 to 2018,jde6j4,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jde6j4/brics_gdp_ranking_from_1960_to_2018/,1.0,0.0,0.0,20023.0,
3829,2020-10-18 17:52:13,1603032733.0,dataengineering,What would be the most optimized and cost effective AWS solution to this portfolio project?,jdhlmt,swapripper,,https://www.reddit.com/r/dataengineering/comments/jdhlmt/what_would_be_the_most_optimized_and_cost/,1.0,9.0,0.0,20032.0,"For a personal project, I'm planning to parse every single reddit comment from these[ pushshift dumps](https://files.pushshift.io/reddit/comments/) for 20 [largest subreddits](https://subredditstats.com/).


Obviously I won't be able to do this with the AWS free tier plan as datasets are HUGE files. Each monthly dump on average is ~10GB compressed JSON file , and I need to filter it based on the field('subreddit').


So I'm asking this sub hoping for help - What is the most efficient and cost effective pipeline I can set up for this project need? Please note that I leave in a third world country so even hourly costs(say $1.00/hr) also will add up very quickly for me. Hence, I want to check folks here to suggest one-time pipeline architecture to minimize the dollar cost.

Let me know if you need any further clarification on the use case and I can provide explanations.

Thanks in advance!"
3830,2020-10-18 19:38:36,1603039116.0,dataengineering,Career advice needed regarding engineering path between data and machine learning engineering?,jdjfpg,CroatianCrystalline,,https://www.reddit.com/r/dataengineering/comments/jdjfpg/career_advice_needed_regarding_engineering_path/,1.0,25.0,0.0,20036.0,"I have a bachelor's in theoretical physics and am currently pursuing a one-year diploma in software engineering. The company I am interning with for 6 months uses Big Data and Data Engineering technologies: Spark, Hadoop, AWS, SQL, Python/Java, ETL, etc.

At the end of the diploma I have the option to study for an extra year and obtain a Master's in AI/Machine Learning. I am debating either staying on with my interning company or pursuing the masters.

What would you advise career-wise? For example, would the data engineering background and the machine learning masters make a career as a machine learning engineer possible? I have no statistics knowledge but I am confident I could teach myself..."
3831,2020-10-18 20:24:36,1603041876.0,dataengineering,Do AWS certifications make up for lack of a degree when it comes to HR?,jdk9jc,SteezeNYC,,https://www.reddit.com/r/dataengineering/comments/jdk9jc/do_aws_certifications_make_up_for_lack_of_a/,1.0,0.0,0.0,20036.0,
3832,2020-10-18 20:42:21,1603042941.0,dataengineering,The Data Janitor Letters - September 2020,jdkl6b,soobrosa,,https://www.reddit.com/r/dataengineering/comments/jdkl6b/the_data_janitor_letters_september_2020/,1.0,0.0,0.0,20039.0,
3833,2020-10-18 21:17:20,1603045040.0,dataengineering,need data in a bad way,jdl881,Interesting_Set7914,,https://www.reddit.com/r/dataengineering/comments/jdl881/need_data_in_a_bad_way/,1.0,1.0,0.0,20039.0,
3834,2020-10-18 21:20:38,1603045238.0,dataengineering,can someone help me get data,jdladj,Interesting_Set7914,,https://www.reddit.com/r/dataengineering/comments/jdladj/can_someone_help_me_get_data/,1.0,3.0,0.0,20039.0,
3835,2020-10-18 21:37:10,1603046230.0,dataengineering,"The 13th edition of the data engineering newsletter is out. This week's release is a new set of articles that focus on the success stories and the importance of Kafka, data quality tools landscape, MLOps tools landscape, schema evolution, and airflow DAG best practices from Twitter, Grab.",jdllch,vananth22,,https://www.reddit.com/r/dataengineering/comments/jdllch/the_13th_edition_of_the_data_engineering/,1.0,0.0,0.0,20041.0,
3836,2020-10-18 22:09:32,1603048172.0,dataengineering,beginner's guide for Data Engineers &amp; Data Architects.,jdm6di,jitendergosain,,https://www.reddit.com/r/dataengineering/comments/jdm6di/beginners_guide_for_data_engineers_data_architects/,1.0,0.0,0.0,20044.0,
3837,2020-10-19 00:55:13,1603058113.0,dataengineering,/r/dataengineering hit 20k subscribers yesterday,jdp74r,TrendingB0T,,https://www.reddit.com/r/dataengineering/comments/jdp74r/rdataengineering_hit_20k_subscribers_yesterday/,1.0,0.0,0.0,20047.0,
3838,2020-10-19 02:12:46,1603062766.0,dataengineering,AI/Data Engineering vs Cybersecurity,jdqi72,squarcia,,https://www.reddit.com/r/dataengineering/comments/jdqi72/aidata_engineering_vs_cybersecurity/,1.0,0.0,0.0,20053.0,"Hi guys, I’m close to graduation and I have to decide which course to take so there are two choices. 
I like AI much more but I’m afraid the market will be saturated, while cybersecurity intrigues me but at the same time I don’t want to go check networks and do very automatic things. 
I like to analyze data much more than networks.
So what would you recommend?"
3839,2020-10-19 06:48:13,1603079293.0,dataengineering,Interview python test with hackerank?,jdul50,pbj800100,,https://www.reddit.com/r/dataengineering/comments/jdul50/interview_python_test_with_hackerank/,1.0,2.0,0.0,20060.0,"Has anyone done one of these and what's it like? Is there a way to test your code as you go? I've literally never taken a python test. I use python everyday at work but my laptop just had to be rebuilt and I'm still waiting on IT to reinstall python for me, so wondering if I should wait to take it until I can use a dev environment on my own computer while I take the test."
3840,2020-10-19 06:59:05,1603079945.0,dataengineering,Good Data science course with projects,jduqc4,jedi_22,,https://www.reddit.com/r/dataengineering/comments/jduqc4/good_data_science_course_with_projects/,1.0,0.0,0.0,20060.0,
3841,2020-10-19 07:02:55,1603080175.0,dataengineering,What kind of computer are you using?,jdus5p,Tamiyo22,,https://www.reddit.com/r/dataengineering/comments/jdus5p/what_kind_of_computer_are_you_using/,1.0,4.0,0.0,20060.0,"Computer science student here. I was lucky enough to be able to borrow my husband's computer this summer to complete my machine learning internship, but with both of us in school currently, I know my mac book air isn't going to cut it for my next round of classes, and any other internships I may have. It has been difficult to say the least. Does anyone have any recommendations?"
3842,2020-10-19 11:57:44,1603097864.0,dataengineering,Apache Druid 0.20.0 officially released! Release notes in the link.,jdy7rt,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/jdy7rt/apache_druid_0200_officially_released_release/,1.0,0.0,0.0,20069.0,
3843,2020-10-19 13:35:40,1603103740.0,dataengineering,News: Streamr has partnered with GSMA. Streamr and GSMA will work together to allow three mobile network operators to monetise their user data ethically through the Streamr #DataUnion framework. Learn more!,jdz9oo,thamilton5,,https://www.reddit.com/r/dataengineering/comments/jdz9oo/news_streamr_has_partnered_with_gsma_streamr_and/,1.0,0.0,0.0,20072.0,
3844,2020-10-19 14:00:05,1603105205.0,dataengineering,Pea Production Ranking | TOP 10 Country from 1961 to 2018,jdzjp4,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jdzjp4/pea_production_ranking_top_10_country_from_1961/,1.0,0.0,0.0,20074.0,
3845,2020-10-19 16:25:25,1603113925.0,dataengineering,Building up production ready architecture with Apache Airflow on AWK EKS,je1l2o,marclamberti,,https://www.reddit.com/r/dataengineering/comments/je1l2o/building_up_production_ready_architecture_with/,1.0,0.0,0.0,20079.0,
3846,2020-10-20 12:40:22,1603186822.0,dataengineering,How to Install Presto on a Cluster and Query Distributed Data on Apache Hive and HDFS,jem97e,njanakiev,,https://www.reddit.com/r/dataengineering/comments/jem97e/how_to_install_presto_on_a_cluster_and_query/,0.0,0.0,0.0,20119.0,
3847,2020-10-20 13:32:00,1603189920.0,dataengineering,Understanding Basics Of SVM With Example And Python Implementation,jemva9,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jemva9/understanding_basics_of_svm_with_example_and/,1.0,0.0,0.0,20122.0,
3848,2020-10-20 13:34:16,1603190056.0,dataengineering,Unsupervised Learning in Machine Learning,jemwd2,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jemwd2/unsupervised_learning_in_machine_learning/,1.0,0.0,0.0,20122.0,
3849,2020-10-20 13:35:33,1603190133.0,dataengineering,An interview with the founders of Monte Carlo about how observability of your data platform contributes to higher data quality and reduces data downtime.,jemwx7,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/jemwx7/an_interview_with_the_founders_of_monte_carlo/,1.0,0.0,0.0,20122.0,
3850,2020-10-20 13:59:35,1603191575.0,dataengineering,Schizophrenia Ranking | TOP 10 Country from 1990 to 2016,jen84u,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jen84u/schizophrenia_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,20123.0,
3851,2020-10-20 15:55:01,1603198501.0,dataengineering,Practicalities of normalization,jeowka,Touvejs,,https://www.reddit.com/r/dataengineering/comments/jeowka/practicalities_of_normalization/,1.0,0.0,0.0,20128.0,
3852,2020-10-20 17:16:38,1603203398.0,dataengineering,are there DE positions that rely less on building ETLs?,jeqcjq,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/jeqcjq/are_there_de_positions_that_rely_less_on_building/,1.0,0.0,0.0,20129.0,
3853,2020-10-20 20:27:52,1603214872.0,dataengineering,Is anyone familiar with XComs in apache airflow?,jeu7po,graciousgroob,,https://www.reddit.com/r/dataengineering/comments/jeu7po/is_anyone_familiar_with_xcoms_in_apache_airflow/,1.0,7.0,0.0,20132.0,"They seem much more confusing than they have to be (compared to variables). Do you know a good tutorial? (i've done a lot of googling but nothing i've found is very helpful.) 

ALternatively, can you describe how to use them easily?

One thing that really confuses me is when to use which syntax:

    context['ti'].xcom_pull(task_ids='**ids**')
    context['task_instance'].xcom_pull(task_ids='**ids**')
    {{jinja templating of same thing}}

    etc. I'm sure there are more.

Why is this so confusing?"
3854,2020-10-20 20:48:57,1603216137.0,dataengineering,"ZD Net - Fluree, the graph database with blockchain inside, goes open source",jeung6,km2day,,https://www.reddit.com/r/dataengineering/comments/jeung6/zd_net_fluree_the_graph_database_with_blockchain/,1.0,0.0,0.0,20132.0,
3855,2020-10-20 21:08:30,1603217310.0,dataengineering,Scraping a table in a PDF and then test the data quality in Python,jev285,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/jev285/scraping_a_table_in_a_pdf_and_then_test_the_data/,1.0,0.0,0.0,20132.0,
3856,2020-10-20 21:51:19,1603219879.0,dataengineering,Help with xcoms in apache airflow?,jevzbh,graciousgroob,,https://www.reddit.com/r/dataengineering/comments/jevzbh/help_with_xcoms_in_apache_airflow/,1.0,0.0,0.0,20135.0,"I am trying to write a pair of operators - one operators and one sensor - that share data via xcoms in apache airflow.

I want to initialize the sensor with an id attribute that gets produced by the operator like this:

    class myOperator(BaseOperator):
        def __init__(self, *args, **kwargs):
            super(myOperator, self).__init__(*args, **kwargs))
        
        def execute(self):
            #do stuff that produces variable ""idval""
            return idval #stores idval as and xcom

    class mySensor(BaseSensorOperator):
        def __init__(self, upstreamTaskId, *args, **kwargs):
            super()__init__(*args, **kwargs)
            **self.idval = kwargs['task_instance'].xcom_pull(task_ids = upstreamTaskId)**
     
    def poke(self):
        #poke the id

However I keep getting errors on the xcom_pull line. For example right now python doesn't know what ""task_instance"" is. Do I have to pass it as an argument?"
3857,2020-10-20 21:59:12,1603220352.0,dataengineering,Enabling Full-text Search with Change Data Capture in a Legacy Application,jew4w9,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jew4w9/enabling_fulltext_search_with_change_data_capture/,1.0,0.0,0.0,20134.0,
3858,2020-10-20 23:55:25,1603227325.0,dataengineering,Is it that hard to build User &amp; Company data models for freemium b2b SaaS?,jeyf3g,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/jeyf3g/is_it_that_hard_to_build_user_company_data_models/,1.0,0.0,0.0,20142.0,
3859,2020-10-21 01:41:44,1603233704.0,dataengineering,DIY Geospatial Data Management Part 2: Building The Geoservice,jf0h4o,bereg0stNPC,,https://www.reddit.com/r/dataengineering/comments/jf0h4o/diy_geospatial_data_management_part_2_building/,1.0,0.0,0.0,20145.0,
3860,2020-10-21 01:47:29,1603234049.0,dataengineering,DIY Geospatial Data Management For $5 A Month Part 2: Building The Geoservice,jf0kxm,bereg0stNPC,,https://www.reddit.com/r/dataengineering/comments/jf0kxm/diy_geospatial_data_management_for_5_a_month_part/,1.0,1.0,0.0,20146.0,
3861,2020-10-21 02:52:30,1603237950.0,dataengineering,New to Data Engineering,jf1qkh,Smashfro96,,https://www.reddit.com/r/dataengineering/comments/jf1qkh/new_to_data_engineering/,1.0,2.0,0.0,20148.0,"Hi guys, I am new to the sub and new to the field. I am a computational mathematics major that graduated in Dec 2018. Since then I have been working as a software developer. I have an interview tomorrow for a junior days engineer. What should I expect and will my major be a benefit for me? Thanks and I look forward to being on this sub!"
3862,2020-10-21 04:21:08,1603243268.0,dataengineering,Trying to switch to Data Engineering. Please critique my resume,jf39du,Scalar_Mikeman,,https://www.reddit.com/r/dataengineering/comments/jf39du/trying_to_switch_to_data_engineering_please/,1.0,14.0,0.0,20153.0,"[Here's my resume](https://i.imgur.com/1YiDOvl.png). My friend let me know about a job in his organization that sounds like it would be a good chance for me to transition into Data Engineering. Honestly aside from making reports a lot of my job is trouble shooting workstations, networking and administration. Decided to leave all that off to try and focus on the role they are looking to fill. Hoping some of the good people here could let me know if there's anything on my resume I'm missing that does not look good, should be rearranged or if I'm just under qualified in general. Aside from what I've put on my resume I've:

Made one open source contribution to scikit learn. Just fixed an error message

Written one article on Medium on how to install and connect to PostGreSQL remotely on a Raspberry Pi

Made one YouTube Video on how to install Go and get Gowitness working on Linux

&amp;#x200B;

Taken these courses:

Version Control with Git - Udacity

 Android Basics: Multiscreen Apps - Udacity

Android Basics: User Interface - Udacity

Android Basics: User Input - Udacity

CS50 - EdX

Introduction to Linux - EdX

Implementing a SQL Data Warehouse - ONLC

Administering  SQL Databases - ONLC

Practical Ethical Hacking - Udemy

&amp;#x200B;

Here is the job description if it helps:

&gt;QUALIFICATIONS: To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.  
&gt;  
&gt;  EDUCATION and/or EXPERIENCE:   
&gt;  
&gt; Undergraduate degree from an accredited university or college; degree in a quantitative or technical discipline is preferred   
&gt;  
&gt;5-7 years of analytical experience, with 3+ years of financial services industry experience preferred • Extensive experience with and strong understanding of relational databases    
&gt;  
&gt;Experience with cloud-based platforms and data query tools    
&gt;  
&gt;CRM knowledge preferred    
&gt;  
&gt;Experience with scrum or other agile methodologies preferred    
&gt;  
&gt;Experience with working in a cloud-based environment    
&gt;  
&gt;  
&gt;  
&gt;INTERPERSONAL/COMMUNICATION/LANGUAGE SKILLS:   
&gt;  
&gt;Personal initiative •  
&gt;  
&gt;Ability to communicate complex information and analysis in written and graphical form   
&gt;  
&gt;Exceptional interpersonal and communication skills in order to work collaboratively across multiple business units  TECHNICAL SKILLS: • Broad overall technical skill set   
&gt;  
&gt;Expert SQL (Microsoft T-SQL or similar)   
&gt;  
&gt;Cloudera Hadoop (Impala or Hive) or similar    
&gt;  
&gt;Experience with R, Python, .NET, or similar preferred  Experience with Tableau Server and/or Desktop, or similar BI tool"
3863,2020-10-21 13:47:09,1603277229.0,dataengineering,Palm Oil Production Ranking | TOP 10 Country from 1961 to 2018,jfaj5e,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jfaj5e/palm_oil_production_ranking_top_10_country_from/,1.0,0.0,0.0,20175.0,
3864,2020-10-21 14:43:26,1603280606.0,dataengineering,Training a model in an area where hard data is scarce,jfb8ah,Flewizzle,,https://www.reddit.com/r/dataengineering/comments/jfb8ah/training_a_model_in_an_area_where_hard_data_is/,1.0,3.0,0.0,20177.0,"I am doing a dissertation project on attribution modelling to measure the impact of marketing spend across platforms (Google, Facebook etc) on business KPIs. Currently looking into various models such as Markov Chains, Shapely Value, Recurrent Neural Network, Deep Neural Network, Bagged Logistic Regression.    


A question that has sprung to mind is how does one go about training an attribution model? Given the scarcity of hard data inherent in the nature of the topic."
3865,2020-10-21 16:29:02,1603286942.0,dataengineering,How hard would the transition be? Analyst -&gt; Engineer,jfcsyb,ivantf15,,https://www.reddit.com/r/dataengineering/comments/jfcsyb/how_hard_would_the_transition_be_analyst_engineer/,1.0,15.0,0.0,20179.0,"Right now, I work as a data analyst at a relatively big company. A lot of what I do is developing dashboards, statistical analysis/visualization/modeling in R and Python. Due to the nature of our department, we get little analytics support so I also end up building a lot of data pipelines to actually get what I need. They're not overly complicated, usually things ranging from standard SQL queries to Python scripts that construct tables via data from APIs. 

After 17 months, I've realized I tend to enjoy the engineering / pure code side of things more than going back and forth with program owners daily. During undergrad, I spent 15 months as a software engineer intern with two companies, so my background is sort of in the dev side of things. Curious for those in the field, how difficult would it be to transition to a role in data engineering?

For context, 23yo, degrees in Data Sci &amp; Math, minors in Stats &amp; Comp Sci. Use R, Python, SQL daily and used to write a lot of Java and C#."
3866,2020-10-21 20:16:58,1603300618.0,dataengineering,Dokra - new Python library for Middleware Injection,jfh5n6,DiratYokra,,https://www.reddit.com/r/dataengineering/comments/jfh5n6/dokra_new_python_library_for_middleware_injection/,1.0,0.0,0.0,20186.0,
3867,2020-10-21 20:53:14,1603302794.0,dataengineering,Python ETL: How to Improve on Pandas?,jfhvnh,infazz,,https://www.reddit.com/r/dataengineering/comments/jfhvnh/python_etl_how_to_improve_on_pandas/,1.0,11.0,0.0,20186.0,"Hi everyone!

In my job the team I am on is in charge of managing data for our division. Part of this is integrating multiple applications' data together (sometimes a database) and storing that in different database.

Side-note: We use multiple database technologies, so I have scripts to move data from Postgres to MSSQL (for example).

Currently what I am using is Pandas to for all of the ETL. I am pulling data from various systems and storing all of it in a Pandas DataFrame while transforming and until it needs to be stored in the database. From there I am using \`.to\_sql()\` and chunking to replace the database table each time the pipeline runs.

This has a few of issues associated with it: 

1. It's difficult to find where data already exists in the database and update it (this is why I'm opting to replace the table each time)
2. The data is small now, but it continues to grow. We will eventually outgrow Pandas
3. Pandas does not allow me to set a primary key column (that I know of)

I have recently been looking at sqllite as a replace for Pandas, but I'm not exactly sure how this is supposed to be done. If anyone uses sqllite I would love to hear how you are doing it or see some code from one of your pipelines."
3868,2020-10-21 21:51:04,1603306264.0,dataengineering,A gentle introduction to the Hive connector for Presto,jfj1hr,bitsondatadev,,https://www.reddit.com/r/dataengineering/comments/jfj1hr/a_gentle_introduction_to_the_hive_connector_for/,1.0,0.0,0.0,20187.0,
3869,2020-10-21 22:43:14,1603309394.0,dataengineering,Question about building a pipeline in AWS,jfk2wx,joehfb,,https://www.reddit.com/r/dataengineering/comments/jfk2wx/question_about_building_a_pipeline_in_aws/,1.0,4.0,0.0,20192.0,"Hi folks,

Would you guys suggest any changes to the simplified design I have below based on your experience (ex: if you had any nightmares with certain products), reducing cost, etc?

**Basic requirements:**

1. Several different types of data sources (nosql, sql, S3, ...)

2. Multiple types of data that each has different combo of data source(s)
Example: data type #1 might be all in source #1, but data type #2 might be spread out across data sources #1, #3, #4 and require joining

3. Each data type should get its data extracted from source(s), joined into one schema if necessary, and then exported to a table, file, whathaveyou.
Example: data type #1 should get data from source #1, and then have its own file.
data type #2 should create 1 file after joining data from sources #1, #3, and #4

4. Because the data sources might have all sorts of data we don't want in the final destination, we need to filter.  
Ex: only extract data where col A = ""I am awesome"" - and only write this to the final destination.
Ex: only extract data that was created in Q2-Q3 2020.

5. This pipeline needs to be able to handle large data - the final output could be several GBs *per data type*.  The data sources might contain a lot more. 

**Design I had come up with:**

1. Use AWS Glue job - 1 job for each data type.  ETL is a solved problem - I don't want to create custom code for this if I can help it.

2. Use AWS step functions to serve as a coordinator - call glue job data type #1 first if needed, then if needed call glue job data type #2, if needed call glue job data type #3, ...

3. If the data source is S3, inside the AWS Glue job use AWS Athena with the JDBC jar imported to filter the data before loading into Glue.

I used Athena just in case because I had found folks saying online that you can't filter data *before* loading into Glue if data is coming from S3.  This way, if it's true, I don't have to pay for additional network transfer fees for data I don't need or time filtering data in memory in glue job. But then again... maybe $ cost for those two might be cheaper than $ cost for Athena scanning all that data...


**Some concerns:**

1. $ - Glue is quite expensive.  I also saw a couple comments here that Athena can go up in costs quickly.

But on the flip side - I for example don't have experience with Spark, and I don't know how much experience we have available configuring and maintaining clusters on EMR.

2. Overkill? - am I choosing overkill products here for this?  

3. It's not clear to me how much data Glue, Athena can handle in one go.   

4. If in the future we want to be more cloud agnostic, this would tie us to AWS pretty heavily."
3870,2020-10-22 01:54:49,1603320889.0,dataengineering,How to progress as a DE,jfnoa8,plodzik,,https://www.reddit.com/r/dataengineering/comments/jfnoa8/how_to_progress_as_a_de/,1.0,18.0,0.0,20201.0,"I've been hired as a junior data engineer and I suck at programming, feel really insecure of what I'm doing. From very confident person I've become afraid of everything, how to overcome those challanges? Any advice that you'd like to share?"
3871,2020-10-22 02:11:15,1603321875.0,dataengineering,Leading 0's on a Unique Identifier,jfnycp,SuccessfulFarmer,,https://www.reddit.com/r/dataengineering/comments/jfnycp/leading_0s_on_a_unique_identifier/,1.0,7.0,0.0,20201.0,"I have a table for drug prescribers that I am working on at work and have an ID (1 of 3)  that has 7 digits with leading 0's. It is being stored as an int but my manager argues it should be varchar to capture the leading 0's.

This makes no sense to me. Is there an angle that I am missing here?

https://preview.redd.it/wihfwasl4ju51.png?width=1130&amp;format=png&amp;auto=webp&amp;s=462ab24291c7cea6b91e82ea6048819a7659b69b"
3872,2020-10-22 04:19:21,1603329561.0,dataengineering,How to Automate Python Workflows in Prefect (Step-by-Step Guide),jfq28c,jimmyle91,,https://www.reddit.com/r/dataengineering/comments/jfq28c/how_to_automate_python_workflows_in_prefect/,1.0,3.0,0.0,20205.0,"As part of my efforts to build in public and to deepen my understanding by teaching, I wrote a guide on how to get started with orchestrating your workflows using Prefect.

If you want to get started with orchestrating and automating your Python workflows, this is the primer I wish I had when I got started.

[https://lejimmy.com/how-to-automate-python-workflows-in-prefect-step-by-step-guide](https://lejimmy.com/how-to-automate-python-workflows-in-prefect-step-by-step-guide)

I don't consider writing my strong suit , please let me know of any and all suggestions for improvement.

Enjoy!"
3873,2020-10-22 05:08:19,1603332499.0,dataengineering,"Interview presentation advice (extract, transform and store this data)",jfqu91,abunavsa,,https://www.reddit.com/r/dataengineering/comments/jfqu91/interview_presentation_advice_extract_transform/,1.0,2.0,0.0,20207.0,"So ive landed a great interview. Im a graduate of Computer Science and the role is for a Graduate Data Engineer.

For the question I have bare in mind these are some of the role reqs:

\- experience of using programming languages / analytical software such as SQL, Python, R and Power BI

\-  writing ETL scripts and code to make sure the ETL process performs optimally.

&amp;#x200B;

The Question:

Basically a governing body collects data from all schools regarding their pupils to allocate funds according to the number of pupils. It will result in millions of records.

The presentation should demonstrate a data engineering solution to  extract, transform and store this data  for analysis.

&amp;#x200B;

I'd like some advice on how to approach this. I have some thoughts already but honestly I think they're either overcomplicated or way too simple.

Regardless here's my thoughts:

1) Do i consider how to acquire the data? If so, a web form which stores the data in an SQL database.

Now I dont really know where to go from there or if its even remotely correct or if its even something to consider 🤦‍♂️

I'm hoping from your advice something will click and I'll realize the way to go"
3874,2020-10-22 05:49:15,1603334955.0,dataengineering,How can I learn about ETL solutions?,jfrh0y,abunavsa,,https://www.reddit.com/r/dataengineering/comments/jfrh0y/how_can_i_learn_about_etl_solutions/,1.0,0.0,0.0,20207.0,"I would like to create a mock data engineering solution to extract, transform and load data of all schools in a country for example. In this process I’m told that python, SQL, R and Microsoft Power BI will play roles. Where could I find more information to approach this solution?"
3875,2020-10-22 07:21:15,1603340475.0,dataengineering,Is MIT CS50 worth it?,jfstao,hyperandaman,,https://www.reddit.com/r/dataengineering/comments/jfstao/is_mit_cs50_worth_it/,1.0,6.0,0.0,20208.0,"Any self taught or non CS data engineers take the MIT CS50 course? Is it worth it? I’ve watched a couple of lectures. It’s amazing! The professor really makes it entertaining and informative but some lectures are 2 hours and the homework is time intensive. 

I am a non CS data analyst/engineer. Wondering if I should continue and work on the homework? Or if I should just buy a python course in Udemy and practice just python alone?"
3876,2020-10-22 08:58:34,1603346314.0,dataengineering,Cloud Data Engineering,jfu2n5,Anblicks,,https://www.reddit.com/r/dataengineering/comments/jfu2n5/cloud_data_engineering/,1.0,0.0,0.0,20208.0,
3877,2020-10-22 12:54:50,1603360490.0,dataengineering,What will help HealthTech companies achieve agility and scale?,jfwqag,AnishTVSNext,,https://www.reddit.com/r/dataengineering/comments/jfwqag/what_will_help_healthtech_companies_achieve/,1.0,1.0,0.0,20210.0,"HealthCare industry is in flux, coping-up with high demands and unique needs to manage patient experience, clinical care, recovery, and wellness. Despite recent advancements in technology and cognitive sciences, the current situation has shown the industry’s vulnerability in catering to the surge in demands. 

While HealthTech companies are incubating disruptive ideas to address the pressing needs, they are finding challenges to scale expertise across cutting edge technologies. A reliable technology partner with extensive HealthCare competency across the value chain, with an ability to spin-up distributed and agile teams become imperative to help realize disruptive ideas.   

[Click here](https://www.linkedin.com/smart-links/AQEKLlSqPkBZVQ/a29559e5-d018-4ba0-b289-9e8eb43f23c7) to learn more about it"
3878,2020-10-22 12:55:52,1603360552.0,dataengineering,Documentation of a data pipeline in python (with a graph),jfwqo0,qchenevier,,https://www.reddit.com/r/dataengineering/comments/jfwqo0/documentation_of_a_data_pipeline_in_python_with_a/,1.0,8.0,0.0,20210.0,"Hello everyone 👋

Struggling to document my python data pipelines, I've made a small python package to help document a pipeline: it builds a graph visualization of the transformation you do and shows the columns which are created at each step.

Here it is: [https://github.com/qchenevier/pandas-pipeline-graphviz](https://github.com/qchenevier/pandas-pipeline-graphviz)

This is a decorator you'll add to your transformation functions.

Here is an example of the output: [https://raw.githubusercontent.com/qchenevier/pandas-pipeline-graphviz/master/examples/03\_apply\_pandas\_pipeline\_decorator.png](https://raw.githubusercontent.com/qchenevier/pandas-pipeline-graphviz/master/examples/03_apply_pandas_pipeline_decorator.png)

&amp;#x200B;

https://preview.redd.it/89vecjvccmu51.png?width=1224&amp;format=png&amp;auto=webp&amp;s=db33721a129956475a1cad350839a01a89e669f0

Have fun with it !"
3879,2020-10-22 14:01:56,1603364516.0,dataengineering,Africa GDP per capita Ranking | TOP 10 Country from 1960 to 2017,jfxi7f,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jfxi7f/africa_gdp_per_capita_ranking_top_10_country_from/,1.0,0.0,0.0,20212.0,
3880,2020-10-22 14:20:32,1603365632.0,dataengineering,Free Live Virtual Workshop: Hazelcast Platform Essentials,jfxq6i,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jfxq6i/free_live_virtual_workshop_hazelcast_platform/,1.0,0.0,0.0,20213.0,"*Disclaimer: I hate advertisement myself. I looked at the Subreddit rules and this post doesn't seem to infringe them. If you're not interested, feel free to ignore.*

This live virtual workshop is targeted at Developers and Architects who want to understand the concepts behind the Hazelcast platform which combines the power of distributed in-memory data and stream processing.

Combining conceptual overviews with hands-on practice, the attendees will be introduced to the fundamentals of the Hazelcast platform and how they can solve real-world use cases.

Course Outline:

* Overview of the Hazelcast Platform
* Hazelcast IMDG  (In-memory data grid)
* Hazelcast Jet (Batch &amp; Real-time Stream processing)
* Hazelcast Cloud
* Real-world use cases and architectures
* Set up and configure Hazelcast
* Hands-on lab
* Enterprise features

[Register for free](https://hazelcast.com/resources/?resourceType=Live+Virtual+Events)"
3881,2020-10-22 16:07:09,1603372029.0,dataengineering,Most Popular social media platforms,jfz9rd,SnooConfections5700,,https://www.reddit.com/r/dataengineering/comments/jfz9rd/most_popular_social_media_platforms/,1.0,0.0,0.0,20215.0,
3882,2020-10-22 16:51:25,1603374685.0,dataengineering,Flink SQL on Ververica Platform | An End-To-End Platform for developing and operating streaming SQL queries,jg0004,Marksfik,,https://www.reddit.com/r/dataengineering/comments/jg0004/flink_sql_on_ververica_platform_an_endtoend/,1.0,0.0,0.0,20216.0,
3883,2020-10-22 17:27:00,1603376820.0,dataengineering,Building for Collaboration,jg0mi9,kelseyfecho,,https://www.reddit.com/r/dataengineering/comments/jg0mi9/building_for_collaboration/,1.0,0.0,0.0,20218.0,
3884,2020-10-22 19:28:13,1603384093.0,dataengineering,On-Premise alternative for Fivetran,jg2y4h,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/jg2y4h/onpremise_alternative_for_fivetran/,1.0,2.0,0.0,20221.0,"Many businesses these days would like their data to stay in their network or cloud. ETL Tools like Fivetran provide just provides a SaaS option. With [Sprinkle](http://sprinkledata.com/), when it comes to an On-Premise setup, the data is run on the customer's virtual machine within the customer's cloud to ensure no data leaves the network boundary.  
[Sprinkle Data](https://www.sprinkledata.com/), a data integration and analytics tool enables On-Premise setup. Not just that, unlike most tools the payment model is not based on the number of rows, the rows are unlimited with Sprinkle. So, the users wouldn't have to worry about the costs as their business and data scales.

But, if you want a SaaS deployment, we offer the same as well. [Sign up](http://sprinkledata.com/) for a Free Trial today and evaluate for yourself."
3885,2020-10-22 20:07:54,1603386474.0,dataengineering,Streaming Geospatial DB (Realtime IoT analytics scenario),jg3rmt,geogaddi55,,https://www.reddit.com/r/dataengineering/comments/jg3rmt/streaming_geospatial_db_realtime_iot_analytics/,1.0,3.0,0.0,20223.0,"Hey all - I'm hoping to possibly gather some architecture ideas for a streaming database of latitude and longitude coordinates. I am only maybe around 15% a data engineer, mostly doing analytics/ML/visualization, but I'm in a shitty position where I'm needing to improve upon an existing PostgreSQL structure which is buckling under the load of around 9000 coordinate records entered per second. Specifically I am running TimescaleDB, but I'm hoping to possibly abandon this DB in lieu of a beefier architecture. I see options like memSQL, GeoMesa, &amp; Magellan, but I don't know anyone with any experience with these tools. Anyone have any insight or experience in this situation they could share?"
3886,2020-10-22 20:08:23,1603386503.0,dataengineering,Looking for feedback on my parallel computing platform,jg3s0i,ScottSpeculatesSome,,https://www.reddit.com/r/dataengineering/comments/jg3s0i/looking_for_feedback_on_my_parallel_computing/,1.0,6.0,0.0,20223.0,"I've been working for a few months on fastmap - a platform to parallelize arbitrary Python code on the cloud. The idea came after I had to leave my laptop open overnight for the umpteenth time to run a massive job. I knew there should be an easier way without having to spin up a spark cluster or build something custom.

In short, the idea is to replace Python's \`map\` builtin with \`fastmap\` on slow lines of code. Fastmap then divides the job into many small tasks and runs those tasks over multiple machines on the cloud as well as multiple processes on your computer.

This is the source for the client Python library, [https://github.com/fastmap-io/fastmap](https://github.com/fastmap-io/fastmap). I'm linking to this instead of the website to avoid too much self-promotion but there are links to the website in the README. 

So far, I've not shared this beyond a few friends so this Reddit post is pretty-darn-close to a debut. As far as I can tell, everything works, and hopefully, 2000+ lines of unit tests mean something! 🤞

My questions are, coming from your backgrounds, what would you want out of something like this? Do you do a lot of work with Numpy/Pandas and need support for those? How about arbitrary 3rd party libraries from pip? Would you ever want to use something like this as a component of an app? (e.g. running jobs continuously instead of as one-offs)"
3887,2020-10-22 20:23:29,1603387409.0,dataengineering,Nested JSON file and query using SQL,jg42vk,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/jg42vk/nested_json_file_and_query_using_sql/,1.0,0.0,0.0,20224.0,"Hi,

I would like to ask for your help. I'm trying to design a data structure for deliveries. It has to have the following:
1. delivery (id, person_id, person_name)
2. stores (id, name, location)
3. destination (address, city, region)
4. products (id, name, stored_id, count, price)
5. timestamp

The products have to be mapped to specific stores and so there will be many nested fields. Finally, this data has to be written in a JSON file. How should I represent the data in JSON format? And how can this be queried using SQL?"
3888,2020-10-22 20:26:50,1603387610.0,dataengineering,Nested JSON and query using SQL,jg45bw,howdeepisyourhouse,,https://www.reddit.com/r/dataengineering/comments/jg45bw/nested_json_and_query_using_sql/,1.0,6.0,0.0,20225.0,"Hi,

I would like to ask for your help. I'm trying to design a data structure for deliveries. It has to have the following:

- delivery (id, person_id, person_name)
- stores (id, name, location)
- destination (address, city, region)
- products (id, name, stored_id, count, price)
- timestamp

The products have to be mapped to specific stores and so there will be nested fields. Finally, this data has to be written in a JSON file. How should I represent the data in JSON format? And how can this be queried using SQL?"
3889,2020-10-22 21:10:53,1603390253.0,dataengineering,Product Qualified Leads (PQLs): Another way to use your data to generate revenue,jg511d,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/jg511d/product_qualified_leads_pqls_another_way_to_use/,1.0,0.0,0.0,20229.0,
3890,2020-10-23 05:20:50,1603419650.0,dataengineering,How long should data QA take?,jge3ts,mathacc,,https://www.reddit.com/r/dataengineering/comments/jge3ts/how_long_should_data_qa_take/,1.0,8.0,0.0,20237.0,"I'm currently working as the sole data engineer on a team with a few data scientists for about a year and half. Recently we launched a big feature and had a lot of problems with data QA, finding out weeks later that there were bugs in the way that we were calculating some of the data. Now going back and doing a thorough check on everything and fixing all bugs has taken several weeks on top. 

What I'm realizing now is that if we had done the proper data QA since the beginning then we could have avoided all these problems but this feature would have taken twice as long to release. The odd thing is that no one around me seems to acknowledge that QA is something that will take a long time to do, my stories are based around maybe an extra day of QA on top of feature development. This is making me second guess myself that maybe I'm just not doing things the right way. So I want to ask other data engineers here how much effort usually goes into their QA process."
3891,2020-10-23 13:17:18,1603448238.0,dataengineering,Data Masking Market Worth 767.0 Million USD by 2022,jgkfx3,pradnya123,,https://www.reddit.com/r/dataengineering/comments/jgkfx3/data_masking_market_worth_7670_million_usd_by_2022/,1.0,2.0,0.0,20250.0,
3892,2020-10-23 13:20:04,1603448404.0,dataengineering,Data engineering internship,jgkh24,Bothurin,,https://www.reddit.com/r/dataengineering/comments/jgkh24/data_engineering_internship/,1.0,10.0,0.0,20250.0,"I just got offered a Data Engineering internship at a large insurance company after applying for a Data Analytics position. Although the process of cleaning and pipelining data seems interesting I ultimately want to be able to use my skills in AI (25% of my degree is in AI, the other 75% is computer science) so I will probably try to make the move from Data Engineering to Data Science quite quickly. Speaking with the head of IT, he says that the there will be no analytics of any kind in the Data Engineering position. Should I still accept this position in hopes of developing useful skills and being able to transfer to the Data Analytics in the future?"
3893,2020-10-23 13:21:13,1603448473.0,dataengineering,Data Masking Market Global Forecast to 2022,jgkhkg,pradnya123,,https://www.reddit.com/r/dataengineering/comments/jgkhkg/data_masking_market_global_forecast_to_2022/,1.0,1.0,0.0,20250.0,
3894,2020-10-23 14:13:32,1603451612.0,dataengineering,Cocoa Beans Production Ranking | TOP 10 Country from 1961 to 2018,jgl4ce,Outrageous_Day_338,,https://www.reddit.com/r/dataengineering/comments/jgl4ce/cocoa_beans_production_ranking_top_10_country/,1.0,0.0,0.0,20252.0,
3895,2020-10-23 17:14:37,1603462477.0,dataengineering,Reports With Airflow,jgnwdq,BalajiShanmugam1144,,https://www.reddit.com/r/dataengineering/comments/jgnwdq/reports_with_airflow/,1.0,5.0,0.0,20256.0,"Hi, 

 Is there any way to send reports with html and css on airflow ?. I am quite familiar with sending reports . But it is not feasible when making html content. It always renders as a plain template. Even if we apply css, it is not getting rendered. Any thoughts?"
3896,2020-10-23 17:18:06,1603462686.0,dataengineering,[video] Apache Beam Explained in 12 Minutes,jgnyqt,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/jgnyqt/video_apache_beam_explained_in_12_minutes/,1.0,8.0,0.0,20256.0,
3897,2020-10-23 18:57:38,1603468658.0,dataengineering,Why hiring a data analyst won't solve your business problems,jgptz9,mkvor8,,https://www.reddit.com/r/dataengineering/comments/jgptz9/why_hiring_a_data_analyst_wont_solve_your/,1.0,2.0,0.0,20261.0,"How do early-stage startups get started with using data? Inquiring minds want to know.

[https://medium.com/swlh/why-hiring-a-data-analyst-wont-solve-your-business-problems-b4effb158d9d?source=friends\_link&amp;sk=82dcd766a70517a47c2b7c60ec2de2b7](https://medium.com/swlh/why-hiring-a-data-analyst-wont-solve-your-business-problems-b4effb158d9d?source=friends_link&amp;sk=82dcd766a70517a47c2b7c60ec2de2b7)"
3898,2020-10-23 22:51:16,1603482676.0,dataengineering,Question: (Python) dataset variable naming conventions for different stages in the pipeline,jgue0e,ChrisIsWorking,,https://www.reddit.com/r/dataengineering/comments/jgue0e/question_python_dataset_variable_naming/,1.0,6.0,0.0,20268.0,"This question could very likely also be asked in /r/datascience but I feel it's /r/dataengineering at the heart of it.

So I manage a low code data management platform that is more UI based.  And within your usual pipelines, you have:

* Store tables - contains the raw data from each load instance, it's timestamped for each load
* Stage tables - contains one load at a time then gets wiped out, this is where transformations happen
* 'Final' tables - the final dumping ground for your data after initial clean and prep

Now from the 'Final' tables the data may go on to some PreMaster and Master tables and then further downstream to export tables from which data is then exported to other systems.  This data is timestamped in some cases as well to know what's been exported and when.

Long drawn out question... what naming conventions do you guys use especially in a python pipeline script for your variables that represent datasets along the way in the pipeline?

When you first import the data do you include \_store or \_load in your variable name? Likewise for your \_stage or \_transformation variables and finally the \_final version?

If I'm missing stages or you guys don't do this let me know!"
3899,2020-10-23 23:12:37,1603483957.0,dataengineering,what's a quick and easy way to query objects in s3?,jguskn,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/jguskn/whats_a_quick_and_easy_way_to_query_objects_in_s3/,1.0,10.0,0.0,20268.0,"We have some small text files in s3 and occasionally have to query but don't need to load into a table. I know loading into Redshift and querying is one option. 

I see that there's s3 select but it's close to useless. You can select rows but can't join or sum a column. That's mainly what I need - a sum. 

I'm looking at an example of a federated query which is supposed to help, but I don't see where they specify the s3 object name here:

[https://docs.aws.amazon.com/redshift/latest/dg/federated\_query\_example.html](https://docs.aws.amazon.com/redshift/latest/dg/federated_query_example.html)

I could also code something in Python +boto3, which I've done a few times, but I wouldn't say it's quick. And anyway my manager is asking; she has to be able to run as well."
3900,2020-10-24 08:35:03,1603517703.0,dataengineering,Udacity Data Engineering Nanodegree template files,jh3l5j,phmark19,,https://www.reddit.com/r/dataengineering/comments/jh3l5j/udacity_data_engineering_nanodegree_template_files/,1.0,13.0,0.0,20280.0,"I want to learn data engineering but I don't have any funds to pay for the udacity's nanodegree. Now, in able to help myself to learn for free, I've searched and found this github repo containing the exercises and projects done in DEND. But what I need are the template files, so that I can get a hands on experience by answering the exercises and projects myself. I have a CS degree and I am already comfortable in software engineering, I also know how to use python and sql. Since the activities are structured, I believe that the template files will suffice to supplement my learning.

Any kind person out there willing to share?"
3901,2020-10-24 09:56:09,1603522569.0,dataengineering,Databricks Eyes Soon IPO,jh4i7v,The-Techie,,https://www.reddit.com/r/dataengineering/comments/jh4i7v/databricks_eyes_soon_ipo/,1.0,10.0,0.0,20280.0,
3902,2020-10-24 14:00:18,1603537218.0,dataengineering,Solar Electricity Ranking | TOP 10 Country from 1985 to 2019,jh71xk,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jh71xk/solar_electricity_ranking_top_10_country_from/,1.0,1.0,0.0,20290.0,
3903,2020-10-24 17:36:57,1603550217.0,dataengineering,"How to become a great ""big data"" engineer ?",jh9yut,GreekYogurtt,,https://www.reddit.com/r/dataengineering/comments/jh9yut/how_to_become_a_great_big_data_engineer/,1.0,5.0,0.0,20297.0," 

So  I'm an SDE fresher, and got a part in the BDPaaS(Big-data-as-a-Service)  team in a Project.Our Project uses Salesforce , Tableau &amp; Big data.  I'm part of big data.

So things related to the project - I will learn on the job.I Have read enough theory on the Hadoop ecosystem.

Should I make a personal project on big data as the project I'm in, has already been built and we keep adding new features.

That can give me a better idea of how things are set up."
3904,2020-10-24 21:41:07,1603564867.0,dataengineering,"Recommendations for good, and paid, DE courses online?",jhe60g,TravellingBeard,,https://www.reddit.com/r/dataengineering/comments/jhe60g/recommendations_for_good_and_paid_de_courses/,1.0,5.0,0.0,20295.0,"I have a large chunk of training budget I can apply to online coursework, plus we get free access to Pluralsight.  However, anyone have recommended sites I can apply this training budget to before the end of the year that they've had success with?

We're talking fundamentals here and the main technologies involved (I'm a long time database admin, so SQL itself will be trivial), after which I can focus on adding more of the nuanced stuff in the new year.  If it matters, our company works mostly with Azure but I understand a large majority of these technologies overlap so there won't be much lost if I learn via AWS or Google as well."
3905,2020-10-24 22:52:49,1603569169.0,dataengineering,"Long time Opensource users? Start contributing to Open Source projects, if you know Python (knowing Airflow is a plus), I can help you contribute your first commit to Airflow :) -- I am Apache Airflow Committer, PMC Member and Release Manager.",jhfg6p,kaxil_naik,,https://www.reddit.com/r/dataengineering/comments/jhfg6p/long_time_opensource_users_start_contributing_to/,1.0,30.0,0.0,20295.0,
3906,2020-10-25 01:51:58,1603579918.0,dataengineering,Transitioning to Data Science,jhiitu,LectricVersion,,https://www.reddit.com/r/dataengineering/comments/jhiitu/transitioning_to_data_science/,1.0,3.0,0.0,20299.0,"I started my career off in analytics (i.e. Excel wizardry with some light SQL), but for the past 5 years I've been a Data Engineer in one form or another. I moved into DE because I wanted to flex my technical muscles a bit, and get more into building infra for data warehouses and writing code for pipelines. To that end, I've done pretty damn well for myself (I'm now in FAANG), but lately have been wondering where I go from here, as I'm not all that passionate about the idea of being a senior-level DE, or a manager.

I've started seriously thinking about transitioning to a DS role since a recent secondment in my current company to support an area of the business with no DE or DS support. I therefore had to pick up responsibilities from both disciplines - building out data pipelines and core tables, and interrogating said core tables for insights that will direct the teams goals and the right metrics to understand customer behaviour and measure our success. When we finally got DS support in the team, my reaction should have been ""Yes! Now I can go back to doing DE stuff full time!"", but it was the complete opposite - I actually felt a little sad that I'd no longer be doing DS stuff!

My company is ridiculously supportive with internal mobility, so I floated the idea to my manager this week who was surprised but 100% on board. He explained that I'd have to go through an interview loop, which is fair enough and what I was expecting. The problem here is that I'm not a maths or stats whizz, and I'm wondering how much this will hold me back? Can anyone recommend any resources to upskill myself in preparation for interviewing and making the switch? Has anyone made the transition from DE to DS (or even vice versa!) and can offer some insights?"
3907,2020-10-25 03:05:01,1603587901.0,dataengineering,Azure/AWS predominant jobs and transfering between them,jhkl25,SWEbyday,,https://www.reddit.com/r/dataengineering/comments/jhkl25/azureaws_predominant_jobs_and_transfering_between/,1.0,3.0,0.0,20301.0,"Hi All,

Just a quick question. I'm primarily working  with aws in my current role but have been receiving interviews and offers at companies using azure. So far Ive been told that working in aws, I can easily adapt to the azure stack. My question is that in 5 years when Im job hunting for senior roles at a  company that works primarily in aws, does it really matter that much or is it harder to transition from azure to aws?"
3908,2020-10-25 09:27:32,1603610852.0,dataengineering,Top 10 Machine Learning Algorithms For Data Scientists,jhpfvg,dhiraj8899,,https://www.reddit.com/r/dataengineering/comments/jhpfvg/top_10_machine_learning_algorithms_for_data/,1.0,0.0,0.0,20306.0,
3909,2020-10-25 12:58:30,1603623510.0,dataengineering,Egg Consumption Ranking | TOP 10 Country from 1961 to 2017,jhrkip,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jhrkip/egg_consumption_ranking_top_10_country_from_1961/,1.0,0.0,0.0,20308.0,
3910,2020-10-25 15:08:47,1603631327.0,dataengineering,Top 10 Most Popular Programming Languages (PYPL) - 2004/ October 2020,jht2us,accappatoiviola,,https://www.reddit.com/r/dataengineering/comments/jht2us/top_10_most_popular_programming_languages_pypl/,1.0,0.0,0.0,20309.0,
3911,2020-10-26 00:39:11,1603665551.0,dataengineering,"Welcome to the 14th edition of the data engineering newsletter. This week's release is a new set of articles that focus on data quality at Microsoft, Operating Pinot at Uber, data science at Hulu &amp; Trivago, Data lake at Grofers, Notebook from Yelp, Spark shuffle optimizer from Linkedin",ji2tp6,vananth22,,https://www.reddit.com/r/dataengineering/comments/ji2tp6/welcome_to_the_14th_edition_of_the_data/,1.0,0.0,0.0,20320.0,
3912,2020-10-26 02:23:13,1603671793.0,dataengineering,Dataverse vs. Airflow?,ji4jt1,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/ji4jt1/dataverse_vs_airflow/,1.0,20.0,0.0,20322.0,"Hi. Anyone here ever used a workflow tool called Dataverse? It's made by Lavastorm. It's a crappy tool I used two years ago at a previous job. 

I just started looking at Airflow, which I've seen others mention here. I noticed that it looks like Dataverse in that it's web-based. People seem to like Airflow. Dataverse wasn't great...the department didn't want to spend more money on a better tool. 

Anyway just wondering if anyone else out there has used it, and can compare to Airflow."
3913,2020-10-26 07:43:41,1603691021.0,dataengineering,Future of data management,ji94sb,metrd,,https://www.reddit.com/r/dataengineering/comments/ji94sb/future_of_data_management/,1.0,0.0,0.0,20328.0,"The Future of Data Management
https://www.linkedin.com/pulse/future-data-management-tim-ward"
3914,2020-10-26 08:20:47,1603693247.0,dataengineering,"Free Data Science Certification Courses -Python, Data Science &amp; Machine Learning Certificate",ji9jug,chase2learn,,https://www.reddit.com/r/dataengineering/comments/ji9jug/free_data_science_certification_courses_python/,1.0,0.0,0.0,20330.0,
3915,2020-10-26 08:23:17,1603693397.0,dataengineering,Which is better CSE or CSE with Specialisation | Difference between CSE and CSE with specialization,ji9ku4,chase2learn,,https://www.reddit.com/r/dataengineering/comments/ji9ku4/which_is_better_cse_or_cse_with_specialisation/,1.0,0.0,0.0,20330.0,
3916,2020-10-26 08:25:39,1603693539.0,dataengineering,Value of Udemy Certificate in India | Are Udemy Certificates worth mentioning in a CV/Resume?,ji9lpr,chase2learn,,https://www.reddit.com/r/dataengineering/comments/ji9lpr/value_of_udemy_certificate_in_india_are_udemy/,1.0,0.0,0.0,20330.0,
3917,2020-10-26 08:27:33,1603693653.0,dataengineering,Pluralsight Free Courses | Pluralsight Premium Subscription For Free | Any Course For Free,ji9mf7,chase2learn,,https://www.reddit.com/r/dataengineering/comments/ji9mf7/pluralsight_free_courses_pluralsight_premium/,1.0,0.0,0.0,20330.0,
3918,2020-10-26 10:27:51,1603700871.0,dataengineering,Webinar: Transform Your Business with AWS Data and Analytics Services,jiavok,suemethen,,https://www.reddit.com/r/dataengineering/comments/jiavok/webinar_transform_your_business_with_aws_data_and/,1.0,0.0,0.0,20332.0,
3919,2020-10-26 10:36:02,1603701362.0,dataengineering,Webinar: Transform Your Business with AWS Data and Analytics Services,jiaynf,suemethen,,https://www.reddit.com/r/dataengineering/comments/jiaynf/webinar_transform_your_business_with_aws_data_and/,1.0,0.0,0.0,20333.0,
3920,2020-10-26 12:18:36,1603707516.0,dataengineering,Motivated Intruder Test Consultancy,jic0oj,Next-Froyo9050,,https://www.reddit.com/r/dataengineering/comments/jic0oj/motivated_intruder_test_consultancy/,1.0,2.0,0.0,20337.0,"Hello!

My company wants to release a public dataset on Kaggle containing customer transactions on the website. However, we are having a hard time to find someone to run a **motivated intruder test** on this dataset to make sure we are complying with GDPR/anonymisation rules.

&amp;#x200B;

&gt;The ‘motivated intruder’ is taken to be a person who starts without any prior knowledge but who wishes to identify the individual from whose personal data the anonymised data has been derived. This test is meant to assess whether the motivated intruder would be successful.  
&gt;  
&gt;The approach assumes that the ‘motivated intruder’ is reasonably competent, has access to resources such as the internet, libraries, and all public documents, and would employ investigative techniques such as making enquiries of people who may have additional knowledge of the identity of the data subject or advertising for anyone with information to come forward. The ‘motivated intruder’ is not assumed to have any specialist knowledge such as computer hacking skills, or to have access to specialist equipment or to resort to criminality such as burglary, to gain access to data that is kept securely.  
&gt;  
&gt;[https://ico.org.uk/media/1061/anonymisation-code.pdf](https://ico.org.uk/media/1061/anonymisation-code.pdf)

&amp;#x200B;

Please get in touch if you provide this kind of service, or knows someone who does."
3921,2020-10-26 12:50:47,1603709447.0,dataengineering,Tomato Production Ranking | TOP 10 Country from 1961 to 2018,jicdkj,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jicdkj/tomato_production_ranking_top_10_country_from/,1.0,0.0,0.0,20338.0,
3922,2020-10-26 17:30:50,1603726250.0,dataengineering,Apache Spark declining trends. The end of the era?,jigmuc,OmegaConstant,,https://www.reddit.com/r/dataengineering/comments/jigmuc/apache_spark_declining_trends_the_end_of_the_era/,1.0,67.0,0.0,20346.0,"I stumbled upon google trends on the ""Apache Spark"" keyword and, as you can see, the trend has been declining over the past 1-2 years.

I suppose its due to new open-source ETL \\ ELT stacks and Python growth, can someone confirm this?

[Google Trends](https://preview.redd.it/09twb0dpjgv51.png?width=2296&amp;format=png&amp;auto=webp&amp;s=b39f79697bb0f45ab13e726e9a125516ff8cecd5)

[https://trends.google.com/trends/explore?date=all&amp;q=%2Fm%2F0ndhxqz](https://trends.google.com/trends/explore?date=all&amp;q=%2Fm%2F0ndhxqz)"
3923,2020-10-26 18:03:52,1603728232.0,dataengineering,Full stack Viz application with graph dB.,jih8rk,powok,,https://www.reddit.com/r/dataengineering/comments/jih8rk/full_stack_viz_application_with_graph_db/,1.0,2.0,0.0,20347.0,"Hi Fellas,

We are looking to create an full stack application .
This is a supply chain management application.
Like if you enter a part number , it will show you where this part is used in all products , where this part is manufactured, etc.Chain of information.

We need to build a custom front end and would be creating our own Graph Viz .
Is neo4j good fit as a database for this or would any cache based database work ?

We will use Open source Viz tools for calling APIs to graph dB to populate the Viz in webpage.So we can't use neo4J bloom or any other private inbuilt Graph DB' Viz tool as this can't be integrated into the front end website.

And we are looking for any resources that would help.

Thanks"
3924,2020-10-26 18:54:42,1603731282.0,dataengineering,SPSS vs SAS: Find Our The Best Statistics Software,jii7yb,Techbiason,,https://www.reddit.com/r/dataengineering/comments/jii7yb/spss_vs_sas_find_our_the_best_statistics_software/,1.0,0.0,0.0,20349.0,
3925,2020-10-26 23:23:09,1603747389.0,dataengineering,Big Data Quality Assurance,jinmhb,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/jinmhb/big_data_quality_assurance/,1.0,0.0,0.0,20355.0,
3926,2020-10-27 03:13:36,1603761216.0,dataengineering,Changelog Dixer v1.2.0,jirpgb,s_t_g_o,,https://www.reddit.com/r/dataengineering/comments/jirpgb/changelog_dixer_v120/,1.0,0.0,0.0,20358.0,
3927,2020-10-27 03:16:02,1603761362.0,dataengineering,Problems with AWS DMS and S3 target out of nowhere,jirquh,gonik,,https://www.reddit.com/r/dataengineering/comments/jirquh/problems_with_aws_dms_and_s3_target_out_of_nowhere/,1.0,1.0,0.0,20358.0,"Hey all!

Today out of the blue we started having issues with an S3 Target on AWS DMS that is in another account that belongs to our organization. When testing the endpoint we get the following error: 

`Test Endpoint failed: Application-Status: 1020912, Application-Message: N/A, Application-Detailed-Message: N/A`

I tried everything I could imagine, recreated the endpoints, the replication task and even the instance with different engine versions. The task has been working flawlessly for more than a week and the IAM roles/policies didn't change at all.

Couldn't find much information with that error message as it's not giving too much detail about what's going on.

Did anyone have the same issue? What else could I check?

Thanks!"
3928,2020-10-27 11:37:34,1603791454.0,dataengineering,"Do you think it is possible to have an interview with George Fraser, Fivetran CEO?",jiy242,an_tonova,,https://www.reddit.com/r/dataengineering/comments/jiy242/do_you_think_it_is_possible_to_have_an_interview/,1.0,2.0,0.0,20370.0,The question is how open is he for interviews and calls?
3929,2020-10-27 12:57:35,1603796255.0,dataengineering,Population ranking | TOP 10 Country from 1950 - 2100,jiyyne,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jiyyne/population_ranking_top_10_country_from_1950_2100/,1.0,0.0,0.0,20373.0,
3930,2020-10-27 15:13:14,1603804394.0,dataengineering,[Airflow] On_failure - get all failed task for notifications,jj0v3q,alpha0519,,https://www.reddit.com/r/dataengineering/comments/jj0v3q/airflow_on_failure_get_all_failed_task_for/,1.0,2.0,0.0,20380.0,"I am in a situation where I have couple of tasks as part of a standard airflow dag, I have a set-up to send on-failure notifications to a slack channel, I currently get an alert for each failed task, is there a way where i can store all the failure notification from on-failure somewhere and send all of it as part of one notification to the channel.

Just trying to understand if it's possible and if it is how can I approach this.

Appreciate the ideas folks."
3931,2020-10-27 15:27:49,1603805269.0,dataengineering,Help me understand it.,jj13f6,StatCold7397,,https://www.reddit.com/r/dataengineering/comments/jj13f6/help_me_understand_it/,1.0,13.0,0.0,20380.0,"Hello everyone,

i just wanted to ask few questions so that maybe you guys will HELP ME understand better.

so i recently got an internship in a company as a data scientist/data engineer. i was given a task where i have to store a video, an audio, an image and a text into a single numpy array combining them all into one. It could also be a series of videos into one numpy array or images or audios or texts.

I could not understand it and asked them why would they want to do that and in answer they told me that it was to make a training set to train the model.

does that make any sense? or i am not understanding it properly. As far as i understand i believe that we just store data lets say videos or images or a text files from different sources into our storage and when we need them for training purpose we extract the files. i still cannot understand why they asked me to convert a video into a numpy array.

I was told that thats what data engineers do, that they extract all these different data files from different sources and make them into one so that we can have a training set.

It would really be great if you guys could help me understand it because i am just starting my data science career and i don’t understand fully how these things work. I was also expecting a lot of data cleaning and SQL queries and maybe work on data pipelines but rather i was told to do this task.

Thanks everyone!"
3932,2020-10-27 15:41:26,1603806086.0,dataengineering,Email blacklist + sending emails airflow,jj1bcx,digichap28,,https://www.reddit.com/r/dataengineering/comments/jj1bcx/email_blacklist_sending_emails_airflow/,1.0,5.0,0.0,20380.0,"Hey, I’m about to finish a couple of pipelines and would be sending out around 2,500 - 3,500 emails on a daily basis as the result of generating certain data. Some emails would be customized to the receiver and some others will be the same for a mailing list.
Around 80% of them will be sent to the inside the organization I work for,  and the others to certain business partners. 

As I’ll be sending that amount of emails and maybe more in the near future, I’m worried about getting the sender email/ip blacklisted. Do you think this might qualify me to get blacklisted ? Should I consider using an email transactional service ? If so, is there any free service you would suggest ? how many sent emails per day do you think might get you in this kind of problems ? Any advice ?

Thanks!"
3933,2020-10-27 16:41:57,1603809717.0,dataengineering,Making a 1 TB dataset of zipped text files searchable?,jj2dm9,sthsthanothersth,,https://www.reddit.com/r/dataengineering/comments/jj2dm9/making_a_1_tb_dataset_of_zipped_text_files/,1.0,11.0,0.0,20384.0,"Basically title. My input is a bunch of zipped files that total around 1 TB when decompressed.

I need to decompress these files and index them into something like azure search?

My current approach is to work with uncompressed data and index them into azure search using spark on azure databricks.

1. Is this the best way to do it?
2. Any input on how to best unzip the data quickly? Each single zipped file could have a size of up to 2GB."
3934,2020-10-27 16:56:31,1603810591.0,dataengineering,The Ultimate Guide to Data Cleaning,jj2n6e,mmanja,,https://www.reddit.com/r/dataengineering/comments/jj2n6e/the_ultimate_guide_to_data_cleaning/,1.0,1.0,0.0,20386.0,
3935,2020-10-27 17:22:05,1603812125.0,dataengineering,🔧 Data Engineering Bootcamp Launch - Intro and Q&amp;A 🔧,jj34k9,soobrosa,,https://www.reddit.com/r/dataengineering/comments/jj34k9/data_engineering_bootcamp_launch_intro_and_qa/,1.0,12.0,0.0,20388.0,"We're launching the world's first coding bootcamp for data engineering!

Pipeline Academy is ready to roll, and your hosts (Daniel Molnar and Peter Fabian) are here to tell you all about it.  


[https://www.meetup.com/pipeline-data-engineering-academy-berlin/events/273973094/](https://www.meetup.com/pipeline-data-engineering-academy-berlin/events/273973094/)"
3936,2020-10-27 18:42:43,1603816963.0,dataengineering,DE Interview with M&amp;C SAATCHI PERFORMANCE,jj4p08,hrish_95,,https://www.reddit.com/r/dataengineering/comments/jj4p08/de_interview_with_mc_saatchi_performance/,1.0,0.0,0.0,20390.0,"Hi, Has anyone given an interview with this company for a *data engineering role*?

If yes, Can you please let me know what was asked in the interview i.e. technical interview?

What level it was? 

And how much salary should I ask for? It's my First Switch."
3937,2020-10-27 19:40:31,1603820431.0,dataengineering,Building A Data Development Environment with lakeFS,jj5thx,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/jj5thx/building_a_data_development_environment_with/,1.0,0.0,0.0,20393.0,
3938,2020-10-27 20:00:54,1603821654.0,dataengineering,Diagram of data warehouse,jj680z,apbcx,,https://www.reddit.com/r/dataengineering/comments/jj680z/diagram_of_data_warehouse/,1.0,11.0,0.0,20392.0,"Hi,

Looking (desperately) for some suggestions :)

I've recently joined to a project that has a data warehouse (SQL Server). Lots of tables, lots of columns (lots of redundant columns), no FKs few PKs.

Project manager is demanding a database diagram. I'm looking for some tools and experimenting as I can but with almost no luck to help me to achieve this goal with a reasonable diagram.

Some tools that I've tried :
Schema crawler
SQL power architect
Lucidchart and some others... 

It's important to:
- have any sort of weak association, at least to have a model to start the refinament.
- be a free to use software.

Would you guys have any suggestion of tools, or methods to start this diagram? 

Any help will be more than welcomed. 

Apologies if anything is wrong with this post and once again thanks for the support :)"
3939,2020-10-27 21:23:47,1603826627.0,dataengineering,FAANG DE Interview,jj7w8j,SnooTangerines1201,,https://www.reddit.com/r/dataengineering/comments/jj7w8j/faang_de_interview/,1.0,34.0,0.0,20396.0,"Hi everyone!

I am looking for opportunities in FAANG for Data Engineering specific roles, including Big Data Engineers. 

I do not have any career prep services as I cannot afford, I also lost a job offer because of COVID, so it will be extremely helpful for your input.

I want to know more about the detailed process of interviewing at FAANG companies for DE concentration. I have heard that not all of them demand Python, some do. I am not sure, it would help if you could clear some things up!

Further, what do you think are the most important topics to be prepped for interviews at these companies.

&amp;#x200B;

Thank you!"
3940,2020-10-27 21:42:05,1603827725.0,dataengineering,Three articles to Read. Three podcasts to Listen. Three videos to Watch. [ThreeAnalytics Newsletter],jj89ei,robbertbrouwers,,https://www.reddit.com/r/dataengineering/comments/jj89ei/three_articles_to_read_three_podcasts_to_listen/,1.0,0.0,0.0,20396.0,
3941,2020-10-28 00:10:08,1603836608.0,dataengineering,Data Teams Is Out,jjb610,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/jjb610/data_teams_is_out/,1.0,6.0,0.0,20400.0,"I’m proud to announce that my latest book, *Data Teams*, is available for purchase. It covers the three teams you need for analytics and how they should work with the rest of the business. This is the first book to really put data engineering at the forefront alongside data science for creating success data projects. You go to the book's website at [http://www.datateams.io/](http://www.datateams.io/) to get more information."
3942,2020-10-28 07:44:08,1603863848.0,dataengineering,Google Cloud Professional Certs 1 month free on Coursera,jji8sz,frenchdic,,https://www.reddit.com/r/dataengineering/comments/jji8sz/google_cloud_professional_certs_1_month_free_on/,1.0,0.0,0.0,20413.0,
3943,2020-10-28 10:47:21,1603874841.0,dataengineering,Deconstructing Dynamo,jjk6wv,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jjk6wv/deconstructing_dynamo/,1.0,0.0,0.0,20420.0,
3944,2020-10-28 13:01:30,1603882890.0,dataengineering,Data Processing in the Edge (Stream and Batch) [Seeking Advice],jjlnn4,vlahunter,,https://www.reddit.com/r/dataengineering/comments/jjlnn4/data_processing_in_the_edge_stream_and_batch/,1.0,2.0,0.0,20424.0,"Hi there,

Lately i have been studying around the data processing side of things. I am a developer and i am trying to understand Architectures, use cases and more.

During the last weeks i have been reading articles and research papers around the Edge Computing domain and i am interested to build a project (if possible to have that in my portfolio for future use) so i have some questions.

Lets say i have some sensors in a Wireless Sensor Network and the data is transfered to a Base Station to an MQTT Broker (I assume i wouldnt build that but i would either find a simulator or find a ready-to-go solution). Would it make sense to use this Device as my primary device of initial analytics? What if i would like to do both Stream Processing Analytics and some Batch Processing? Would that make sense according to your experience? Reason for that would be to have minimal Internet connectivity and only broadcast minimal amounts of data or events/warnings if needed from the Base Station to my laptop. Keep in mind that instead of using a Gateway as a Base Station, i would use a Linux Machine, capable of running heavy computations if needed.

&amp;#x200B;

So far, as i see it and if i understand it correct, it could look like the following flow.

**Sense Data** (Sensor) ==&gt; **MQTT Broker** (Base Station) ==&gt; **Stream Process Data** (Spark or something different) ++ **Intermediate Data Storage** for Batch Processing Later (Cassandra, MongoDB or something different) ==&gt; Run some tasks in Python as a Daemon to **Batch Process** ==&gt; **Save/Send Data** after being processed to a DB ==&gt; In case the Stream Processing gives an incident **broadcast an event back to my machine** over Network.

The questions are, am i on the right track? Can i build something like that and avoid the Cloud? What else do you feel is missing according to your experience?  


Thanks in advance

Best"
3945,2020-10-28 13:17:54,1603883874.0,dataengineering,Snowflake vs Redshift,jjlv27,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/jjlv27/snowflake_vs_redshift/,1.0,0.0,0.0,20424.0,"On the basic level, the right data warehouse for your business should be filtered based on three criteria. Integration, Pricing and Security.

https://preview.redd.it/2zqyj55nktv51.png?width=1920&amp;format=png&amp;auto=webp&amp;s=c2701f3f8acc3691f68ed8aeeacb9ee36e7eb944

If you are looking to choose between Snowflake and Redshift as your warehousing option, then this is the pit stop for your business. Study, analyze and compare the data warehouses for yourself. [Snowflake vs Redshift](https://www.sprinkledata.com/docs/snowflake-vs-redshift/index.html?utm_source=reddit&amp;utm_medium=snowflakevsredshift)"
3946,2020-10-28 13:23:39,1603884219.0,dataengineering,Daily Meat Consumption Ranking | TOP 10 Country from 1961 to 2013,jjlxkb,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jjlxkb/daily_meat_consumption_ranking_top_10_country/,1.0,0.0,0.0,20424.0,
3947,2020-10-28 14:06:07,1603886767.0,dataengineering,NEED HELP BashOperator dosen't run bash file requires SUDO privileges,jjmhhj,Zodijackly,,https://www.reddit.com/r/dataengineering/comments/jjmhhj/need_help_bashoperator_dosent_run_bash_file/,1.0,1.0,0.0,20423.0," Hello there :))

i have script called CC that collects the data and push it into a data warehouse . I created a dag for it

         Task_I = BashOperator(         task_id=""CC"",         run_as_user=""koa"",         bash_command=""sudo /home/koa/CC""   ) 

and I've added the permission to run it without typing password by modifying /etc/sudoers

     koa ALL = (ALL) NOPASSWD: /home/koa/CC 

however when the task fails in airflow and the log states that a password is needed

    {bash_operator.py:153} INFO - Output: {bash_operator.py:157} INFO - sudo: a terminal is required to read   the password; either use the -S option to read from standard input or configure an askpass   helper {bash_operator.py:159} INFO - Command exited with return code 1 

It will be great if you can help me guys , i'am new to airflow and have been struggling with this for the last few hours"
3948,2020-10-28 15:11:43,1603890703.0,dataengineering,Crawler Structure,jjnghm,Low-Somewhere-211,,https://www.reddit.com/r/dataengineering/comments/jjnghm/crawler_structure/,1.0,14.0,0.0,20425.0,"Hey

I am going to write a crawler for a specific website, which I have a list of URLs in it. I want to do it in the asynchronous method and use python libraries, but I don't know how to design the structure of the project and use what technologies for it. I would be very thankful if you recommend any resource for it."
3949,2020-10-28 18:36:00,1603902960.0,dataengineering,The Most Popular Databases - 2006/2020,jjr5p9,accappatoiviola,,https://www.reddit.com/r/dataengineering/comments/jjr5p9/the_most_popular_databases_20062020/,1.0,6.0,0.0,20434.0,
3950,2020-10-28 21:41:38,1603914098.0,dataengineering,How to Enrich Your Snowflake Data Using External Functions,jjusig,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/jjusig/how_to_enrich_your_snowflake_data_using_external/,1.0,1.0,0.0,20439.0,
3951,2020-10-28 22:09:05,1603915745.0,dataengineering,Opinions on data scientists using Airflow,jjvbox,ploomber-io,,https://www.reddit.com/r/dataengineering/comments/jjvbox/opinions_on_data_scientists_using_airflow/,1.0,78.0,0.0,20438.0,"Hello everyone,

I've been reading posts on Apache Airflow for data science/machine learning projects and I spotted a common trend. It looks like a lot of companies struggle to get ""research code"" to work in a production system like Airflow. My hypothesis is that when data scientists finish working on a data pipeline, an engineering team might require them to deliver their work as an Airflow pipeline (a lot of companies have Airflow installations these days and try to leverage them as much as possible). But this posits a huge burden for data scientists who now have to detail with a lot of details to make their pipeline work and often have to rewrite big portions of the code until ""it works"".

Has anyone had a similar experience? Any approaches to make Airflow more accessible to data scientists?"
3952,2020-10-28 22:55:57,1603918557.0,dataengineering,Choosing the right database - ease of query - Azure,jjw732,Alf4598,,https://www.reddit.com/r/dataengineering/comments/jjw732/choosing_the_right_database_ease_of_query_azure/,1.0,1.0,0.0,20442.0,"Hi,

&amp;#x200B;

I need to setup 2 databases in Azure. The first one will contain a list of clients and related informations, the second one will be bigger and will contain informations about clients purchases. 

&amp;#x200B;

But I have 2 big constraints:

\-the database must be able to store a fast growing amount of data that can lead to very big data

\-NON TECH employees at the company must be able to query the databases and extract csv or xls files for their daily tasks. Those employees have never coded before nor have used SQL.

\-the data must be stored on Azure

&amp;#x200B;

1) What would you guys recommend? Should I go for a classic Azure SQL database or something else (like a data warehouse service?)

Is there any way for those technical employees to easily query the database? Any combination of additional services that you know about?

&amp;#x200B;

2) If we were to remove this contraint of non SQL querying, what would be the best azure database service to accomplish that? Speed, ability to handle lot of data, reliability are the most important factors in this case

&amp;#x200B;

Thank you very much, any help or advice is really appreciated"
3953,2020-10-28 23:09:32,1603919372.0,dataengineering,"[video] Outstanding conversation on the future of data. With: ex-CEO Snowflake, CEO Noteable (ex Netflix), GP a16z, CEO Fishtowndata (dbt), CEO Fivetran",jjwgd1,fhoffa,,https://www.reddit.com/r/dataengineering/comments/jjwgd1/video_outstanding_conversation_on_the_future_of/,1.0,0.0,0.0,20442.0,
3954,2020-10-29 06:55:03,1603947303.0,dataengineering,Best programming language to learn in 2021,jk3yky,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jk3yky/best_programming_language_to_learn_in_2021/,1.0,0.0,0.0,20455.0,
3955,2020-10-29 07:10:43,1603948243.0,dataengineering,Notebook-based TDD?,jk45pg,ColdPorridge,,https://www.reddit.com/r/dataengineering/comments/jk45pg/notebookbased_tdd/,1.0,4.0,0.0,20456.0,"Hopefully this is the right sub for this, I figured many folks here have experience with both the DS and eng side.

I’m currently a DS, but in my last role I was a SWE and picked up some really nice habits of using test-driven development (TDD). Lately I’ve been wondering how to integrate that approach into my current work as a DS prototyping analytics pipelines.

For background, my current team is very classically DS. Mostly PhDs, with very little engineering training, if any. The stack used tends to be primarily Spark in Python and Scala, usually written in a notebook like Jupyter or Zeppelin. Inputs are usually HDFS files or Hive, and outputs are generally dashboards and more Hive tables. 

My goal is to help provide options for the team to reduce development time and increase confidence in results with better testing along the way. Previously all of my TDD was done in a normal IDE, but I know from experience that you will not be able to convince an entire team of seasoned DS to leave notebooks for an IDE, so I’m looking to meet them in their element.  Are there any tools or practices that you have found particularly useful/easy to get other team members on board with? Is notebook-based TDD even worth pursuing?"
3956,2020-10-29 12:48:12,1603968492.0,dataengineering,How do I enter the data engineering field? Is there a sequence to follow or do even fresh graduates could directly participate in the field?,jk7sn5,Pervert_Spongebob,,https://www.reddit.com/r/dataengineering/comments/jk7sn5/how_do_i_enter_the_data_engineering_field_is/,1.0,14.0,0.0,20464.0,
3957,2020-10-29 13:03:55,1603969435.0,dataengineering,Colon and Rectum Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jk7zde,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jk7zde/colon_and_rectum_cancer_death_rates_ranking_top/,1.0,0.0,0.0,20465.0,
3958,2020-10-29 15:32:00,1603978320.0,dataengineering,An interview about the Cyral platform and how it enforces data security as code for protecting databases and object storage in the cloud.,jka2lz,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/jka2lz/an_interview_about_the_cyral_platform_and_how_it/,1.0,0.0,0.0,20473.0,
3959,2020-10-29 20:11:54,1603995114.0,dataengineering,How to get prepared for a 'big data' internship technical interview?,jkfbme,msv5450,,https://www.reddit.com/r/dataengineering/comments/jkfbme/how_to_get_prepared_for_a_big_data_internship/,1.0,3.0,0.0,20483.0,"I am having a second round interview with an insurance company for a big data internship position. This is my first interview ever for a big data role. The first round interview was a take home exam about generic data science stuff with Jupyter Notebook. However, the big data tools like Spark are a mystery to me.

The comapny collects massive amounts of data from vehicles and they work with distributed, parallel technologies like Hadoop, spark and Kafka to analyze the data. The interviewers will probably ask me how I would make a distributed framework to digest and analyze millions of rows of data. I only know basic stuff about Hadoop and AWS.

What are the typical questions that the employers ask for an entry level position like this in big data? How can I better prepare myself? What should I review?"
3960,2020-10-29 20:17:11,1603995431.0,dataengineering,"Your AWS Lambda Function Failed, What Now?",jkfffl,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/jkfffl/your_aws_lambda_function_failed_what_now/,1.0,7.0,0.0,20483.0,
3961,2020-10-29 22:05:56,1604001956.0,dataengineering,Why Become A Data Engineer,jkhkhz,70sechoes,,https://www.reddit.com/r/dataengineering/comments/jkhkhz/why_become_a_data_engineer/,1.0,7.0,0.0,20482.0,"Hey everyone,

I've recently created a video explaining why I think one would go into Data Engineering. I would love it if you could take a look and let me know your thoughts about the points I was making.

You can find it here : [https://www.youtube.com/watch?v=S34QfQaMxDE&amp;t](https://www.youtube.com/watch?v=S34QfQaMxDE&amp;t)

Thanks anyway!"
3962,2020-10-30 00:16:51,1604009811.0,dataengineering,"Gretel Blueprints, making it easy to anonymize and balance datasets with just a few clicks.",jkk4gj,alig80,,https://www.reddit.com/r/dataengineering/comments/jkk4gj/gretel_blueprints_making_it_easy_to_anonymize_and/,1.0,0.0,0.0,20489.0,"Working with developers using [Gretel beta](https://console.gretel.cloud/login) and open-source [synthetic data libraries](https://github.com/gretelai/gretel-synthetics), one of the top requests we have heard is developers asking us to make it incredibly simple to get started on use cases including [data anonymization](https://gretel.ai/platform/synthetics), [privacy engineering](https://gretel.ai/blog/fast-data-cataloging-of-streaming-data-for-fun-and-privacy), and [data balancing](https://gretel.ai/blog/improving-massively-imbalanced-datasets-in-machine-learning-with-synthetic-data). 

Blueprints are collections of sample code and sample datasets that utilize Gretel's SDKs that can be easily adapted to solve customer-specific use cases: [https://gretel.ai/blog/introducing-gretel-blueprints](https://gretel.ai/blog/introducing-gretel-blueprints)"
3963,2020-10-30 01:37:22,1604014642.0,dataengineering,Actian Avalanche vs Snowflake,jklkui,CantGoogleMe,,https://www.reddit.com/r/dataengineering/comments/jklkui/actian_avalanche_vs_snowflake/,1.0,2.0,0.0,20491.0,"Anyone with experience these? 

Employer is looking at standing up a cloud warehouse and was going to go snowflake, but is now looking at actian avalanche.

any obvious pro/cons of one over the other? (cost, speed, etc.?)"
3964,2020-10-30 06:56:16,1604033776.0,dataengineering,How to choose the right ETL tool for your business,jkqmb7,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/jkqmb7/how_to_choose_the_right_etl_tool_for_your_business/,1.0,1.0,0.0,20496.0,"Here are the key advantages and disadvantages for the **top ETL tools in the market**. These comparisons have been made on the tool's ability to connect to **data sources and warehouses, SQL/Python transformations, BI Visualization code, Portable transformation code, Embedding Dashboards into other applications, Jupyter notebook based pipelines, Deployment capabilities**.  


[**Top 5 ETL tools for 2021**](https://www.sprinkledata.com/docs/top-5-etl-tools-for-2021/index.html?utm_source=reddit&amp;utm_medium=top5etl)

&amp;#x200B;

https://preview.redd.it/qoju00j0y5w51.png?width=3808&amp;format=png&amp;auto=webp&amp;s=c170b86e1f0b4074e9de081b19eed3ba83e6aa34"
3965,2020-10-30 11:36:24,1604050584.0,dataengineering,Resources for learning BIML for SSIS,jktpq7,K0NGO,,https://www.reddit.com/r/dataengineering/comments/jktpq7/resources_for_learning_biml_for_ssis/,1.0,0.0,0.0,20508.0, Started a new job that uses BIML for generating and maintaining SSIS packages. I've read some online introductory material about BIML but I was wondering if yall know any good courses/books that cover beginner and advanced concepts.
3966,2020-10-30 13:29:28,1604057368.0,dataengineering,What is Logistic Regression and how does it work?,jkv17e,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jkv17e/what_is_logistic_regression_and_how_does_it_work/,1.0,0.0,0.0,20512.0,
3967,2020-10-30 13:52:28,1604058748.0,dataengineering,Goldman Sachs virtual internship insidesherpa | virtual internship india,jkvbvg,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jkvbvg/goldman_sachs_virtual_internship_insidesherpa/,1.0,0.0,0.0,20514.0,
3968,2020-10-30 15:12:03,1604063523.0,dataengineering,Almond Production Ranking | TOP 10 Country from 1961 to 2018,jkwhc2,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jkwhc2/almond_production_ranking_top_10_country_from/,1.0,0.0,0.0,20515.0,
3969,2020-10-30 15:27:03,1604064423.0,dataengineering,"are you a ""data engineer"" who has to fix things that break in production?",jkwpy9,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/jkwpy9/are_you_a_data_engineer_who_has_to_fix_things/,1.0,22.0,0.0,20515.0,"Hi. I interviewed recently for a position called ""data research analyst"", and they said it's really more of a data engineer role, would I be comfortable with that? I said yeah, sure. 

To me (and I think you would agree, more or less), DE means building ETLs, infrastructure, etc. But as I asked more about it, they said all their ETLs are built, I just need to maintain them (fix things that break), ensure data panels are delivered to clients, and ***when I have time***, find ways to improve the process. 

I did production support for 10 years before I moved over to data. That's enough! They really should call the role ""production support"" or ""data operations"". I'm not gonna take it. 

Anyway, are any of you fixing production stuff the breaks?"
3970,2020-10-30 17:28:51,1604071731.0,dataengineering,Detailed reporting that shows how accurate your synthetic data’s statistical distributions and correlations are.,jkyvgm,alig80,,https://www.reddit.com/r/dataengineering/comments/jkyvgm/detailed_reporting_that_shows_how_accurate_your/,1.0,0.0,0.0,20519.0,Gretel Synthetics includes a performance report that shows you just how well the distributions in your training data were maintained in your new synthetic data. [https://medium.com/gretel-ai/synthetic-data-performance-report-e5a3cd6b1e6d](https://medium.com/gretel-ai/synthetic-data-performance-report-e5a3cd6b1e6d)
3971,2020-10-30 18:11:54,1604074314.0,dataengineering,"Regression analysis with its types, objectives and applications",jkzot0,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jkzot0/regression_analysis_with_its_types_objectives_and/,1.0,0.0,0.0,20519.0,
3972,2020-10-30 18:24:26,1604075066.0,dataengineering,Pipeline monitoring tool,jkzxfy,MasterEpictetus,,https://www.reddit.com/r/dataengineering/comments/jkzxfy/pipeline_monitoring_tool/,1.0,8.0,0.0,20519.0,"This might be a basic question.  I'm trying to get a hold of all inbound and output pipelines connecting to our Snowflake database. We have Fivetran, DMS, Google cloud functions/engine/run, microservices, Looker reporting, PBI reporting, etc. I'd be interested in not only listing them all, but understand what their status is. What are some tools that would help with that? Thanks."
3973,2020-10-30 21:34:55,1604086495.0,dataengineering,"If you had access to thousands of hours of online language teaching videos (1-on-1 and group class English teaching), what use case is there for data engineering and machine learning?",jl3k3e,randomrandom341,,https://www.reddit.com/r/dataengineering/comments/jl3k3e/if_you_had_access_to_thousands_of_hours_of_online/,1.0,0.0,0.0,20535.0,What would you build using all this video data? Input appreciated!
3974,2020-10-30 21:53:55,1604087635.0,dataengineering,Data lake help,jl3wt2,adrayic,,https://www.reddit.com/r/dataengineering/comments/jl3wt2/data_lake_help/,1.0,0.0,0.0,20534.0,"My company is looking to start collecting data from some of our products out in the field.  These products are built with STM32 MCU's that we can communicate with via a remote cellular link.  The data we want to collect is a mix of MCU peripheral data, sensor data, event / log data, images and audio recordings.  We would like to eventually use this data for remote monitoring, predictive analytics and AI / machine learning.

I have spent a bit of time researching the various cloud options but there seems to be many different ways to do this and I want to make sure I pick one that won't leave me hamstrung down the road.

The first step is simply to start collecting the raw data.  I believe a data lake is well suited for this (we use Azure currently but AWS would work fine as well).  

Assuming a data lake is the right tool for this, the recommended folder structure for storing data is something like: 

/project-name/raw-files/sensor/year/month/day/hour/minute/

What is the best way to actually get the data into the data lake?  I have read about the Azure IoT hub (and AWS IoT) but are these necessary?  It seems i would need to set up multiple cloud services to eventually get the data to the data lake and that just seems overly complicated. 

I'm very new to this so any help would be greatly appreciated!"
3975,2020-10-30 22:00:51,1604088051.0,dataengineering,Distributed File Systems,jl41js,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jl41js/distributed_file_systems/,1.0,0.0,0.0,20534.0,
3976,2020-10-31 04:21:01,1604110861.0,dataengineering,Airflow repo template,jlagzn,youngstunna24,,https://www.reddit.com/r/dataengineering/comments/jlagzn/airflow_repo_template/,1.0,0.0,0.0,20544.0,"I've worked with 4 different Airflow setups in my career now and I keep a Github template repo of my preferred setup for a local dev environment and codebase, figured I'd share it for my fellow data engineers to see! Any feedback or PRs would be very welcome :)"
3977,2020-10-31 04:37:00,1604111820.0,dataengineering,"Airflow codebase template - run locally by cloning and running ""make start-airflow""",jlapkw,youngstunna24,,https://www.reddit.com/r/dataengineering/comments/jlapkw/airflow_codebase_template_run_locally_by_cloning/,1.0,11.0,0.0,20544.0,
3978,2020-10-31 13:36:52,1604144212.0,dataengineering,"Tracheal, Bronchus &amp; Lung Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017",jlgwws,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jlgwws/tracheal_bronchus_lung_cancer_death_rates_ranking/,1.0,0.0,0.0,20558.0,
3979,2020-10-31 16:04:36,1604153076.0,dataengineering,"New to data engineering, but tasked with something that will require me to store huge quantities of data.",jliwse,Lostwhispers05,,https://www.reddit.com/r/dataengineering/comments/jliwse/new_to_data_engineering_but_tasked_with_something/,1.0,6.0,0.0,20557.0,"Hi, so the title explains what my situation is. 

The use-case is this - we have a web app that draws data from our production database to serve the right content. We're trying to collect more specific info about each user's actions, including the various API calls they make. 

My first thought was to store the info on these API calls that get made into our production DB, because that's where all the rest of the data resides and easily gets stored, but this would likely affect the performance of our product substantially. 

My question is therefore where all this data can be stored such that it can still be readily accessed for things like analytics and whatnot. The data I'll be storing will be quite huge, and would clog up our production db if we stored it there."
3980,2020-10-31 17:03:25,1604156605.0,dataengineering,"Binomial Distribution: Definition, Pdf, properties and application",jljush,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jljush/binomial_distribution_definition_pdf_properties/,1.0,0.0,0.0,20558.0,
3981,2020-10-31 18:44:56,1604162696.0,dataengineering,How to Learn Python for Data Science? The Best Ways,jlllv2,Techbiason,,https://www.reddit.com/r/dataengineering/comments/jlllv2/how_to_learn_python_for_data_science_the_best_ways/,1.0,0.0,0.0,20565.0,
3982,2020-10-31 19:40:58,1604166058.0,dataengineering,"Apache Flink: Stateful Functions Demo deployed on AWS Lambda (Stateful Serverless, FaaS)",jlmleg,Marksfik,,https://www.reddit.com/r/dataengineering/comments/jlmleg/apache_flink_stateful_functions_demo_deployed_on/,1.0,0.0,0.0,20565.0,
3983,2020-10-31 20:28:18,1604168898.0,dataengineering,Looking for advice on how to select a model for my final year project in attribution modelling,jlnfnz,Flewizzle,,https://www.reddit.com/r/dataengineering/comments/jlnfnz/looking_for_advice_on_how_to_select_a_model_for/,1.0,2.0,0.0,20566.0,"I am currently in the literature review stage of my project, where I need to understand where the science is up to in my chosen field of Attribution modelling for digital marketing in order to select a model (or models) to implement. the channels included are Google/Bing search/display, PPC, Paid social, affiliate marketing, influencer marketing, etc. The data used will be both simulated and actual data, provided by the company I am doing it for, and maybe I can find some datasets that fit the bill online too. 

If your reading this and are able to answer any of the questions below I would be extremely grateful.  


&amp;#x200B;

1. There are a lot of options, Markov chains appear a lot in the literature, but then again so do Shapley values, bagged logistic regression, multivariate point process, Hidden Markov models, and others. I'm sure it will become clearer with time, but I'm not sure what the best option would be. I need to be able to validate my decision in the report, but currently cant see the wood for the trees! Could anyone provide any insight into the benefits of particular models or the best way to be thinking about things? 
2. When it comes to tuning/ fitting the models to KPIs, would this be done in a machine learning sense where you trained the model with data on KPIs before and after a channel has been added (or had more spent on it), then tested it by having it make a prediction on how much a channel would contribute to a KPI, and comparing its prediction to the actual KPI, and doing that a bunch of times until its predictions were good?
3. Would there be different models for different KPIs?

  
I ultimately want to be able to ask the model will adding X channel have an effect on Y KPI.   


If anyone can provide any useful insight that I haven't explicitly asked about, I would also be very grateful for that.   


Thanks for reading!"
3984,2020-10-31 22:17:16,1604175436.0,dataengineering,Readable Scala Code in Apache Spark (4 attempts),jlpbgw,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/jlpbgw/readable_scala_code_in_apache_spark_4_attempts/,1.0,0.0,0.0,20567.0,
3985,2020-11-01 04:38:20,1604198300.0,dataengineering,Can someone point in right direction for learning to get / clean / organize nasa satellite data (weather and other related things ),jlv568,756677755677,,https://www.reddit.com/r/dataengineering/comments/jlv568/can_someone_point_in_right_direction_for_learning/,1.0,1.0,0.0,20572.0,Any good turials or other resources
3986,2020-11-01 13:29:12,1604230152.0,dataengineering,Apricot Production Ranking | TOP 10 Country from 1961 to 2018,jm0rsi,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jm0rsi/apricot_production_ranking_top_10_country_from/,1.0,0.0,0.0,20577.0,
3987,2020-11-01 18:52:26,1604249546.0,dataengineering,From Reactor to Coroutines,jm5hh1,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jm5hh1/from_reactor_to_coroutines/,1.0,0.0,0.0,20589.0,
3988,2020-11-01 21:15:28,1604258128.0,dataengineering,Getting to attached to code,jm81zs,joshtree41,,https://www.reddit.com/r/dataengineering/comments/jm81zs/getting_to_attached_to_code/,1.0,33.0,0.0,20591.0,"I’ve had this problem recently. 

I write something very cool and elegant then realize that it’s not actually the most efficient/easiest/best way to do it. 

Then I try to invent reasons that I should keep it in its original form. Which results in over-complication.

In my most recent battle with this anti-pattern I wanted to keep a method that was using some cool 🐼 DataFrame code golf over adding a sub query to an initial sql pull. The class pulls data from a MySQL database, does some transform, applies some business logic, then loads it into an analytics Redshift database. 

I caught this one earlier on so I’m going to rewrite it before pushing it out, but it’s such a pain haha.

Anyone else have this problem?"
3989,2020-11-02 01:05:27,1604271927.0,dataengineering,"The 15th edition of the data engineering newsletter is out. This week's articles that focus on how to structure data org? Bulldozer from Netflix, Lime's data catalog, How autonomous racecar crashed? Is it time for Decision Scientists?.",jmc4pq,vananth22,,https://www.reddit.com/r/dataengineering/comments/jmc4pq/the_15th_edition_of_the_data_engineering/,1.0,0.0,0.0,20595.0,
3990,2020-11-02 13:20:07,1604316007.0,dataengineering,Breast Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jmloqj,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jmloqj/breast_cancer_death_rates_ranking_top_10_country/,1.0,0.0,0.0,20603.0,
3991,2020-11-02 15:29:28,1604323768.0,dataengineering,2021 — Rockstar Data Engineer Roadmap,jmne7s,_kaabachi,,https://www.reddit.com/r/dataengineering/comments/jmne7s/2021_rockstar_data_engineer_roadmap/,1.0,0.0,0.0,20606.0,
3992,2020-11-02 15:46:30,1604324790.0,dataengineering,Supervised Learning in Machine Learning,jmnnpf,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jmnnpf/supervised_learning_in_machine_learning/,1.0,0.0,0.0,20606.0,
3993,2020-11-02 16:57:17,1604329037.0,dataengineering,5 Critical Elasticsearch Metrics to Monitor,jmovs6,iamondemand,,https://www.reddit.com/r/dataengineering/comments/jmovs6/5_critical_elasticsearch_metrics_to_monitor/,1.0,2.0,0.0,20607.0,
3994,2020-11-02 20:32:38,1604341958.0,dataengineering,Bivariate analysis| How to analyze data using spss (part-10),jmt8fh,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jmt8fh/bivariate_analysis_how_to_analyze_data_using_spss/,1.0,0.0,0.0,20615.0,
3995,2020-11-02 20:46:26,1604342786.0,dataengineering,Making it easy to anonymize and balance datasets with just a few clicks.,jmtj2g,alig80,,https://www.reddit.com/r/dataengineering/comments/jmtj2g/making_it_easy_to_anonymize_and_balance_datasets/,1.0,0.0,0.0,20615.0,Blueprints provide example code for solving specific use cases and data challenges: [https://github.com/gretelai/gretel-blueprints](https://github.com/gretelai/gretel-blueprints)
3996,2020-11-03 05:08:07,1604372887.0,dataengineering,Advice,jn2nv4,Jagmeetoff,,https://www.reddit.com/r/dataengineering/comments/jn2nv4/advice/,1.0,21.0,0.0,20628.0,What would you tell your younger self who's just getting started in DE and working their first job in the field?
3997,2020-11-03 05:25:28,1604373928.0,dataengineering,What is the main constraint on running larger YARN jobs and how do I increase it?,jn2xfp,Anxious_Reporter,,https://www.reddit.com/r/dataengineering/comments/jn2xfp/what_is_the_main_constraint_on_running_larger/,1.0,0.0,0.0,20628.0,"What is the main constraint on running larger YARN jobs (Hadoop version HDP-3.1.0.0 (3.1.0.0-78)) and how do I increase it? **Basically, want to do more (all of which are pretty large) sqoop jobs concurrently**.

I am currently *assuming* that I need to increase the Resource Manager heap size (since that is what I see going up on the Ambari dashboard when I run YARN jobs). How to add more resources to RM heap / why does RM heap appear to be such a small fraction of total RAM available (to YARN?) across the cluster?

Looking in Ambari: YARN ""cluster memory"" is 55GB, but ""RM heap"" is only 900MB.

Could anyone with more experience tell me what is the difference and which is the limiting factor in running more YARN applications (and again, how do I increase it)? Should I add more nodes or is there some config that can be changed to use more of this cluster memory in the RM heap? Is that even advisable (ie. if the cluster memory is X, what percent should be allocated to the RM heap)?

Thanks"
3998,2020-11-03 05:56:37,1604375797.0,dataengineering,How to do Data Visualization in Python for Data Science,jn3edv,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jn3edv/how_to_do_data_visualization_in_python_for_data/,1.0,0.0,0.0,20633.0,
3999,2020-11-03 09:06:06,1604387166.0,dataengineering,Best ETL tool with potential,jn5v35,sharavanan_,,https://www.reddit.com/r/dataengineering/comments/jn5v35/best_etl_tool_with_potential/,1.0,4.0,0.0,20632.0,"I'd like to understand the best ETL tool for the future and the functionalities that offer, doesn't matter of it's a tad small, I'd like to understand the power and efficiency mainly."
4000,2020-11-03 12:32:58,1604399578.0,dataengineering,Career advice,jn81xh,Appropriate_Cover_92,,https://www.reddit.com/r/dataengineering/comments/jn81xh/career_advice/,1.0,18.0,0.0,20636.0,I am a Master's student pursuing a course focused on Machine learning and data science. But I am more interested in the Data Engineering profile. I have had past work experiences in BI and tools like SSRS and Power BI. What should I absolutely learn in order to be a confident data engineer?
4001,2020-11-03 12:47:58,1604400478.0,dataengineering,Bulk loading data into azure cosmos gremlin or any graph database,jn87lr,psykiran_ms,,https://www.reddit.com/r/dataengineering/comments/jn87lr/bulk_loading_data_into_azure_cosmos_gremlin_or/,1.0,0.0,0.0,20639.0,"Hi guys ,

How are you loading data into graph databases for analytical purposes.

Weekly load.

We will be using a Viz tool to take the data from graph database, but I'm not seeing a way to load data in a bulk fashion instead of inserting with queries.

What do you do ?"
4002,2020-11-03 14:01:58,1604404918.0,dataengineering,Cassava Production Ranking | TOP 10 Country from 1961 to 2018,jn92jq,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jn92jq/cassava_production_ranking_top_10_country_from/,1.0,0.0,0.0,20639.0,
4003,2020-11-03 17:08:32,1604416112.0,dataengineering,Help installing Apache Airflow,jnbvjo,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jnbvjo/help_installing_apache_airflow/,1.0,4.0,0.0,20646.0,"I followed the start guide very closely: https://airflow.apache.org/docs/stable/start.html 

I created an Airflow directory in my Coding directory: `/Users/amit/Coding/Airflow`

I set the path in my .zprofile: 
`export AIRFLOW_HOME=""/Users/amit/Coding/Airflow""`

When I tried to `initdb` I got the following error: `ImportError: cannot import name 'resolve_types' from 'attr' (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/attr/__init__.py)` 

I tried to start the webserver using `airflow webserver -p 8080 and I got the same error. 

Does anyone know what I did wrong in the installation process?"
4004,2020-11-03 18:47:16,1604422036.0,dataengineering,DB Link like feature in Azure Synapse?,jndrk7,ksubrent,,https://www.reddit.com/r/dataengineering/comments/jndrk7/db_link_like_feature_in_azure_synapse/,1.0,0.0,0.0,20649.0,Is it possible to select data from a table in a separate Synapse instance within the same subscription? Similar to the Oracle equivalent as a DB Link. I do not want to call a data pipeline to accomplish this. Thanks!
4005,2020-11-03 21:11:20,1604430680.0,dataengineering,Concurrent reads on shared HDF5 causing corruptions. Any solutions besides using a DB?,jngnjf,throwaway_chef1,,https://www.reddit.com/r/dataengineering/comments/jngnjf/concurrent_reads_on_shared_hdf5_causing/,1.0,2.0,0.0,20655.0,"We were getting a lot of data from a vendor and this IO strain happens for every compute. To get around this IO problem, I put data on HDF5 on a shared file system as a central source of data for multiple users. On the shared file system, multiple users/servers/nodes can access this data at will for their computations as opposed to each process retrieving the data from the vendor (20 mins).

This data is updated at midnight when no users can read it. I CANNOT use a DB due to IT restrictions.

it seems like HDF5 can do single write, multi-read, but it looks to have bugs in that function since file can become corrupted


My questions are:
1) In python, are HDF's able to support concurrent reads? I'm getting mixed answers depending on h5py or pytables.
2) Is there a more resilient file format for concurrent reads?
3) How do I avoid data corruption? And what causes this corruption in the first place?)

Thanks"
4006,2020-11-03 21:19:02,1604431142.0,dataengineering,A data pipeline that will automatically transform datasets so they can be safely used,jngt1i,alig80,,https://www.reddit.com/r/dataengineering/comments/jngt1i/a_data_pipeline_that_will_automatically_transform/,1.0,1.0,0.0,20656.0,Developers want to work with data that closely mirrors what’s in production. [https://gretel.ai/blog/auto-anonymize-production-datasets-for-development](https://gretel.ai/blog/auto-anonymize-production-datasets-for-development)
4007,2020-11-04 07:54:52,1604469292.0,dataengineering,The Most Powerful Data Analytics Platforms For You,jnraqh,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jnraqh/the_most_powerful_data_analytics_platforms_for_you/,1.0,1.0,0.0,20669.0,
4008,2020-11-04 08:07:10,1604470030.0,dataengineering,DE experience for applying to jobs,jnrgiu,fidegty_bleep,,https://www.reddit.com/r/dataengineering/comments/jnrgiu/de_experience_for_applying_to_jobs/,1.0,1.0,0.0,20669.0,"I am applying for jobs as DE and have total 3 years of experience overall and with big data , though i have good technology stack under my command and in resume , i am still not getting selected for any round after submitting resume. I am not applying in any startup so is 3 year of experience not enough to go for big companies ? Do they only take consider someone with 4-6 years of experience?"
4009,2020-11-04 08:18:57,1604470737.0,dataengineering,Intel Snaps Up Cnvrg.io,jnrlz6,The-Techie,,https://www.reddit.com/r/dataengineering/comments/jnrlz6/intel_snaps_up_cnvrgio/,1.0,0.0,0.0,20669.0,
4010,2020-11-04 09:00:32,1604473232.0,dataengineering,Foundations of Software Engineering,jns4ed,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/jns4ed/foundations_of_software_engineering/,1.0,1.0,0.0,20669.0,
4011,2020-11-04 13:42:40,1604490160.0,dataengineering,DuckDB – The little OLAP database that could. TPC-DS Benchmark Results and First Impressions,jnva2l,dingopole,,https://www.reddit.com/r/dataengineering/comments/jnva2l/duckdb_the_little_olap_database_that_could_tpcds/,1.0,0.0,0.0,20672.0,
4012,2020-11-04 15:20:04,1604496004.0,dataengineering,Cervical Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jnwjt8,Agreeable-Victory694,,https://www.reddit.com/r/dataengineering/comments/jnwjt8/cervical_cancer_death_rates_ranking_top_10/,1.0,0.0,0.0,20672.0,
4013,2020-11-04 17:41:31,1604504491.0,dataengineering,Streaming Vitess at Bolt,jnyvs0,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jnyvs0/streaming_vitess_at_bolt/,1.0,0.0,0.0,20676.0,
4014,2020-11-04 17:48:34,1604504914.0,dataengineering,Learn Flink: Hands-on Training provided by the Flink community,jnz05v,Marksfik,,https://www.reddit.com/r/dataengineering/comments/jnz05v/learn_flink_handson_training_provided_by_the/,1.0,0.0,0.0,20676.0,
4015,2020-11-04 21:17:45,1604517465.0,dataengineering,Delta Lake? what’s next: an rdbms?,jo2ypu,Upitor,,https://www.reddit.com/r/dataengineering/comments/jo2ypu/delta_lake_whats_next_an_rdbms/,1.0,6.0,0.0,20680.0,"Point of the title: can anybody explain the advantages and disadvantages of Delta Lake vs a traditional datawarehouse on an rdbms? It seems to me like we are close to coming full circle, and reinviting the wheel? 

I deal have fairly large datasets (fx 8 million new rows per day for some datasets), where the source data changes over time, so being able to delete and update data seems desirable. But if I were to choose between the two approaches, why would I choose one or the other?"
4016,2020-11-04 23:31:36,1604525496.0,dataengineering,Explain to me what a data model really is in data engineering / science,jo5go8,ChrisIsWorking,,https://www.reddit.com/r/dataengineering/comments/jo5go8/explain_to_me_what_a_data_model_really_is_in_data/,1.0,7.0,0.0,20679.0,"Trying to learn the lingo here.  I hear and see the term ""data model"" and ""data modeling"" used a lot in the data science and engineering space.

I come from the business intelligence space so data model to me is like a Power BI model that relates your dimensional tables to your fact tables.

I was viewing a product demo with a guy who said he's creating a ""data model"" when all it really was is a new SQL table or view transformed from data in some other source tables.  Maybe the whole pipeline was the model but then I see pipelines as their own thing.

What do you guys mean by data model?!?!"
4017,2020-11-04 23:38:12,1604525892.0,dataengineering,How Yotpo Drives Data Observability with Monte Carlo,jo5l2p,mkvor8,,https://www.reddit.com/r/dataengineering/comments/jo5l2p/how_yotpo_drives_data_observability_with_monte/,1.0,0.0,0.0,20679.0,"The company, not simulation. :)  

For Yotpo, an e-commerce marketing platform, data downtime led to sleepless nights, costly fire drills, and unreliable analytics dashboards. At the end of the day, they needed a solution to tell them what went wrong in their data pipelines and how to fix it. See how monitoring their data helped them achieve data reliability. 

RSVP here: [https://my.demio.com/ref/vRkTMA2PpWoUTFXh](https://my.demio.com/ref/vRkTMA2PpWoUTFXh)

https://preview.redd.it/sdb7hk8hlax51.png?width=1200&amp;format=png&amp;auto=webp&amp;s=48bdae1400520e96c463a4426df33b2480359c4f"
4018,2020-11-05 01:34:18,1604532858.0,dataengineering,Papermill via BashOperator keeps failing,jo7mce,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jo7mce/papermill_via_bashoperator_keeps_failing/,1.0,0.0,0.0,20682.0,"I'm trying to run the following DAG: 

    notebook_in_path = '/Users/amit/Coding/Projects/AmazonOrderHistoryAirflow/AmazonOrderHistoryAirflow.ipynb'
    notebook_out_path = '/Users/amit/Coding/Projects/AmazonOrderHistoryAirflow/AmazonOrderHisotryAirflow{{execution_date}}.ipynb'

    t2 = BashOperator(
	    task_id = 'run_notebook',
	    bash_command = 'papermill {} {} -p alpha 0.6 -p l1_ratio 0.1'.format(notebook_in_path,notebook_out_path),
	    dag = dag
	    )

I got the syntax from Papermill's documentation: 
https://github.com/nteract/papermill https://pypi.org/project/papermill/ 

However, I got the following error and I don't know what it means: https://i.imgur.com/HLwcmL2.png"
4019,2020-11-05 02:51:52,1604537512.0,dataengineering,"Posted in datascience, but maybe anyone here has done something similar already?",jo8wr5,plodzik,,https://www.reddit.com/r/dataengineering/comments/jo8wr5/posted_in_datascience_but_maybe_anyone_here_has/,1.0,0.0,0.0,20687.0,
4020,2020-11-06 20:06:40,1604686000.0,dataengineering,Overall Data Pipeline Solution: Beginner,jpa2c7,Culpgrant21,,https://www.reddit.com/r/dataengineering/comments/jpa2c7/overall_data_pipeline_solution_beginner/,1.0,13.0,0.0,20749.0,"I am new to data engineering and would like to get a better understanding of how something like this would work in a company. Ideally, I am looking for a rough solution to the steps you would take (very high level).

Problem: Your company starts using a new marketing email automation tool. They want to get all the data from it and build a Tableau/Power BI dashboard for the data. The marketing automation tool has an API to pull data from. The data needs to be updated once a day. The number of rows is in the millions (\~20).

&amp;#x200B;

So would the idea be to call the API overnight load the data into a SQL database and connect the data visualization tool to the SQL database?

What would be your way of solving this problem? What tools would you use?

&amp;#x200B;

Thank you all I am interested to learn more!"
4021,2020-11-07 10:52:25,1604739145.0,dataengineering,Switching to Oauth &amp; openID,jpnkdo,Rubbervuist,,https://www.reddit.com/r/dataengineering/comments/jpnkdo/switching_to_oauth_openid/,1.0,0.0,0.0,20764.0,"Hi

I used to query my financial data through Power Query in Power BI. Recently I've switched to doing it through a python script running on Google Cloud functions, triggered by Cloud Scheduler. (is this the best way?) It saves a .csv file to GCStorage.

The party that provides the data I'm after is switching to oAuth 2.0 using either implicit or authorization code flow. I believe this means that somewhere in this flow a browser is opened where username and password must be entered. Also I need to give a redirect uri to this party, I'm not sure how to implement this in my current setup.

Anyone have an idea? More info about the API can be found here. https://accounting.twinfield.com/webservices/documentation/#/ApiReference/Authentication/OpenIdConnect"
4022,2020-11-07 12:52:25,1604746345.0,dataengineering,Uterine Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jpowbg,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jpowbg/uterine_cancer_death_rates_ranking_top_10_country/,1.0,0.0,0.0,20768.0,
4023,2020-11-07 14:21:27,1604751687.0,dataengineering,Solutions Architect to Data Engineer?,jppyao,plexex,,https://www.reddit.com/r/dataengineering/comments/jppyao/solutions_architect_to_data_engineer/,1.0,10.0,0.0,20771.0,"I'm currently a junior in undergrad and recently got an offer for a Solutions Architect Intern role at AWS this summer. From what I know, it's more of a customer facing IT role, and while it is for a FAANG company, I'm not sure if this is the right internship to take for me, as I've been pursuing data engineering roles and have been trying to get into data engineering, studying data structures, algorithms, databases, SQL, etc.

The only other offer I have right now is a data engineering internship for a smaller company with less pay/benefits. However, this seems to be more aligned with setting me up for a career in data engineering.

Any insight as to whether I should still accept the AWS internship? Is there any flexibility there as far as where it may take me? Do you think it still aligns well with my career goals of getting into data engineering?
I know it's just an internship but itd be nice to get some insight to make a good decision."
4024,2020-11-07 17:41:13,1604763673.0,dataengineering,Can I get more details on my Broken Dag error in Airflow?,jpswhj,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/jpswhj/can_i_get_more_details_on_my_broken_dag_error_in/,1.0,5.0,0.0,20772.0,"New jr data engineer here, with a broken dag!

This is the error I see, (also I'm running on ec2 and not in docker). Wondering how I can get more info on this broken dag other then the vague ""match\_ad\_unit"" which is in almost every line of the code.

&amp;#x200B;

https://preview.redd.it/s3ki9skm8ux51.png?width=1242&amp;format=png&amp;auto=webp&amp;s=99f01b3060efd451b621d1f933d3b39abf4eeb26"
4025,2020-11-07 20:35:19,1604774119.0,dataengineering,[MongoDB] How to add a subdocument array to an existing document without it,jpvi1n,timfcrn,,https://www.reddit.com/r/dataengineering/comments/jpvi1n/mongodb_how_to_add_a_subdocument_array_to_an/,1.0,0.0,0.0,20776.0,
4026,2020-11-07 21:10:30,1604776230.0,dataengineering,Favorite part about DE?,jpw70s,Jagmeetoff,,https://www.reddit.com/r/dataengineering/comments/jpw70s/favorite_part_about_de/,1.0,10.0,0.0,20779.0,What makes you really interested in DE?
4027,2020-11-07 23:11:34,1604783494.0,dataengineering,"Airflow error: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.",jpyfqk,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jpyfqk/airflow_error_pendingdeprecationwarning_the/,1.0,4.0,0.0,20782.0,"As soon as I start the airflow webserver, I get the error in the title: https://i.imgur.com/BxJu8nE.png

I noticed in my graph that both of the following tasks are showing as no status: https://i.imgur.com/Q3XnHCf.png 

I have the following task in my dag:

    t3 = PostgresOperator(
	    task_id = 'create_table_postgres',
	    postgres_conn_id = 'amazon_order_history_aws',
	    sql = '''
	    DROP TABLE IF EXISTS amazon.purchases_aws; 
	    CREATE TABLE amazon.purchases_aws (
	    OrderID int not null primary key,
	    OrderDate date,
	    Category varchar(50),
	    `Condition` varchar(50),
	    Seller varchar(50),
	    ListPricePerUnit numeric(10,2),
	    PurchasePricePerUnit numeric(10,2),
	    Quantity int,
	    ShipDate date,
	    Carrier varchar(50),
	    ItemSubtotal numeric(10,2),
	    Tax numeric(10,2),
	    ItemTotal numeric(10,2),
	    OrderYear int,
	    OrderMonth int,
	    OrderDay int,
	    OrderDayIndex int,
	    OrderDayName varchar(50)
	    );''',
	    dag = dag
    )

    hook_copy_table = airflow.hooks.postgres_hook.PostgresHook('amazon_order_history_aws')

    def import_csv_postgres():
	    """"""
	    Imports CSV into Postgres RDS
	    """"""
	    sql = ""DELETE FROM amazon.purchases_aws; COPY amazon.purchases_aws FROM STDIN WITH CSV DELIMITER ',', HEADER;""
	    hook_copy_table.copy_expert(sql, csv_output_path, open = open)

    t4 = PythonOperator(
	    task_id = 'import_csv_postgres',
	    python_callable = import_csv_postgres,
	    provide_context = False,
	    dag = dag
    )

I don't have a duplicate task anywhere in my dag as this suggested: https://stackoverflow.com/questions/63973944/getting-error-while-making-a-new-dag-in-apache-airflow

I have no idea what is happening."
4028,2020-11-07 23:28:36,1604784516.0,dataengineering,Slowly changing dimensions in depth (21:33),jpyqr9,SnirD,,https://www.reddit.com/r/dataengineering/comments/jpyqr9/slowly_changing_dimensions_in_depth_2133/,1.0,0.0,0.0,20783.0,
4029,2020-11-08 13:19:51,1604834391.0,dataengineering,Potato Consumption Ranking | TOP 10 Country from 1961 to 2013,jqa5ow,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jqa5ow/potato_consumption_ranking_top_10_country_from/,1.0,1.0,0.0,20800.0,
4030,2020-11-08 19:42:57,1604857377.0,dataengineering,How do I run an Airflow DAG located in another directory?,jqfv2w,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jqfv2w/how_do_i_run_an_airflow_dag_located_in_another/,1.0,12.0,0.0,20808.0,"I have a `project_directory` that contains all of my scripts. I use this directory to push my code to a Github repo. Right now, when I use Airflow, I manually copy my DAGs from my `airflow_home directory` and put them into `project_directory`. 

How do I eliminate this redundancy? Can I create a DAG in `airflow_home` that runs the DAG in `project_directory`? Or is there another way that this is accomplished? 

Thank you"
4031,2020-11-08 20:50:28,1604861428.0,dataengineering,Columnar VS Row based DB query,jqh327,redder_ph,,https://www.reddit.com/r/dataengineering/comments/jqh327/columnar_vs_row_based_db_query/,1.0,8.0,0.0,20808.0,"From a performance POV what is the diff between the following queries?

`select qty, amt from product; (RDBMS)`    

`select qty, amt from product; (Columnar DB)`"
4032,2020-11-08 23:37:40,1604871460.0,dataengineering,Airflow + AWS EC2: Configure Security Group,jqk5nw,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jqk5nw/airflow_aws_ec2_configure_security_group/,1.0,6.0,0.0,20814.0,"I'm trying to follow the guide here https://medium.com/@abraham.pabbathi/airflow-on-aws-ec2-instance-with-ubuntu-aff8d3206171 but the author skims over the Configure Security Group section. 

He writes the following: 
&gt;When it asks for setting up security group, add the rule to open port 8080 to public as that’s the port through which you can connect to airflow server.

So, would I add a Custom TCP and enter 8080 in the  `Port Range`: https://i.imgur.com/3K3MfOs.png"
4033,2020-11-08 23:53:10,1604872390.0,dataengineering,Can anyone provide some beginner project ideas using AWS cloud infra ?,jqkg4m,Beast-UltraJ,,https://www.reddit.com/r/dataengineering/comments/jqkg4m/can_anyone_provide_some_beginner_project_ideas/,1.0,7.0,0.0,20814.0,I really want to get my hand dirty in data engineering but something in AWS. Since I use AWS a-lot as cloud support engineer. Which data would be the best place to start. Do I need to deploy some database to store the data there ? Any advice would be appreciated.
4034,2020-11-09 00:40:44,1604875244.0,dataengineering,"Welcome to the 16th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Data quality at Airbnb, Data version control tools, Spotify experimentation framework, Uber’s proposal to add remote shuffle for Spark.",jqlar7,vananth22,,https://www.reddit.com/r/dataengineering/comments/jqlar7/welcome_to_the_16th_edition_of_the_data/,1.0,0.0,0.0,20814.0,
4035,2020-11-09 09:23:40,1604906620.0,dataengineering,Is it even possible to get a job without spark/Hadoop experience?,jqt8gv,pbj800100,,https://www.reddit.com/r/dataengineering/comments/jqt8gv/is_it_even_possible_to_get_a_job_without/,1.0,19.0,0.0,20823.0,"I've been a ""data engineer"" (questioning if I can even call myself that anymore. Sad face) for around 2 years at a small company. I use python, SQL and AWS to build small scale pipelines moving data around between platforms. I've been looking for a new job because I know I'm missing a lot of key technologies like ETL tools, big data frameworks etc. My company just hasn't had a need for them and I'm the only engineer (first engineering job). My whole reason is to leave and get this experience somewhere else but seems impossible to get hired. I'm super eager to learn and I've made that clear. I WANT to be challenged in my next role. I've had so many interviews now where I've been told that someone would love to hire me as I'm the type of person they're looking for, but im just not technically qualified enough. Didn't think it'd be this hard as everyones got to start somewhere. I didn't study in a relevant field (bachelor's in math) so I don't know if I'm just super behind everyone else that's looking for junior roles?  I'm searching only by entry level stuff as I'm happy with that and want a senior mentor, but even these jobs want someone that sounds more like a senior engineer. So frustrating... I might be ranting more than looking for advice tbh. I know I should have done personal projects outside of work to pick up these skills but my current job is so busy I'm already working until 8-9pm most nights so when I'm not working, data is the last thing I want to think about, as much as I love it!"
4036,2020-11-09 11:30:21,1604914221.0,dataengineering,Are there any no-code solutions for extracting data from excel spreadsheets?,jqumr7,danbcooper,,https://www.reddit.com/r/dataengineering/comments/jqumr7/are_there_any_nocode_solutions_for_extracting/,1.0,24.0,0.0,20829.0,"I receive about 100 emails every day, each containing a spreadsheet or csv with different formats. I need to pull data from specific cells from each file and load them into mssql.

My plan was to build a python script for each type of file and schedule them in airflow.

But I feel like this is going to be a nightmare for a future handover. Are there any no-code alternatives?  The files being kind of unstructured really limits the options..

Thanks!"
4037,2020-11-09 12:49:38,1604918978.0,dataengineering,Prostate Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jqviox,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jqviox/prostate_cancer_death_rates_ranking_top_10/,1.0,0.0,0.0,20829.0,
4038,2020-11-09 14:43:08,1604925788.0,dataengineering,The Data Janitor Letters - October 2020,jqwxhx,soobrosa,,https://www.reddit.com/r/dataengineering/comments/jqwxhx/the_data_janitor_letters_october_2020/,1.0,0.0,0.0,20830.0,"Testing SQL, open source databases, ETL Batch with Kafka, data discovery, binary search, Google Analytics data and a multi-touch attribution model.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-october-2020](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-october-2020)"
4039,2020-11-09 15:49:26,1604929766.0,dataengineering,"From Aligned to Unaligned Checkpoints: Checkpoints, Alignment, and Backpressure in Apache Flink",jqxwnq,Marksfik,,https://www.reddit.com/r/dataengineering/comments/jqxwnq/from_aligned_to_unaligned_checkpoints_checkpoints/,1.0,0.0,0.0,20834.0,
4040,2020-11-09 18:03:27,1604937807.0,dataengineering,Online Pantalk Zürich 2020 - Data as a Product,jr09m5,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/jr09m5/online_pantalk_zürich_2020_data_as_a_product/,1.0,1.0,0.0,20842.0,"Join us on the next online Pantalk on 17.11.2020 6.30 pm CET

Our CTO, Samuel Benz is going to talk about the importance of data transparency in companies and why teams should be made responsible for their data and be declared data owners. He’ll present a use case of a company that implements parts of the vision to stream Apache Kafka as a self-service. ⁠

🤞🏼Hope to see you there!  

More information here ➡️ https://www.pantalks.ch/ 

Livestream here ➡️ [https://www.youtube.com/watch?v=5jwQwFt9fIg&amp;feature=youtu.be](https://www.youtube.com/watch?v=5jwQwFt9fIg&amp;feature=youtu.be)

Cheers, 

SPOUD Agoora"
4041,2020-11-09 18:47:26,1604940446.0,dataengineering,Airflow managed solution - workflow deployment,jr14tq,digichap28,,https://www.reddit.com/r/dataengineering/comments/jr14tq/airflow_managed_solution_workflow_deployment/,1.0,8.0,0.0,20844.0,"I’m trying to understand which is the best way (best practices) to deploy and execute the workflows using an airflow managed solution (Astronomer, Cloud composer, etc ).

With a self airflow deployment using Docker containers I created a volume with the following structure:

* Volume:

- dags
- scripts (Python scripts )

And according to what I have read, in a managed solution the only thing you upload are the dags and not the scripts. Am I wrong ? If I’m right, where should the scripts reside and be executed ?"
4042,2020-11-09 20:56:49,1604948209.0,dataengineering,Your data tests failed! Now what? (Blog),jr3qqp,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/jr3qqp/your_data_tests_failed_now_what_blog/,1.0,0.0,0.0,20848.0,
4043,2020-11-09 21:14:34,1604949274.0,dataengineering,How to pass the Apache Spark 3.0 Certification - New Udemy Course,jr43wq,gwadson,,https://www.reddit.com/r/dataengineering/comments/jr43wq/how_to_pass_the_apache_spark_30_certification_new/,1.0,0.0,0.0,20848.0,"  

Dear fellow data professionals,

Here is something new to help you grow your career for only 9.99€  
 [https://www.udemy.com/course/apache-spark-3-databricks-certified-associate-developer/?couponCode=8A079EDBB6E015968642](https://www.udemy.com/course/apache-spark-3-databricks-certified-associate-developer/?couponCode=8A079EDBB6E015968642)

The skills you gained in the current course allow you to start working with Apache Spark and Databricks. 

As a beginner in a given technology, it is often necessary to prove your skills to potential employers.

The **Databricks Certified Associate Developer for Apache Spark 3.0 certification** can help you prove your beginner's skill in Apache Spark.

I have a new course that can help you earn a Databricks Certification.

After passing the exam this August, I created this course to help everybody earn an Apache Spark Certification.

The course covers all skills required by Databricks to pass the exam, such as:

· Basic understanding of the Spark Architecture

· Basic understanding of the Adaptive Query Execution

· Understanding of the Execution Hierarchy of Apache Spark

· Writing and Partitioning DataFrames with schemas

· Understanding different Deployment Modes

· Selecting, Renaming, and Manipulating columns

· Filtering, Dropping, Sorting, and Aggregating Rows

· Joining, Reading, Writing, and Partitioning DataFrames

· Working with UDFs and Spark SQL function

· Handling missing data

· **AND MUCH MORE**

You can take the course using the link below 

[https://www.udemy.com/course/apache-spark-3-databricks-certified-associate-developer/?couponCode=8A079EDBB6E015968642](https://www.udemy.com/course/apache-spark-3-databricks-certified-associate-developer/?couponCode=8A079EDBB6E015968642)

I hope you find the course helpful and that the course supports the grow your career.

Best regards and good luck  
 Wadson"
4044,2020-11-09 21:26:48,1604950008.0,dataengineering,Data + AI Summit Europe 2020 training check it out,jr4ckj,adamrabinovitch,,https://www.reddit.com/r/dataengineering/comments/jr4ckj/data_ai_summit_europe_2020_training_check_it_out/,1.0,0.0,0.0,20848.0,
4045,2020-11-09 23:42:41,1604958161.0,dataengineering,Airflow Sensor Timeout. What are some reasons?,jr747d,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/jr747d/airflow_sensor_timeout_what_are_some_reasons/,1.0,4.0,0.0,20853.0,"Hello! Jr Data Engineer here w. an airflow question.

So I'm combining 5 dags we have into 1 daily dag. I've just created it and now  I'm trying to test on our dev airflow server but I'm getting a lot of Sensor TimeOut failures, specifically: 

raise AirflowSensorTimeout('Snap. Time is OUT.') 

(Also, the 5 dags I'm combining are still running on the master airflow server, wondering if that someone effects this). 

What are some reasons for this timeout? And how can it be prevented?"
4046,2020-11-09 23:54:32,1604958872.0,dataengineering,Migrating existing airflow project into Astronomer,jr7cfg,pagenotdisplayed,,https://www.reddit.com/r/dataengineering/comments/jr7cfg/migrating_existing_airflow_project_into_astronomer/,1.0,2.0,0.0,20853.0,"I'm trying out Astronomer's 14 day free trial. I already have my own github repo with an airflow project, that I currently manage myself (using docker, running it on a GCP compute engine), however I could really use the devops help that comes with using Astronomer, since I'm not a sophisticated data engineer.

Their quickstart guide https://www.astronomer.io/docs/cloud/stable/get-started/quickstart/ walks through setting up a new project / example DAG. However, I cannot find (in the quickstart, or elsewhere) out how to migrate an existing project into Astronomer. I don't want to have 2 separate directory for my 1 airflow deployment, and I don't want to copy all of my code into a new directory. 

I guess my question then is - is it safe to run `astro dev init` from my main / already existing github repo / airflow project? I am worried that this will overwrite some previous files (Dockerfile, .env, etc., the files created by `astro dev init` mostly...). Or, should I create a new empty repo, init it with both github and astronomer, and then move my existing code into this new repo (and, i guess, not use the old repo anymore).

I'm up against the clock on this 14 day free trial, so any help is appreciated on this!"
4047,2020-11-10 03:39:04,1604972344.0,dataengineering,Whats your companies ETL/job deployment process?,jrbg3t,importpandaaspd,,https://www.reddit.com/r/dataengineering/comments/jrbg3t/whats_your_companies_etljob_deployment_process/,1.0,17.0,0.0,20860.0,"Recently joined an enterprise scale company and the ETL/job deployment process is marked with lots of red tape. I come from a small company where I would be the owner of the entire infrastructure where I'd be able to schedule and manage every DAG.

How does your company manage and schedule its workflows?"
4048,2020-11-10 04:24:33,1604975073.0,dataengineering,Where can I find Geospatial Data Sources that are updated daily?,jrc8ef,engineer_of_data,,https://www.reddit.com/r/dataengineering/comments/jrc8ef/where_can_i_find_geospatial_data_sources_that_are/,1.0,3.0,0.0,20861.0,"I want to do a side project working with a large amount of geospatial data. The ideal dataset would be something that is updated daily.

I'd like to be able to pull this data from an API, process it in Dask, and then put the results of the computations and maybe a few visualizations on a dashboard. I'm also going to orchestrate this using Apache Airflow on an AWS EKS cluster. The Dask processing and computations would be running on a separate Dask cluster on EC2."
4049,2020-11-10 11:41:55,1605001315.0,dataengineering,How to Access Google Analytics with Python,jri67z,underback007,,https://www.reddit.com/r/dataengineering/comments/jri67z/how_to_access_google_analytics_with_python/,1.0,0.0,0.0,20875.0,
4050,2020-11-10 11:47:31,1605001651.0,dataengineering,Access Google Analytics with Python,jri8b9,underback007,,https://www.reddit.com/r/dataengineering/comments/jri8b9/access_google_analytics_with_python/,1.0,1.0,0.0,20875.0,"Best guide on how to set up and use the Google Analytics API.

You can now automate all your BI reports!"
4051,2020-11-10 12:19:41,1605003581.0,dataengineering,"Pod Launching failed: Pod took too long to start, Failed to run KubernetesPodOperator secret",jrilia,Key-Coat-3406,,https://www.reddit.com/r/dataengineering/comments/jrilia/pod_launching_failed_pod_took_too_long_to_start/,1.0,1.0,0.0,20877.0,
4052,2020-11-10 14:17:42,1605010662.0,dataengineering,An interview about the Tree Schema data catalog platform and using it to quickly get visibility into your data assets.,jrjywa,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/jrjywa/an_interview_about_the_tree_schema_data_catalog/,1.0,0.0,0.0,20882.0,
4053,2020-11-10 14:27:10,1605011230.0,dataengineering,Maize Consumption Ranking | TOP 10 Country from 1961 to 2013,jrk363,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jrk363/maize_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,20882.0,
4054,2020-11-10 17:25:35,1605021935.0,dataengineering,Opening/Closing ODBC connections to SQL databases in Jupyter notebooks,jrmv26,ChrisIsWorking,,https://www.reddit.com/r/dataengineering/comments/jrmv26/openingclosing_odbc_connections_to_sql_databases/,1.0,5.0,0.0,20892.0,"Hi All,

In some Jupyter Notbooks, I'm using pyodbc to connect to some sql server databases and run some t-sql queries.

I 've defined a function which takes a sql query string as an argument, then opens an odbc connection to the database, executes the sql query (able to use t-sql in this case), then closes the connection object, and finally returns the results of the query as a pandas dataframe object.

Within the notebooks, especially for research and analysis purposes, you may never know how many queries you'll be executing and the order may go back and forth between several cells.  Thus, I felt opening and closing the database connection within the function was the proper way to handle this.

My question is... if running the 'execute sql' function several times, opening and closing the connection each time, am I creating some kind of chaotic connection activity and driving the database admins crazy? 

 Would it be more preferable to open a connection outside the function and then close it a the end of the notebook?  I think that might be more error prone as perhaps not everyone would run that initial open connection cell and also likely for get to run the close connection when they're done."
4055,2020-11-10 18:46:11,1605026771.0,dataengineering,"Using Jupyter Notebooks as an ""all in one"" deployment testing &amp; documentation solution?",jroef0,ChrisIsWorking,,https://www.reddit.com/r/dataengineering/comments/jroef0/using_jupyter_notebooks_as_an_all_in_one/,1.0,27.0,0.0,20895.0,"I'm trying to automate some pipeline deployment testing.  I have to make sure that select data in the Development and Production environments either matches or looks reasonable with explanations for any anomalies.

The data in both environments is in SQL Server databases.  

Right now the testing involves running queries in SQL Server Mgmt Studio, exporting data to excel spreadsheet, and then doing some lookups or compares.  We also keep a Microsoft OneNote or Word doc where we summarize the results of each test.

I've recently been learning Jupyter Notebooks and feel the ability to combine code, display some results as an output preview, export full test result files (executed by code) and of course markdown for explanations would be perfect.  I see the jupyter notebooks can be exported as HTML or PDF files which could replace the OneNote and Word docs.

Wanted to run this by you guys if case you either know or see some pitfalls of using jupyter notebooks to perform 'deployment testing' as I've described."
4056,2020-11-10 19:33:46,1605029626.0,dataengineering,Avo’s Ultimate Tracking Plan Template (w/ Downloadable Worksheet),jrpc99,kelseyfecho,,https://www.reddit.com/r/dataengineering/comments/jrpc99/avos_ultimate_tracking_plan_template_w/,1.0,0.0,0.0,20898.0,
4057,2020-11-10 22:28:26,1605040106.0,dataengineering,"Application Data Engineer, Cloud Systems- Google",jrsuzl,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/jrsuzl/application_data_engineer_cloud_systems_google/,1.0,7.0,0.0,20906.0,"**I have seen this job positing, is this a Data Engineer role at google, or do that have more 'traditional' data engineering roles? Interested I guess in what Data Engineering Careers look like at google and if this is a good start?**"
4058,2020-11-10 23:48:24,1605044904.0,dataengineering,How is Snowflake so much better for development cycles than traditional databases used for Data Warehousing,jruf8w,fhoffa,,https://www.reddit.com/r/dataengineering/comments/jruf8w/how_is_snowflake_so_much_better_for_development/,1.0,1.0,0.0,20910.0,
4059,2020-11-11 03:08:21,1605056901.0,dataengineering,Setup for home server,jry3dt,BruToine,,https://www.reddit.com/r/dataengineering/comments/jry3dt/setup_for_home_server/,1.0,10.0,0.0,20919.0,"Hello everyone,

&amp;#x200B;

I want to setup my own server for DE / DS projects but I'm not sure

where to start. What kind of technologies would you use for a pipeline

that need to scrape a website every weeks to train a model. I will also

need to host an API for making predictions.

&amp;#x200B;

Thanks"
4060,2020-11-11 05:55:29,1605066929.0,dataengineering,Airflow with multiple ec2 instances,js0ua5,furiousnerd,,https://www.reddit.com/r/dataengineering/comments/js0ua5/airflow_with_multiple_ec2_instances/,1.0,21.0,0.0,20930.0,"Was wondering what best practices were for handling multiple jobs in production.

I have two sequential jobs that have different memory and environment requirements so they're set up on different instances. I was thinking of having a third instance that just uses airflow and kicks off the jobs on those two instances. Is this how airflow is meant to be used here?

I guess what I'm also wondering is what's the best way to handle aws credentials? I have them on all the instances, but this feels like a security risk. Is there a way to store them on one and push them only when needed? What's considered best practice?

Appreciate the advice!"
4061,2020-11-11 08:04:12,1605074652.0,dataengineering,Effect of blockchain on programming languages especially python,js2ptt,chase2learn,,https://www.reddit.com/r/dataengineering/comments/js2ptt/effect_of_blockchain_on_programming_languages/,1.0,0.0,0.0,20935.0,
4062,2020-11-11 08:05:10,1605074710.0,dataengineering,What is Logistic Regression and how does it work?,js2qbo,chase2learn,,https://www.reddit.com/r/dataengineering/comments/js2qbo/what_is_logistic_regression_and_how_does_it_work/,1.0,0.0,0.0,20935.0,
4063,2020-11-11 08:05:55,1605074755.0,dataengineering,Understanding Basics Of SVM With Example And Python Implementation,js2qop,chase2learn,,https://www.reddit.com/r/dataengineering/comments/js2qop/understanding_basics_of_svm_with_example_and/,1.0,0.0,0.0,20935.0,
4064,2020-11-11 08:07:13,1605074833.0,dataengineering,Unsupervised Learning in Machine Learning,js2ras,chase2learn,,https://www.reddit.com/r/dataengineering/comments/js2ras/unsupervised_learning_in_machine_learning/,1.0,0.0,0.0,20935.0,
4065,2020-11-11 14:14:17,1605096857.0,dataengineering,Lip and Oral Cavity Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,js72dv,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/js72dv/lip_and_oral_cavity_cancer_death_rates_ranking/,1.0,0.0,0.0,20947.0,
4066,2020-11-11 22:31:20,1605126680.0,dataengineering,DAG stuck in up_for_retry,jsg4i8,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/jsg4i8/dag_stuck_in_up_for_retry/,1.0,12.0,0.0,20956.0,"Hello! Jr Data Engineer here!

I'm working on a dag someone last ran successfully back in 2019, for the past week I've been trying to get it to run again, and this time I made some changes and it looks as if its going to work but now its stuck in up\_for\_retry. This is the dags arguments:

&gt;default\_args = {  
 'owner': 'airflow',  
 'depends\_on\_past': False,  
 'start\_date': datetime(2018, 12, 28),  
 'email\_on\_failure': True,  
 'email\_on\_retry': False,  
 'retries': 4,  
 'retry\_delay': timedelta(minutes=15),  
 'provide\_context': True,  
 'on\_failure\_callback': task\_fail\_slack\_alert}

Also, the Duration is showing 3 min 15 sec, but its been like this for over an hour

https://preview.redd.it/fgv2he048oy51.png?width=714&amp;format=png&amp;auto=webp&amp;s=bd7bfd950327c6e94a4b0c496c829629ca9b597a

The only change I made here is delete 'sla'. I'm wondering if it could be because the start date is 2018? Should I change to yesterdays date perhaps? but I feel it should still run and not be up\_for\_retry. 

Thoughts, suggestions ... always welcome!"
4067,2020-11-11 22:50:48,1605127848.0,dataengineering,[INTERVIEW] Data streaming gaps,jsgimq,redder_ph,,https://www.reddit.com/r/dataengineering/comments/jsgimq/interview_data_streaming_gaps/,1.0,7.0,0.0,20958.0,"I was asked this question in a data engineering interview.

A streaming data pipeline writes discrete payloads to kafka. Consumers on the backend process and store it in Redshift. Data is sent to the pipeline over the internet from wireless devices. Let's assume, the internet goes down for a half hour and devices don't send data. After the internet is back online the 30 minutes worth of data is sent to the pipeline. 

How should analytics team or algorithms handle this gap in data in Redshift? For example, if a car sends gps info every second, but stops sending data for half an hour due to the outage, how would a car's position be analyzed during the half hour and after the half hour? 

Or if a diabetic patient has an insulin monitor that tracks insulin levels and cannot send data for a half hour due to the outage. There is a possibility that the patient's insulin levels drop to near fatal levels during the 30 minutes. How would analytics handle such issues?"
4068,2020-11-12 01:49:05,1605138545.0,dataengineering,Lakeless Data Warehouse: tech stack and architecture for a single source of truth,jsjjkq,anahnarciso,,https://www.reddit.com/r/dataengineering/comments/jsjjkq/lakeless_data_warehouse_tech_stack_and/,1.0,0.0,0.0,20963.0,
4069,2020-11-12 04:06:28,1605146788.0,dataengineering,Trying to get into data eng,jslxt8,Anunoby3,,https://www.reddit.com/r/dataengineering/comments/jslxt8/trying_to_get_into_data_eng/,1.0,2.0,0.0,20966.0,If you could provide me some (as harsh as possible) feedback that'd make my day!
4070,2020-11-12 11:09:12,1605172152.0,dataengineering,What do you think?,jsrkrr,Jagmeetoff,,https://www.reddit.com/r/dataengineering/comments/jsrkrr/what_do_you_think/,1.0,1.0,0.0,20974.0,"""You can have data without information, but you cannot have information without data."""
4071,2020-11-12 13:32:53,1605180773.0,dataengineering,working on python code to detect PII in english and french- need suggestions and recommendations,jst3ga,lnx2n,,https://www.reddit.com/r/dataengineering/comments/jst3ga/working_on_python_code_to_detect_pii_in_english/,1.0,2.0,0.0,20979.0,"Hi, I am looking to write a PII module in python for our telecom data in english and french. I have an english one using scrubadub. 

Any other suggestions for english and french ( I didnt figure this out yet).

Any specific things to watch out for from your experiences?"
4072,2020-11-12 14:00:05,1605182405.0,dataengineering,Tea Consumption Ranking | TOP 10 Country from 1961 to 2013,jstemw,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jstemw/tea_consumption_ranking_top_10_country_from_1961/,1.0,0.0,0.0,20979.0,
4073,2020-11-12 16:36:26,1605191786.0,dataengineering,How to find correlation using spss| Correlation analysis,jsvni0,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jsvni0/how_to_find_correlation_using_spss_correlation/,1.0,0.0,0.0,20983.0,
4074,2020-11-12 19:04:17,1605200657.0,dataengineering,How important is job title?,jsydoc,writingaballetnovel,,https://www.reddit.com/r/dataengineering/comments/jsydoc/how_important_is_job_title/,1.0,27.0,0.0,20987.0,"Hey all,

I'm a jr. data analyst at my company, but the job I do is really closer to data engineer I think (I manage CI pipelines for our Android/iOS apps, deployment/database migration of new code to production server, develop in-house ETL, configure databases, clean/prepare/aggregate data, create new schema, sketch out data flow, and am also considered the DevOps person on my small 10 person team). 

I recently asked my manager for a title change to Data Engineer since I believe I'm mid-level now (they hired me with 0 experience fresh out of a data sceince bootcamp, but I do have a formal background in stats/mathematics as well). 

Well they're offering me Data Analyst (no jr) and a 16% raise. To be honest, I think they may be offering me Analyst and not Engineer because they can't pay me market rate (I work for a small startup, and right now care a little less about salary because in my role I'm allowed a ton of freedom and my company is very supportive, I get to learn all these things and really get my hands dirty, and so right now making a little less is a tradeoff I'm willing to take. Also COVID has really impacted the company). 

16% is really generous I think, and I'm happy with the money. If they were offering me DE, it would be like the 10th percentile for salary. For a DA I'd be making like the 90th percentile for salary in my market. 

But I'm not sure how important the title is for my career. Would it be worth it to try and ask if I could have the title of DE but I'd be happy to keep the level of compensation? I think I want my career path to be building toward Data Architect / continue with Data Engineer or maybe even Cloud Architect. I'm not sure if having ""Data Analyst"" as my formal title will impact me when I'm applying for future roles. I'll still have the skillset, just not the title. 

So how important is the title?"
4075,2020-11-12 20:00:04,1605204004.0,dataengineering,Bringing Reliable Data &amp; AI to the Cloud: A Q&amp;A with Apache Spark creator Matei Zaharia,jszh1f,mkvor8,,https://www.reddit.com/r/dataengineering/comments/jszh1f/bringing_reliable_data_ai_to_the_cloud_a_qa_with/,1.0,0.0,0.0,20988.0,
4076,2020-11-12 23:09:51,1605215391.0,dataengineering,Databricks launches SQL Analytics,jt3auz,rmrfaccount,,https://www.reddit.com/r/dataengineering/comments/jt3auz/databricks_launches_sql_analytics/,1.0,6.0,0.0,20999.0,
4077,2020-11-13 00:51:56,1605221516.0,dataengineering,Data Engineer and Compliance,jt588u,Gabooll,,https://www.reddit.com/r/dataengineering/comments/jt588u/data_engineer_and_compliance/,1.0,4.0,0.0,21001.0,"Is it a data engineer's duty to ensure that data is used in a compliant manner to all laws like Canada's CASL Anti-Spam law or only laws similar CCPA, GDPR. Or if this should fall onto another role/person."
4078,2020-11-13 03:57:37,1605232657.0,dataengineering,Data Engineering Code Snippets?,jt8b97,greg_on_data,,https://www.reddit.com/r/dataengineering/comments/jt8b97/data_engineering_code_snippets/,1.0,1.0,0.0,21010.0,"I feel like I spend a lot of time at my job creating simple functionality that I'm sure people have coded more efficiently. Like finding new ftp files and uploading them into S3, creating another cloudfront template for a lambda with a chron trigger, etc...

Are there any websites that have either code snippets on certain tasks or good examples of ETL/ELT processes? Or do most people just end up googling part of their solution until they can get a script together than works?"
4079,2020-11-13 04:23:28,1605234208.0,dataengineering,Best Data Visualization Free Tools to Get Better Insights,jt8q6s,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jt8q6s/best_data_visualization_free_tools_to_get_better/,1.0,0.0,0.0,21011.0,
4080,2020-11-13 05:45:37,1605239137.0,dataengineering,Best Data Visualization Free Tools to Get Better Insights,jt9yze,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jt9yze/best_data_visualization_free_tools_to_get_better/,1.0,0.0,0.0,21014.0,
4081,2020-11-13 06:16:34,1605240994.0,dataengineering,Data Engineer using Talend or Data Analyst using Spark/SQL,jtaffh,datakei,,https://www.reddit.com/r/dataengineering/comments/jtaffh/data_engineer_using_talend_or_data_analyst_using/,1.0,7.0,0.0,21017.0,"I'm searching for new grad roles and have a couple options. One would be work as a DE using Talend as the main tool, and the other would be working as a Data Analyst but doing about 30% reporting/dashboarding with the other 70% being ETL with Spark / SQL. I was reading [this post](https://www.reddit.com/r/dataengineering/comments/jsydoc/how_important_is_job_title/) from earlier today on how much a job title matters, and it seems like the consensus is that early in your career having the title of DE is a big plus. However, it seems like the Data Analyst position is more technical than the Data Engineer position so I'm curious: would it be better to have a DE title but work with Talend, or have a Data Analyst title and actually work with Spark?"
4082,2020-11-13 07:17:47,1605244667.0,dataengineering,Business research for professional engineering men looking to gain weight (and/or build muscle),jtb9xv,Jagmeetoff,,https://www.reddit.com/r/dataengineering/comments/jtb9xv/business_research_for_professional_engineering/,1.0,0.0,0.0,21018.0,"We notice that professionals and people who generally work in offices are our main clientele and we're working to better understand their exact problems so we can best incorporate solutions specific to them.

Our business helps busy professional men gain weight (and build muscle if that’s one of their goals) without feeling unmotivated after work, stressed on time/work responsibilities, or knowing what to eat.

If you resonate with any of this, honest answers are appreciated.

1. As busy engineers, what are the 2 biggest issues you’re dealing with? (Work related)
2. (If this applies to you) Regarding wanting to gain weight (and/or build muscle), what frustrates you more than anything else, and why?

Thanks so much in advance - looking forward to reading your answers!"
4083,2020-11-13 14:34:22,1605270862.0,dataengineering,Nasopharynx Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jtg6u0,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jtg6u0/nasopharynx_cancer_death_rates_ranking_top_10/,1.0,0.0,0.0,21027.0,
4084,2020-11-13 17:28:47,1605281327.0,dataengineering,Blueprints are collections of sample code and datasets that utilize Gretel's SDKs that solve Data Engineering use cases.,jthlxi,alig80,,https://www.reddit.com/r/dataengineering/comments/jthlxi/blueprints_are_collections_of_sample_code_and/,1.0,0.0,0.0,21029.0,"Blueprints make it incredibly simple to get started on use cases including [data anonymization](https://gretel.ai/platform/synthetics), [privacy engineering](https://gretel.ai/blog/fast-data-cataloging-of-streaming-data-for-fun-and-privacy), and [data balancing](https://gretel.ai/blog/improving-massively-imbalanced-datasets-in-machine-learning-with-synthetic-data). [https://github.com/gretelai/gretel-blueprints](https://github.com/gretelai/gretel-blueprints)"
4085,2020-11-13 18:48:55,1605286135.0,dataengineering,Python / Airflow question - combining tasks,jtj8ga,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/jtj8ga/python_airflow_question_combining_tasks/,2.0,0.0,0.0,21030.0,"Hello! Jr Data Engineer here!

So I have to link 5 tasks together which would be easy but the tasks currently run with loops and I'm still pretty new to all this.... And of course I want to make sure they are properly streaming.

This is the current setup of the tasks:

&gt;\# ingest\_daily  
for i in range(len(Util.transfer\_files)):  
ingest\_daily1\[i\] &gt;&gt; ingest\_daily2\[i\] &gt;&gt; ingest\_daily3\[i\] &gt;&gt; ingest\_daily4\[i\] &gt;&gt; ingest\_daily5\[i\]  
  
&gt;  
&gt;\# match\_daily\_dag  
for tname in match\_daily1:  
match\_daily1\[tname\] &gt;&gt; match\_daily2\[tname\]  
&gt;  
&gt;match\_daily2\['match\_ad\_unit'\] &gt;&gt; match\_daily3  
  
&gt;  
&gt;\# dmatch\_daily\_dag  
for key in dmatch\_daily2.keys():  
dmatch\_daily1\[key\] &gt;&gt; dmatch\_daily2\[key\]  
  
&gt;  
&gt;\# il\_daily\_dag  
for tname in il\_daily1:  
il\_daily1\[tname\] &gt;&gt; il\_daily2\[tname.replace('\_backfill', '')\]  
&gt;  
&gt;il\_daily1\['network\_impressions'\] &gt;&gt; il\_daily3  
  
&gt;  
&gt;\# benchmarks\_daily\_dag  
benchmarks\_daily1 &gt;&gt; benchmarks\_daily3.values()  
for key in benchmarks\_daily3.keys():  
benchmarks\_daily2\[key\] &gt;&gt; benchmarks\_daily3\[key\]  
&gt;  
&gt;benchmarks\_daily3.values() &gt;&gt; benchmarks\_daily4

&amp;#x200B;

I want them to start in the order they're showing so would I do this:

&gt;for i in range(len(Util.transfer\_files)):  
ingest\_daily1\[i\] &gt;&gt; ingest\_daily2\[i\] &gt;&gt; ingest\_daily3\[i\] &gt;&gt; ingest\_daily4\[i\] &gt;&gt; ingest\_daily5\[i\]  
&gt;  
&gt;for tname in match\_daily1:  
match\_daily1\[tname\] &gt;&gt; match\_daily2\[tname  
&gt;  
&gt;match\_daily2\['match\_ad\_unit'\] &gt;&gt; match\_daily3  
&gt;  
&gt;for key in dmatch\_daily2.keys():  
dmatch\_daily1\[key\] &gt;&gt; dmatch\_daily2\[key\]

&amp;#x200B;

This doesn't seem right. Any advice is appreciated, again really new to all this. Thanks! :)"
4086,2020-11-13 18:50:12,1605286212.0,dataengineering,Large column load failure in Azure DW,jtj9ef,JaysExcellentWorkThr,,https://www.reddit.com/r/dataengineering/comments/jtj9ef/large_column_load_failure_in_azure_dw/,1.0,6.0,0.0,21030.0,"Hey DataEngineering! I've got a Azure SQL table I'm moving from the traditional DB into a Azure Synapse Analytics (DW) table for long term storage and so it can be removed from our production DB. This is a system table for a deprecated system we used to used (Salesforce, for more info). I've got a column in this table that is a varchar(max), and its massive. The MAX(LEN(FIELD) is 1585521. I've tried using Data Factory to move the table into the DW, but it fails on that massive column. I modeled the DW table to be a mirror of the production DB table, but it fails! I changed the DW column that is failing to nvarchar(max), but its still failing (thought it might be non-unicode causing the failure). Any ideas? Its confusing me because the data exists in our production DB, but won't be nice and peacefully move to our DW. Any input appreciated!"
4087,2020-11-13 18:52:31,1605286351.0,dataengineering,Scala users: do you use recursive functions for DE tasks?,jtjb3n,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/jtjb3n/scala_users_do_you_use_recursive_functions_for_de/,1.0,20.0,0.0,21030.0,"Hi all. I started to learn Scala from a Udemy course and the instructor is talking about recursive functions. He emphatically says to use functions and **avoid for loops**. 

I'm used to imperative Python, so I'm just curious - is this approach in Scala common for DE tasks, like building ETLs and such? 

Or is it just a comp sci construct that has other applications?"
4088,2020-11-13 20:20:04,1605291604.0,dataengineering,"Combining 4 Dags, using other Dags sensors?",jtl1xy,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/jtl1xy/combining_4_dags_using_other_dags_sensors/,1.0,2.0,0.0,21031.0,"Hello, new jr data engineer here.

&amp;#x200B;

I've combined 4 dags into one but now I'm in the process of linking them together. I'm unsure what exactly is being asked of me when I'm told:

&amp;#x200B;

[\(il\_daily\) .... This query doesn't need sensors because it can use the next task \(see next pic\)](https://preview.redd.it/xnqfobsxt1z51.png?width=551&amp;format=png&amp;auto=webp&amp;s=011c80bbd71aa4ca75e6dec1e22e30bc4ed7e497)

&amp;#x200B;

[\(ingest\_daily\)...and the non-backfull task as upstream dependencies](https://preview.redd.it/3kaa96a0u1z51.png?width=627&amp;format=png&amp;auto=webp&amp;s=c84b65234aefffa0e6ff6b1733e300894bf8a392)

&amp;#x200B;

&amp;#x200B;

This is one of the first pic's task:

&amp;#x200B;

il\_daily1 = {tname: SnowflakeDataSensor(

conn\_id='snowflake\_default',

task\_id=f'{tname}\_data\_exists',

mode='reschedule',

poke\_interval=60,  # 60 sec

timeout=900,  # 15 min

retries=6,

retry\_delay=timedelta(minutes=60),

sql=f""""""

SELECT COUNT(\*) &gt; 0

FROM {Table('daily').full\_name}}

&amp;#x200B;

This is how the ingest\_daily task is setup to run:

&amp;#x200B;

for i in range(len(transfer\_files)):

ingest\_daily1\[i\] &gt;&gt; ingest\_daily2\[i\] &gt;&gt; ingest\_daily3\[i\] &gt;&gt; ingest\_daily4\[i\] &gt;&gt; ingest\_daily5\[i\]

&amp;#x200B;

and the il\_daily:

&amp;#x200B;

for tname in il\_daily1:

il\_daily1\[tname\] &gt;&gt; il\_daily2\[tname.replace('\_backfill', '')\]



il\_daily1\['network\_impressions'\] &gt;&gt; il\_daily3

&amp;#x200B;

What exactly is being asked of me when I'm told: the il\_daily query doesn't need sensors because it can use ingest\_daily and the non-backfull task as upstream dependencies ?

&amp;#x200B;

  \[1\]: [https://i.stack.imgur.com/VlrNG.png](https://i.stack.imgur.com/VlrNG.png)

  \[2\]: [https://i.stack.imgur.com/TQY0w.png](https://i.stack.imgur.com/TQY0w.png)"
4089,2020-11-13 21:33:13,1605295993.0,dataengineering,Combine several CSV files,jtmigf,kalyanivr,,https://www.reddit.com/r/dataengineering/comments/jtmigf/combine_several_csv_files/,1.0,5.0,0.0,21034.0,"Combine several CSV files fetched from several URLs, with different column names and columns( number of columns in all the files are different).

&amp;#x200B;

I have several CSV files downloaded using a Python script from a given list of URLs.

These files have slightly different content with different columns present. I want to combine the whole data into a single file so that I can preprocess it and analyze it. 

&amp;#x200B;

I am looking to append the data in each csv file into a single file. If any columns are not present then put null values or blanks in the corresponding field for that column.

&amp;#x200B;

If this is not the best way to do it, please suggest a good way to do it. Something that can be used in real life use cases with Big Data. Also if am missing out on any additional information needed, please let me know. 

&amp;#x200B;

Thanks!"
4090,2020-11-14 03:39:30,1605317970.0,dataengineering,"API Data pipeline, using AWS Lambda and AWS S3",jtt92c,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/jtt92c/api_data_pipeline_using_aws_lambda_and_aws_s3/,1.0,16.0,0.0,21041.0,"Hi Everyone,

If you are looking for an easy to setup and simple way to automate, schedule and monitor a 'small' API data pull on the cloud, serverless functions are a good option. In this post I try to cover what a serverless function can and cannot do, what its pros and cons are and walk through a simple API data pull project. Using AWS Lambda and AWS S3. Any feedback is appreciated. 

[https://www.startdataengineering.com/post/pull-data-from-api-using-lambda-s3/](https://www.startdataengineering.com/post/pull-data-from-api-using-lambda-s3/)"
4091,2020-11-14 06:27:10,1605328030.0,dataengineering,Recommend a Data Warehouse/OLAP,jtvres,Silicon-Data,,https://www.reddit.com/r/dataengineering/comments/jtvres/recommend_a_data_warehouseolap/,1.0,14.0,0.0,21044.0,"We have a rather simple on-premise setup with Hadoop-based data lake and Presto to run SQL queries over files in HDFS and HIVE tables. As our company grows we are ready for the data warehouse to serve wider segment of needs.

Could you recommend an open source software for data warehouse to store and analyze clickstream data - several billions of events per day?   
I wonder if there's any open source software that implements Kimball architecture."
4092,2020-11-14 12:52:51,1605351171.0,dataengineering,Onion Consumption Ranking | TOP 10 Country from 1961 to 2013,ju08di,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/ju08di/onion_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21049.0,
4093,2020-11-14 18:38:23,1605371903.0,dataengineering,Set Dev environment,ju4rgd,sebastiansrikanth,,https://www.reddit.com/r/dataengineering/comments/ju4rgd/set_dev_environment/,1.0,1.0,0.0,21061.0,How do we set development environment to test python SDK for Amazon Forecast(read data from s3 or export results to s3) without connecting to AWS just to test code.
4094,2020-11-14 20:41:12,1605379272.0,dataengineering,"If You’re Using Kafka With Your Microservices, You’re Probably Handling Retries Wrong",ju6ugu,nfrankel,,https://www.reddit.com/r/dataengineering/comments/ju6ugu/if_youre_using_kafka_with_your_microservices/,1.0,6.0,0.0,21063.0,
4095,2020-11-15 01:12:59,1605395579.0,dataengineering,UI Developer to Data Engineer,jubduk,ML_begENGinner,,https://www.reddit.com/r/dataengineering/comments/jubduk/ui_developer_to_data_engineer/,1.0,2.0,0.0,21067.0,"Hello everyone, I need suggestion on career change. I have been UI developer since 5 years. Do you recommend changing to Data Engineer role now? 

I actually want to learn new skills like Java Spring, Mongo DB etc to extend beyond just UI but I'm confused between these or getting into Data Engineer roles by learning Spark, SQL etc. 

Are my current UI skills useful for Data Engineer position?

My skills: Angular JS, Core Java, SQL

Please recommend. Also kindly let me know if you have any suggestions. Thanks in advance."
4096,2020-11-15 01:19:49,1605395989.0,dataengineering,need career advice,jubhlw,ML_begENGinner,,https://www.reddit.com/r/dataengineering/comments/jubhlw/need_career_advice/,1.0,2.0,0.0,21068.0,Do you recommend changing to Data Engineer from UI?
4097,2020-11-15 03:21:55,1605403315.0,dataengineering,How should I setup the following pipeline?,juddlh,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/juddlh/how_should_i_setup_the_following_pipeline/,1.0,4.0,0.0,21073.0,"I'm working on a person a personal project to gain skills to become a data engineer. Here's what I've currently done: 

1. Obtained CSV from NYC OpenData. 
2. Cleaned data in R. 
3. Performed visualization in R using ggplot.
4. Persisted data into mongoDB. 

How would this be done on the job at your workplace? Why is there a need to persist the data into a database when the data is already stored in R? It seems like once I cleaned the data to create a final dataframe, the engineering side of this project was done. How can I make this better? Instead of using a CSV, I'm trying to retrieve data using the Socrata API. I'm having trouble with this. What else can I do?"
4098,2020-11-15 04:49:17,1605408557.0,dataengineering,Swe to data engineer,juenla,revlentOne,,https://www.reddit.com/r/dataengineering/comments/juenla/swe_to_data_engineer/,1.0,5.0,0.0,21075.0,I have an interview next week and so I'm looking to for tips/advice from anyone who has move from swe to data engineer. What was the interview like? The transition? Did you know R before?
4099,2020-11-15 12:08:40,1605434920.0,dataengineering,10 Key Skills Every Data Engineer Needs | Hacker Noon,jujq4a,mamamamu,,https://www.reddit.com/r/dataengineering/comments/jujq4a/10_key_skills_every_data_engineer_needs_hacker/,1.0,0.0,0.0,21120.0,
4100,2020-11-15 12:18:27,1605435507.0,dataengineering,What are the best practices for providing data for the data science team?,jujthb,phpfaber,,https://www.reddit.com/r/dataengineering/comments/jujthb/what_are_the_best_practices_for_providing_data/,5.0,6.0,2.0,21120.0,"Hey folks,

Say, we have SaaS which consists of multiple microservices. Each microservice has its own DB (Postgres mainly) and now we've got a data team who started on their tasks. Currently, they read from DB directly (well, almost directly, through AWS data lake) and, of course, each change in DB schema causes issues.

What are the best practices for providing data for the data science team? 

One way I am thinking of is to decouple the data needed for data science. So we could use Postgres views or triggers prepared especially for data science. What do you think about this? 

Any other ways?"
4101,2020-11-15 13:15:46,1605438946.0,dataengineering,Other Pharynx Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,jukdnb,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jukdnb/other_pharynx_cancer_death_rates_ranking_top_10/,1.0,0.0,0.0,21122.0,
4102,2020-11-15 13:18:28,1605439108.0,dataengineering,Consume kubernetes secret from KubernetesPodOperator (Airflow),jukemb,Key-Coat-3406,,https://www.reddit.com/r/dataengineering/comments/jukemb/consume_kubernetes_secret_from/,0.0,6.0,2.0,21122.0," I'm setting up an Airflow environment on Google Cloud Composer for testing. I've added some secrets to my namespace, and they show up fine.

    secret_token = secret.Secret(
        deploy_type='env',
        deploy_target='SQL_CONN',
        secret='m-secrets',
        key='token')
    
    YESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)
    
    with models.DAG(
            dag_id='composer_set_controlm_secret_kubernetes_pod',
            schedule_interval=datetime.timedelta(days=1),
            start_date=YESTERDAY) as dag:
    
      kubernetes_secret_vars_ex = kubernetes_pod_operator.KubernetesPodOperator(
            task_id='ex-kube-secrets',
            name='ex-kube-secrets',
            image='eu.gcr.io/$PROJECT/$DOCKER_IMG:latest',
            namespace='default',
            cmds=['python'],
            arguments=['call_api.py'],
            secrets=[secret_token]
      )

 As you can se above, I am running a docker image which call my **call\_api.py** program, I would like to print the secret passed via the KubernetesPodOperator as below :

*call\_api.py*

    if __name__ == '__main__':
    print($secret_token) ====&gt; how can I do this ?

Can't find any valuable resource on the documentation..."
4103,2020-11-15 15:43:33,1605447813.0,dataengineering,Your thoughts on Kubeflow?,jum27f,de1pher,,https://www.reddit.com/r/dataengineering/comments/jum27f/your_thoughts_on_kubeflow/,14.0,8.0,0.0,21126.0,"Hi all,

I'm about to be joining a new team which is using Kubeflow for their ML stuff. After reading a bit about Kubeflow, I'm a little sceptical about it. It allows you to package individual steps (e.g. data prep, model training, etc) into separate docker files and execute them separately. I'm not entirely sure how this is better than, say plain Airflow which can also run on kubernetes and run docker operators. Additionally, based on my observations so far, Kubeflow appears to be poorly documented. 

I'm curious to hear your impressions.

Thanks"
4104,2020-11-15 17:45:40,1605455140.0,dataengineering,API call with weird date format in R,junuee,[deleted],,https://www.reddit.com/r/dataengineering/comments/junuee/api_call_with_weird_date_format_in_r/,0.0,2.0,0.0,21128.0,
4105,2020-11-15 18:53:49,1605459229.0,dataengineering,DE Mentor,juoze1,vinsanity1603,,https://www.reddit.com/r/dataengineering/comments/juoze1/de_mentor/,22.0,46.0,0.0,21131.0,"Hi guys,

I’m currently a data analyst who wants to shift or at least learn more about data engineering. I have a good experience working with SQL, Python, and some BI tools. However, the problem is that I’m the first data person in the startup where I’m currently working at, and sometimes I question if the things I’m doing are correct or not, or if I’m in the right path of doing data engineering. We also don’t have a Head of Data or some sort to guide me. 

I’ve been mostly self-taught and I don’t have any problem putting in the work. I just feel like a mentor can be of great help in learning. 

Thanks"
4106,2020-11-15 22:31:17,1605472277.0,dataengineering,GCP Data Eng. on Coursera - some questions,jusvue,gutterandstars,,https://www.reddit.com/r/dataengineering/comments/jusvue/gcp_data_eng_on_coursera_some_questions/,1.0,11.0,0.0,21134.0,"I've just completed week 1 of the course and it's been good so far.       
How do things pave out from week 2? Any particular things to watch out for or that I should study along with course to. Do better at it?      
Thanks."
4107,2020-11-15 22:51:05,1605473465.0,dataengineering,"The 17th edition of the @data_weekly newsletter is out. This week's release focus on Databook at Uber, Flink Forward recap, Linkedin's new JVM ML Lib, PyTorch &amp; MlFlow integration, A Data Engineer's take of Postgres, and Picnic's DWH",jut96p,vananth22,,https://www.reddit.com/r/dataengineering/comments/jut96p/the_17th_edition_of_the_data_weekly_newsletter_is/,0.0,0.0,0.0,21135.0,
4108,2020-11-16 04:41:05,1605494465.0,dataengineering,What tool should I pick for a non-production and maybe one-time data processing workflow?,juyyng,nartb,,https://www.reddit.com/r/dataengineering/comments/juyyng/what_tool_should_i_pick_for_a_nonproduction_and/,4.0,31.0,0.0,21147.0,"I've worked on Data teams in the past but at companies processing large amounts of data on a continual basis that needed to be extendable in all kinds of ways.

I'm now finding myself helping out with a data processing workflow for a small research group at my university.

The general workflow will require around 1TB of satellite data to be downloaded from a server. The files are split into about 800 groups. For each group, I'll have to process the data to produce some output/s.

So the groups can be run in parallel. There are also a few steps for the processing in each group that can be parallelized but which then need to be combined to produce an output.

The processing will involve some data conversion tool from NASA available as a CLI as well as some R scripts which I haven't had a chance to see yet so I'm not sure how well they could be ported to something like Python.

I'm hoping to make use of Google Cloud or some other provider to automate this workflow. The main goal is to avoid having someone in the lab manually doing each step as it's quite tedious. There's definitely less of a concern for compute time.

The workflow *may* need to be run again and there may be other workflows that would benefit from having a good automation system already sorted out.

I'm wondering what tools it makes sense for me to use.

I've worked with Kubeflow, Airflow, etc. for projects at work but wasn't involved in setting them up.

Would tools like these be overkill for what I want to do? Are there other tools that would help with easily writing a DAG in order to simplify the code and make use of parallelization? Should I just write Python that runs on a cloud instance instead and hack my way towards parallelization?

Let me know if there are any details that would be helpful!

Thanks :)"
4109,2020-11-16 05:06:49,1605496009.0,dataengineering,Non-Relational DB's for Data Engineering,juzcio,jduran9987,,https://www.reddit.com/r/dataengineering/comments/juzcio/nonrelational_dbs_for_data_engineering/,2.0,6.0,0.0,21148.0,"I've asked a similar question before but still a bit confused, especially as I continue to learn about DE related database work.

I am not sure what the importance is,  or where non-relational db's fit in the work of a DE.  A lot of the DB work/learning materials I've been exposed to only involve RDBMS (things like OLTP vs. OLAP, query optimizations, etc.).

It may be that I have focused more on the BI/Data Warehousing/SQL side of DE rather than the scripting/ETL side.  My question is, what need does a DE have for tech such as graph and no-sql databases. 

Thanks!"
4110,2020-11-16 05:10:56,1605496256.0,dataengineering,"Databricks have released a new product, SQL Analytics",juzel6,Jiarca,,https://www.reddit.com/r/dataengineering/comments/juzel6/databricks_have_released_a_new_product_sql/,25.0,6.0,0.0,21148.0,
4111,2020-11-16 05:59:03,1605499143.0,dataengineering,Best Ways of How to Learn Python For Data Science,jv03ir,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jv03ir/best_ways_of_how_to_learn_python_for_data_science/,0.0,0.0,0.0,21147.0,
4112,2020-11-16 06:03:01,1605499381.0,dataengineering,Best Ways of How to Learn Python For Data Science,jv05n5,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jv05n5/best_ways_of_how_to_learn_python_for_data_science/,1.0,2.0,0.0,21147.0,
4113,2020-11-16 06:11:51,1605499911.0,dataengineering,Best Ways of How to Learn Python For Data Science,jv0a47,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jv0a47/best_ways_of_how_to_learn_python_for_data_science/,0.0,0.0,0.0,21147.0,
4114,2020-11-16 06:11:58,1605499918.0,dataengineering,Best Ways of How to Learn Python For Data Science,jv0a6d,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jv0a6d/best_ways_of_how_to_learn_python_for_data_science/,0.0,0.0,0.0,21147.0,
4115,2020-11-16 06:12:04,1605499924.0,dataengineering,Best Ways of How to Learn Python For Data Science,jv0a8j,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jv0a8j/best_ways_of_how_to_learn_python_for_data_science/,0.0,0.0,0.0,21147.0,
4116,2020-11-16 06:12:19,1605499939.0,dataengineering,Best Ways of How to Learn Python For Data Science,jv0ad4,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/jv0ad4/best_ways_of_how_to_learn_python_for_data_science/,0.0,0.0,0.0,21147.0,
4117,2020-11-16 06:23:27,1605500607.0,dataengineering,ETL?,jv0fvq,hatfield1992,,https://www.reddit.com/r/dataengineering/comments/jv0fvq/etl/,0.0,2.0,0.0,21147.0,"I hope this is allowed and if not please don’t jump all over me :(  

I have a class project that I must email a list of leads to attend a Talend webinar (Talend is an ETL cloud solution)

The email templates my teacher provided are AWFUL....super long, look spammy, I hate it....I want them to be natural and concise. I’ve looked up all my prospects and so I’m going to have some personalization in every email but I just want a simple....boom here’s my value proposition, attend the webinar if you want to know more...or like “if you’re having trouble with_________, you should look into Talend ____ blah blah blah.” 

This issue is I know NOTHING about technology, zip. So I don’t know how to provide a value proposition that would be important to you? 

Does anyone have any suggestions on a simple email? I’ve been trying to learn about it for 2 hours but there’s so much too it and WAY over my head. I don’t even know if all data engineers know what an ETL is? Lol I’m hopeless &amp; please don’t attack me, I don’t know how nice everyone is in this sub 🙃"
4118,2020-11-16 10:04:55,1605513895.0,dataengineering,Is this a DE task or a DA task?,jv34ng,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jv34ng/is_this_a_de_task_or_a_da_task/,0.0,16.0,0.0,21157.0,"Using Python, I ingested data into MySQL. Before doing so, I formatted the dates, removed any spaces, replaced `NULL` values with `'UNKNOWN'`. 

While creating plots in R, I noticed a problem with the `age` column, namely that there were too many values and incorrect values https://i.imgur.com/MwnWzWg.png 

So, is this something that should have been fixed by a Data Engineer or is this something a Data Analyst would change for his/her analysis?"
4119,2020-11-16 12:18:23,1605521903.0,dataengineering,Geo coordinates - what data type?,jv4i3e,[deleted],,https://www.reddit.com/r/dataengineering/comments/jv4i3e/geo_coordinates_what_data_type/,1.0,0.0,0.0,21160.0,
4120,2020-11-16 13:10:32,1605525032.0,dataengineering,Bean Consumption Ranking | TOP 10 Country from 1961 to 2013,jv51z4,Acceptable_Truth_194,,https://www.reddit.com/r/dataengineering/comments/jv51z4/bean_consumption_ranking_top_10_country_from_1961/,0.0,0.0,0.0,21160.0,
4121,2020-11-16 15:21:43,1605532903.0,dataengineering,How to Import 100M+ rows of data into DynamoDB in under 30 minutes!,jv6mwr,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/jv6mwr/how_to_import_100m_rows_of_data_into_dynamodb_in/,32.0,7.0,0.0,21163.0,
4122,2020-11-16 16:50:58,1605538258.0,dataengineering,Effect of blockchain on programming languages especially python,jv7zwf,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jv7zwf/effect_of_blockchain_on_programming_languages/,1.0,0.0,0.0,21162.0,
4123,2020-11-16 16:52:58,1605538378.0,dataengineering,What is Logistic Regression and how does it work?,jv812r,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jv812r/what_is_logistic_regression_and_how_does_it_work/,1.0,0.0,0.0,21162.0,
4124,2020-11-16 16:54:15,1605538455.0,dataengineering,Understanding Basics Of SVM With Example And Python Implementation,jv81ta,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jv81ta/understanding_basics_of_svm_with_example_and/,1.0,0.0,0.0,21162.0,
4125,2020-11-16 17:17:27,1605539847.0,dataengineering,Installing python packages on Azure Synapse (mini demo).,jv8gdd,MenziesTheHeretic,,https://www.reddit.com/r/dataengineering/comments/jv8gdd/installing_python_packages_on_azure_synapse_mini/,2.0,0.0,0.0,21162.0,
4126,2020-11-16 17:23:02,1605540182.0,dataengineering,How mitigating event-time skewness can reduce checkpoint failures and task manager crashes in Flink,jv8jsl,Marksfik,,https://www.reddit.com/r/dataengineering/comments/jv8jsl/how_mitigating_eventtime_skewness_can_reduce/,1.0,0.0,0.0,21162.0,
4127,2020-11-16 17:29:16,1605540556.0,dataengineering,What do you do when only partial data is missing?,jv8nka,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/jv8nka/what_do_you_do_when_only_partial_data_is_missing/,0.0,3.0,0.0,21162.0,"Have a look at this SQL query: https://i.imgur.com/vBHg1hz.png Most of the values in that row are correct and I don't want to throw this record out because a couple of values were incorrect. 

The reason why this is important is that I couldn't insert the record into MySQL as integers / numeric because SQL can't convert string of non-numbers and null values into numbers. Likewise, replacing the `null` values with zero wouldn't be correct in this instance since the data refers to coordinates. I had to leave them as strings. 

**Question:** What happens when you setup a pipeline and some of the newer data (even only a few records) that you ingest is incorrect, as above? For example, my entire python script didn't finish because of this error. I had to change the table schema to accommodate this issue. So, what happens on the job? How do you deal with this?"
4128,2020-11-16 19:42:40,1605548560.0,dataengineering,Gretel is building the GitHub for Data!,jvb5zt,alig80,,https://www.reddit.com/r/dataengineering/comments/jvb5zt/gretel_is_building_the_github_for_data/,8.0,4.0,0.0,21166.0,"Its pretty awesome to see approaching privacy problems as an engineering challenge and building tools for developers, vs. regulations and compliance [https://techcrunch.com/2020/11/16/gretel-announces-12m-series-a-to-make-it-easier-to-anonymize-data/](https://techcrunch.com/2020/11/16/gretel-announces-12m-series-a-to-make-it-easier-to-anonymize-data/)"
4129,2020-11-17 00:30:14,1605565814.0,dataengineering,Does anyone use sparklens?,jvgs30,blue_sky_time,,https://www.reddit.com/r/dataengineering/comments/jvgs30/does_anyone_use_sparklens/,1.0,0.0,0.0,21170.0,"I was looking for tools to help me analyze my spark jobs and AWS resource usage, and found sparklens.  I was curious if people use it?  One thing was to help me pick the right number of nodes to use on AWS (to prevent over provisioning and spending too much)

[http://sparklens.qubole.com/](http://sparklens.qubole.com/)"
4130,2020-11-17 01:32:12,1605569532.0,dataengineering,What exactly are you DE guys doing with SQL? Book recommendation?,jvhvzx,ChrisIsWorking,,https://www.reddit.com/r/dataengineering/comments/jvhvzx/what_exactly_are_you_de_guys_doing_with_sql_book/,27.0,44.0,0.0,21173.0,"Hi All,

I'm an intermediate SQL user in the BI analyst sense.

Been seeing a lot of people saying DE at many companies is very heavy in SQL.  I'd really like to get a better sense of what flavor of SQL is used most often and what are the types of things being done with SQL?

* Are tables and views being created?
* Stored procedures?
* Deriving data values based on data in other columns?
* Converting data types?
* Creating classifications/categorizations?

Any book recommendations for SQL that would cover what needs to be known for DE?"
4131,2020-11-17 04:32:48,1605580368.0,dataengineering,How do you deal with schema changes?,jvkthi,TheSqlAdmin,,https://www.reddit.com/r/dataengineering/comments/jvkthi/how_do_you_deal_with_schema_changes/,3.0,12.0,0.0,21177.0,"I'm trying to understand that in a Dara warehouse or big data, if we are synchronizing data from OLTP tables, then how do we handle the schema changes like adding a new column, new column with default values?"
4132,2020-11-17 06:38:11,1605587891.0,dataengineering,Suggestions need to learn data modeling,jvmpb8,TheSqlAdmin,,https://www.reddit.com/r/dataengineering/comments/jvmpb8/suggestions_need_to_learn_data_modeling/,1.0,4.0,0.0,21182.0,"I would like to learn some data modeling concepts. Im a beginner in this area. So it wants to create a data model for a data warehouse, where should I start? 

Any course recommendation(mostly from Udemy or EDX :) Or any good blogs?"
4133,2020-11-17 07:04:37,1605589477.0,dataengineering,Aren't Data Lake and Data Warehouse the same?,jvn2tz,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/jvn2tz/arent_data_lake_and_data_warehouse_the_same/,7.0,15.0,0.0,21183.0,"Still not sure about the differences between a data lake and a data warehouse?

We have differentiated both on the basis of structure, approach, architecture, schema and storage.

[Data lake vs Data warehouse - 5 traditional differentiators](https://www.sprinkledata.com/docs/Snowflake-vs-Redshift-vs-BigQuery-vs-Hive-vs-Athena/index.html?utm_source=reddit_171120&amp;utm_medium=datalakevsdatawarehouse)

https://preview.redd.it/0y1mug72eqz51.png?width=1084&amp;format=png&amp;auto=webp&amp;s=5e8f8a92bce095b22cb476a920d43216ce4cad7d"
4134,2020-11-17 07:10:54,1605589854.0,dataengineering,How to analyze data using SPSS (for beginners)-part 1,jvn5z3,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jvn5z3/how_to_analyze_data_using_spss_for_beginnerspart_1/,0.0,0.0,0.0,21183.0,
4135,2020-11-17 13:18:16,1605611896.0,dataengineering,Data Protection Market to Reach 119.95 Billion USD by 2022,jvrbkd,pradnya123,,https://www.reddit.com/r/dataengineering/comments/jvrbkd/data_protection_market_to_reach_11995_billion_usd/,1.0,0.0,0.0,21198.0,
4136,2020-11-17 13:32:17,1605612737.0,dataengineering,Kedro 6 Months In: Thoughts and Take Homes after using Kedro in various projects.,jvrh8q,geeklk83,,https://www.reddit.com/r/dataengineering/comments/jvrh8q/kedro_6_months_in_thoughts_and_take_homes_after/,12.0,0.0,0.0,21198.0,
4137,2020-11-17 13:37:32,1605613052.0,dataengineering,Cocoa Bean Consumption Ranking | TOP 10 Country from 1961 to 2013,jvrji7,Honest_Palpitation_2,,https://www.reddit.com/r/dataengineering/comments/jvrji7/cocoa_bean_consumption_ranking_top_10_country/,0.0,2.0,0.0,21198.0,
4138,2020-11-17 13:52:35,1605613955.0,dataengineering,Data parsing explained,jvrpo8,depressioncat11,,https://www.reddit.com/r/dataengineering/comments/jvrpo8/data_parsing_explained/,0.0,0.0,0.0,21198.0,
4139,2020-11-17 14:56:47,1605617807.0,dataengineering,"Edge Computing: All you need to know This computing reduces the amount of data actually at risk. If the device is compromised, it will only contain the locally collected data. Edge computing only sends relevant information to the cloud, so hacking is not possible and it can be prevented...",jvsinv,genuinematters,,https://www.reddit.com/r/dataengineering/comments/jvsinv/edge_computing_all_you_need_to_know_this/,1.0,0.0,0.0,21201.0,
4140,2020-11-17 16:14:00,1605622440.0,dataengineering,Airflow tasks dependency,jvtnob,Key-Coat-3406,,https://www.reddit.com/r/dataengineering/comments/jvtnob/airflow_tasks_dependency/,0.0,4.0,1.0,21205.0,"Hey guys, I have a beginner question below :

**I would like to run p3 only once when p1 &amp; p2 are finished with success.**

From the airflow documentation, I have tried this :

    [p1,p2] &gt;&gt; p3

The problem with the code above is that p3 is running twice, once after the end of p1 and the second after the end of p2.

Thanks in advance for your help !"
4141,2020-11-17 17:46:50,1605628010.0,dataengineering,Columnar storage vs. Delta Lake,jvv8yn,BeggarInSpain,,https://www.reddit.com/r/dataengineering/comments/jvv8yn/columnar_storage_vs_delta_lake/,3.0,8.0,0.0,21209.0,"Hi!

What you think about the newest fad in data engineering which brings ACID transactions to Apache Spark? When you would prefer it over parquet format?

Cheers!"
4142,2020-11-17 18:12:44,1605629564.0,dataengineering,How to upload 50 OpenCV frames into cloud storage within 1 second,jvvq3i,Bala_venkatesh,,https://www.reddit.com/r/dataengineering/comments/jvvq3i/how_to_upload_50_opencv_frames_into_cloud_storage/,1.0,0.0,0.0,21209.0,"I have published a new article about ""How to upload 50 OpenCV frames into cloud storage within 1 second"" which is very useful for data engineers to set up a data pipeline for computer vision applications. 

Blog URL:- [https://medium.com/@venkateshpnk22/how-to-upload-50-opencv-frames-into-cloud-storage-within-1-second-653ee73d7711](https://medium.com/@venkateshpnk22/how-to-upload-50-opencv-frames-into-cloud-storage-within-1-second-653ee73d7711)"
4143,2020-11-17 19:00:26,1605632426.0,dataengineering,Chaos Data Engineering,jvwmgq,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/jvwmgq/chaos_data_engineering/,6.0,2.0,0.0,21209.0,
4144,2020-11-17 19:23:08,1605633788.0,dataengineering,Airflow dagrun_timeout callback,jvx2hz,Dminor77,,https://www.reddit.com/r/dataengineering/comments/jvx2hz/airflow_dagrun_timeout_callback/,1.0,3.0,0.0,21209.0,"Hi,

Is there any callback available for dagrun_timeout and if yes then  how can I pass custom parameters to the callback from a custom_function imported from custom_package which is used in PythonOperator.

So my use case is, I have a dag which consist of 3 task which are executed in a group -
`start &gt;&gt; [spider_a, spider_b, spider_c]`

If the dag takes more than x minutes I want to stop the execution and return the number of requests made both successful (200 ok) and unsuccessful to the callback.

Thank you"
4145,2020-11-17 19:52:42,1605635562.0,dataengineering,Inconsistency in DE Interviews,jvxn5s,floydhead11,,https://www.reddit.com/r/dataengineering/comments/jvxn5s/inconsistency_in_de_interviews/,26.0,45.0,0.0,21209.0,"I have given 4 Final rounds, 3 more pre-final rounds, numerous coding/modeling rounds, and countless HR calls.

None of my rounds were consistent with what was mentioned in the JD and none of the rounds were consistent among each other for a similar LoE DE role for Product or Consulting company.

Some were very Leetcode heavy where they barely asked any SQL.
Some were hell bent on knowing Python (including descriptions of module methods, decorator usage, etc).
Some were heavy on Data Modeling but didn't care about SQL or code. But if you did model, they want NoSQL solutions when you've not even touched NoSQL in the past.
The rest involved all of the above in the most intense sense. 

I fail to understand how one should even prepare for such interviews, especially as I hold a DE position already.
Most of these positions have a very similar JD so it is hard to analyse up front.

They want end to end members who know APIs, CAP, ETL diagnostics, Indexes, Partitioning, SQL and NoSQL maestros, AWS/Redshift, Python, Airflow, DAGs, Tableau/R, Snowflake, and perform BFS/DFS/DP/Regex within 30 mins for people with &lt;3 YoE 

I am willing to concede that I am lacking in the skills and continually improving. But this is also disheartening. Am I just underqualified or doing something wrong?

If you are one of the members who actually do all or even half the above, can you point me in the right direction? Also, how did you land up and what was your background?"
4146,2020-11-17 19:56:46,1605635806.0,dataengineering,Interfacing with Data Analytics - where is the business logic located?,jvxpvs,joshtree41,,https://www.reddit.com/r/dataengineering/comments/jvxpvs/interfacing_with_data_analytics_where_is_the/,3.0,4.0,0.0,21209.0,"I’ve found this to be pretty dynamic lately. 

My team builds pipelines to bring data into our Redshift DB from various sources. 

We perform transformations on the data. These transformations have inherent business logic. 

The analytics team then uses the data we give them and they write queries and apply more business logic.

Lately I have been thinking about the line between the business logic applied in the pipeline versus the business logic applied by the analysts. 

There seems to be an optimal trade-off where a certain chunk of the business logic is best handled by the Data Eng team and another chunk is handled by the Data Analytics team. 

But this optimal point can be fickle, and defiantly seems to be different depending on the nature of the data, the nature of the pipeline, and the nature of the reporting needs. 

Has anyone else battled with this trade-off? 

And recommendations on how to efficiently find this trade-off?"
4147,2020-11-17 20:38:17,1605638297.0,dataengineering,"How DAGs grow: Deeper, wider, and thicker",jvyjo8,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/jvyjo8/how_dags_grow_deeper_wider_and_thicker/,1.0,0.0,0.0,21210.0,
4148,2020-11-17 20:44:28,1605638668.0,dataengineering,Load Named Entity Recognition (NER) data into Elasticsearch,jvyo2c,alig80,,https://www.reddit.com/r/dataengineering/comments/jvyo2c/load_named_entity_recognition_ner_data_into/,2.0,0.0,0.0,21211.0,In this Blueprint we create a simple workflow to perform Named Entity Recognition (**NER**) on sample data using Gretel and load the records into Elasticsearch - useing **docker** to start up a local **Elasticsearch**. [https://gretel.ai/blog/load-ner-data-into-elasticsearch](https://gretel.ai/blog/load-ner-data-into-elasticsearch)
4149,2020-11-17 21:32:25,1605641545.0,dataengineering,The Art of EXPLAIN ANALYZE,jvzm3n,themikep82,,https://www.reddit.com/r/dataengineering/comments/jvzm3n/the_art_of_explain_analyze/,8.0,5.0,0.0,21214.0,"Hello DE subreddit,

I'm doing some database optimization for work and I'd like some broad guidance on how to approach it. So I'm employing the time-honored tradition of ""asking people who are smarter than me""

I plan on finding some resources on interpreting query planner output and diving into that rabbithole (suggestions welcome), but what I really need is a little bit of a North Star to help me understand what types of things I should be looking for.

Any tips or tricks or ""things you wish you'd known"" from the outset?

What are the first/most impactful things you look for when looking at EXPLAIN ANALYZE output?

Do you look for different things in OLTP vs OLAP environments (i.e postgres vs Redshift)?

Any nuggets of wisdom you've gained from your experience over the years?

I'm hoping to get a little discussion around practical advice on this topic that I can learn from.

Thanks so much!"
4150,2020-11-17 23:23:18,1605648198.0,dataengineering,A Network Engineer's approach to Data Mesh,jw1ubb,chykn1,,https://www.reddit.com/r/dataengineering/comments/jw1ubb/a_network_engineers_approach_to_data_mesh/,2.0,0.0,0.0,21219.0,"https://preview.redd.it/jph2ag1mbvz51.jpg?width=753&amp;format=pjpg&amp;auto=webp&amp;s=6d5d60b6275e652229e83137fddcfecc9e7df007

TLDR - I work with a ton of data sources in the infrastructure realm, ended up developing an open source hybrid service/data mesh project to help organize everything.

[https://github.com/adhdtech/DRP](https://github.com/adhdtech/DRP)

Will start off by admitting that I'm definitely not a ""big data"" person.  However, coming from an infrastructure background I understand the pain of having to deal with APIs from various vendor.  Reconciling information about a given entity spread across multiple management systems can be a nightmare.

To make things a little easier, I took a page from the network engineering playbook.  The concept is simple enough; for networks we use dynamic routing protocols to advertise routes.  Figured the same approach could be applied to our data sources over a service mesh.

The resulting project is called DRP (Declarative Resource Protocol).  In DRP, a given data source joins the mesh using DNS SRV records and advertises its classes, capabilities, etc.  That information is propagated across the mesh and made available to other services.  Instead of relying on a manually updated modeling system to aggregate class info, we can now see the environment as it really is - a collection of loosely federated data sets.

A few months ago I came across one of Zhamak Dehghani's talks on data mesh.  According to her description, sources should exhibit 6 characteristics.  I was surprised to find that this project covers them, at least as I understand them.

* Shared / Discoverable - Classes advertised in a UML style format via the mesh
* Addressable - Data set paths advertised via the mesh
* Trustworthy - Only primary sources should advertise a given dataset
* Self-describing - Sources declare their classes, capabilities, etc
* Inter-operable - Consumers can use REST or mesh specific protocol
* Secure - RPC calls are authenticated and include caller info for granular authorization

Would love to hear any thoughts on this.  There's a simple demo site you can play with, [https://jmesh.io](https://jmesh.io).  Enter any username/password.  I wanted to be able to navigate the mesh like a filesystem, so there's a DRP Shell applet.  Old habits die hard!"
4151,2020-11-18 00:10:25,1605651025.0,dataengineering,[video] Trevor Noah on data storytelling (recording available only for one week),jw2s6q,fhoffa,,https://www.reddit.com/r/dataengineering/comments/jw2s6q/video_trevor_noah_on_data_storytelling_recording/,3.0,3.0,0.0,21219.0,
4152,2020-11-18 00:27:58,1605652078.0,dataengineering,Any thoughts on explainable AI?,jw34b1,illhamaliyev,,https://www.reddit.com/r/dataengineering/comments/jw34b1/any_thoughts_on_explainable_ai/,1.0,3.0,0.0,21221.0,"that's it, that's the post. LOL

But also, any one used it? preferably on small teams!"
4153,2020-11-18 02:14:03,1605658443.0,dataengineering,How do I write up a project map(idk what it's actually called) for transforming my data from excel to snowflake\tabealu?,jw51bx,TrainquilOasis1423,,https://www.reddit.com/r/dataengineering/comments/jw51bx/how_do_i_write_up_a_project_mapidk_what_its/,8.0,4.0,0.0,21225.0,"I have posted a few times here that I got my first data analyst job, for a sales team, about a year ago, and I have been trying to transition my role into more of a data engineer since my teams data ETL process is... rage inducing. Recently the impossible happened and the execs of my company decided they wanted to transition everything from excel files on a network to snowflake and use tableau as the main visualization tool. This is music to my ears and I want to try and take lead on this as much as possible as it sounds like a wonderful learning experience and resume padder.

I would like to write up a project road map outlining all the work we would need to do to make sure it's done right. Problem is I don't really know how to do that. I'm still young and have not done anything like this before, but very willing to put in the extra hours to learn and make it work. I am learning SQL, and snowflake through online courses right now, and have solid python knowledge as well.

I can give some specifics about my current data ETL process if need be, but mostly I'm looking for learning materials on relational databases and organizing data flow from multiple sources. Plus anything on organizing my thoughts in a more official business case way. Any advice or resources you could give to help me plan out this transition would be greatly appreciated."
4154,2020-11-18 02:54:58,1605660898.0,dataengineering,Data Management Advice,jw5qnz,Filmerandeditorguy,,https://www.reddit.com/r/dataengineering/comments/jw5qnz/data_management_advice/,3.0,3.0,0.0,21226.0,"I am looking for advice for people that have been in this field and understand best practice.

I graduated just under a year ago and landed my first job as a data analyst. I am the only data analyst at my company but I would not call the company small by any means. I work under our IT directer who manages our servers, etc.. and wants to move the company to focus more on our data for decision making. Since working with data is fairly new to this company and since this is my first job I don't have the best understanding of how top tier companies operate with efficiency and accuracy. Our financial department uses Quickbooks and our production teams use excel to track their projects. It seems fairly outdated for the size of our company and I want to push for updating our systems and processes. I am curious if it makes sense to pull Quickbooks data into Power BI and create reports or create a data warehouse and collect all the data from all of the sources then create queries to pull needed data for certain reports from the data warehouse. I am curious what other options there are and what some pros and cons are."
4155,2020-11-18 09:28:08,1605684488.0,dataengineering,"How to analyze data using SPSS (part-2), how to input data in SPSS",jwbgps,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jwbgps/how_to_analyze_data_using_spss_part2_how_to_input/,0.0,1.0,0.0,21236.0,
4156,2020-11-18 11:37:30,1605692250.0,dataengineering,"Moving data(images, videos, sql-dbs etc) from many IOT devices to Remote Storage?",jwcw39,rafay_a_k,,https://www.reddit.com/r/dataengineering/comments/jwcw39/moving_dataimages_videos_sqldbs_etc_from_many_iot/,1.0,6.0,0.0,21236.0,"Hey guys first time posting here as I am making a foray into data engineering for my project. I have deployed a few Jetson Nano devices from which I would like to, from time-to-time, get all the collected data which includes images, videos, SQL-dbs to an on-premises remote, from there maybe push to a GCP storage, AWS or azure and finally delete data backed up data from the Nano devices.

One thing I've tried is using DVC ([https://dvc.org/](https://dvc.org/))  it has a great CLI that allows me to push to literally any remote of my choice (ssh, HDFS, GCP, S3, etc) but one of the major downsides is that it is dependent on Git to track changes to data since its mainly a data versioning tool, without involving Git data is kinda not humanly readable. Secondly, the Python API lacks significantly.  Using it for my purpose is like hammering a square peg in a round hole.

Further looking at this online Apache Airflow and Luigi came up often, but I'm not sure these do what I want, which is basically automate backing up of certain targets residing on Jetson devices and monitoring if the backup was successful.

Any help pointing me in the right direction would be greatly appreciated."
4157,2020-11-18 12:08:27,1605694107.0,dataengineering,Manage Jupyter Notebook and JupyterLab with Systemd,jwd7xu,njanakiev,,https://www.reddit.com/r/dataengineering/comments/jwd7xu/manage_jupyter_notebook_and_jupyterlab_with/,1.0,0.0,0.0,21238.0,
4158,2020-11-18 14:04:18,1605701058.0,dataengineering,Honey Consumption Ranking | TOP 10 Country from 1961 to 2013,jwelcd,Honest_Palpitation_2,,https://www.reddit.com/r/dataengineering/comments/jwelcd/honey_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21241.0,
4159,2020-11-18 14:23:45,1605702225.0,dataengineering,#idataengineer podcast 001 Ramzi Al-Aruri,jweugf,soobrosa,,https://www.reddit.com/r/dataengineering/comments/jweugf/idataengineer_podcast_001_ramzi_alaruri/,7.0,0.0,0.0,21241.0,"Not one of today's data engineers grew up as a kid imagining becoming one. I definitely didn't. We, data engineers, are all different, but we all have a story. In the first part of our micro-podcast, [Ramzi Al-Aruri](https://ramziaruri.com) speaks about his journey and why he loves [dbt](https://www.getdbt.com) and [SeekWell](https://seekwell.io). 

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-001](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-001)"
4160,2020-11-18 15:12:56,1605705176.0,dataengineering,How to Build a Career in Artificial Intelligence as a Fresher,jwfj2i,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/jwfj2i/how_to_build_a_career_in_artificial_intelligence/,0.0,0.0,0.0,21243.0,
4161,2020-11-18 15:49:58,1605707398.0,dataengineering,Best Setup for a Small Team/Budget,jwg3av,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/jwg3av/best_setup_for_a_small_teambudget/,1.0,19.0,0.0,21244.0,"I’m on a small data team where I’m the only engineer. There’s also a scientist and an analyst plus a couple other roles. 

Our current setup is in Azure. We have a VM for airflow, Postgres, and a couple other for things like tableau. Recently our company asked if we can decrease our budget, or cut down on our resources and monthly spending. 

I’m not really sure what to do here. We have a pretty beefy VMs for airflow and Postgres, but they seem necessary. Also maintaining all this infrastructure is kind of a hard with such a small crew. Is there a better way to spend less and also take away the overhead of maintenance from us?"
4162,2020-11-18 16:25:32,1605709532.0,dataengineering,"Looking to interview some Data engineers, Analytics leads and Data Analysts",jwgo0w,1ewish,,https://www.reddit.com/r/dataengineering/comments/jwgo0w/looking_to_interview_some_data_engineers/,1.0,0.0,0.0,21245.0,"Hi DE crew, we (Dataform) are back again, looking for Analytics Leads, Data Engineers and Data Analysts to interview and get some feedback on some new content and marketing copy.

If you'd be up for chatting to us please let us know using this [form](https://forms.gle/bXJCRsy1aGeKNsnE8). The interview won't take longer than 20 minutes and we're offering a **$40 Amazon gift voucher** for anyone who takes part in one of the interviews.  


Thanks in advance!  


[https://forms.gle/bXJCRsy1aGeKNsnE8](https://forms.gle/bXJCRsy1aGeKNsnE8)"
4163,2020-11-18 16:45:38,1605710738.0,dataengineering,New courses on distributed systems and elliptic curve cryptography (by Martin Kleppmann),jwh0qv,nfrankel,,https://www.reddit.com/r/dataengineering/comments/jwh0qv/new_courses_on_distributed_systems_and_elliptic/,26.0,10.0,0.0,21245.0,
4164,2020-11-18 18:32:55,1605717175.0,dataengineering,I started an open-sourced Data Engineering book,jwiyy8,oleg_agapov,,https://www.reddit.com/r/dataengineering/comments/jwiyy8/i_started_an_opensourced_data_engineering_book/,7.0,15.0,0.0,21247.0,"Hey data engineers!

I'm very interested in the field of Data Engineering for quite some time now. My main occupation is Data Analysis and all the related stuff (like BI tools, ETL, etc). But more and more I find myself digging into DE side of analytics: setting up the correct Spark configuration, migrating the data, writing maintainable ETL scripts, watching cluster health, etc.

At some point, I spotted – I like Data Engineering more than Data Analysis. So I've decided to switch :)

The road to becoming a full-fledged Data Engineer is hard and takes time. So I've decided to note my existing and new knowledge into an open book, so the next generation of data engineers will have an example of a man who has done that already.

So here it is – my open-sourced book about Data Engineering!

[https://github.com/oleg-agapov/data-engineering-book](https://github.com/oleg-agapov/data-engineering-book)

Right now it has an Introduction section with two chapters. And currently, I'm working on a chapter called ""Intro to Databases"". It is going slowly but forward. In the end, it is a marathon, not a sprint :)

Will be glad if you check it out, leave a comment or simply give me a GitHub star (so I know that my work is in demand). Thanks for your attention!"
4165,2020-11-18 18:35:54,1605717354.0,dataengineering,An interview with Isima's CEO about their integrated data management platform that simplifies and accelerates delivery of data products.,jwj13d,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/jwj13d/an_interview_with_isimas_ceo_about_their/,1.0,0.0,0.0,21247.0,
4166,2020-11-18 19:35:32,1605720932.0,dataengineering,"Let's discuss, notebooks on production, yes or no?",jwk7dh,RamalloLeaks,,https://www.reddit.com/r/dataengineering/comments/jwk7dh/lets_discuss_notebooks_on_production_yes_or_no/,3.0,7.0,0.0,21246.0,"I was reading this post [https://martinfowler.com/articles/productize-data-sci-notebooks.html](https://martinfowler.com/articles/productize-data-sci-notebooks.html)

But also, some time ago I read this one from Netflix [https://netflixtechblog.com/notebook-innovation-591ee3221233](https://netflixtechblog.com/notebook-innovation-591ee3221233)

WDYT? I have my opinion, but I am really interested in your opinion and experiences about this topic."
4167,2020-11-18 22:21:51,1605730911.0,dataengineering,Is this a Data Engineering task?,jwnk29,jduran9987,,https://www.reddit.com/r/dataengineering/comments/jwnk29/is_this_a_data_engineering_task/,5.0,3.0,0.0,21250.0,"So I have to create a reoccurring job that reconciles user data that lives in two different places.  It's a job that I can easily get done using Airflow and framing it as an ETL/ELT pipeline..

This has absolutely nothing to do with metrics, reports, data science, etc... but I really enjoy working on tasks that makes my colleagues lives easier. 

Do projects like this normally fall on a Data Engineer or a backend engineer? (I know DE is a subset of backend, but i mean the application backend vs. analytics data engineering)"
4168,2020-11-19 06:58:18,1605761898.0,dataengineering,Improving Data Engineering Workflows with End-to-End Data Observability,jwwgs0,mkvor8,,https://www.reddit.com/r/dataengineering/comments/jwwgs0/improving_data_engineering_workflows_with/,1.0,2.0,0.0,21258.0,"What tools do you use for data quality monitoring/data observability? 

[https://towardsdatascience.com/improving-data-engineering-workflows-with-end-to-end-data-observability-2fea930d8333](https://towardsdatascience.com/improving-data-engineering-workflows-with-end-to-end-data-observability-2fea930d8333)"
4169,2020-11-19 09:39:23,1605771563.0,dataengineering,Data Pipelines with Flink SQL on Ververica Platform,jwyiql,Marksfik,,https://www.reddit.com/r/dataengineering/comments/jwyiql/data_pipelines_with_flink_sql_on_ververica/,0.0,0.0,0.0,21262.0,
4170,2020-11-19 09:55:22,1605772522.0,dataengineering,Logistic Regression Machine Learning Classification in Python and Sklearn,jwypcr,dhiraj8899,,https://www.reddit.com/r/dataengineering/comments/jwypcr/logistic_regression_machine_learning/,1.0,0.0,0.0,21262.0,
4171,2020-11-19 12:35:17,1605782117.0,dataengineering,Does anyone know how to submit a SQL Query to a Databricks cluster through its rest api and get back the result,jx0gkn,YerwolEkim,,https://www.reddit.com/r/dataengineering/comments/jx0gkn/does_anyone_know_how_to_submit_a_sql_query_to_a/,3.0,9.0,0.0,21263.0,I'm stuck and any information will help
4172,2020-11-19 14:10:26,1605787826.0,dataengineering,It’s A Data War. And Here’s Your War Room,jx1l1i,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/jx1l1i/its_a_data_war_and_heres_your_war_room/,1.0,0.0,0.0,21265.0,
4173,2020-11-19 14:40:33,1605789633.0,dataengineering,Spices Consumption Ranking | TOP 10 Country from 1961 to 2013,jx1z43,Honest_Palpitation_2,,https://www.reddit.com/r/dataengineering/comments/jx1z43/spices_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21267.0,
4174,2020-11-19 15:19:47,1605791987.0,dataengineering,Build conditional blocks for your own requirements,jx2jf4,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/jx2jf4/build_conditional_blocks_for_your_own_requirements/,1.0,0.0,0.0,21267.0,"[Sprinkle](https://www.sprinkledata.com/) enables users to build [Conditional blocks](https://www.sprinkledata.com/docs/segment-conditional-builders/index.html?utm_source=reddit_191120&amp;utm_medium=conditionalbuilder) of their own requirements. This enables users to apply “AND/OR” actions between conditional blocks or even within a single conditional block. In the below example, the first conditional block consists of “SellerCity-Mumbai” **OR** “BuyerCity-Mumbai.”

**AND** in the second conditional block, the “Category” is marked as “Electronics” and “Subcategory” is marked as “Kitchen Storage and containers” with an “OR” condition in between.

**AND** in the third conditional block, the “Order date” is in the “Last 10 weeks” AND  the “Order status” is “Buyer Cancelled”

With “**And**” condition for these three conditional blocks, they give the meaning that, Orders that have been made in the past 10 weeks that's been cancelled by the buyer on electronics or kitchen storage and containers where the seller city is Mumbai or the buyer city is Mumbai.

https://preview.redd.it/w5pu2btg67061.png?width=1363&amp;format=png&amp;auto=webp&amp;s=9f683ccb0626fc0815eed692c8b9d20623a0f0b4"
4175,2020-11-19 21:59:23,1605815963.0,dataengineering,Tracking Schemas in a S3 Data Lake?,jx9zyn,exact-approximate,,https://www.reddit.com/r/dataengineering/comments/jx9zyn/tracking_schemas_in_a_s3_data_lake/,4.0,8.0,0.0,21270.0,"I have been struggling with this for some time, researching different solutions. 
Basically, given a bunch of json structured s3 files in different folders on a data lake, do any tools exist for monitoring the json structure of the files?

How do you make sure the stuff you are dumping on s3 conforms to the same schema.

ps. Yes, I understand it is NoSQL but some level of visibility and monitoring is very useful"
4176,2020-11-19 23:26:23,1605821183.0,dataengineering,"How AI, ML, and Big Data Analytics Fit Into a Non-Tech Company",jxbo2c,D3Vtech,,https://www.reddit.com/r/dataengineering/comments/jxbo2c/how_ai_ml_and_big_data_analytics_fit_into_a/,1.0,0.0,0.0,21275.0,
4177,2020-11-20 01:24:42,1605828282.0,dataengineering,The challenge of testing Data Pipelines,jxdsrj,quality_engineer,,https://www.reddit.com/r/dataengineering/comments/jxdsrj/the_challenge_of_testing_data_pipelines/,1.0,4.0,0.0,21278.0,"The challenge of testing data pipelines from the perspective of traditional application development. 

[https://medium.com/slalom-build/the-challenge-of-testing-data-pipelines-4450744a84f1?source=friends\_link&amp;sk=a661e11b6ce3dfc61932984ad8a77b6f](https://medium.com/slalom-build/the-challenge-of-testing-data-pipelines-4450744a84f1?source=friends_link&amp;sk=a661e11b6ce3dfc61932984ad8a77b6f)"
4178,2020-11-20 09:37:22,1605857842.0,dataengineering,Data engineering hackathon/competitions,jxkvw1,RstarPhoneix,,https://www.reddit.com/r/dataengineering/comments/jxkvw1/data_engineering_hackathoncompetitions/,1.0,3.0,0.0,21289.0,Is there any website where I can find hackathon and competitions related to data engineering or big data ?
4179,2020-11-20 10:56:29,1605862589.0,dataengineering,Available transportation API? And COVID19 restrictions in countries API?,jxlqqe,Majestic-Jump,,https://www.reddit.com/r/dataengineering/comments/jxlqqe/available_transportation_api_and_covid19/,1.0,0.0,0.0,21289.0,
4180,2020-11-20 13:06:58,1605870418.0,dataengineering,Looking for a data catalog demo site.,jxn5in,walnutmercury,,https://www.reddit.com/r/dataengineering/comments/jxn5in/looking_for_a_data_catalog_demo_site/,1.0,6.0,0.0,21293.0,"Hi everyone.

Doing some research into open sourced data catalogs (Bonus if there are any that can redact sensitive info). I’d love to showcase some to my company, but am stuck in a world where access to Git and Python code is blocked without special approval.

Does anyone know of a good platform that I can run some of these tools on I.e Amunsden , Databook etc.

Thanks in advance!"
4181,2020-11-20 13:53:06,1605873186.0,dataengineering,Have you ever taken a take-home hiring challenge that you liked?,jxnowz,five4three2,,https://www.reddit.com/r/dataengineering/comments/jxnowz/have_you_ever_taken_a_takehome_hiring_challenge/,1.0,43.0,0.0,21295.0,"Hi all,

I'm trying to figure out a good hiring challenge for a data engineering position.   This will be a take-home challenge, it should only take a few hours.

Personally, if I'm excited about the job, I can enjoy take-home's.  If I'm not, I kind of hate them, especially when applying to many roles.  We're a small start-up, so at our current capacity, I see them as a necessary-evil: we're tried a few different hiring pipelines and the take home works the best.

&amp;#x200B;

To keep it simple, the role would be for a data engineer with:

* Strong python
* Strong SQL
* Experience building pipelines that process data regularly
* Strong software experience/instincts 

Has anyone taken a data engineering challenge they like?

&amp;#x200B;

I am thinking something around:

* Identifying a well-structured datasets they have to ingest (csvs somewhere that update regularly)
* Building a SQLite database to store this data.
* Pipeline, built in python, that ingests and updates the database.

Any comments?"
4182,2020-11-20 14:04:52,1605873892.0,dataengineering,HIV Infection Ranking | TOP 10 Country from 1990 to 2017,jxnuiq,Honest_Palpitation_2,,https://www.reddit.com/r/dataengineering/comments/jxnuiq/hiv_infection_ranking_top_10_country_from_1990_to/,1.0,0.0,0.0,21295.0,
4183,2020-11-20 15:04:10,1605877450.0,dataengineering,"Violations Registry - Data Breach Registry - GDPR compliance, manage Data Breaches with our tool: Powered by New Web Network Solutions",jxonim,fanpolee,,https://www.reddit.com/r/dataengineering/comments/jxonim/violations_registry_data_breach_registry_gdpr/,1.0,0.0,0.0,21295.0,
4184,2020-11-20 15:42:33,1605879753.0,dataengineering,Top 10 Data Science Tools You Should Know,jxp7rl,Techbiason,,https://www.reddit.com/r/dataengineering/comments/jxp7rl/top_10_data_science_tools_you_should_know/,1.0,0.0,0.0,21296.0,
4185,2020-11-20 17:52:30,1605887550.0,dataengineering,How do you choose a the right data warehouse/lake?,jxrhmv,mkvor8,,https://www.reddit.com/r/dataengineering/comments/jxrhmv/how_do_you_choose_a_the_right_data_warehouselake/,1.0,2.0,0.0,21297.0,"The release of Databricks' SQL Analytics and Snowflake's Snowpark are continuing to blur the lines between warehouses and lakes. 

**How do you determine whether to go the warehouse, lake, or even ""lakehouse"" route?** Inquiring minds want to know...

[https://www.montecarlodata.com/how-to-build-your-data-platform-choosing-a-cloud-data-warehouse/](https://www.montecarlodata.com/how-to-build-your-data-platform-choosing-a-cloud-data-warehouse/)"
4186,2020-11-20 18:32:02,1605889922.0,dataengineering,Apache Airflow 2.0 Youtube Series,jxs98f,kaxil_naik,,https://www.reddit.com/r/dataengineering/comments/jxs98f/apache_airflow_20_youtube_series/,1.0,6.0,0.0,21296.0,"🎊 Airflow 2.0 Youtube Series is OUT 🎊  


Time to discover the NEW AWESOME features of Airflow 2.0!  
6 HANDS-ON videos are available now!  


You will learn:  
\- The Stable REST API  
\- The Smart Sensors  
\- The TaskFlow API, new way of coding DAGs!  
\- The TaskGroups (Much better than SubDAGs 😎)  
\- Refreshed UI with auto-refresh in Graph View  
\- The HA Scheduler -- THIS ONE IS 🔥🔥🔥🔥🔥

  
And the new KubernetesExecutor/PodOperator video is coming out soon !  


ACCESS THE VIDEOS HERE: [https://www.youtube.com/playlist?list=PLCi-q9vYo4x-PESoBcXN0tXCMgzh5c\_Pj](https://www.youtube.com/playlist?list=PLCi-q9vYo4x-PESoBcXN0tXCMgzh5c_Pj)  


Enjoy!"
4187,2020-11-20 20:23:19,1605896599.0,dataengineering,Confusion about interview,jxugd8,TheBusBoy,,https://www.reddit.com/r/dataengineering/comments/jxugd8/confusion_about_interview/,1.0,1.0,0.0,21297.0,"I just got out of a technical interview and I have to say that I am thoroughly confused and looking for input about whether my confusion is justified.


I contacted a company about a Data Engineering job around 3 weeks ago that listed SQL, NoSQL, and Python as well as AWS skills as their primary need.


I made it through the HR representative and was pushed on to a meeting with the Data Platform Lead.


I talked about my experience with ETL/ELT with this person and said that my primary interest was in data-pipeline creation specifically as it pertains to building out data lakes and downstream data consumers. This interview seemed to go great and they told me to expect an invite to do a technical interview (that would be small project-based problem with a senior data engineer on their team).


I thought things were going great, so I brushed up on some pandas skills (as I mostly lived in the Spark world in my previous job) as well as some NoSQL and generic difference between SparkSQL and SQL.


The interview starts, and I'm told that we're going to be building a RestAPI endpoint using an ORM to solve some generic db read get request problem.


I'm really confused at this point, as per my experience this problem really doesn't reflect someone's utility as a DE. I'm not saying it isn't tangentially related, but designing a really simple OLTP-based model and then connecting it to a RESTapi seemed to be pretty far out of the problems I expect to be solving on a day-to-day basis.

Am I justified in my confusion for thinking this technical interview with a Senior Data Engineer would be modeling/python (probably pandas/numpy)/ sql/nosql aggregation based? I feel like what I really just completed was an interview for a backend engineer and doesn't reflect my abilities as a DE.

NOTE: I don't think that being able to spin up a quick flask rest-api isn't a useful skill for a DE to have, but I have it pretty low in terms of expectation on day-to-day role ."
4188,2020-11-21 00:29:03,1605911343.0,dataengineering,dbt Fundamentals (free official training),jxz07j,fhoffa,,https://www.reddit.com/r/dataengineering/comments/jxz07j/dbt_fundamentals_free_official_training/,1.0,5.0,0.0,21299.0,
4189,2020-11-21 00:48:34,1605912514.0,dataengineering,I would like to take a regular backup of the Production database and restore or dump it to the UAT database.,jxzc7t,shubhvv,,https://www.reddit.com/r/dataengineering/comments/jxzc7t/i_would_like_to_take_a_regular_backup_of_the/,1.0,3.0,0.0,21299.0,"We have a production database that sits at AWS RDS Postgres (Instance1), similarly UAT at AWS(pre-prod) RDS Postgres (Instance2), both in the same region and under the same AWS account. I would like to automate (monthly), **export pg\_dump** from production env and import it to pre-prod env.

I can write a bash script and schedule a CRON job. BUT, are there any dedicated AWS services that can help me to automate this?"
4190,2020-11-21 01:10:17,1605913817.0,dataengineering,ELT is not the disruption- Data Engineering is!,jxzpth,nonab8,,https://www.reddit.com/r/dataengineering/comments/jxzpth/elt_is_not_the_disruption_data_engineering_is/,1.0,2.0,0.0,21300.0,"In our recent conversations with a few VCs, we have heard how they think ELT is the next big disruption. Having seen the NoSQL and Hadoop waves before, we see this to not be true. In this post, we attempt to demystify the euphoria around ELT and give our view on why it is actually Data Engineering that is the next big disruption.

We're keen to hear from fellow data engineers what they think. 

[https://www.prophecy.io/blogs/to-vcs-elt-is-not-the-disruption-data-engineering-is](https://www.prophecy.io/blogs/to-vcs-elt-is-not-the-disruption-data-engineering-is)"
4191,2020-11-21 02:24:31,1605918271.0,dataengineering,Is it time for you to build data warehouse or modernize your data warehouse?,jy0yeo,knowillence,,https://www.reddit.com/r/dataengineering/comments/jy0yeo/is_it_time_for_you_to_build_data_warehouse_or/,1.0,2.0,0.0,21302.0,"Are you encountering these issue with respect to your on-premise database and data warehouse? If so then it is a good time to think about how to modernize the database.

[http://knowillence.com/blog/data-warehouse/pain-points-in-your-data-warehouse/](http://knowillence.com/blog/data-warehouse/pain-points-in-your-data-warehouse/)"
4192,2020-11-21 07:22:15,1605936135.0,dataengineering,Schema Management Tool?,jy5bpy,Silicon-Data,,https://www.reddit.com/r/dataengineering/comments/jy5bpy/schema_management_tool/,1.0,2.0,0.0,21305.0,"I'm fed up with constantly fixing mismatched schemas from upstream. 
Do you know any way or software to check database migrations from various frameworks like django, ruby on rails etc against some common rules and conformity to at least two downstream schemas, e.g. hive tables or avro.

If there's nothing available our group can start an open source project to address this need.

I'm thinking of it integrated into CI/CD process to become a part of build or could be just a github hook to trigger validation."
4193,2020-11-21 13:20:13,1605957613.0,dataengineering,Cheese Consumption Ranking | TOP 10 Country from 1961 to 2013,jy96lp,Honest_Palpitation_2,,https://www.reddit.com/r/dataengineering/comments/jy96lp/cheese_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21314.0,
4194,2020-11-21 14:52:46,1605963166.0,dataengineering,Fuzzy-wuzzy Algorithm,jya8hh,1987_akhil,,https://www.reddit.com/r/dataengineering/comments/jya8hh/fuzzywuzzy_algorithm/,1.0,1.0,0.0,21315.0,Anyone in this group has worked on matching algorithms or fuzzy-wuzzy.
4195,2020-11-21 15:29:49,1605965389.0,dataengineering,How many nodes should I use for the HiBench ML Benchmark (Bayesian) using Spark?,jyaqof,blue_sky_time,,https://www.reddit.com/r/dataengineering/comments/jyaqof/how_many_nodes_should_i_use_for_the_hibench_ml/,1.0,0.0,0.0,21315.0,"I am running some tests with the [HiBench benchmarks](https://github.com/Intel-bigdata/HiBench) , and I am not sure how many nodes I should use to get the fastest performance.  Some specs on this test:

* My input data size is 75 GB. 
* Running on AWS m5.4xlarge instances (16 vCPUs).
* Running with Spark

Any tips on what people normally do to select the number of nodes when they want to maximize performance (but being conscious of costs)?"
4196,2020-11-21 17:54:45,1605974085.0,dataengineering,"How to analyze data using SPSS (part-2), how to input data in SPSS",jyczsr,touhidkf,,https://www.reddit.com/r/dataengineering/comments/jyczsr/how_to_analyze_data_using_spss_part2_how_to_input/,1.0,2.0,0.0,21316.0,
4197,2020-11-21 19:05:58,1605978358.0,dataengineering,What's new in Airflow 2.0 — broken down into six ~10 min videos by Marc Lamberti,jye9im,rywalker,,https://www.reddit.com/r/dataengineering/comments/jye9im/whats_new_in_airflow_20_broken_down_into_six_10/,1.0,0.0,0.0,21317.0,
4198,2020-11-22 02:40:42,1606005642.0,dataengineering,I'm Officially a Data Engineer!!!,jym5v1,itsjustnae,,https://www.reddit.com/r/dataengineering/comments/jym5v1/im_officially_a_data_engineer/,1.0,37.0,0.0,21335.0,"I recently just got the news for promotion from an Application Developer to a Data Engineer, any advice you guys could give me. My new role starts in about two weeks."
4199,2020-11-22 04:48:16,1606013296.0,dataengineering,I've done some bad things - Transitionning from local files and Power Query to Azure and PQ then Python. Advices needed.,jyo4ia,Kiwizqt,,https://www.reddit.com/r/dataengineering/comments/jyo4ia/ive_done_some_bad_things_transitionning_from/,1.0,2.0,0.0,21339.0,"Hi folks, 


Context:

I'm in need of advices. Understand that I am not in charge of decisions but with solid evidences, I could change the obviously wrong path we've taken.

So far, I have assumed a role akin to a Business Analyst within our operational subsection.

We've been limited to locally exporting in csv our datas which contains thousands of daily inputs from our agents and in turn manually transform them in excel to do stuff in excel for the next day.

With PowerQuery, I have been able to automate lots of these tasks through data manipulation but it still isn't enough.

Recently, I have been tasked to build a database -NSFL incoming-, that I have built so far within powerquery's datamodel.


However, the main datascientist and I have grown extremely tired of manually exporting data all day from multiple sources and as such, we've been granted a nice VM aswell as an Azure DB.

Current problem:

Our datas can get gathered from a Rest API 3, SFTP and from another azure db.

I fully understand that we'd need one of you guys on out team to sucessfully lead that project, but that won't be done to my misfortune.

So far, I've only dealt with data manipulation within PowerQuery and I'm learning Python, but I reckon I won't be able to match my PQ skills within 6 months of hard practice.

My question is as follow:

What would you do/train for ? What Azure service would you purchase in order to have constant data streaming ?

As I understand it, both SFTP and REST datas could stream into our Azure DB, which I then could connect from PowerQuery to do transformations. 

To do that I'd need tokens, which I do have and solid data model. Having done so in PQ+PBI, I believe that isn't an insurmountable challenge.

Both of these flows (SFTP and REST) would need to stream into power BI reports.

Is there anything that i'm not seeing ? What would you require from your direction ? What skills are needed to ensure correct datas ?"
4200,2020-11-22 05:59:25,1606017565.0,dataengineering,What do you need to become a Python developer in 2021?,jyp4kv,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jyp4kv/what_do_you_need_to_become_a_python_developer_in/,1.0,0.0,0.0,21340.0,
4201,2020-11-22 06:04:51,1606017891.0,dataengineering,Best programming language to learn in 2021,jyp7eb,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jyp7eb/best_programming_language_to_learn_in_2021/,1.0,0.0,0.0,21341.0,
4202,2020-11-22 08:54:24,1606028064.0,dataengineering,Best-option analysis of cloud services,jyrddb,redder_ph,,https://www.reddit.com/r/dataengineering/comments/jyrddb/bestoption_analysis_of_cloud_services/,1.0,0.0,0.0,21345.0,"Data engineers who are tasked with picking a cloud platform based on a set of requirements, how do you approach this analysis?

Not a lot of engineers have AWS/Azure/GCP consultants to hold their hands through such analysis. In such a situation, how long do you take to come up with a recommendation? What resources do you leverage for your analysis?

For example, if you are tasked with recommending a cloud Datawarehouse service. Do you compare all the DW services offered by AWS/Azure/GCP/Oracle, etc. How do you determine the cost effectiveness of your choice?"
4203,2020-11-22 10:38:45,1606034325.0,dataengineering,The Most Prominent Ways to Learn Data Science With R,jysgvv,Techbiason,,https://www.reddit.com/r/dataengineering/comments/jysgvv/the_most_prominent_ways_to_learn_data_science/,1.0,0.0,0.0,21348.0,
4204,2020-11-22 14:55:45,1606049745.0,dataengineering,Tomato Consumption Ranking | TOP 10 Country from 1961 to 2013,jyv7zu,Honest_Palpitation_2,,https://www.reddit.com/r/dataengineering/comments/jyv7zu/tomato_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21351.0,
4205,2020-11-22 16:15:41,1606054541.0,dataengineering,"Terminology (LA, L0, L1, L2)",jywaiy,BeggarInSpain,,https://www.reddit.com/r/dataengineering/comments/jywaiy/terminology_la_l0_l1_l2/,1.0,1.0,0.0,21353.0,"Hey there!

I should probably know that but I'm bit confused how people use the basic division of DWH by levels and want to make this clear for myself too.

I've been always thinking about LA as a Data Lake or Kafka topic, L0 as ODS or Staging, L1: core DWH, L2: data-marts.

Am I utterly wrong?"
4206,2020-11-22 20:13:57,1606068837.0,dataengineering,Normalization and OLTP vs. OLAP,jz0cqm,breathe_dah,,https://www.reddit.com/r/dataengineering/comments/jz0cqm/normalization_and_oltp_vs_olap/,1.0,14.0,0.0,21365.0,"In my data management class, we've started talking about OLTP vs. OLAP and the access patterns of each. The main points are that OLTP requires high performance, as these are the systems that our users interact with, and there's often a large concurrent number of transactions. OLAP on the other hand requires flexibility in order to view the data in different ways, is often read-only, and performance isn't as critical. This all makes sense to me.

&amp;#x200B;

From these points, our professor then draws the following conclusions: OLTP should be *denormalized*, and OLAP should be *normalized*. This still makes sense to me on the surface - it would follow that normalization would have a penalty on speed and complexity due to multiple joins (bad for OLTP), and would enable more sophisticated views of the data (good for OLAP).

&amp;#x200B;

But when I start thinking about it more, it doesn't really seem like that would work. If we denormalize OLTP, wouldn't that leave the door open for data redundancy and inconsistency? And if we normalize OLAP, wouldn't aggregations become much more difficult to write? And then I read about star schema and how that's somewhat denormalized, which seems to contradict what we're learning in lecture, and when I started digging deeper it seems like every single resource I can find on the internet is saying the opposite of what we're being taught. I seriously doubt my professor is just straight up wrong, but it's getting very confusing to sort through when I'm getting told directly conflicting things.

&amp;#x200B;

So what are y'all's thoughts on this? Is there a right answer to (de)normalization and OLTP/OLAP? Or is this a case of differing schools of thought / it depends on the situation?"
4207,2020-11-22 20:54:29,1606071269.0,dataengineering,GKE Secrets OR Google Secret manager,jz13fj,Key-Coat-3406,,https://www.reddit.com/r/dataengineering/comments/jz13fj/gke_secrets_or_google_secret_manager/,1.0,8.0,0.0,21364.0,Does anyone know in which case choose Kubernetes secrets instead of google secret manager and the reverse ? Differences between the two ?
4208,2020-11-22 21:25:20,1606073120.0,dataengineering,Can someone explain the major data engineering tools and how they fit together?,jz1nrh,calijag18,,https://www.reddit.com/r/dataengineering/comments/jz1nrh/can_someone_explain_the_major_data_engineering/,1.0,4.0,0.0,21367.0,"I'm relatively new to the field and am having trouble understanding what each of these tools and platforms do, how they fit together, where they overlap, which ones are competing solutions for the same task, and which ones are meant to be used together. Would really appreciate a clarification or a link to a good resource. Also feel free to mention any tools/platforms that I missed.

Hadoop
Apache Spark
Databricks
Snowflake
AWS S3 (and Azure/GCP analogs)
AWS Redshift (and Azure/GCP analogs)
AWS Aurora/ RDS (and Azure/GCP analogs)
Tableau

Thanks and apologies if it's a bit elementary."
4209,2020-11-23 00:28:17,1606084097.0,dataengineering,"The 18th edition of data engineering is out. Microsoft’s ML model governance, Google’s MinDiff, Slack’s Airflow migration, Doordash’s scaling ML feature store, Pinterest’s journey from Lambda to Kappa architecture &amp; What is data mesh?",jz514r,vananth22,,https://www.reddit.com/r/dataengineering/comments/jz514r/the_18th_edition_of_data_engineering_is_out/,1.0,0.0,0.0,21372.0,
4210,2020-11-23 09:14:58,1606115698.0,dataengineering,Health / Weight gain as an engineering professional,jzd1it,Jagmeetoff,,https://www.reddit.com/r/dataengineering/comments/jzd1it/health_weight_gain_as_an_engineering_professional/,1.0,11.0,0.0,21390.0,"Does anyone else have experience/goals of going from ""underweight"" or ""skinny"" to gaining weight while working as a busy engineer?  
If so, what tips can you give and if not, what's your experience like?"
4211,2020-11-23 14:04:47,1606133087.0,dataengineering,Pork Consumption Ranking | TOP 10 Country from 1961 to 2013,jzgeoh,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/jzgeoh/pork_consumption_ranking_top_10_country_from_1961/,1.0,0.0,0.0,21395.0,
4212,2020-11-23 15:58:51,1606139931.0,dataengineering,Running Airflow in a Docker container,jzi2fy,clintdk,,https://www.reddit.com/r/dataengineering/comments/jzi2fy/running_airflow_in_a_docker_container/,1.0,5.0,0.0,21396.0,"I'm trying to setup Airflow to schedule tasks and scripts for the company I'm working for. I was looking at the following infrastructure:

\- Setting up an EC2 server and running a Docker environment on it

\- Setting up a container with Airflow within that Docker environment

\- Setting up several containers that will get directed by Airflow and will run scraping tasks for instance

I don't know if this will be a very efficient way to set up our architecture so I am asking for advice or suggestions :) TIA!"
4213,2020-11-23 16:05:21,1606140321.0,dataengineering,Sports realtime API,jzi6ej,pmol87,,https://www.reddit.com/r/dataengineering/comments/jzi6ej/sports_realtime_api/,1.0,3.0,0.0,21396.0," Hi, Someone knows any realtime service (free or paid) as API to get match data like passes, fouls, shots as well as other types of events throughout the sporting event?"
4214,2020-11-23 16:22:33,1606141353.0,dataengineering,Oracle Data Integration Connectors,jzigjy,Zestyclose-Hand-7319,,https://www.reddit.com/r/dataengineering/comments/jzigjy/oracle_data_integration_connectors/,1.0,0.0,0.0,21397.0," My first post here, and sorry in advance if etiquette is off.   I'm helping a company set up a modern D&amp;A platform.  They currently store custom SQL in Tableau server and execute the queries directly against production OLTP oracle (!).  I'm trying to create a pipeline architecture that at a minimum replicates the oracle DBs to Snowflake, and then do transforms and analytics there.  We did this at prior employer (very large), and used Goldengate.

  
 This is a smaller company, who not only hates oracle, but doesn't want the expense of a Goldengate license.  So I went looking for alternatives.  There are several, but they all involve some sort of license (Fivetran, Talend, HVR, Hevo), which might be what we have to do.   I'm still looking for something more open source that could work with TOS, or dbt and does both full syncs and CDC.  This seems to be available for MSSQL, MySQL, and Postgres, but not for Oracle (especially the CDC part).

  
 Does anyone know of a good open source connector for oracle CDC that does not require licensing a more heavyweight transformation tool?  Thanks in advance..."
4215,2020-11-23 16:41:36,1606142496.0,dataengineering,Unable to fetch celery task object in Airflow with CeleryExecutor,jzis4q,Inmate4587_,,https://www.reddit.com/r/dataengineering/comments/jzis4q/unable_to_fetch_celery_task_object_in_airflow/,1.0,4.0,0.0,21399.0,"I am new to airflow and trying to distribute the tasks parallelly to celery workers using **CeleryExecutor** module and I am able to achieve it via following code. But however, I am **unable to fetch** task object of each celery task

When used celery previously without airflow, I use to fetch the task object using delay  
 keyword but since here in airflow I am not using the delay shortcut how do I fetch each task object?

**Note** :- Each of the celery worker has its own task object.

As show below, I am trying to fetch the status\_code  
 of each\_url and every url is passed to different workers simultaneously as i am using CeleryExecutor

Please suggest how can i fetch the celery task object  


    urls = [website1, website2, website3, website4, website5, website6]
        
        dag_id = 'test'
        dag = DAG(dag_id, description='test DAG',
                  schedule_interval=""@once"", start_date=datetime(2018, 11, 1), catchup=False)
        
        def my_sleeping_function(url):
            r = requests.head(url)
            result[url] = r.status
        
        run_this_bash_first = BashOperator(
                                task_id='run_this_bash_first',
                                bash_command='echo start',
                                dag=dag)
        run_this_bash_last = BashOperator(
                                task_id='run_this_bash_last',
                                bash_command='echo ""run_id={{ run_id }} | dag_run={{ dag_run }}""',
                                dag=dag)
        
        result = {}
        index = 0
        for each_url in urls:
            index += 1
            task_python = PythonOperator(
                    task_id= str(index),
                    python_callable=my_sleeping_function,
                    op_kwargs={'url': each_url},
                    dag=dag)
            run_this_bash_first.set_downstream(task_python)
        
        task_python.set_downstream(run_this_bash_last)

&amp;#x200B;

https://preview.redd.it/gl1gwjgq40161.png?width=388&amp;format=png&amp;auto=webp&amp;s=26a722521ea61cacb67bb019a6d335e6693293d1"
4216,2020-11-23 16:48:36,1606142916.0,dataengineering,Data Democratization Infographic,jziwke,AppointmentTight4756,,https://www.reddit.com/r/dataengineering/comments/jziwke/data_democratization_infographic/,1.0,7.0,0.0,21400.0,
4217,2020-11-23 16:54:42,1606143282.0,dataengineering,Analysing historical and live data with ksqlDB and Elastic Cloud,jzj0hu,rmoff,,https://www.reddit.com/r/dataengineering/comments/jzj0hu/analysing_historical_and_live_data_with_ksqldb/,1.0,0.0,0.0,21400.0,
4218,2020-11-23 18:22:25,1606148545.0,dataengineering,Kafka IDE is released as Open Beta today! 🎉,jzkoqd,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/jzkoqd/kafka_ide_is_released_as_open_beta_today/,1.0,0.0,0.0,21404.0,"[Kafka IDE](https://kafkaide.com/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=open-beta&amp;utm_term=dataengineering) is a desktop client similar to Tableau or Looker that queries Apache Kafka directly. It aims to be a better alternative to kafkacat, kafka manager or similar.

You can query particular time windows using offsets (you can use natural language to specify dates) as well as smart schema inference for those folks who deal with plain JSON messages.

It also let you peek into some Apache Kafka specific configurations for your cluster and topics.

Let us know if you find it useful or if you encounter any issue while trying it out! You can leave your comment here or send us an email at [team@kafkaide.com](mailto:team@kafkaide.com)."
4219,2020-11-23 18:22:58,1606148578.0,dataengineering,Artificial Intelligence vs Machine Learning vs Data Science,jzkp33,chase2learn,,https://www.reddit.com/r/dataengineering/comments/jzkp33/artificial_intelligence_vs_machine_learning_vs/,1.0,0.0,0.0,21404.0,
4220,2020-11-23 18:37:18,1606149438.0,dataengineering,Curious about best data integration tools,jzkzfk,AppointmentTight4756,,https://www.reddit.com/r/dataengineering/comments/jzkzfk/curious_about_best_data_integration_tools/,1.0,4.0,0.0,21405.0,Can the reddit community recommend me the best data integration software?
4221,2020-11-23 20:19:29,1606155569.0,dataengineering,The GitHub of Data,jzn4b0,alig80,,https://www.reddit.com/r/dataengineering/comments/jzn4b0/the_github_of_data/,1.0,2.0,0.0,21407.0,"Having access to and learning from real world data is the key to rapid innovation, yet the modern enterprise devotes precious little time to it. Data is either completely walled off from broad access, or is left open for all to copy within an enterprise. [https://greylock.com/the-github-of-data/](https://greylock.com/the-github-of-data/)"
4222,2020-11-23 21:14:26,1606158866.0,dataengineering,"[Kafka] Insulator, an open source tool to interact with Kafka",jzo8b2,andrewinci,,https://www.reddit.com/r/dataengineering/comments/jzo8b2/kafka_insulator_an_open_source_tool_to_interact/,1.0,0.0,0.0,21409.0,"Insulator is an opensource and multi-platform UI to make easier interact with a Kafka cluster. 

Easily configure your cluster using SSL, SASL or Plaintext for local development.

Main features are: Avro consumer with easy search functionality, Avro producer, Schema registry viewer and so much more.

  
Check it out at [https://andrea-vinci.github.io/Insulator/](https://andrea-vinci.github.io/Insulator/)"
4223,2020-11-24 00:13:01,1606169581.0,dataengineering,"A practical guide to creating differentially private, synthetic data with Python and TensorFlow",jzru3u,alig80,,https://www.reddit.com/r/dataengineering/comments/jzru3u/a_practical_guide_to_creating_differentially/,1.0,0.0,0.0,21412.0,How To Create Differentially Private Synthetic Data [https://towardsdatascience.com/how-to-create-differentially-private-synthetic-data-ae1a04cff1b9](https://towardsdatascience.com/how-to-create-differentially-private-synthetic-data-ae1a04cff1b9)
4224,2020-11-24 02:38:10,1606178290.0,dataengineering,Is the apply function in koalas effective at distributed computing?,jzug32,nouseforaname888,,https://www.reddit.com/r/dataengineering/comments/jzug32/is_the_apply_function_in_koalas_effective_at/,1.0,0.0,0.0,21415.0,"The reason why I’m asking is I’m planning on creating a huge table of data and am computing lots of the business logic. 

Does the apply function distribute well/take advantage of parallel processing effectively? 

How does koalas compare to pyspark for this method?"
4225,2020-11-24 04:09:00,1606183740.0,dataengineering,I want to learn how to build a full end to end pipeline in order to expand my skillset as more than just a Data Analyst. What's the best approach to take/learn how to do so?,jzvy15,pipeline_prospect,,https://www.reddit.com/r/dataengineering/comments/jzvy15/i_want_to_learn_how_to_build_a_full_end_to_end/,1.0,5.0,0.0,21419.0,"I work as a Data Analyst in a smaller tech company and I want to grow my skillset and be more than just an end state consumer of data. For context, my company currently runs a Stitch &gt; Redshift &gt;dbt workflow and I always feel like I hit a wall whenever I encounter a system that we need to integrate that doesn't have an out of box connector for Stitch.  Given the fact that I always need to lean on our already overworked data science team for them to build out custom integrations, I want to learn how to be more self-sufficient in these scenarios and overall just learn how to build a automated date pipeline with technology like S3 and EC2/Lambdas.

My biggest hurdle at the moment is simply figuring out how/where to start. Skillset wise, I'm pretty decent with python and I've built a handful of small scripts that have loaded data from various APIs into a Postgres DB using Pandas/sqlalchemy for personal projects, but that's a stark difference when compared to a fully automated pipeline that gets refreshed automatically on a regular basis and runs in a real world business context.

Do y'all have any advice on how tackle this kind of project? Ideally, I'd like to work with similar technologies that I work with in my day job and would hope to produce something like the following workflow

&amp;#x200B;

1. Extract data from an API and land it into an S3 bucket on regular, automated cadence. The hosting/orchestration of running code to do this on a regular cadence is the most foreign part of this to me at the moment.
2. Move data from S3 into Redshift (or an alternative, free/cheap option like RDS)
3. Use dbt to model said data and load it back into my data store. This part is pretty easier for me as it's basically what I spend 50% of my day on already.

As mentioned above my biggest hurdle right now is the lack of familiarity with the scheduling/automation/cloud hosting pieces. I've never done anything with EC2 or Lambdas given that my job is mainly data modeling and analysis and the entirety of server management/hosting code  is pretty much unknown territory for me. Any advice here would be greatly appreciated!"
4226,2020-11-24 08:11:49,1606198309.0,dataengineering,What do you use for data governance?,jzzkm0,vektor888,,https://www.reddit.com/r/dataengineering/comments/jzzkm0/what_do_you_use_for_data_governance/,1.0,30.0,0.0,21431.0,"Hi,

I am currently working for a company that wants to have a metadata catalog in order to list their assets (data is on S3 buckets, processed with spark and exposed on hive/redshift.

I have tried Apache Atlas, but I am not impressed by it. Despite it can be easily integrated with airflow and there is a spark plugin for it, end users don't like its UI and I agree with them when they say it's quite unusable for a non tech person.

What do you use in your company that allows to keep track of data lineage (hopefully in a semi-automatic way like atlas) and at the same time is easy to use for end users?
 
(Open source and self-hosted solutions would be better)"
4227,2020-11-24 11:11:25,1606209085.0,dataengineering,Building a fully automated Medium stats pipeline to track my writing performance,k01p4i,feliche93,,https://www.reddit.com/r/dataengineering/comments/k01p4i/building_a_fully_automated_medium_stats_pipeline/,1.0,0.0,0.0,21437.0,"Combining Prefect, Selenium, Pandas and Metabase to measure and manage my writing success on Medium"
4228,2020-11-24 11:12:27,1606209147.0,dataengineering,Building a fully automated Medium stats pipeline to track my writing performance,k01pio,feliche93,,https://www.reddit.com/r/dataengineering/comments/k01pio/building_a_fully_automated_medium_stats_pipeline/,1.0,2.0,0.0,21437.0,"Combining Prefect, Selenium, Pandas and Metabase to measure and manage my writing success on Medium"
4229,2020-11-24 13:24:08,1606217048.0,dataengineering,Spirits Consumption Ranking | TOP 10 Country from 1961 to 2014,k039oo,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k039oo/spirits_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21442.0,
4230,2020-11-24 14:54:44,1606222484.0,dataengineering,Business applications of Lambda architecture (batch+stream pipelines)?,k04gnf,MadMaxReddy,,https://www.reddit.com/r/dataengineering/comments/k04gnf/business_applications_of_lambda_architecture/,1.0,5.0,0.0,21443.0,"Hi all, I'm a non-tech student who recently started a journey in data analytics. While I was working on understanding the basics of data and pipelines, stumbled upon the type of pipelines that are out there. It's getting quite difficult to understand the business applications of these types of pipelines

In my understanding, there are 3 types of pipelines - batch, streaming, and lambda (mix of both). (I also read a bit about delta and kappa architectures)

Whereas the batch pipeline runs and transforms data at regular intervals, the streaming pipeline does the data transformations on the fly (with apps like Spark)

Why does a company still maintain these 2 different pipelines where the streaming pipeline could do the job of transformations on the fly and the data can still be consumed towards the end of the day from streamed data (by disposing of the batch pipeline).

Do companies use the streaming layer and batch layer for different purposes? If yes, can companies get rid of batch pipelines and just have streaming pipelines in place (since the data is transformed anyway in the streaming pipe)?

Can you help me with some real-life examples?"
4231,2020-11-24 16:16:47,1606227407.0,dataengineering,Help with automating data download from web,k05re1,ineednoBELL,,https://www.reddit.com/r/dataengineering/comments/k05re1/help_with_automating_data_download_from_web/,1.0,13.0,0.0,21447.0,"I recently applied for a data engineer intern position where I am tasked to write a python program to download files from a https webpage, both regularly and user-specific. I realised the data in question is not linked on the page. And it is user specific. For example, selecting A from the list along with any specific date, and then clicking the “download” button will give me the data i want. I dont see any links on the inspect button? 

It is my first time doing data crawling, and i have never worked with automating data download from the web. Im actually having a hard time understanding the (inspect) elements on the page. I found a few libraries which support crawling, but im not so sure if my approach is on the right direction. It will be nice if anyone can give me some advice or direction to get started. Thanks in advance!"
4232,2020-11-24 16:22:08,1606227728.0,dataengineering,What is needed to be a full stack developer in 2021,k05upt,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k05upt/what_is_needed_to_be_a_full_stack_developer_in/,1.0,0.0,0.0,21448.0,
4233,2020-11-24 16:39:09,1606228749.0,dataengineering,Airflow task_id not found,k065ar,Inmate4587_,,https://www.reddit.com/r/dataengineering/comments/k065ar/airflow_task_id_not_found/,1.0,0.0,0.0,21449.0,"I am trying to set a unique id for each of the celery task in airflow. but for some reason it throws me the following error. I even tried with **uuid** module and the same error pops up.

    Traceback (most recent call last):
      File ""/home/mahesh/anaconda3/bin/airflow"", line 37, in &lt;module&gt;
        args.func(args)
      File ""/home/mahesh/anaconda3/lib/python3.7/site-packages/airflow/utils/cli.py"", line 76, in wrapper
        return f(*args, **kwargs)
      File ""/home/mahesh/anaconda3/lib/python3.7/site-packages/airflow/bin/cli.py"", line 547, in run
        task = dag.get_task(task_id=args.task_id)
      File ""/home/mahesh/anaconda3/lib/python3.7/site-packages/airflow/models/dag.py"", line 1263, in get_task
        raise TaskNotFound(""Task {task_id} not found"".format(task_id=task_id))
    airflow.exceptions.TaskNotFound: Task 893370 not found
    

**Following is my DAG**

    
dag_id = 'test'
        dag = DAG(dag_id, description='test DAG',
                  schedule_interval=None, start_date=datetime(2018, 11, 1), catchup=False)
        
        def my_sleeping_function(**context):
            url = context['url']
            r = requests.head(url)
            return {url : r.status_code}
        
        def fetch_final_result(**context):
            task_instance = context['task_instance']
            ans = []
            print(""paypal is "", uuid_list)
            for i in uuid_list:
                data = task_instance.xcom_pull(task_ids=i)
                ans.append(data)
            print(""uber is "", ans)
            return ans
        
        
        run_this_bash_last = PythonOperator(
                    task_id= 'last',
                    python_callable=fetch_final_result,
                    # op_kwargs={'url': 'asd'},
                    dag=dag,
                    provide_context=True)
        
        
        urls = [website1, website2, website3, website4, website5, website6]
        
        for i in urls:
            index += 1
            ind_id = str(random.randint(1, 100000000000000))
            uuid_list.append(ind_id)
            task_python = PythonOperator(
                    # task_id = ind_id,
                    task_id = ind_id,
                    python_callable=my_sleeping_function,
                    op_kwargs={'url': i},
                    dag=dag,
                    provide_context=True)
        task_python.set_downstream(run_this_bash_last)

Please suggest how to overcome this"
4234,2020-11-24 18:56:41,1606237001.0,dataengineering,.NET ETL - free Community edition and .NET 5 support added to actionETL,k08sbb,envobi,,https://www.reddit.com/r/dataengineering/comments/k08sbb/net_etl_free_community_edition_and_net_5_support/,1.0,0.0,0.0,21453.0,"&amp;#x200B;

https://preview.redd.it/tvepbelqx7161.png?width=100&amp;format=png&amp;auto=webp&amp;s=20cb1021a787a9838bae2b25e176b5f5310c4708

actionETL is a high performance, code-first .NET ETL library. The [latest release](https://docs.envobi.com/articles/release-notes.html#version-0390) adds:

* [Free Community edition](https://envobi.com/community-edition/)
   * Commercial use allowed
   * Best-Effort support included
   * Unlimited data volumes
* .NET 5 support

Check out actionETL [features](https://envobi.com/) and the [extensive documentation](https://docs.envobi.com/), and get the above Community edition or use the [free 30-day trial](https://envobi.com/trial/) to test it out.

Cheers!  
Kristian Wedberg  
Disclaimer: I'm the architect of the framework."
4235,2020-11-24 19:18:13,1606238293.0,dataengineering,A Development Environment for Data with CI/CD,k098ko,ydr-,,https://www.reddit.com/r/dataengineering/comments/k098ko/a_development_environment_for_data_with_cicd/,1.0,0.0,0.0,21453.0,
4236,2020-11-24 20:40:46,1606243246.0,dataengineering,Data + AI Summit Europe 2020 Highlights - Data Mechanics Blog,k0awec,gingerbeardmayn,,https://www.reddit.com/r/dataengineering/comments/k0awec/data_ai_summit_europe_2020_highlights_data/,1.0,1.0,0.0,21454.0,
4237,2020-11-24 20:44:06,1606243446.0,dataengineering,"Entity Resolution: Data Standardization, Duplicate Detection, and Data Merging",k0ayxh,ZoeHawkins,,https://www.reddit.com/r/dataengineering/comments/k0ayxh/entity_resolution_data_standardization_duplicate/,1.0,0.0,0.0,21454.0,"Entity Resolution is the process of comparing key attributes of customer or partner records to identify the same or completely different entities. How to use Dgraph capability to store, dynamically link, and traverse nodes to achieve each process's needs: [Article](https://dgraph.io/blog/post/introducing-entity-resolution/)."
4238,2020-11-24 20:51:45,1606243905.0,dataengineering,Real-time decision-making with streaming SQL for Apache Kafka,k0b4j8,lensesio,,https://www.reddit.com/r/dataengineering/comments/k0b4j8/realtime_decisionmaking_with_streaming_sql_for/,1.0,0.0,0.0,21455.0,
4239,2020-11-24 22:46:50,1606250810.0,dataengineering,Advice for a Beginner?,k0df02,rabbitemperor,,https://www.reddit.com/r/dataengineering/comments/k0df02/advice_for_a_beginner/,1.0,3.0,0.0,21459.0,"I'm a student with mostly an ML background who's looking to dive in deeper into big data/data engineering.

I've read some of the posts on here about getting started mostly from a few years ago, but I was wondering what kind of experience companies want and what's the most valuable types of experience/creds (Are big data/data engineering certs any good?) How much ML is used/will be used? And what's the job outlook for this profession looking like (is there a shortage right now or will there be in the future and at what level is that shortage).

I've heard all kinds of wildly different answers and was wondering what people who actually know what they're talking about think."
4240,2020-11-24 22:57:44,1606251464.0,dataengineering,New Amazon Managed Workflows for Apache Airflow!,k0dmab,knowsuchagency,,https://www.reddit.com/r/dataengineering/comments/k0dmab/new_amazon_managed_workflows_for_apache_airflow/,1.0,0.0,0.0,21459.0,
4241,2020-11-25 00:27:27,1606256847.0,dataengineering,Introducing Amazon Managed Workflows for Apache Airflow,k0faxp,GrayMsr,,https://www.reddit.com/r/dataengineering/comments/k0faxp/introducing_amazon_managed_workflows_for_apache/,1.0,23.0,0.0,21462.0,
4242,2020-11-25 01:32:08,1606260728.0,dataengineering,DE on a $0 Budget,k0gfqm,AphoticSeagull,,https://www.reddit.com/r/dataengineering/comments/k0gfqm/de_on_a_0_budget/,1.0,10.0,0.0,21465.0,"My company has purely transactional data which has been somewhat split up to support a microservices architecture.

My goal is to create a ""poor man's"" version of
* a Data Dictionary
* Data Lineage
and
* Data Observability

... before I even begin architecting a Data Lake, while at the same time hunt down SMEs and stewards, and shoot for low-hanging fruit before really ramping up the seven Vs.

Marts / warehouse / tailored Data Viz to come later.

The last thing I need is a new team of data scientists being pressured to produce value using mystery, undocumented data on data pipelines designed by software engineers.

Advice? Looking for a few breadcrumbs on how to start the above with no budget with the intent to prove value, get budget approved, and then EASILY import existing documentation into the new system (Alex, Alation, Collibra, whatever)."
4243,2020-11-25 01:34:59,1606260899.0,dataengineering,Apologies if this doesn’t belong here but can anyone give me any good Data Engineering Projects to get a portfolio going or just some deeper experience. I am a BI Developer at the minute but really want to move into a purely Data Engineering role.,k0ghmr,Apex-AlphaPredator,,https://www.reddit.com/r/dataengineering/comments/k0ghmr/apologies_if_this_doesnt_belong_here_but_can/,1.0,7.0,0.0,21465.0,"Basically I use SQL Server and Python which I’m very mediocre with. That being said I can use python to connect to API’s, manipulate and create data frames and then insert said data into SQL sever tables. 

I’ve created a database for coronavirus data but really that’s just three tables: countries, provinces and daily_stats.

Can anyone recommend a good solid end to end project for data engineering?"
4244,2020-11-25 04:37:41,1606271861.0,dataengineering,The 5 timeless data books for analytics engineers,k0jg92,anhthong00,,https://www.reddit.com/r/dataengineering/comments/k0jg92/the_5_timeless_data_books_for_analytics_engineers/,1.0,0.0,0.0,21469.0,
4245,2020-11-25 14:11:43,1606306303.0,dataengineering,#idataengineer podcast 002 Tamás Németh,k0r326,soobrosa,,https://www.reddit.com/r/dataengineering/comments/k0r326/idataengineer_podcast_002_tamás_németh/,1.0,0.0,0.0,21480.0,"We, data engineers, are all different, but we all have a story. In the second part of our micro-podcast, [Tamás Németh](https://twitter.com/treff7es) speaks about his journey and why you should always  ask: 'why' -- even if it comes to real-time.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-002](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-002)"
4246,2020-11-25 14:13:08,1606306388.0,dataengineering,Coffee Consumption Ranking | TOP 10 Country from 1961 to 2013,k0r3qv,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k0r3qv/coffee_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21480.0,
4247,2020-11-25 14:41:45,1606308105.0,dataengineering,A 2020 Reader's Guide to The Data Warehouse Toolkit,k0rhm2,levelworm,,https://www.reddit.com/r/dataengineering/comments/k0rhm2/a_2020_readers_guide_to_the_data_warehouse_toolkit/,1.0,1.0,0.0,21483.0,
4248,2020-11-25 16:33:18,1606314798.0,dataengineering,Any reviews about Dremio on Kubernetes?,k0t7jl,unnitechm,,https://www.reddit.com/r/dataengineering/comments/k0t7jl/any_reviews_about_dremio_on_kubernetes/,1.0,0.0,0.0,21488.0,"Does Dremio on Kubernetes scale to 100Gbs of data. 

What will be the configuration for 400 concurrent requests?

Any issues anyone has encountered."
4249,2020-11-25 16:42:56,1606315376.0,dataengineering,[Rookie questions] Transformation possibilities in a data pipeline,k0td75,MadMaxReddy,,https://www.reddit.com/r/dataengineering/comments/k0td75/rookie_questions_transformation_possibilities_in/,1.0,9.0,0.0,21488.0,"So I'm a nontechie (marketer) who recently joined a company that is a no-code data pipeline. While making a presentation on data and pipelines I was asked some questions by the panel that sorta contradicts what I read online. So I thought I'll ask the experts here.

  
(A snapshot of a slide where I explained how modern data pipelines are built)

https://preview.redd.it/gudhtdy7jd161.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=c79074541199a83da8a341cd1df8bea27172da12

Q1. A person in the panel said there are **some data transformations** that **can't be done before** loading data into the warehouse. As per her, transformations like aggregations and joins are only possible after the data is in the warehouse and can't be done beforehand. Whereas transformations like converting data types, cleansing, and deduplication are possible before the data is loaded (I agree with this).

Is the above statement about aggregations and joins accurate?  


Q2. My presentation said data sent via Kafka to the Transformation stage is transformed in realtime. A person in the panel asked how is data that is ingested via Kafka to the Transformation stage transformed on the fly? Because with the streaming data pipeline data is always coming in and if transformations are run, then it's not accurate. 

How is it possible to transform data on the fly?

Q3. Is it okay to build batch pipelines in the way shown above? - Where you stream database data via Kafka and try to perform batch operations? Or does batch pipeline have to pull data in a way that's not a Pub/sub model?"
4250,2020-11-25 17:37:15,1606318635.0,dataengineering,Airflow DAG auto-refreshes every 30 seconds,k0ubru,Inmate4587_,,https://www.reddit.com/r/dataengineering/comments/k0ubru/airflow_dag_autorefreshes_every_30_seconds/,1.0,1.0,0.0,21489.0,"I have a **DAG script in my airflow** and it auto-refreshes 30 seconds  
. I want to either **disable it (or) if possible set to a higher value.**

Also, as suggested [here](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=63407290), I have set min\_file\_process\_interval  
 and dag\_dir\_list\_interval  
 to higher values and restarted the airflow server, but again the DAG is getting refreshed in next 30 seconds.

https://preview.redd.it/zsavug8eoe161.png?width=413&amp;format=png&amp;auto=webp&amp;s=041622931a36d787afdd14fa635c11cbdf5cbc3a

**Please suggest a workaround to disable this auto-refresh or delay it to more time interval. Thanks**"
4251,2020-11-25 18:53:54,1606323234.0,dataengineering,"Drowning in YAML, conf, xml, configuration.",k0vsnq,jduran9987,,https://www.reddit.com/r/dataengineering/comments/k0vsnq/drowning_in_yaml_conf_xml_configuration/,1.0,8.0,0.0,21494.0,"The thing I miss about working on web development projects in being able to jump right into the code.

I love Data Engineering, but the time I spend actually writing code to manipulate data is nothing compared to the time setting up environments and configuring files.  I wanted to learn a bit of Hadoop but instead the last few days I've spent confused over the installation process (I did it, just not sure what I did).  

I know there are things like Cloudera that allow you to spin up a Hadoop cluster, or Databricks for Spark work.. but aren't DE's expected to know how to set these things up in the real world? 

I guess I just want a clear picture of the expectations.. I don't mind configuring something here and there, but I really want to spend most of my time with the data."
4252,2020-11-26 02:14:30,1606349670.0,dataengineering,Built my first SSIS package from scratch today.,k14fm9,Not-NedFlanders,,https://www.reddit.com/r/dataengineering/comments/k14fm9/built_my_first_ssis_package_from_scratch_today/,1.0,5.0,0.0,21507.0,"I built my first SSIS package by myself today *without* help from anyone on my team at work and damn that felt good! 

I’m a Jr. BI Developer and this year has been struggle after struggle. I landed my first data job as an entry level DE last fall but lost my job when I was furloughed in the Spring due to COVID-19. Finding another entry level job was hard given the state of the world, but I finally did it after being unemployed for 6 months. 

SSIS was something I was struggling with before I lost my job in the spring, so today being able to come in and churn out a new package in under a day by myself felt like a major win. It feels damn good to be back into the swing of things."
4253,2020-11-26 04:45:47,1606358747.0,dataengineering,10 Key Skills Every Data Engineer Needs | Hacker Noon,k16t1m,zakariahs1,,https://www.reddit.com/r/dataengineering/comments/k16t1m/10_key_skills_every_data_engineer_needs_hacker/,1.0,0.0,0.0,21511.0,
4254,2020-11-26 07:34:42,1606368882.0,dataengineering,Use cases of pipelines where data is directly loaded to the visualization stage.. skipping the T (transformation) and L(load to storage) in the ETL stages,k19am7,MadMaxReddy,,https://www.reddit.com/r/dataengineering/comments/k19am7/use_cases_of_pipelines_where_data_is_directly/,1.0,5.0,0.0,21514.0,"Hi all!

Anyone here worked on sending **data directly to visualizing tools like Metabase or PBI or KafkaIDE?** I think this is only required if we need to monitor certain events.   


But **wondering if it's possible to visualize data without any transformations and loading** to the data lake or warehouse process.

Thanks."
4255,2020-11-26 08:13:34,1606371214.0,dataengineering,"What do you know about ""Drill down"" on Segments and Dashboards",k19tng,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/k19tng/what_do_you_know_about_drill_down_on_segments_and/,1.0,0.0,0.0,21514.0,"As users keep creating reports for every level of data hierarchy, it happens to be a siloed process. Say for example, an ecommerce company generates a report on the **""OrderDate\_Week""** and **""Sum\_OrderLineAmount""**

https://preview.redd.it/ri6v795bzi161.png?width=1387&amp;format=png&amp;auto=webp&amp;s=800a66c3bd7357d747ab8e54a4b51fd4a793ce0b

What Drill down feature actually does is, on clicking the required **""Sum\_OrderLineAmount"",** it displays the distribution of this sum as per your requirement, say here **""BuyerCity""**, **""SellerCity""**, **""Category""** and **""Sum\_OrderLineAmount""** is opted. 

https://preview.redd.it/1ldyai4szi161.png?width=1389&amp;format=png&amp;auto=webp&amp;s=da27f784b3a801e9110dcd8bb284aaf754170ff6

This feature actually help the users to save time on report creation and allow more time for analysis, to learn more, access the following link [**Drill Down Feature In Segments And Dashboards**](https://www.sprinkledata.com/docs/drill-down-feature-in-segments-and-dashboards/index.html?utm_source=reddit_261120&amp;utm_medium=drilldown)"
4256,2020-11-26 08:22:48,1606371768.0,dataengineering,Do data engineers get paid less than SWE?,k19y4e,antisocial72,,https://www.reddit.com/r/dataengineering/comments/k19y4e/do_data_engineers_get_paid_less_than_swe/,1.0,24.0,0.0,21514.0,"Currently am undergrad student thinking about going into data engineering, but I've been reading on Blind that DE is less respected and has slightly lower pay than SWE, and it seems that people shit on it for being boring work. I don't know if blind is full of biased software engineers, but I was wondering what data engineers think of this career in comparison to software engineering."
4257,2020-11-26 09:31:56,1606375916.0,dataengineering,Why Data Engineering Needs Automated Testing,k1auks,david_ok,,https://www.reddit.com/r/dataengineering/comments/k1auks/why_data_engineering_needs_automated_testing/,1.0,0.0,0.0,21518.0,
4258,2020-11-26 12:01:27,1606384887.0,dataengineering,4 Benefits Of Hybrid Data Management With AWS,k1cm9e,arminham1967,,https://www.reddit.com/r/dataengineering/comments/k1cm9e/4_benefits_of_hybrid_data_management_with_aws/,1.0,0.0,0.0,21522.0,
4259,2020-11-26 13:21:46,1606389706.0,dataengineering,RCEP GDP Ranking | TOP 10 Country from 1960 to 2018,k1dkut,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k1dkut/rcep_gdp_ranking_top_10_country_from_1960_to_2018/,1.0,0.0,0.0,21524.0,
4260,2020-11-26 17:11:40,1606403500.0,dataengineering,Using company Learning budget,k1gtiv,anotherdataredittor,,https://www.reddit.com/r/dataengineering/comments/k1gtiv/using_company_learning_budget/,1.0,2.0,0.0,21527.0,"Hey folks,

I have 300$ company budget left for professional development which would expire by the end of year. What are some of subscriptions that I can purchase? Some options that I have seen are pluralsight, linuxacademy, udemy(specific course)
I work as a data engineer in my day job and would be just looking to enhance my career path.

Need some suggestions from community."
4261,2020-11-26 17:12:53,1606403573.0,dataengineering,How do you guys learn things?,k1gu8v,Silver-Thing,,https://www.reddit.com/r/dataengineering/comments/k1gu8v/how_do_you_guys_learn_things/,1.0,24.0,0.0,21527.0,"I feel overwhelmed by the amount of things I could possibly learn. Currently working as a consultant based in Eastern Europe. Despite my young age, I am perceived by an expert but I just don't feel like being experienced engineer yet. There is so many technologies to learn and I yet don't feel like knowing deeply the things I am using on a daily basis (AI, AWS, GCP, K8S, Spark, Kafka, Hadoop stack to name a few).

I have A TON of articles, courses bookmarked. With 10 hours of spare time weekly I don't feel like watching a presentation or reading an article is moving me forward in meaningful direction. I used to take notes but I never had time to revisit them as I basically kept on learning new things. Back in college I used to spend 10+ hours A DAY learning things to get the best job I could. And back then there weren't so many technologies to grasp. 

My main issue is that I am usually asked to design new piece of a system or design entirely new pipeline. Either way I don't feel competent enough to tell someone what're the viable options. Without deep understanding it's hard to tell which technologies provide e. g. exactly once semantics, how they handle failures, how easy it is to maintain them. 

How do you guys stay sharp nowadays? I would appreciate any advice."
4262,2020-11-26 18:47:39,1606409259.0,dataengineering,The PostgresOperator: All you need to know,k1ikny,marclamberti,,https://www.reddit.com/r/dataengineering/comments/k1ikny/the_postgresoperator_all_you_need_to_know/,1.0,0.0,0.0,21531.0,
4263,2020-11-26 20:00:20,1606413620.0,dataengineering,Black Friday discounts for data engineering courses?,k1k0ro,Usurper__,,https://www.reddit.com/r/dataengineering/comments/k1k0ro/black_friday_discounts_for_data_engineering/,1.0,2.0,0.0,21530.0,"Hi,

Do you know any good Black Friday discounts regarding data engineering courses? I'm considering Real Python (python programming) or DataQuest (data engineering)."
4264,2020-11-26 20:57:31,1606417051.0,dataengineering,Strategies for reading ~100k xml files in spark,k1l44d,sthsthanothersth,,https://www.reddit.com/r/dataengineering/comments/k1l44d/strategies_for_reading_100k_xml_files_in_spark/,1.0,8.0,0.0,21534.0,"Context: I am using spark for an ETL task and the input is around 1-2 terrabytes of text distributed across tens of thousands of xml files.

Problem: Seems the over head at the driver level for loading such a large number of files is extremely high. I am using databricks' spark-xml adapter with a list of the files I am interested in reading as input to spark's read. The job seems to be stuck at the driver level.

What I already tried:
Having looked online, I started to read about the ""hadoop small files problem"". I am learning that spark inherently doesn't work well when the number of files is too many (or too small, tbh I don't completely understand this)

I am not sure if that's the problem or I am just running out of memory at driver level. I see a lot of Garbage Collection messages in the driver log.

Any inputs or any ideas how to debug/optimize this?"
4265,2020-11-27 05:46:24,1606448784.0,dataengineering,Thinking of starting a Youtube channel on Data Engineering. Ideas?,k1tqbs,stym06,,https://www.reddit.com/r/dataengineering/comments/k1tqbs/thinking_of_starting_a_youtube_channel_on_data/,1.0,23.0,0.0,21549.0,"I've been working for more than a year in a data engineering team, and have been learning a lot of stuff in this domain - primarily Hadoop, Spark, data pipelines, etc. Recently I've been thinking of starting a Youtube channel wherein I'll stream while making a project or while how to learn some technology or something like that.

Suggestions and ideas are welcome."
4266,2020-11-27 07:23:59,1606454639.0,dataengineering,Survey Analytics Help,k1v2oi,scross4565,,https://www.reddit.com/r/dataengineering/comments/k1v2oi/survey_analytics_help/,1.0,1.0,0.0,21551.0,"Hi

Any resources to study how to perform statistical analysis on Data patterns of various Surveys. We have built a tool to capture data for Survey and there are about 5 surveys which will be distributed across our customers/stakeholders to assess pre, during and post covid experiences so we would know our strength/weakness ? Any suggestions or recommendations on how to handle Survey analytics

Thanks

ANS"
4267,2020-11-27 09:47:02,1606463222.0,dataengineering,How to Learn Python for Data Science? The Best Ways,k1wwb2,Techbiason,,https://www.reddit.com/r/dataengineering/comments/k1wwb2/how_to_learn_python_for_data_science_the_best_ways/,1.0,0.0,0.0,21558.0,
4268,2020-11-27 13:21:18,1606476078.0,dataengineering,Mutton and Goat Meat Consumption Ranking | TOP 10 Country from 1961 to 2013,k1zf8e,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k1zf8e/mutton_and_goat_meat_consumption_ranking_top_10/,1.0,0.0,0.0,21563.0,
4269,2020-11-27 14:44:21,1606481061.0,dataengineering,[Question] Creating a data ecosystem.,k20gf3,gamerfreakish,,https://www.reddit.com/r/dataengineering/comments/k20gf3/question_creating_a_data_ecosystem/,1.0,5.0,0.0,21564.0,"I'm a full-stack developer and now am working for a small company.  Despite my lack of experience in data-engineering, the company has bestowed responsible for establishing an ecosystem of data reports. 

From raw-data to BI-Tools/Data-science, periodically.

Apology for oversimplifying everything and a bit ambiguous, but my current plan is to upload all the raw data to BigQuery —there are myriad types of data from excel sheet to the old version of SQL— then use DBT to query those raw data into a specific table, then feeds those data to DataStudio.

My questions are;

* How to automate the query? I know BiqQuery has a native scheduler system but will it work well with DBT? What's the best approach.
* I notice that when importing data to Datastudio, it doesn't recognize the datatype checking/changing these datatypes are painful, is there a way to automate this? 
* Instead of reinventing the wheel, is there a standard pipeline that uses Google products/DBT that I could adapt?"
4270,2020-11-27 15:17:44,1606483064.0,dataengineering,2nd Data Engineering (Azure) Job Interview - Help me prepare !,k20xi2,huuugaaa,,https://www.reddit.com/r/dataengineering/comments/k20xi2/2nd_data_engineering_azure_job_interview_help_me/,1.0,0.0,0.0,21564.0,"Hey r/dataengineering 

So I’m having a second interview regarding a data engineering job (azure) in a city/company I love, it’s my dream job and I want to ensure I’m prepared as much as possible.

Background: 
-Data Science background academically (Masters) . 
-7 months interning where I learned basic cloud azure tools like data lake, factory, some spark on databricks.
-Then they gave me a full time where I’ve been working for 4 months as a data science engineer where I’ve been working a lot on azure as well , especially on data warehousing and sql servers. 

I don’t have any Microsoft certifications , only some general udemy data engineering courses .

I’m preparing some project examples I can present at the interview , but I know I am not the most experienced data engineer. Did anyone go through a data engineering interview recently who could give me some feedback? All insights are welcome.

Thanks a lot!"
4271,2020-11-27 15:50:24,1606485024.0,dataengineering,Need guidance/path for AWS Data Engineering,k21fgl,OtherDegree3593,,https://www.reddit.com/r/dataengineering/comments/k21fgl/need_guidancepath_for_aws_data_engineering/,1.0,7.0,0.0,21565.0,"I want to transit my career to Data Engineering on AWS platform. Currently stuck in a stagnant service desk analyst role (5.5 years of experience). Have had some hands-on on Java and SQL Server 8 years ago at a training institute but never got the programmer job and had to settle for tech support roles.

In a state of paralysis on where to start. Whether to start with Python/Java programming or with databases on AWS. Unlike Azure Data I do not see a curated path for AWS. Also unable to figure out what Associate certification to pursue, SA or Developer.

Someone, please guide me.. my time is running out 🙏"
4272,2020-11-27 15:57:16,1606485436.0,dataengineering,Get free managed Kafka credits with DataOps,k21jb6,lensesio,,https://www.reddit.com/r/dataengineering/comments/k21jb6/get_free_managed_kafka_credits_with_dataops/,1.0,0.0,0.0,21566.0,
4273,2020-11-27 17:03:56,1606489436.0,dataengineering,Planning to enroll in Udacity Data Engineer Nanodegree due to 75% Off,k22p8f,Hawkeyee96,,https://www.reddit.com/r/dataengineering/comments/k22p8f/planning_to_enroll_in_udacity_data_engineer/,1.0,13.0,0.0,21567.0," Just got to know about the sale from here [Udacity's School of Data Science](https://onlinecourseing.com/udacity-school-of-data-science/).

I always wanted to take this nanodegree but as Udacity is pretty expensive I couldn't afford it. Now since its on sale I am heavily considering their Data Engineer or Data Scientist Nanodegree. One of my friend took their data analyst nanodegree last year and have landed in a job as a data analyst. I think this course will be a good start for me.

Any suggestions?"
4274,2020-11-27 18:14:54,1606493694.0,dataengineering,"For your current data engineering role, which would you say was more important for your interview: 1) data structures and algorithms 2) understanding of big data tools 3) systems design",k24191,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k24191/for_your_current_data_engineering_role_which/,1.0,10.0,0.0,21568.0,"Interested in seeing what employers of the DE field put more focus on for your guys’ current role. For me, it was an entry level DE job little focus on data structure and algos and much more focus on my understanding of conceptual/high level understanding of data engineering tools in general (ie: Hadoop, hive, spark, etc). 

Please state the seniority of the role and also size of company. Mine was entry level DE for a very large enterprise (+5000 employees)

[View Poll](https://www.reddit.com/poll/k24191)"
4275,2020-11-27 18:20:58,1606494058.0,dataengineering,Which Informatica free trial contains PowerCenter?,k245ij,data_fren,,https://www.reddit.com/r/dataengineering/comments/k245ij/which_informatica_free_trial_contains_powercenter/,1.0,0.0,0.0,21568.0,"I will be using Informatica PowerCenter at work soon and I'd like to download a free trial to learn it before I get it installed. Which of these free trials contains PowerCenter?  How come there is no ""PowerCenter trial""?

&amp;#x200B;

Thanks!"
4276,2020-11-27 22:42:36,1606509756.0,dataengineering,31yo no relevant education. Where do I start in order to land an entry-level job? (And is it doable?),k2995p,Jonathanplanet,,https://www.reddit.com/r/dataengineering/comments/k2995p/31yo_no_relevant_education_where_do_i_start_in/,1.0,15.0,0.0,21573.0,"I recently started a udemy course on mysql, hoping I might be able to get a job. But the more I delve into data the more lost I feel. And job requirements seem to want a complex skillset.. 

My current plan is to finish the mysq course, then take an etl course, a data visualization course and a data modeling or warehousing course. I barely understand what some of these mean, but they seem to go together?



Basically:

Is there a basic skillset I can learn on my own from udemy (or YouTube, books, or something else) in order to land an entry level job? 

Would trying to do the same for software development instead be any easier?

Any help or advice is deeply appreciated 🙂"
4277,2020-11-27 23:17:29,1606511849.0,dataengineering,Investigating Data Engineering,k29vj3,Deadgrunt12,,https://www.reddit.com/r/dataengineering/comments/k29vj3/investigating_data_engineering/,1.0,0.0,0.0,21574.0,"Hello, I am a Software Developer with roughly 7 years of full time experience. Nearly all of my professional experience has been with building web apps on either ASP.NET/ .NET Core MVC. I did taken an internship using Scala back in 2012/13 and I also took my intro courses in Python when I was at school for Computer Science and I took a refresher course a couple of month back, but that is about it. I don't have much experience with data engineering and I have been having trouble finding where I should start. I have searched on here, but most of the relevant posts are either for someone with some existing data engineering/science skills or have no experience at all.

I guess I am wondering where should one start if they have experience with programming and are comfortable with Python but don't have any experience with data engineering. I am not sure if this is a career path I want to go down, but I thought it would be worth a shot to check it out and see if it is something I would like. I am willing to spend some money on courses/books. I saw that DataCamp has a Black Friday sale and I didn't know if that would be a good place to start. I also saw Udacity has a sale, but I can't see myself forking out $400+ for a Data Engineering nanodegree I don't even know if I will even end up using. I am open to any resources/recommendations any of you may have. Thanks!"
4278,2020-11-27 23:31:43,1606512703.0,dataengineering,Question about the limitations of using AWS Lambda to extract XML data.,k2a4n6,sandhulk145,,https://www.reddit.com/r/dataengineering/comments/k2a4n6/question_about_the_limitations_of_using_aws/,1.0,9.0,0.0,21574.0,"My team was tasked with providing a proof-of-concept for an ETL Pipeline. I was tasked with the backend and migrating the local setup to AWS. 

So how the system is supposed to work is that every time a folder containing the required data is uploaded to the S3 bucket (the trigger), the Lambda function extracts the XML and goes to work on the database in Redshift. 

Now the job isn’t to simply move records from one place to another. We have to add new columns to the table as it is processing. We are literally building out the table as it’s going through. So all tags and sub tags of the XML have to be checked and if they don’t already exist, they are added to the table as varchar(255) columns. This is done recursively. Do not ask me why they wanted this built this way. My team knows it’s wildly inefficient and dumb, but that’s what we were assigned to do. 

Now our local setup (which was our local machines and a MySQL server) was able to process one file and the add all the tags as columns in about 25 minutes. This resulted in a 428-row table with a massive number of columns built from scratch. 

Move it over to AWS, it can’t get through one record in the 15 minute-maximum Lambda allows. I had to change some of the code to retrieve the file from the S3 buckets s convert the MySQL-specific code to Postgres-specific code. Functionally, the code is still the same. 

I know EC2 would’ve been the better tool for this kind of task (the company didn’t want to use it), but I didn’t expect it to run this badly in Lambda. My question is did my team overestimate the processing power of Lambda? Because I’m starting to think Lambda simply wasn’t built for something that intense."
4279,2020-11-28 04:55:25,1606532125.0,dataengineering,What can is the difference between a data engineer and data migration engineer?,k2fnau,emerican_coder,,https://www.reddit.com/r/dataengineering/comments/k2fnau/what_can_is_the_difference_between_a_data/,1.0,0.0,0.0,21583.0,"I have a technical interview coming up for a data migration engineer role but I'm having trouble on what to study up on. Any suggestions and resources you guys can recommend? Here's the job description of that helps

https://preview.redd.it/f1vwmmj8bw161.png?width=882&amp;format=png&amp;auto=webp&amp;s=cafeb27f96335f8611efa8c6023d2601a7d04f6e"
4280,2020-11-28 05:05:55,1606532755.0,dataengineering,difference between data engineer and data migration engineer?,k2fthj,emerican_coder,,https://www.reddit.com/r/dataengineering/comments/k2fthj/difference_between_data_engineer_and_data/,1.0,0.0,0.0,21585.0,"I have a technical interview coming up for a data migration engineer role but I'm having trouble figuring out what to study. Any suggestions and resources you guys can recommend? Here's the job description if that helps.

https://preview.redd.it/djpt6tz4dw161.png?width=882&amp;format=png&amp;auto=webp&amp;s=64681f7d3f0ce292d8974a14a4e65890766e0b66"
4281,2020-11-28 05:08:47,1606532927.0,dataengineering,Data Engineers who came from a mostly database administration role before; what route did you take?,k2fv59,TravellingBeard,,https://www.reddit.com/r/dataengineering/comments/k2fv59/data_engineers_who_came_from_a_mostly_database/,1.0,8.0,0.0,21585.0,"I'm having a hard time finding a way to set a training plan for myself.  I'm a SQL Server database admin of 10+ years.  I already have a head start on one of the technical requirements for data engineering, the SQL language itself.  Our company provides a generous training allowance, in addition to access on Pluralsight.

Currently I am learning technologies our Cloud team needs that touches SQL.  So Docker/Kubernetes, cloud technologies (Azure in our case).

I plan to get basic certifications in both.  But from there, I'm stuck on what next.  I have experience in transforming data with SSIS (SQL Server Integration Services), a pretty powerful tool, even today.  But I guess I'm paralyzed on choices on how to approach the pipelines I need to push and pull that data to and from where I need beyond basic powershell scripts.

For any former database admins (Oracle, SQL Server, etc), what plan did you put in place to get from there to your current data engineering role?"
4282,2020-11-28 08:12:27,1606543947.0,dataengineering,Please suggest the best Udemy courses for DE,k2imvu,vendetta33,,https://www.reddit.com/r/dataengineering/comments/k2imvu/please_suggest_the_best_udemy_courses_for_de/,1.0,5.0,0.0,21590.0,"Hi guys,

On the web, there are a lot of resources to learn Data Engineering skills. Actually there are far too many out there and confuses the hell out of a newcomer. Can you guys suggest the best Udemy courses which helped you acquire a new skill for Data Engineering?

I specifically mentioned Udemy as it's the most affordable option for many to learn a skill ($9.99 rocks). Please share your thoughts."
4283,2020-11-28 14:39:51,1606567191.0,dataengineering,Esophageal Cancer Death Rates Ranking | TOP 10 Country from 1990 to 2017,k2n072,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k2n072/esophageal_cancer_death_rates_ranking_top_10/,1.0,0.0,0.0,21592.0,
4284,2020-11-28 18:53:27,1606582407.0,dataengineering,Top 7 Best Programming Language for Data Science,k2r0j5,Techbiason,,https://www.reddit.com/r/dataengineering/comments/k2r0j5/top_7_best_programming_language_for_data_science/,1.0,0.0,0.0,21599.0,
4285,2020-11-28 19:22:57,1606584177.0,dataengineering,Data Warehousing for Business Intelligence Specialization on Coursera?,k2rkjp,strahan47,,https://www.reddit.com/r/dataengineering/comments/k2rkjp/data_warehousing_for_business_intelligence/,1.0,2.0,0.0,21600.0,"Has anyone done this course?

[https://www.coursera.org/specializations/data-warehousing](https://www.coursera.org/specializations/data-warehousing)

I'm coming to the end of the first course in the specialisation Database Management Essentials [https://www.coursera.org/learn/database-management?specialization=data-warehousing](https://www.coursera.org/learn/database-management?specialization=data-warehousing)

I've learned quite a bit, but tbh the course is a bit of a drag and I'm not sure if I'm going to continue with the remaining 4 coursers for a few reasons:  


* It's  badly structured. There are multiple modules in each week...e.g. they refer to module 6 and 7 in week 4. And assignment instructions are along the lines of ""to complete Module 7, check out module 5.""  As a result it's hard to navigate. 
* Multiple assignments per week...e.g. an assignment after each module in week 3 (module 4 and 5 in week 3). Just have one at the end. 
* Each assignment requires you to download multiple txt files to complete it. e.g. the questions are in one sheet, while a diagram you need to refer to is in another sheet, that's located in another section of the Coursera site. 
* Peer grading: you often have to save your work as a screenshot and a text file and submit both. You also have to have multiple windows open why you correct your peers' work

Looking at course 2 Data Warehouse Concepts, Design, and Data Integration, some students are complaining about having to install some software, and the assignment is not in line with the current version of it. 

I would absolutely hate to be coming to this course as a complete n00b - I know a lot of SQL - so I easily skimmed through that part of course 1. 

Overall, the course was handy for learning about database modelling and the remaining weeks (5 and 6) look a bit more concise. Otherwise the course is very verbose and long-winded, and could be structured much better. 

Anyone know of another course that deals with database design, ETL etc.?"
4286,2020-11-28 20:35:24,1606588524.0,dataengineering,Where does DE sit in your organization?,k2sxie,SardaarG,,https://www.reddit.com/r/dataengineering/comments/k2sxie/where_does_de_sit_in_your_organization/,1.0,3.0,0.0,21600.0,"We are a mid-size company and had a very ad-hoc way of managing our data and pipelines. I have been playing the data engineer role for marketing related data science projects. I know there are a couple of other folks like me working in Finance and operations as well. Now there are talks of making DE central for the organization under IT rolling up to CTO. I am currently reporting to the head of marketing strategy rolling up to the CMO and might be moving into the central DE team. Ours was a setup which was working fine. All this shifting made me curious about how DE is organized in other companies. 

What is your structure?"
4287,2020-11-28 21:12:29,1606590749.0,dataengineering,There are many DE jobs that are not dealing with the latest and greatest technologies,k2tm26,drecklia,,https://www.reddit.com/r/dataengineering/comments/k2tm26/there_are_many_de_jobs_that_are_not_dealing_with/,1.0,27.0,0.0,21602.0,"I just want to share with everyone that is looking to become DE and wondering what techs to learn, what cloud to use, etc., that there is more than Airflow, Spark, Redshift, Snowflake and big data in general.

Below is some quick and dirty survey I did of jobs available that include the following keywords. Note that this means there will be overlaps, for example a job might have Redshift and Spark but here I'm counting them individually. I'll be dividing this in what I would call old and new tech as well (why? because I've seen many comments in this sub saying that DE is relatively new, but DE has existed for a long time under different names such as Data Developer, BI Analyst, Analytics Developer, etc).

Reminder that this is incredibly subjective and not comprehensive at all, and all I want to say that there are more jobs than what usually gets asked in this sub. Also ""total"" is only for convenience and not for interpretation.

Old:

Name | Job count
:--|--:
informatica | 3,543
ssis | 3,333
ssas | 1,259
obiee | 608
oracle | 1,414
mssql | 1,213
powerbi | 3,311
tableau | 17,705
sas | 20,774
cognos | 2,604
essbase | 885
db2 | 2,403
total | 59,052

New:

Name | Job count
:--|--:
spark | 2,337
hadoop | 9,031
redshift | 4,264
bigquery | 1,331
snowflake | 2,811
looker | 1,840
airflow | 2,386
total | 24,000

It's very likely you'll get paid more if you know the ""new"" stack because you'll be in the crossroad of SWE and InfraOps, but if you find it hard to break into the field, there is nothing wrong with starting with the ""old"" stack."
4288,2020-11-28 22:32:34,1606595554.0,dataengineering,Understanding Window Functions,k2v2aw,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/k2v2aw/understanding_window_functions/,1.0,13.0,0.0,21603.0,"Window functions are one the most powerful features of SQL. They are extremely useful in analytics and for performing operations which cannot be done easily with standard group by, subquery and filters. 

If you have ever thought that window functions are confusing, then this post is for you.

[https://www.startdataengineering.com/post/6-concepts-to-clearly-understand-window-functions/](https://www.startdataengineering.com/post/6-concepts-to-clearly-understand-window-functions/)

Any comments or feedback is appreciated. Hope this provides value to someone."
4289,2020-11-28 23:43:15,1606599795.0,dataengineering,What kind of data warehousing architecture should I be expected to know when applying to jobs in 2020?,k2wc7v,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k2wc7v/what_kind_of_data_warehousing_architecture_should/,1.0,5.0,0.0,21605.0,"Obviously data warehousing architecture is entirely dependent on the company but I just honestly don't know what I should be expected to know for a data engineering job in 2020 because there are so many variations of data warehousing architectures:   


there are out-dated 'classic' ETL data warehousing architectures and also many variations of 'modern' ELT architectures... honestly wtf am i expected to know nowadays in terms of data warehousing architectures...  


Here is my current understanding of data warehousing architecture. Not sure if it's correct or modern or old. Need some insight:  
\&gt; data is coming from many different data sources (apis, files, etc)

\&gt; raw data from all sources land in a datalake (ie: s3 bucket)

\&gt; data is cleansed and sent to a staging datalake (ie: another s3 bucket) 

\&gt; data from staging datalake is sent to a data warehouse (ie: redshift, bigquery, presto, snowflake, etc)

\&gt; data from data warehouse can be used for whatever (ie: analytical purposes, reporting purposes, etc)"
4290,2020-11-29 00:35:33,1606602933.0,dataengineering,what’s your process for admitting something into your data warehouse?,k2x9d8,adappergentlefolk,,https://www.reddit.com/r/dataengineering/comments/k2x9d8/whats_your_process_for_admitting_something_into/,1.0,7.0,0.0,21609.0,"since letting everything into your data warehouse leads to data swamps, I think it would be a good idea to institute a formal process that would need to be done for anything that is set up to be ingested into the wh. I’m thinking for each table ingested each field has to be documented on a doc page with the business meaning and possible values, plus documenting the higher level business process it is part of. In order to do this would sometimes involve chasing uncooperative database owners but I believe this is worth it as otherwise we cannot build any trustworthy aggregations on top of the ingested data since we don’t know what it really means. What are you experiences implementing something like this in a typical business where the data warehouse is bolted onto a primary business with lots of operational dbs and teams?"
4291,2020-11-29 14:10:10,1606651810.0,dataengineering,Poultry Meat Consumption Ranking | TOP 10 Country/Region from 1961 to 2013,k38e8m,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k38e8m/poultry_meat_consumption_ranking_top_10/,1.0,0.0,0.0,21648.0,
4292,2020-11-29 15:59:00,1606658340.0,dataengineering,"Azure Databricks, how to learn to use practically?",k39snv,Gazpage,,https://www.reddit.com/r/dataengineering/comments/k39snv/azure_databricks_how_to_learn_to_use_practically/,1.0,4.0,0.0,21653.0,"Not sure whether better do ask this in an Azure or Spark subreddit, but I thought I might get responses appropriate to our use cases here.

We have Azure Databricks set up and working, and not had any problems following along the tutorials, but I don't feel they really let me know how to use in practice. I would appreciate any recommendations, but a couple of questions to give an example of the kind of thing I don't know.

When I close a cluster down, I get a warning that all data will be lost. However, the workspace seems to remember that the blob storage was mounted and if I rerun the full notebook I get an error at this step: Should I be separating my mounting steps into a different notebook? Or should I unmount at the end of a notebook? Is this mounted for the entire subscription, or just me as a user?

I have successfully written out last dataframes as a parquet in blob storage. Databricks splits it into many different files. How do I keep track of this? Do I need a different folder for each output, or do I rely on databricks to know this? Will this knowledge be retained if I close the cluster? Is there an easy way to view the contents of a folder from within databricks where it displays as a single file; i.e. Output.parquet rather than the 20+ files I see in the actual Blob container.

Any thoughts appreciated."
4293,2020-11-29 17:02:35,1606662155.0,dataengineering,How to Become a Data Analyst in 2021?,k3aqz6,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k3aqz6/how_to_become_a_data_analyst_in_2021/,1.0,0.0,0.0,21652.0,
4294,2020-11-29 17:04:09,1606662249.0,dataengineering,Artificial Intelligence vs Machine Learning vs Data Science,k3arug,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k3arug/artificial_intelligence_vs_machine_learning_vs/,1.0,0.0,0.0,21652.0,
4295,2020-11-29 17:05:39,1606662339.0,dataengineering,What do you need to become a Python developer in 2021?,k3asqu,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k3asqu/what_do_you_need_to_become_a_python_developer_in/,1.0,0.0,0.0,21652.0,
4296,2020-11-29 18:47:01,1606668421.0,dataengineering,GCP data engineer certification vs AWS data certification,k3cjk3,dna_o_O,,https://www.reddit.com/r/dataengineering/comments/k3cjk3/gcp_data_engineer_certification_vs_aws_data/,1.0,12.0,0.0,21654.0,"Hi,

I'm currently trying to move to the DE from software engineering.  In the meantime, I'm preparing for GCP certification but I see most of the Job posting only on AWS or Microsoft Data Azure.   


Can someone please let me know is it worth doing GCP certification or should I look into AWS or Microsoft Azure.

Thanks"
4297,2020-11-29 20:39:56,1606675196.0,dataengineering,reading a Spark Timestamp in Azure Datawarehouse / Synapse,k3en3m,Omar_88,,https://www.reddit.com/r/dataengineering/comments/k3en3m/reading_a_spark_timestamp_in_azure_datawarehouse/,1.0,0.0,0.0,21659.0,"Hello Friends,

I have an ETL pipeline where we push some data into parquet via Databricks. One of the meta columns I set is the extract datetime, which is the date the file lands in blob storage set by an on-prem script. 

Now, when reading the data into Synapse via Polybase I attempt to manually set the datatype as datetime but it's not able to read it in. After reading some blogs/posts it seems that MS SQL it seems reading any sort of timestamp is an issue. 

has anyone had any experience in reading timestamps into a SQL DW or DB? 

my work around was to change my column into a string then cast it back into a datetime column.

my issue with this is that I'm changing the structure of my datetime column in my datalake."
4298,2020-11-29 21:59:58,1606679998.0,dataengineering,"The 19th edition of the @data_weekly is out. The edition focus on Data Quality @Airbnb, Dynamic Data Testing, @Medium story on how counting is a hard problem, Opinionated view on AWS managed Airflow, Challenges in Deploying ML application.",k3g3ng,vananth22,,https://www.reddit.com/r/dataengineering/comments/k3g3ng/the_19th_edition_of_the_data_weekly_is_out_the/,1.0,0.0,0.0,21666.0,
4299,2020-11-29 22:30:05,1606681805.0,dataengineering,Hackerrank.com SQL certification,k3gne7,Mental-Hovercraft848,,https://www.reddit.com/r/dataengineering/comments/k3gne7/hackerrankcom_sql_certification/,1.0,1.0,0.0,21668.0,Hi! Is there SQL(basic) certification still at hackerrank.com? I cannot find it. I see  a lot of other types of certification and no one SQL. Thanks for reply.
4300,2020-11-30 01:42:44,1606693364.0,dataengineering,"Do faang DE interviews even ask about big data tools? Or is it just SQL, python and systems design?",k3k5qr,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k3k5qr/do_faang_de_interviews_even_ask_about_big_data/,1.0,12.0,0.0,21679.0,"My goal is to get a DE job at a FAANG (or similar tier) company and I’ve heard that DE interviews for FAANG cover mainly SQL, python and systems design. Do faang companies even test candidate’s knowledge on big data tools? (ie: spark, hive, Hadoop, etc). Also would you guys say that DE interviews for companies that are basically in the same tier as faang (well known startups: zoom, tiktok, Snapchat, Uber, etc) also test for primarily SQL, python and systems design?

Input appreciated. Thanks"
4301,2020-11-30 03:42:38,1606700558.0,dataengineering,How do I copy a Jupyter Notebook from my local machine to an EC2 instance?,k3m6d6,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/k3m6d6/how_do_i_copy_a_jupyter_notebook_from_my_local/,1.0,3.0,0.0,21685.0,"I tried exporting my notebook as a python file, copying the contents, and then pasting them into a file with an `ipynb` extension but I got and error when trying to execute it in Airflow: 

&gt;ERROR - Notebook does not appear to be JSON: '\n# In[ ]:\n\n\nimport numpy as np\nim

Does anyone know how I can copy the contents into a file on an EC2? 

I found the following https://unix.stackexchange.com/questions/106480/how-to-copy-files-from-one-machine-to-another-using-ssh but I'm not exactly sure how to use it."
4302,2020-11-30 13:21:02,1606735262.0,dataengineering,Stomach Cancer Death Rate Ranking | TOP 10 Country from 1990 to 2017,k3u09h,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k3u09h/stomach_cancer_death_rate_ranking_top_10_country/,1.0,0.0,0.0,21701.0,
4303,2020-11-30 13:28:19,1606735699.0,dataengineering,Real-Time Performance Monitoring with Flink SQL: AdTech Use Case,k3u4aa,Marksfik,,https://www.reddit.com/r/dataengineering/comments/k3u4aa/realtime_performance_monitoring_with_flink_sql/,1.0,0.0,0.0,21701.0,
4304,2020-11-30 13:30:12,1606735812.0,dataengineering,"Am I on the right track to building an ""open-source""-esque (cheap) Data Lake/ Data Warehouse on AWS?",k3u57o,OneOverNever,,https://www.reddit.com/r/dataengineering/comments/k3u57o/am_i_on_the_right_track_to_building_an/,1.0,12.0,0.0,21701.0,"I'm trying to avoid snowflake, bigquery, alteryx and all that sort.  Hoping someone has gone down this route, whether professionally or by hobby.

So far my idea is formulating around building a postgresql DB on a docker image and loading it from ECS.  I'll use airflow to build my ETLs and knime to provide data mart support.  However, I'm under the impression that I won't scale past 1TB aprox.

So two questions:

1) Am I on the right track?

2) Because of my scaleability issue, should I switch to hadoop? My end client is a live dashboard -- I'll be using batch, real-time, and near real-time info.  

Any tips, opinions, or suggestions help :)

Thanks!!"
4305,2020-11-30 13:40:07,1606736407.0,dataengineering,Transcribe Your Zoom Meetings For Free with AWS!,k3u9x6,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/k3u9x6/transcribe_your_zoom_meetings_for_free_with_aws/,1.0,0.0,0.0,21701.0,
4306,2020-11-30 16:19:36,1606745976.0,dataengineering,Hello! can anyone help me with my degree? only takes 2 minutes :),k3wlm8,theresaoctober,,https://www.reddit.com/r/dataengineering/comments/k3wlm8/hello_can_anyone_help_me_with_my_degree_only/,1.0,0.0,0.0,21708.0,"I am currently doing a statistics module in my Computer Science degree course and i'm a bit stuck. I need to collect data first hand and my usual methods are out the window (due to COVID we are only allowed to collect data online, so if you have 2 minutes to spare, would you mind filling out this questionnaire?:

[https://forms.gle/zDoLHD8GboL5djcG6](https://forms.gle/zDoLHD8GboL5djcG6)

It would mean a massive deal to me, and I appreciate your time! Thanks :)"
4307,2020-11-30 17:39:49,1606750789.0,dataengineering,"Updating existing SlackClient package in docker container, then getting 'no module named 'slackclient'",k3y4j5,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/k3y4j5/updating_existing_slackclient_package_in_docker/,1.0,1.0,0.0,21710.0,"Hello! New jr data engineer here!

We currently have slackclient==1.3.2 in our requirements.txt file but I need to change to version 2

 (I'm trying to get slack notifications working with the SlackApiPostOperator and seeing if this might help)

When testing locally I changed slackclient==2.9.3 in requirements.txt file, did the following docker commands:

 **docker-compose down**

**docker-compose build --no-cache**

**docker-compose run --rm webserver initdb**

**docker-compose up -d**

and now all my dags are failing saying `""No module named slackclient""` 

Am I missing a step?

https://preview.redd.it/y7nth8x9de261.png?width=1680&amp;format=png&amp;auto=webp&amp;s=ca8cd84a9c092ee9b12081d6e9c86ce96a25c871"
4308,2020-11-30 17:56:24,1606751784.0,dataengineering,What exactly does being proficient in SQL entail?,k3ygzc,Severe-Childhood1382,,https://www.reddit.com/r/dataengineering/comments/k3ygzc/what_exactly_does_being_proficient_in_sql_entail/,1.0,40.0,0.0,21711.0,"Hello there. Pretty sure this is kind of a noob question, sorry about that! I recently transitioned to a role that requires me to be more hands on with data from a role that was more management oriented. Since then I've been beefing up my technical capabilities further - a few months of python, some ETL and SQL.

I keep reading on here about proficiency in SQL, and considering how that in itself comes with a lot of other DB related skills - I was wondering when I can decide that I am in fact, proficient? What exactly does proficiency in SQL entail?

Thanks v much! :)"
4309,2020-11-30 18:01:45,1606752105.0,dataengineering,Fun Data Engineering Projects,k3yl60,scatterbrained_mugen,,https://www.reddit.com/r/dataengineering/comments/k3yl60/fun_data_engineering_projects/,1.0,11.0,0.0,21711.0,"I'm trying to come up with a data engineering project to tackle on my own for fun and to get more comfortable using existing tools - not for a portfolio or resume. 

I was wondering what you guys would *personally* consider as an engaging data engineering project to tackle if there was no extrinsic incentive besides for your own enjoyment and learning.

For those who are already working as data engineers, do you consider some of the projects that you work on as ""fun""? If so, what sort of projects?"
4310,2020-11-30 19:23:27,1606757007.0,dataengineering,Children living with AIDS (0-14),k40bbj,shanrap,,https://www.reddit.com/r/dataengineering/comments/k40bbj/children_living_with_aids_014/,1.0,0.0,0.0,21716.0,
4311,2020-11-30 21:25:10,1606764310.0,dataengineering,"Airflow - Saving file to /usr/local/airflow/repo showing /root/ instead, and getting Permission Error",k4331k,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/k4331k/airflow_saving_file_to_usrlocalairflowrepo/,1.0,0.0,0.0,21723.0,"Hello! New Jr. Data Engineer here with an airflow questions!

( Also for the record this worked last week I swear! )

So I'm doing an api call and saving the results to a .txt file. 

    request = requests.get('https://sandbox.boot.com/api/' + endpoint, headers=headers)

    file_path = open(os.environ[""HOME""]+f""/repo/airflow_dag_output/test.txt"", 'w')
file_path.write(request.text)

But this is the error I'm seeing in Airflow:

     File ""/usr/local/airflow/repo/plugins/boot_plugin/boot_hooks.py"", line 19, in execute_authorized_request     
    
    file_path = open(os.environ[""HOME""]+f""/repo/airflow_dag_output/test.txt"", 'w') 
    
    PermissionError: [Errno 13] Permission denied: '/root/repo/airflow_dag_output/test.txt'

Why is it showing **/root**/ and not **/usr/local/airflow**?  And how can I change so it writes, this was working find last week but now I'm completely at a loss"
4312,2020-11-30 23:17:27,1606771047.0,dataengineering,Anyone who attempted Cloudera DE575 certification?,k45h7v,redder_ph,,https://www.reddit.com/r/dataengineering/comments/k45h7v/anyone_who_attempted_cloudera_de575_certification/,1.0,3.0,0.0,21729.0,"Hoping to get input from anyone who has attempted DE575. I am interested in understanding the exam environment.  In-depth info about the exam is hard to find on the web. I have been able to piece the following info from cloudera community, about the environment:

* OS is centos (This was from a community reply in 2015, so not sure if this is still the case)
*  The exam is on a cloud hosted, multi-node cluster. 
*  Each user is given their own CDH cluster 

My question:

1. How do you access the multi-node cluster? Are you provided with ssh credentials to ssh into the terminal?
2. For any db access are you also provided with all the db connection strings before you begin?
3. Where are the questions displayed? Do they need to be accessed in a browser or are they provided as a document, pdf?"
4313,2020-12-01 02:43:24,1606783404.0,dataengineering,What are your most utilized libraries/modules?,k49mce,DataD23,,https://www.reddit.com/r/dataengineering/comments/k49mce/what_are_your_most_utilized_librariesmodules/,1.0,3.0,0.0,21736.0,"I thought it would be cool to have somewhat of a curated list of the most utilized libraries/modules everyone uses to get their ""Data Engineering"" work done. Some of the most common ones that come to mind for me are:

requests

urllib3

datetime

json

csv

sys

pandas

numpy

Does anyone have anything else to add to the list?"
4314,2020-12-01 03:44:49,1606787089.0,dataengineering,What is needed to be a full stack developer in 2021,k4ar23,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k4ar23/what_is_needed_to_be_a_full_stack_developer_in/,1.0,0.0,0.0,21738.0,
4315,2020-12-01 03:45:29,1606787129.0,dataengineering,Artificial Intelligence vs Machine Learning vs Data Science,k4arh7,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k4arh7/artificial_intelligence_vs_machine_learning_vs/,1.0,0.0,0.0,21738.0,
4316,2020-12-01 06:10:34,1606795834.0,dataengineering,Quick question on Airflow installation,k4dbok,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/k4dbok/quick_question_on_airflow_installation/,1.0,6.0,0.0,21743.0,"In the installation docs https://airflow.apache.org/docs/stable/start.html it says to do the following: 

    # airflow needs a home, ~/airflow is the default,
    # but you can lay foundation somewhere else if you prefer
    # (optional)
    export AIRFLOW_HOME=~/airflow

    # install from pypi using pip
    pip install apache-airflow

    # initialize the database
    airflow initdb

    # start the web server, default port is 8080
    airflow webserver -p 8080

    # start the scheduler
    airflow scheduler

    # visit localhost:8080 in the browser and enable the example dag in the home page

Above, when they `export AIRLFLOW_HOME=~/airflow`, do they assume that the directory that you created to house Airflow is called `airflow`? If I housed Airflow in `/Users/me/Airflow` would I then use `export AIRLFLOW_HOME=~/Airflow`?"
4317,2020-12-01 08:56:48,1606805808.0,dataengineering,"Implementation process, Usage Scenarios: Data Lake vs Data Warehouse",k4ftkr,brrdprrsn,,https://www.reddit.com/r/dataengineering/comments/k4ftkr/implementation_process_usage_scenarios_data_lake/,1.0,3.0,0.0,21746.0,"Hi,

I would very much value any inputs you could provide on one/more of the below topics.

Thank you in advance for any help you can provide...

1. Any recommended reading on the broad stages of implementation of Data Lakes vs Data Warehouses? Which of the 2 types of projects are typically larger, costlier and/or more time consuming (esp. for the number of data sources that are typically seen in mid, or large enterprises)? 
2. How time consuming is the Dimensional Modeling process (and validation) as a % of the overall time / effort? What are the common difficulties encountered during this process of creating a Data Warehouse model?
3. Are Warehouses typically implemented only once you have implemented a Data Lake? 
4. Is it common to BI / Reporting built directly off Data Lakes?"
4318,2020-12-01 12:14:00,1606817640.0,dataengineering,A Data Prediction For 2021,k4i9kh,soobrosa,,https://www.reddit.com/r/dataengineering/comments/k4i9kh/a_data_prediction_for_2021/,1.0,2.0,0.0,21753.0,"The data suggests that data engineering is here to stay, and having observed how data science has evolved as a profession, and especially how first-movers gained an almost unfair advantage compared to whoever tries to get into the game today... it is safe to say that the right time to start data engineering is yesterday.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/data-trend-report-prediction-for-2021](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/data-trend-report-prediction-for-2021)"
4319,2020-12-01 14:01:09,1606824069.0,dataengineering,Barley Consumption Ranking | TOP 10 Country from 1961 to 2013,k4jklu,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k4jklu/barley_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21756.0,
4320,2020-12-01 15:12:49,1606828369.0,dataengineering,An interview about how the Equalum platform is architected to provide streaming data integration workflows with a no-code interface.,k4kkiq,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/k4kkiq/an_interview_about_how_the_equalum_platform_is/,1.0,0.0,0.0,21758.0,
4321,2020-12-01 17:27:46,1606836466.0,dataengineering,Recommendations for dataframe validation/rules engine library?,k4myqh,Evilcanary,,https://www.reddit.com/r/dataengineering/comments/k4myqh/recommendations_for_dataframe_validationrules/,1.0,9.0,0.0,21768.0,"As our data moves through our pipeline, I'd like to be able to apply certain rules and assign warnings/errors to it at different stages.

Data is primarily manipulated in dataframes and the dataframes represent a file from a customer. 

Ideally I can start with just storing rules that apply to high level scopes (scopes being things like file level, customer level, user level, row level).

I'd like to be able to define rules like 

\-""If the sum of the column is less than this, then add a flag to the file level scope.""

\-""If the column is outside of this threshold, add a flag to the row level scope""

So I'd end up with a secondary dataframe that looks like that has a many-to-many relationship (1 scope/identifier can have many different flags / warnings with it). Something like

ScopeLevel, Identifier, File,Flag

Row,123,FileA,Column8 is null 

Row,123,FileA,Column6 is outside of boundary

File,FileA,FileA,Sum of total is less than 500

This is all a bit pseudo-codey, but just to get the idea I'm looking at across. I can visualize how I can write this and how to store the rules and join the rules onto the dataframe.  But I can also see how this could become a bit hard to maintain.  So with that said, are there any libraries that could do something similar without reinventing the wheel?  I've seen CLIPSPY, venmo/business\_rules, durable\_rules, pyke, and something databricks was messing with called dataframe-rules-engine, but I'm not sure any of them fit my use case.  

Ideally it would easily work with dataframes and vectorized operations since each dataframe will have millions of rows.

 Any thoughts or suggestions before I start building this out?"
4322,2020-12-01 17:38:18,1606837098.0,dataengineering,Data Mesh Applied: How to Move Beyond the Data Lake with lakeFS,k4n6as,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/k4n6as/data_mesh_applied_how_to_move_beyond_the_data/,1.0,0.0,0.0,21768.0,
4323,2020-12-01 18:41:45,1606840905.0,dataengineering,Any advice for an anxious college student?,k4ohue,busynoodle,,https://www.reddit.com/r/dataengineering/comments/k4ohue/any_advice_for_an_anxious_college_student/,1.0,9.0,0.0,21773.0,"So I'd like to preface this by saying that I only really got into programming in my second year of college after I shifted my degree course from economics to cs. Hence, I'm not as smart or experienced as my classmates with regard to programming  Right now, I'm a fourth year graduating student. And I still honestly don't know what tech-related career I want to pursue. I've dabbled and still continue to experiment the different facets of the industry like web dev and data science. But I don't know if it's for me. However recently, I was able to get the opportunity to be a data engineer intern for 2 months for a live-streaming platform and I really enjoyed the job. I enjoyed figuring out how each pipeline worked. Not to mention, I find learning cloud platforms like AWS and GCP to be interesting. So I decided to maybe try focusing my energy on data engineering since I think I like it. 

I would just like to ask you guys if you have any advice for an anxious learner like me who's still finding his way in the industry. I've been looking for books and roadmaps to follow but with so much information available it's hard to filter everything out and know where to start."
4324,2020-12-01 20:16:12,1606846572.0,dataengineering,Python For Machine Learning and Data Science,k4qiub,poonddetatte,,https://www.reddit.com/r/dataengineering/comments/k4qiub/python_for_machine_learning_and_data_science/,1.0,0.0,0.0,21780.0,
4325,2020-12-01 23:19:24,1606857564.0,dataengineering,PySpark: I think my GroupBy is applying the function on the entire column instead of unique subgroups,k4udr5,usernaame001,,https://www.reddit.com/r/dataengineering/comments/k4udr5/pyspark_i_think_my_groupby_is_applying_the/,1.0,1.0,0.0,21781.0,"I have a dataframe with columns 'asin' and 'price'. Here are the first 20 rows:

    +----------+-----+ |asin      |price| +----------+-----+ |B006I74FRS|27.99| |B006I74FRS|null | |B006I74FRS|41.95| |B006I74FRS|null | |B006I74FRS|24.99| |B006I74FRS|null | |B006I74FRS|58.0 | |B006I74FRS|null | |B006I74FRS|null | |B006I74FRS|5.49 | |B006I74FRS|null | |B006I74FRS|30.0 | |B006I74FRS|null | |B006I74FRS|15.99| |B006I74FRS|null | |B006I74FRS|null | |B006I74FRS|null | |B006I74FRS|3.99 | |B006I74FRS|19.99| |B006I74FRS|9.55 | +----------+-----+ 

What I am trying to do is for every product ID (under the column called 'asin'), find the count of rows/prices that are associated with that product ID. So I did

    df.groupby('asin').count() 

but then the output came out as

    +----------+-------+ |asin      |count  | +----------+-------+ |B007ATO2Z8|1000000| |B005BFKR2A|1000000| |B006I74FRS|1000000| |B00FQITYO4|1000000| |B00B105J0E|1000000| |B00F9RNZ24|1000000| |B006IIZOZO|1000000| |B007ECGY02|1000000| |B00GMRMCBA|1000000| +----------+-------+ 

I think the groupby is being applied to the entire price columns as opposed to on each unique asin subgroup. I think this because when I do .summary().collect() on the resulting dataframe, the mean is 1000000 and the standard deviation is 0. Also, when I do collect\_list() instead of count during the groupby, every list has the same length. How can I tackle this situation?"
4326,2020-12-01 23:42:25,1606858945.0,dataengineering,Analytic sandbox on aws,k4uups,iffexibility,,https://www.reddit.com/r/dataengineering/comments/k4uups/analytic_sandbox_on_aws/,1.0,0.0,0.0,21782.0,"Hi everyone, I need help setting up an analytic sandbox.

I want to create an analytic sandbox i.e different environments on aws; a development environment, test environment and deployment environments.

In these environments, I want to be able to access the data-lake and make necessary data manipulate and data preparation. 

Please any suggestions on how I can implement this on AWS"
4327,2020-12-02 01:44:51,1606866291.0,dataengineering,"May be a dumb question but is: snowflake, hive, bigquery, redshift, terradata considered as datalakes or data warehouses?",k4x8mv,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k4x8mv/may_be_a_dumb_question_but_is_snowflake_hive/,1.0,24.0,0.0,21788.0,"Maybe I’m thinking too much into this but I’ve noticed that some people consider bigquery a DWH while others consider it a datalake. This got me in my head and thinking about what I considered what for all the other platforms. 

I consider the following as DWH: 

- snowflake
- presto
- Hadoop/hive
- redshift
- bigquery
- terradata


And the following as datalake:

- aws s3
- Google cloud storage 
- azure blob storage

I’m confusing myself and overthinking. I feel like I don’t know anything anymore"
4328,2020-12-02 02:27:59,1606868879.0,dataengineering,anyone with experience interviewing data engineers for faang (or of faang tier) want to help me by providing me a mock data engineering interview?,k4y0j2,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k4y0j2/anyone_with_experience_interviewing_data/,1.0,4.0,0.0,21787.0,"I have 2 years experience as a data engineer (not in tech) looking for a middle level position. My dream is to be a data engineer at a faang or similar tier company. The company work for is a very large but because it is not in tech, they are no where near as rigorous with the interview process in terms of sql and data structures/algos and systems design. I was wondering if anyone (preferably with experience interviewing data engineers for faang) would be kind enough to provide me a mock interview. I’ve heard of the coding interview site that pairs you up and sets up a mock interview. I’m not looking for that. I’m specifically looking to practice for a data engineering interview with someone that has experience interviewing candidates for this position at a faang tier company. This may be asking for to much but it’s worth a shot. Thanks."
4329,2020-12-02 02:31:35,1606869095.0,dataengineering,AWS Managed Airflow Setup issue,k4y2ur,OkieDaddy,,https://www.reddit.com/r/dataengineering/comments/k4y2ur/aws_managed_airflow_setup_issue/,1.0,3.0,0.0,21787.0,"Have any of you guys been successful yet in setting up MWAA? I'm admin in my AWS environment, yet I keep getting a vague/useless error when trying to do the setup. It will sit there at creating for a couple of hours, then just bomb out. Nothing written to cloudwatch that I can tell. Just curious if it's affecting anyone else."
4330,2020-12-02 07:35:02,1606887302.0,dataengineering,AWS S3 Strong Consistency,k538q5,Bazencourt,,https://www.reddit.com/r/dataengineering/comments/k538q5/aws_s3_strong_consistency/,1.0,9.0,0.0,21800.0,Amazon announced today during reInvent that [S3 now has strong consistency](https://aws.amazon.com/s3/consistency/) (used to be eventual consistency) and there isn't any change in price. Pretty strong flex on their part.
4331,2020-12-02 07:58:58,1606888738.0,dataengineering,I need a job :),k53l4n,blcksrx,,https://www.reddit.com/r/dataengineering/comments/k53l4n/i_need_a_job/,1.0,3.0,0.0,21800.0,I need a job with a Visa sponseship :))
4332,2020-12-02 08:11:18,1606889478.0,dataengineering,Glue Elastic Data Views,k53rh2,Bazencourt,,https://www.reddit.com/r/dataengineering/comments/k53rh2/glue_elastic_data_views/,1.0,0.0,0.0,21800.0,"[New functionality in AWS Glue](https://aws.amazon.com/glue/features/elastic-views/) allows you to create materialized views between various AWS data sources and have the views automatically refreshed when the data updates. For example, you can use it to create a materialized view in Redshift from a datasource in ElasticSearch. Coming soon will be support for RDS and Aurora data sources. Stupid simple, but very powerful."
4333,2020-12-02 11:58:22,1606903102.0,dataengineering,Microsoft Create: Data Online event,k56ist,AdiPolak,,https://www.reddit.com/r/dataengineering/comments/k56ist/microsoft_create_data_online_event/,1.0,3.0,0.0,21803.0,"Hi,

I'm helping to organize [Microsoft Create: Data](https://aka.ms/createdata). During the event, we have [Holden Karau](https://twitter.com/holdenkarau) joining for an interview with [Cheryl Adams](https://www.linkedin.com/in/cheryl-adams-cloudarchitect/), Holden will cover her journey with Spark and will share insights and best practices on how to join an open-source project related to distributed data.

\[Jacek Laskowski\]([https://twitter.com/jaceklaskowski?ref\_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor](https://twitter.com/jaceklaskowski?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)) will share how you can use Delta Lake from Spark SQL commands.  

&amp;#x200B;

\[Tim Berglund\]([https://twitter.com/tlberglund?ref\_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor](https://twitter.com/tlberglund?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)) will talk about picking the right distributed database and how to navigate NoSql tools to find the right one for the tasks.

&amp;#x200B;

We will also have a workshop on Azure Synapse Serverless SQL with Simon Reddy, Panel about data for good, and more.

&amp;#x200B;

Would love to know if you have any questions for our speakers, happy to take all your questions with them so they can address it live during the event.

&amp;#x200B;

You are all welcome to join us, this is an online free event, but requires registration.

&amp;#x200B;

Thank you 🙌"
4334,2020-12-02 14:10:46,1606911046.0,dataengineering,Liver Cancer Death Rate Ranking | TOP 10 Country from 1990 to 2017,k583pz,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k583pz/liver_cancer_death_rate_ranking_top_10_country/,1.0,1.0,0.0,21802.0,
4335,2020-12-02 14:50:53,1606913453.0,dataengineering,#idataengineer podcast 003 Sejal Vaidya,k58n2u,soobrosa,,https://www.reddit.com/r/dataengineering/comments/k58n2u/idataengineer_podcast_003_sejal_vaidya/,1.0,0.0,0.0,21803.0,"In the third part of our micro-podcast, [Sejal Vaidya](https://github.com/sejalv) speaks about how data needs to serve business, and that deployment is king.  


[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-003](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-003)"
4336,2020-12-02 16:13:42,1606918422.0,dataengineering,Why Data Scientist Love to do Data Science in Python?,k59xug,Techbiason,,https://www.reddit.com/r/dataengineering/comments/k59xug/why_data_scientist_love_to_do_data_science_in/,1.0,0.0,0.0,21806.0,
4337,2020-12-02 16:47:47,1606920467.0,dataengineering,A contest to win free dev accounts of our ClickHouse-based SaaS product,k5aj0p,xlpz,,https://www.reddit.com/r/dataengineering/comments/k5aj0p/a_contest_to_win_free_dev_accounts_of_our/,1.0,2.0,0.0,21809.0,
4338,2020-12-02 17:13:52,1606922032.0,dataengineering,Monte Carlo launches Data Observability Platform to prevent broken data pipelines,k5b078,mkvor8,,https://www.reddit.com/r/dataengineering/comments/k5b078/monte_carlo_launches_data_observability_platform/,1.0,0.0,0.0,21810.0,
4339,2020-12-02 18:05:15,1606925115.0,dataengineering,Data Science in Python,k5c0am,Techbiason,,https://www.reddit.com/r/dataengineering/comments/k5c0am/data_science_in_python/,1.0,0.0,0.0,21810.0,
4340,2020-12-02 18:44:34,1606927474.0,dataengineering,Is Hadoop/Hive/HDFS considered a data warehouse or data lake?,k5ctt1,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k5ctt1/is_hadoophivehdfs_considered_a_data_warehouse_or/,1.0,1.0,0.0,21814.0,"I'm wondering if my labelling in parenthesis is considered correct below. If an interviewer asks me to explain my company's data ingestion process and I explain to them the following, would you say that i know what I'm talking about or not at all.:  


raw data sources (transactional data) &gt; operational Informix database (operational db) &gt; Informix sends raw csv data files every day to on-prem server (datalake) &gt; python script runs batch spark load job to loads csv files by cleansing/mapping the data to temporary staging tables in Hive (staging area is the temp daily hive tables) &gt; finally spark batch load jobs inserts from daily staging hive tables to historical hive tables (these historical hive tables we use as our data warehouse).   


Can you use Hive tables for both the staging process and also as the data warehouse?"
4341,2020-12-02 19:18:24,1606929504.0,dataengineering,How To Create Differentially Private Synthetic Data,k5djs0,alig80,,https://www.reddit.com/r/dataengineering/comments/k5djs0/how_to_create_differentially_private_synthetic/,1.0,0.0,0.0,21814.0,"A practical guide to creating differentially private, synthetic data with Python and TensorFlow [https://gretel.ai/blog/how-to-create-differentially-private-synthetic-data](https://gretel.ai/blog/how-to-create-differentially-private-synthetic-data)"
4342,2020-12-02 20:59:25,1606935565.0,dataengineering,"Video: How to get started with our free, hosted, cross platform Spark UI &amp; Spark History Server. This is but our first milestone towards replacing the Spark UI entirely with new metrics and visualizations. I'd love your feedback :) JY @Data Mechanics",k5fq97,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/k5fq97/video_how_to_get_started_with_our_free_hosted/,1.0,0.0,0.0,21821.0,
4343,2020-12-02 22:48:47,1606942127.0,dataengineering,How do you prevent broken data pipelines?,k5i1vf,mkvor8,,https://www.reddit.com/r/dataengineering/comments/k5i1vf/how_do_you_prevent_broken_data_pipelines/,1.0,0.0,0.0,21825.0,"Any best practices? 

[https://towardsdatascience.com/how-do-you-prevent-broken-data-pipelines-326f3c6d239e](https://towardsdatascience.com/how-do-you-prevent-broken-data-pipelines-326f3c6d239e)"
4344,2020-12-03 01:28:49,1606951729.0,dataengineering,Legacy db migration to a new system,k5l7af,plodzik,,https://www.reddit.com/r/dataengineering/comments/k5l7af/legacy_db_migration_to_a_new_system/,1.0,2.0,0.0,21827.0,"Hey, any suggestions for a good read on legacy system database migration to a new system? The common pitfalls, keeping integrity constraints etc? Thanks!"
4345,2020-12-03 02:09:16,1606954156.0,dataengineering,Some questions about Lyft's Amundsen as a data portal (user restrictions and flatfile cataloging),k5lyby,Anxious_Reporter,,https://www.reddit.com/r/dataengineering/comments/k5lyby/some_questions_about_lyfts_amundsen_as_a_data/,1.0,1.0,0.0,21827.0,"Anyone with any experience using Amundsen know about the following?...

1. Is there a way to **restrict certain datasets from being discoverable by the general base of users** in Amundsen? In my case, we have siloed teams that have certain data assets that only they should be able to search / know about while they do have some other assets that they do want to have discoverable by other teams/users in the org.
2. Can Amundsen catalog data that only exists as **loose files on FTP and other storage servers** (I noticed ""CSV"" in the integrations page, but not sure how that works or if other flat file sources can be hooked in)?

I've seen the presentation here ([https://youtu.be/m1B-ptm0Rrw?t=734](https://youtu.be/m1B-ptm0Rrw?t=734)) and looked around the Amundsen site ([https://www.amundsen.io/amundsen/](https://www.amundsen.io/amundsen/)). But was not able to understand if the above could be done.

The use case here is that we want to have Amundsen act as a general data portal for our org. In our case, we have various research teams that have their own set of data assets (which we as admins want to be able to search / discover), yet we don't want / can't have these various teams being able to discover ALL of the data assets of everyone else (only certain data that those teams deem ""public""). This is similar to ""public / private"" data works for Organizations in CKAN ([https://ckan.org/](https://ckan.org/)), but interested in trying Amundsen.   

Thanks"
4346,2020-12-03 09:34:17,1606980857.0,dataengineering,Work Experience in Job Descriptions,k5styq,phmark19,,https://www.reddit.com/r/dataengineering/comments/k5styq/work_experience_in_job_descriptions/,1.0,0.0,0.0,21832.0,"Hello everyone,

If a job post does not include the amount of experience required, does it mean that they are willing to entertain anyone that can do the job, given that they satisfy technical and the rest of the requirements in the JD?

I'm thinking if I should include those kind of Jobs^ in my list when looking for a DE position. I'm confident with my CS fundamentals, and is planning to build an end to end DE project before reaching out recruiters. And maybe if the question above is true, then I will pick a DE Stack among those JD that I can focus on."
4347,2020-12-03 10:32:45,1606984365.0,dataengineering,Choosing best storage strategy &amp; file format on S3 for simple reporting,k5ti0y,lam_bd85,,https://www.reddit.com/r/dataengineering/comments/k5ti0y/choosing_best_storage_strategy_file_format_on_s3/,1.0,3.0,0.0,21833.0,"I'm designing a simple reporting solution for my web app and I want to use S3, Athena and Quicksight for that purpose. My web app is emitting events about orders placed on my website to Kinesis and I'll be storing them in S3.

Now I have different storage strategies to choose from:

1. 1 event, 1 file
2. Many events, 1 file - but then how should I partition them? Per hour? Per day? Per week?

What would be the best file format (parquet? other alternatives?) and storage strategy given my app is emitting:

* a) few events, say 1000 per 24h
* b) mediocre amount of events, say 10 000 per 24h
* c) a lot of events, say 10 000 000 per 24h

I'm also curious what would be the pricing x performance considerations in that case."
4348,2020-12-03 14:18:02,1606997882.0,dataengineering,Is there an organized catalogue for all the steps in a data pipeline that shows the tools necessary (in each step) to have an end-to-end data engine?,k5w5z3,OneOverNever,,https://www.reddit.com/r/dataengineering/comments/k5w5z3/is_there_an_organized_catalogue_for_all_the_steps/,1.0,3.0,0.0,21835.0,"The question is asking for everything, but anything would help :)"
4349,2020-12-03 14:49:10,1606999750.0,dataengineering,Cream Consumption Ranking | TOP 10 Country from 1961 to 2013,k5wljt,Enough-Barracuda-344,,https://www.reddit.com/r/dataengineering/comments/k5wljt/cream_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21835.0,
4350,2020-12-03 16:15:27,1607004927.0,dataengineering,Cream Consumption Ranking | TOP 10 Country from 1961 to 2013,k5xy9v,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k5xy9v/cream_consumption_ranking_top_10_country_from/,1.0,0.0,0.0,21839.0,
4351,2020-12-03 18:04:43,1607011483.0,dataengineering,How do you manage the CREATE statements of your data warehouse tables?,k5zz4q,L3GOLAS234,,https://www.reddit.com/r/dataengineering/comments/k5zz4q/how_do_you_manage_the_create_statements_of_your/,1.0,3.0,0.0,21840.0,"Hello. Every time we have to make a new integration, we first create manually the new table. After that, we use to store that query in a GitLab repo, so that if we need it again, we don't have to extract it using SQL queries. 

We are now moving from this approach to using the tool YoYo Migrations, which allows us to keep a record of what has been done and some other advantages.

However, I was wondering if there is a better approach to manage this thing. In case it is useful, our stack is Airflow, Snowflake and Qlick.

Thanks"
4352,2020-12-04 11:45:04,1607075104.0,dataengineering,"Machine Learning Model Serving Overview (Seldon Core, KFServing, BentoML, MLFlow)",k6hyla,usefulcalamity,,https://www.reddit.com/r/dataengineering/comments/k6hyla/machine_learning_model_serving_overview_seldon/,1.0,6.0,0.0,21869.0,"Hi Everyone,

TLDR; I’m looking for a way to provide Data Scientists with tools to deploy a growing number of models independently, with minimal Engineering and DevOps efforts for each deployment. After considering several model serving solutions, I found Seldon Core to be the most suitable for this project’s needs

Full Overview:

[https://medium.com/everything-full-stack/machine-learning-model-serving-overview-c01a6aa3e823?source=friends\_link&amp;sk=a1a2e9d27c62913e0218d3a8bc252896](https://medium.com/everything-full-stack/machine-learning-model-serving-overview-c01a6aa3e823?source=friends_link&amp;sk=a1a2e9d27c62913e0218d3a8bc252896)"
4353,2020-12-04 13:57:15,1607083035.0,dataengineering,"Surprisingly Made Final Round Entry Level DE Interview, Need Help",k6jkyj,ksuszko,,https://www.reddit.com/r/dataengineering/comments/k6jkyj/surprisingly_made_final_round_entry_level_de/,1.0,1.0,0.0,21872.0,"A few weeks ago, I had an interview for a technology associate program, being considered for a data analyst and data engineering role. My passion is data science and I have a lot of deep learning project experience from this past year, but with only my Master's being completed and the inability to obtain a PhD due to financial restraints, I am trying to find a way to break into a data-related job from my current job in the traditional Mechanical Engineering industry (undergrad in Mechanical Engineering, masters in systems engineering and data analytics).

&amp;#x200B;

The initial interview I had went fairly poor, with me struggling to answer some of the technical questions (I've never taken a formal Data Structures and Algorithms course, I am mainly self taught in my coding abilities) and the manager interviewing me seeming to be confused that I was trying to switch from Mechanical Engineering to a data type job, but to my shock, I moved to the final round for the data engineering position. The only part of the interview that went well is when I spoke to a current data scientist at the company who is in the program I am applying for (this year the program is not interviewing for a data science position), and she was really impressed by my deep learning projects and publications.

&amp;#x200B;

My current strengths:

1. Python - my strongest coding language and I can explain coding solutions using it fairly well
2. SQL - I am fairly decent at it and have never failed a hackerrank portion containing it and can answer most questions
3. Machine Learning, Neural Networks, Natural Language Processing specialty (what the company I am applying for uses mostly for its deep learning applications)

&amp;#x200B;

Does anybody have any advice on what things I should try to cram study this weekend to have a shot at landing the position? What are the most things to know for an entry level DE position? I am not sure if it is better to focus on data structures and algorithms (ironically I have used many in project experience without knowing what I was doing was called) or to try to study about the types of databases and tools generally used.

&amp;#x200B;

Thanks!"
4354,2020-12-04 14:10:56,1607083856.0,dataengineering,Larynx cancer Death Rate Ranking | TOP 10 Country from 1990 to 2017,k6jrja,IndividualReveal8238,,https://www.reddit.com/r/dataengineering/comments/k6jrja/larynx_cancer_death_rate_ranking_top_10_country/,1.0,0.0,0.0,21872.0,
4355,2020-12-04 15:00:40,1607086840.0,dataengineering,Where do you run your Hadoop cluster on,k6kgak,powok,,https://www.reddit.com/r/dataengineering/comments/k6kgak/where_do_you_run_your_hadoop_cluster_on/,1.0,2.0,0.0,21873.0,"And where do you think the market will learn towards in the future

[View Poll](https://www.reddit.com/poll/k6kgak)"
4356,2020-12-04 16:50:50,1607093450.0,dataengineering,Need help understanding a Data Engineering Project Requirement,k6m9o2,Uttasarga,,https://www.reddit.com/r/dataengineering/comments/k6m9o2/need_help_understanding_a_data_engineering/,1.0,11.0,0.0,21876.0,"Hey Everyone,  


Recently, I have received a Coding activity from a Firm which requires me to scan through a Json file for various Data Sources related to COVID-19 Data. After the Scan, I have to make sure that the Data is segregated according to each states; and the Data should be transferred into PostGRE SQL Data base.  


I have to schedule a job for this data transfer pipeline using Apache Airflow. This is the understanding I had from the requirement and if someone can help me if I missed any thing from this. Please find the  additional details below:

**Problem Statement:**

*Imagine you are part of a data team which wants to bring in daily data for*

*COVID-19 for all the states. Your team has to design a daily workflow that would*

*run at a specific time and would bring in all the data into the system.*

**Website:** [**https://healthdata.gov/data.json**](https://healthdata.gov/data.json)

**Data Schema**

*1. The link contains json data for various data sources. You would have to scan*

*through and filter any COVID related data*

*2. Design a schema for every state and store data in the respective tables per*

*state*

*3. Apply various indexing technique on PostGres to enable fast searching*

*While developing a solution think about the following :*

*1. DAG performance and efficiency*

*2. Concepts of Distributed Computing*

*3. Your choice of schema design 1 NF, 2 NF, 3NF*

*4. Utilize any async process while performing any loads*

*5. How would you scale DAG with increase in data volume*

*6. Logging and monitoring if any failure happens*

*7. Object oriented design*

**Requirements:**

*1. Use Python 3 for developing the solution*

*2. Utilize Apache Airflow to design a daily dag that would run every day.*

*3. Create a task within the dag to iterate through the json and download the*

*locally.*

*4. Create task to load the files into PostGres Schema*

*5. Optimize your dag performance by achieving max parallelism locally. You*

*could utilize parallelism for task, dag concurrency, thread pool or*

*max\_threads*

*6. Follow the ETL process of Extract, Transform and Load*

*7. Each dag task should be independent and should be able to run individually.*

*8. Implement unit or integration test*

*9. Containerize your application inside a docker container. Use docker-compose*

*if required*

**Bonus points:**

*You could create any visualization on top if your data stored in your database*

*Visualization notebook using any standard Python library such as matplotlib, seaborn,*

*etc.*

*Example:*

*While querying*

[*https://healthdata.gov/search/type/dataset?query=covid+state+of+maryland&amp;sort\_by=ch*](https://healthdata.gov/search/type/dataset?query=covid+state+of+maryland&amp;sort_by=ch)

*anged&amp;sort\_order=DESC candidates can find a number of COVID resources for the*

*state of Maryland. Similar searches on other states could lead to finding right resources*

*per state that you will use for data ingestion.*"
4357,2020-12-04 19:38:41,1607103521.0,dataengineering,TerminusDB 4.0 - Versioned Data and Content Management in a Box,k6pjmx,EverythingIsNail,,https://www.reddit.com/r/dataengineering/comments/k6pjmx/terminusdb_40_versioned_data_and_content/,1.0,1.0,0.0,21878.0,
4358,2020-12-04 20:39:55,1607107195.0,dataengineering,Data is overwhelmed,k6qqrg,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/k6qqrg/data_is_overwhelmed/,1.0,6.0,0.0,21881.0,"Internet is now filled with all types of data and If at all I wanted to learn Data Engineering. It is really confusing me how to excel in this field. I'm stuck with a regular day job along with trying to pursue the right path, rather than learning some random youtube videos, courses, etc.. Is there anyone like me trying to learn everything but learning nothing. 2020 is gone like this. I learned nothing. At least 2-year rock-solid plan I need to make a good switch for a high-paying job. I don't have any urgency for any job change.

I don't even know - version control, Jenkins, SQL(very basic only), not even wrote single line of code, etc.. so I feel like surviving is getting a tougher daily basis.

my daily job goes like managing the people and do organizing between teams and handling client calls. Even If I learn I can't implement it anyway, So learning becomes useless. I have my own server Dell T30, bought 2020 to learn and implement. But nothing happened so far. at least i wanted a good change."
4359,2020-12-04 21:57:32,1607111852.0,dataengineering,Let’s talk about Snowflake. What are your thoughts on it?,k6s8xb,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k6s8xb/lets_talk_about_snowflake_what_are_your_thoughts/,3.0,47.0,0.0,21886.0,"Snowflake is the hype new DWH. But always hearing mixed opinions: other tools can do the same thing vs it makes things a lot easier. Im not asking whether or not they are doing something new. Rather, do you think what they are doing is valuable and if they will see growth.

[View Poll](https://www.reddit.com/poll/k6s8xb)"
4360,2020-12-04 22:04:29,1607112269.0,dataengineering,Introducing the 5 Pillars of Data Observability,k6sdwi,mkvor8,,https://www.reddit.com/r/dataengineering/comments/k6sdwi/introducing_the_5_pillars_of_data_observability/,6.0,1.0,0.0,21886.0,
4361,2020-12-05 02:07:53,1607126873.0,dataengineering,Looking for ideas for running many short and long tasks,k6wwzr,ryptophan,,https://www.reddit.com/r/dataengineering/comments/k6wwzr/looking_for_ideas_for_running_many_short_and_long/,1.0,5.0,0.0,21889.0,"Currently each short (&lt;5s) task and long task is put into separate celery queues, and workers execute tasks after they are put in their corresponding queue.

The tasks are inserted through an api request. There could be some scheduling, but not required. 

I have motivations for wanting to break the tasks into two (feature  will, model training) or more steps.

An inference/short task would then be split into two short tasks...

I could probably accomplish this with celery, but I'm interested in how things like Airflow and Beam might be of use. I think I have read that Airflow is not ideal for many short tasks...

I can offer some more description if I have left anything out.

Thanks!

Edit: celery is an asynchronous task queue for use in python, for those unaware."
4362,2020-12-05 13:32:34,1607167954.0,dataengineering,OSS DB Focused on Data Engineers New Release - TerminusDB 4.0,k75yyy,EverythingIsNail,,https://www.reddit.com/r/dataengineering/comments/k75yyy/oss_db_focused_on_data_engineers_new_release/,1.0,0.0,0.0,21901.0,
4363,2020-12-05 16:30:01,1607178601.0,dataengineering,"Get to know Workflows, Google Cloud’s serverless orchestration engine",k78f3a,dream-fiesty,,https://www.reddit.com/r/dataengineering/comments/k78f3a/get_to_know_workflows_google_clouds_serverless/,1.0,9.0,0.0,21906.0,
4364,2020-12-06 02:28:59,1607214539.0,dataengineering,"Another ""How to Become a Data Engineer"" Post",k7j81z,cookoojeffy,,https://www.reddit.com/r/dataengineering/comments/k7j81z/another_how_to_become_a_data_engineer_post/,1.0,24.0,0.0,21922.0,"Hi. I would first like to apologize for this post as it seems like people in the community are tired of seeing posts like this and if this post isn't really fluid as I am pretty much almost venting right now. But, I am lost and overwhelmed trying to become a data engineer.

I just graduated from uni (in California) back in July with a Math: Probability and Statistics BS degree and haven't done anything substantial to add to my empty resume. I was stuck between wanting to become a data scientist or a data analyst. But later down the road, I discovered what a data engineer generally does and it really intrigued me and am fully committed and passionate to becoming one. My goal right now is to become an entry level or Junior Data Engineer. 

In terms of experience in practical skills, I barely have any. In the past two months, I learned basic SQL from ""Sams Teach Youself SQL"" and basic fundamentals of Python from  ""Learning Python"" by Mark Lutz and that's pretty much all the practical skills I have so far. SQL is something I can understand quicker than Python as it is not as code heavy. I heard that SQL and Python are the two biggest essential skills for a data engineer and am not sure how to expand my knowledge from the basics and fundamentals (especially Python) to something more advanced so that I can confidently add them to my resume. Some resources would greatly be appreciated. 

But, programming is something that I really struggle with. I know the BASIC fundamentals of Python such as  types, functions, and classes but don't really know how to further expand from that. I know part of expanding is doing projects but I don't know how to begin that's related to data engineering. 

I looked into the roadmaps and learning paths that people have posted here and it just seems so daunting and overwhelming especially for someone like me who has no experience in the field. However, I heard that a lot of the things you learn as a data engineer comes from working on the job. So, what skills should I be focusing on or prioritizing on to actually land an interview/job? 

And of course any resources are really appreciated. Thank you very much for actually reading this if you did."
4365,2020-12-06 04:06:20,1607220380.0,dataengineering,Careers similar to DE,k7krc7,pressYESforchicken,,https://www.reddit.com/r/dataengineering/comments/k7krc7/careers_similar_to_de/,1.0,7.0,0.0,21922.0,"Hello all,

I’m currently a BI analyst but find myself doing quite a bit of low level DE work, and have found that I actually enjoy systems issues more than business operations. I’m starting the Georgia Tech OMSCS program in Jan. and plan to follow the systems engineering track. 

Since nearly all “entry level” DE positions require experience with systems I don’t currently work with, what other positions could I pursue to set myself up for DE work in the future? Systems engineer, SWE, Backend engineer? A stepping stone position so to speak. 

Thank you in advance."
4366,2020-12-06 07:23:11,1607232191.0,dataengineering,"learning Kubernetes , Good Idea",k7npgi,angadaws,,https://www.reddit.com/r/dataengineering/comments/k7npgi/learning_kubernetes_good_idea/,1.0,8.0,0.0,21925.0,"Is learning Kubernetes for a data engineer role a good idea ? 
Data engineer roles that need Spark , Airflow/Nifi for data pipelines"
4367,2020-12-06 09:45:44,1607240744.0,dataengineering,I see a lot of different reviews of data engineering and what it is or isn’t. I listed below what I do and wondering for all of you who say I am not a data engineer and say what I do is so pathetic and boring ....what can I do to be like,k7piq5,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/k7piq5/i_see_a_lot_of_different_reviews_of_data/,1.0,19.0,0.0,21931.0,"I use python to make requests to various source applications, store the data in a data lake (S3), and transform and load the data as fact/dim , denormalized or replicated tables in snowflake. I use AWS/python to string it all together. I work on making the python code reusable and eventually maybe a meta data pipeline framework swe can use to replicate different product backend for analytics. I consider how our analytics IP can be used in product ( very lightly , nothing acted on yet ). I gather requirements from stakeholders in the data needs and shed a perspective on its value to the business. I read logs daily to see how 14 pipelines performed and fix bugs. I manage the computation resources to keep the system up , and am mindful of the “cost” of our pipelines against the value to business. I used to be an analyst. I tie out our data warehouse with source applications in different departments. Use threading /batches/ get clever with avoiding api limits to avoid additional costs. 

What steps can I take to be more valid."
4368,2020-12-06 19:16:39,1607274999.0,dataengineering,Useful resources for Data Engineers,k7xo0q,oleg_agapov,,https://www.reddit.com/r/dataengineering/comments/k7xo0q/useful_resources_for_data_engineers/,1.0,12.0,0.0,21950.0,
4369,2020-12-06 21:16:25,1607282185.0,dataengineering,Intentional homicide rate (1950-2019),k7zzo6,shanrap,,https://www.reddit.com/r/dataengineering/comments/k7zzo6/intentional_homicide_rate_19502019/,1.0,0.0,0.0,21953.0,
4370,2020-12-06 21:18:37,1607282317.0,dataengineering,What to prepare for Python Data Engineer role?,k8015j,veryuncanny_,,https://www.reddit.com/r/dataengineering/comments/k8015j/what_to_prepare_for_python_data_engineer_role/,1.0,2.0,0.0,21953.0,"I am a data engineer and I am trying to switch my job. I have 2 years of experience and have an interview lined up for a python data engineer job. Their requirement is mostly python, unix/linux and git. In my current job, I have mostly worked on only sql and etl tools. Where should I prepare from? What all to expect in the interview?"
4371,2020-12-06 22:07:52,1607285272.0,dataengineering,Data pipeline architecture,k80z6w,redder_ph,,https://www.reddit.com/r/dataengineering/comments/k80z6w/data_pipeline_architecture/,1.0,8.0,0.0,21954.0,"I am working on an pipeline that has both ingest and transform in a single pipeline. So, the architecture looks something like: 

Rest layer(Ingestion) -&gt; producer servc -&gt; Kafka -&gt; consumer servc(transform) -&gt; Database.

This is a real-time, streaming pipeline with high data traffic. I feel like it would be more efficient to decouple the ingestion from the transform.  Am I overthinking this or would it make sense to do something like this?

Ingestion:

* Rest layer -&gt; producer servc -&gt; Kafka(Pull) -&gt; consumer servc -&gt; S3 writer-&gt; S3

Transform

* S3 -&gt; S3 reader -&gt; producer servc -&gt; RabbitMQ(Push) -&gt; consumer servc(transform) -&gt; Database.

This way the transform services are not overwhelmed by the speed at which data is ingested."
4372,2020-12-06 22:16:43,1607285803.0,dataengineering,Registered Nurse in need of advice,k8151q,YodaClimbs,,https://www.reddit.com/r/dataengineering/comments/k8151q/registered_nurse_in_need_of_advice/,1.0,14.0,0.0,21954.0,"I work for a small privately owned hospital in the west. I've recently been hired for what's called an Oncology Clinical Trials Nursing position. I'm in the process of trying to wrangle all the information that is expected to flow through us and for which we are responsible. While I don't have a background in CS I've at least read books on Python, Database Management Systems, Health information management and Data Structures and Algorithms.

A coworker and I am responsible for moving data from 5 different possible medical record systems onto web based database systems for the clinical trials. They hire nurses because we are also responsible for annotating these transactions. Up until this point the positions have been held by older nurses and the solution to this has been to write these notes either in the margins of faxed papers or in an Excel document with a date in one cell and another massive cell for all the text, printed out and then thrown in a massive stack of papers in reverse chronological order in a binder.

I'm in a position where I can establish a new record system as the only person to hold this job left early this year and had a heart attack, so we have no explanation of previous methods. 

Now I'm hoping to save these ""Progress Notes"" in an electronic format. To keep my coworker happy it obviously needs to murder a tree and be stored in a confined area with other flammable data records. But since the notes are often typed it seems reasonable enough to keep their initial electronic form.  As far as I can think, the notes need the date, the person creating it, and the text. To make notes more searchable I think a section for tags would be good.

So the question is what's the best way to save the notes so that they're searchable? We are limited to Microsoft office products, having no ability to download and install programs and limited web connectivity. (Basically limited to Wiki, government websites, and the bing search engine)

My thoughts are to store these files in just basic text files with dates, patient and nurse information being saved in the name and just entering the text in a consistent format, delimiting data with carriage returns and delimiting notes with some common symbol. The only record that these files haven't been altered though is by keeping originals in hardcopy.

 My second thought is saving these in an Excel Worksheet Table with one column for dates, one for the massive text note, with everybody saving and password protecting their own file. Finally I wonder if an Access Database would accomplish this task the best? I am not familiar with Access, most of my DBMS experience is open source, either being MySQL or an early Hierarchical language called MUMPS.

These patient's can be on trials for years it would be nice to collect all these Natural Language Data in something other than the piles of Borg like programs that is modern Health Information Science. Or just thrown into charts to be dug through in the slowest O(n) time imaginable."
4373,2020-12-07 00:12:45,1607292765.0,dataengineering,"@data_weekly 20th edition focus on S3 strong read-on-writes consistency, @ApachePinot 0.6.0, @thoughtworks Data Mesh principles, @Adobe experience with Iceberg, @LinkedInEng Lambda-less architecture, @FT platform journey, and more.",k83b4k,vananth22,,https://www.reddit.com/r/dataengineering/comments/k83b4k/data_weekly_20th_edition_focus_on_s3_strong/,1.0,0.0,0.0,21957.0,
4374,2020-12-07 01:07:54,1607296074.0,dataengineering,11 Tools for Productive Data Analytics Professionals - The full stack data scientist toolkit,k84awa,data-leon,,https://www.reddit.com/r/dataengineering/comments/k84awa/11_tools_for_productive_data_analytics/,1.0,1.0,0.0,21957.0,
4375,2020-12-07 10:31:21,1607329881.0,dataengineering,Top 5 free or affordable dashboard and SQL reporting tools for 2021,k8cunt,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/k8cunt/top_5_free_or_affordable_dashboard_and_sql/,1.0,1.0,0.0,21974.0,"The world of business seeks data analytics and most big enterprises and businesses would be able to afford a dedicated tool for ETL and visualization. We have sorted and compared the best dashboard and SQL reporting tool that's start-up friendly in terms of affordability.

[Top 5 free or affordable dashboard and SQL reporting tools](https://www.sprinkledata.com/docs/top-5-free-or-affordable-dashboard-and-sql-reporting-tools/index.html?utm_source=reddit_071220&amp;utm_medium=sqlreportingtool)

https://preview.redd.it/7wyp2uef7q361.png?width=1336&amp;format=png&amp;auto=webp&amp;s=144ad986df454be081de0f37c23ab0fa1b0c09e1"
4376,2020-12-07 11:18:23,1607332703.0,dataengineering,Top 5 Real World Artificial Intelligence Applications,k8de3d,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k8de3d/top_5_real_world_artificial_intelligence/,1.0,0.0,0.0,21977.0,
4377,2020-12-07 11:19:32,1607332772.0,dataengineering,Artificial Intelligence vs Machine Learning vs Data Science,k8dejn,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k8dejn/artificial_intelligence_vs_machine_learning_vs/,1.0,0.0,0.0,21977.0,
4378,2020-12-07 11:24:10,1607333050.0,dataengineering,Learn Machine learning and data science,k8dgdk,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k8dgdk/learn_machine_learning_and_data_science/,1.0,0.0,0.0,21977.0,
4379,2020-12-07 11:25:01,1607333101.0,dataengineering,What is needed to be a full stack developer in 2021,k8dgp0,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k8dgp0/what_is_needed_to_be_a_full_stack_developer_in/,1.0,0.0,0.0,21977.0,
4380,2020-12-07 11:26:29,1607333189.0,dataengineering,Best programming language to learn in 2021,k8dh8n,chase2learn,,https://www.reddit.com/r/dataengineering/comments/k8dh8n/best_programming_language_to_learn_in_2021/,1.0,0.0,0.0,21977.0,
4381,2020-12-07 12:46:24,1607337984.0,dataengineering,a short blog on Random forest model for Machine learning,k8ef5s,HairComprehensive322,,https://www.reddit.com/r/dataengineering/comments/k8ef5s/a_short_blog_on_random_forest_model_for_machine/,1.0,0.0,0.0,21984.0,
4382,2020-12-07 13:00:20,1607338820.0,dataengineering,How to implement real-time rankings for data stream?,k8elch,60ri,,https://www.reddit.com/r/dataengineering/comments/k8elch/how_to_implement_realtime_rankings_for_data_stream/,1.0,3.0,0.0,21984.0,"Hi,

I'm working on a system that processes stream of tweets, extracts entities from the records and saves entity occurrences as time-series data in the database (5 minute granularity). My stack is Kafka-Spark Structured Streaming-Cassandra. I want to add a functionality to the system, that will also provide top k entities for the past hour or so (in terms of occurrences). These rankings should ideally update very frequently with incoming new data. As Cassandra doesn't seem as a good choice here (as I need sorting based on aggregated values), I'm wondering how should I approach this? I'm open to both using my current stack and using some other tools to achieve this. Any ideas are appreciated, as I'm really stuck on this right now :) Thanks in advance!"
4383,2020-12-07 14:03:17,1607342597.0,dataengineering,THE RIGHT DMP in 2021 (DATA MANAGEMENT PLATFORM),k8ffeb,ResearchInsights,,https://www.reddit.com/r/dataengineering/comments/k8ffeb/the_right_dmp_in_2021_data_management_platform/,1.0,0.0,0.0,21985.0,
4384,2020-12-07 16:31:04,1607351464.0,dataengineering,Top 4 Resume Projects?,k8hlz1,serious_joe_92,,https://www.reddit.com/r/dataengineering/comments/k8hlz1/top_4_resume_projects/,1.0,0.0,0.0,21988.0,"If you could only list 4 DE projects on your resume, what would you choose to showcase?"
4385,2020-12-07 16:53:05,1607352785.0,dataengineering,Major feature released for Kafka IDE later this week. Guess what's coming for a chance to win 30 USD Amazon voucher.,k8hzt9,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/k8hzt9/major_feature_released_for_kafka_ide_later_this/,1.0,0.0,0.0,21987.0,"Launching [Kafka IDE](https://kafkaide.com/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=major-feature-giveaway&amp;position=top&amp;utm_term=dataengineering) Open Beta across many desktop platforms and many Apache Kafka installations has been a challenging task. Our first releases were not as stable as we would loved to, and we are aware that this could have caused some frustration. We are working hard on making the user experience more enjoyable.

To alleviate some of that frustration, we will be shipping later this week a new release with more stability enhancements and a **new major feature that will definitely make your Apache Kafka experience more enjoyable**. We have also decided to run a series of giveaways while the Beta Phase is open, this one being the first one.

To be elegible, reply here or to this [tweet](https://twitter.com/kafkaide/status/1335881607111200768) at our official twitter account at [https://twitter.com/kafkaide](https://twitter.com/kafkaide) with your guess. Those who guess what's the new upcoming feature will be elegible for a chance to win the 30USD Amazon Voucher. The winner will be announced next Friday.

**What is Kafka IDE?**

[Kafka IDE](https://kafkaide.com/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=major-feature-giveaway&amp;position=bottom&amp;utm_term=dataengineering) is a desktop client similar to Tableau or Looker that queries Apache Kafka directly. It aims to be a better alternative to kafkacat, kafka manager or similar.

You can query particular time windows using offsets (you can use natural language to specify dates) as well as smart schema inference for those folks who deal with plain JSON messages."
4386,2020-12-07 16:53:21,1607352801.0,dataengineering,Fastest way to move large amounts of data from SQL Server to BigQuery,k8i008,jaredstufft,,https://www.reddit.com/r/dataengineering/comments/k8i008/fastest_way_to_move_large_amounts_of_data_from/,1.0,16.0,0.0,21987.0,"Hi everyone,

&amp;#x200B;

I'm building an ETL pipeline in Airflow between a remote SQL Server instance and Google BigQuery. One particular table is quite large (10+ billion records). The ETL occurs hourly for the previous hour which is fast enough for new data. However, backfilling the table has proven to be very slow. The airflow backfill command is using the hourly backfill which means the tasks will take weeks to finish because of BigQuery API quotas, but the data set is too large to just extract the whole table into a flat file and upload manually.

&amp;#x200B;

What strategies are people using nowadays to move large amounts of data out of SQL Server quickly?"
4387,2020-12-07 17:23:04,1607354584.0,dataengineering,"any ""BigData"" Datasets to learn data processing",k8ij5n,pras29gb,,https://www.reddit.com/r/dataengineering/comments/k8ij5n/any_bigdata_datasets_to_learn_data_processing/,1.0,6.0,0.0,21987.0,"Fellow data-engineers, where can I find resources  which provide bigdata sets to learn data processing. This can help me to learn spark and other data processing technologies."
4388,2020-12-07 19:01:31,1607360491.0,dataengineering,Webinar Smart meter analytics in Demand-Response programs,k8kgyl,Revolutionary_Long71,,https://www.reddit.com/r/dataengineering/comments/k8kgyl/webinar_smart_meter_analytics_in_demandresponse/,1.0,0.0,0.0,21987.0,"Our suggested approach is based on the knowledge of the customer electricity usage behavior gathered from smart meters.

In the webinar we will generate hourly load profiles, then we will cluster users with the similar load profiles and finally select the groups with higher conversion rate for demand-response programs

[Register](https://www.isthari.com/webinar-demand-response?utm_source=reddit&amp;utm_medium=dataengineering&amp;utm_campaign=demand-response)

&amp;#x200B;

https://preview.redd.it/ls7ehnhgqs361.png?width=1283&amp;format=png&amp;auto=webp&amp;s=bf4c4fb2c55fb8b9bf96d119f02b21621f0c76a3"
4389,2020-12-07 20:23:44,1607365424.0,dataengineering,Is it feasible to use the Vertical Pod Autoscaler with Airflow on a task level?,k8m6o5,raybb,,https://www.reddit.com/r/dataengineering/comments/k8m6o5/is_it_feasible_to_use_the_vertical_pod_autoscaler/,1.0,3.0,0.0,21990.0,
4390,2020-12-08 12:20:59,1607422859.0,dataengineering,Are Oracle Cloud services really cheaper than AWS as they claim?,k91tye,Irajk,,https://www.reddit.com/r/dataengineering/comments/k91tye/are_oracle_cloud_services_really_cheaper_than_aws/,1.0,8.0,0.0,22007.0,"I came across this link of Oracle which claims it is in every aspect better than AWS and cheaper.  
[https://www.oracle.com/cloud/economics/](https://www.oracle.com/cloud/economics/)  
It seems too good to be true. 

In other sources, I've always seen AWS is the cheaper option.

Of course then you might ask ""regarding which service?""

In the link it claims superiority in all services in price and quality. But my usage is mostly storage (hundreds of giga-bytes) with high I/O and limited computation.   


Do both of these cloud services support transfer of my structured MS SQL databases and tables   
and work with them on cloud? Can I have my C# program (as a job/ running frequently) for managing databases on these services, working with the stored data as I do it locally?  


Thank you for your answers."
4391,2020-12-08 12:38:44,1607423924.0,dataengineering,I built a free tool to help engineers document any database from CLI,k921dy,duyenla257,,https://www.reddit.com/r/dataengineering/comments/k921dy/i_built_a_free_tool_to_help_engineers_document/,1.0,18.0,0.0,22007.0,"A little bit of background first, I was an engineer at a fast-growing tech agency. Projects came to us every day and it was my job to document the database schema, fields' relationship, and all other nitty-gritty metadata.

It was a PAIN in the neck to do that using Microsoft Word. Imagine you have to scroll through hundreds of pages just to find the correct field, then change it, then do it A HUNDRED TIMES more because the BA team constantly changes their requests.

The worst part of it is sharing with our clients. To save time, I document the database in a way that only technical people like me can understand (or...well...just me). It turns out the piece is completely useless when read by our business clients, who understood nothing and found it hard to navigate around. I know that Word's formatting can be nice, but it just takes too much time for a dev like me to learn the ins and outs. So nope.

So after I resigned from that agency and moved to another (don't get me wrong, it's a great place to work, this is a personal matter), I decided to build a tool to help me get the job done.

You can check it out here: [https://dbdocs.io](https://dbdocs.io/)

**The project started off fairly simple: it must meet 2 big criteria:**

* Must be easy for me to generate. I hate spending time tweaking and editing on Word -&gt; I decide that I MUST use code to generate it.
* Must be business-friendly. The generated result must have a clear and friendly format so even non-tech people know what is what -&gt; I consulted my designer friend and went with the table-based design. 

**So what can the tool do?** It's fairly new so only can do a few things for now:

* First, you will need to define the database structure using a standardized language. I choose DBML since it's fairly simple to learn.

https://preview.redd.it/fy7guypyyx361.png?width=529&amp;format=png&amp;auto=webp&amp;s=7330fa37df7f010a10cbeebe5d58ca275322593d

* Then I use the tool + CLI to quickly generate a website for the database document. It takes only a few lines of commands.

https://preview.redd.it/ihwkedppyx361.png?width=532&amp;format=png&amp;auto=webp&amp;s=0b02f129fc0728de18a374c2a603dcfd7281c691

* In the document, you can view the ERD, fields name, type, references and notes.

https://preview.redd.it/htbu9s8vyx361.png?width=1919&amp;format=png&amp;auto=webp&amp;s=8374ae09a0ec8077e202ac4828d4f54d529a44c8

* You can share it pretty easily using the document's URL. Remember to set a password for it by CLI.

P/s:  Since this is a new side project, I haven't got time to think of pricing for it. Just hope to hear your initial impression and feedback so I can continue improving it! Hope this free project is helpful for someone out there."
4392,2020-12-08 15:28:24,1607434104.0,dataengineering,Programming Languages for Data Science,k945ej,Techbiason,,https://www.reddit.com/r/dataengineering/comments/k945ej/programming_languages_for_data_science/,1.0,0.0,0.0,22011.0,
4393,2020-12-08 17:42:38,1607442158.0,dataengineering,Data Engineering Python ETL and SQL,k96gkp,Rough_Let_4118,,https://www.reddit.com/r/dataengineering/comments/k96gkp/data_engineering_python_etl_and_sql/,1.0,3.0,0.0,22013.0,"Hi Folks, 

Can anybody please help me with the approach/solution to the below problem 

    m0	p0	start 0.712 
    m0	p1	start 0.841 
    m0	p2	start 1.523 
    m0	p2	end 1.966 
    m0	p1	start 2.856 
    m0	p2	start 3.347 
    m0	p2	end 3.567 
    m0	p1	start 3.800 
    m0	p2	start 4.618 
    m0	p2	end 5.497 
    m0	p1	start 5.961 
    m0	p2	start 6.324 
    m0	p2	end 6.673 
    m0	p1	end 7.233 
    m0	p1	end 7.533 
    m0	p1	end 7.933 
    m0	p1	end 8.333 
    m0	p0	end 9.933 

a row m1:p1:start:2.984 means, machine m1 starts process p1 at timestamp 2.984.

Goal:

1. Design a table schema for this data to be used by data scientist to query metrics such as process with max average elapsed time and they can plot each process.
2. Design a ETL in python to load data to above data model /table.

Follow-up

1. How to optimize process to parse the file and load to table. Can it be done with constant memory.
2. There can be multiple machine m0..mN, each machine can have millions of process entries. How you will scale."
4394,2020-12-08 18:02:06,1607443326.0,dataengineering,Now Published: Getting Started With Apache Spark on Kubernetes (Data + AI Summit 2020),k96tvp,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/k96tvp/now_published_getting_started_with_apache_spark/,1.0,0.0,0.0,22015.0,
4395,2020-12-08 18:33:55,1607445235.0,dataengineering,Getting Started with Spark on Kubernetes - Data + AI Summit 2020 Talk - Video now published :),k97g5f,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/k97g5f/getting_started_with_spark_on_kubernetes_data_ai/,1.0,0.0,0.0,22016.0,"Hope it's useful to some of you!

Topics covered in the talk:

\- Architecture  
\- Pros/Cons  
\- Best practices  
\- Live Demo  
\- Future Works

Video + slides: [https://databricks.com/session\_eu20/getting-started-with-apache-spark-on-kubernetes](https://databricks.com/session_eu20/getting-started-with-apache-spark-on-kubernetes)

Highlights of the conference: [https://www.datamechanics.co/blog-post/data-ai-summit-europe-2020-highlights](https://www.datamechanics.co/blog-post/data-ai-summit-europe-2020-highlights)"
4396,2020-12-08 18:57:40,1607446660.0,dataengineering,The Rules for Data Processing Pipeline Builders,k97xc0,vkazanov,,https://www.reddit.com/r/dataengineering/comments/k97xc0/the_rules_for_data_processing_pipeline_builders/,1.0,3.0,0.0,22016.0,
4397,2020-12-08 20:29:05,1607452145.0,dataengineering,DE Interview @Facebook,k99sy2,Suitable-Chemistry-9,,https://www.reddit.com/r/dataengineering/comments/k99sy2/de_interview_facebook/,1.0,37.0,0.0,22021.0,"I am super excited that I got invited to do a technical interview with Facebook. But also... I am a nervous wreck.

I am very confident on my SQL but I haven’t touched Python in two years. I deployed many projects in it a while ago but it’s super rusty now.

My questions for Reddit:
- How long should I study for before having the interview?
- I got referred, so the position I am interviewing for is not exactly open yet, but they are expecting that it will open in January and want me to be ready. Is it dumb/rude if I give available dates almost a month from now? I have the holidays coming up + the GMAT in early January + month close (12 hr days) at my current job.

I’ve read that I should really go through the resources provided by my recruiter + go through Glassdoor interview section. Any advice other than that?"
4398,2020-12-08 20:57:46,1607453866.0,dataengineering,Cloud Architecture - Discussion,k9ae4j,70sechoes,,https://www.reddit.com/r/dataengineering/comments/k9ae4j/cloud_architecture_discussion/,1.0,1.0,0.0,22022.0,"Hey everyone,

I currently work in a country where they have restrictions over data being stored in the cloud so most of my work has been with Hadoop on-prem.

That being said, I wanted to gain more experience with cloud providers, I'm currently working on getting an AWS certification. What I wanted to know is if someone who is working as a data engineering at a company where the services are hosted on cloud can share the stack that is currently being used and what his/her role is on a daily basis.

Also, do you believe this huge shift to the cloud will dissolve the need for administrators?"
4399,2020-12-08 21:44:26,1607456666.0,dataengineering,What rituals/workflows does your team follow?,k9be57,ercish,,https://www.reddit.com/r/dataengineering/comments/k9be57/what_ritualsworkflows_does_your_team_follow/,1.0,2.0,0.0,22024.0,"Transparently, I'm doing research for an article on ""data team rituals"" and I was hoping I could get some input from this subreddit. What rituals/workflows does your team follow (daily/weekly/monthly/quartlerly) that help keep you on track? Also, how have things changed this year? 

I won't use anything without your permission, I'm just looking for a temperature check/to spark discussion."
4400,2020-12-08 21:56:50,1607457410.0,dataengineering,Need help with ETL Process for Personal Dashboard Project,k9bn4a,_work_redditor_,,https://www.reddit.com/r/dataengineering/comments/k9bn4a/need_help_with_etl_process_for_personal_dashboard/,1.0,4.0,0.0,22024.0,"The plan is to pull IMDB information into a local Postgres database for manipulation and ultimately visual presentation through Tableau Public.

However, my ETL process is extremely resource (and time) consuming. I think it would work fine for a small dataset, but I'd like to be able to run the ingestion process daily to pick up any new or updated records. The dataset is 24.5 Million Records for just the one subject area example below, and I'd really like to process more IMBD datasets.

**ETL Overview:**

external table &gt;&gt; staging view to remove nulls &gt;&gt; staging table &gt;&gt; comparison (insert/update) table &gt;&gt; data-warehouse table &gt;&gt; [TBD] reporting views and smaller report-centric tables

**Example ETL path:**

EXT_IMDB_BASE_TITLE_AKAS &gt;&gt; SV_IMDB_BASE_TITLE_AKAS &gt;&gt; ST_IMDB_BASE_TITLE_AKAS &gt;&gt; X_IMDB_BASE_TITLE_AKAS_CHG &gt;&gt; DW_IMDB_BASE_TITLE_AKAS

**Example Process:**

ext_imdb_base_title_akas_etl.sql

&gt; TRUNCATE TABLE EXT_IMDB_BASE_TITLE_AKAS;
&gt; 
&gt; /* CHARACTER CASE OF THE FOLLOWING LOAD SEQUENCE MUST REMAIN */
&gt;
&gt; COPY EXT_IMDB_BASE_TITLE_AKAS FROM PROGRAM 'curl ""https://datasets.imdbws.com/title.akas.tsv.gz"" | gunzip' QUOTE '|' DELIMITER E'\t' NULL '\N' CSV HEADER;
&gt; 
&gt; ANALYZE EXT_IMDB_BASE_TITLE_AKAS;
&gt; 
&gt; COMMIT;

st_imdb_base_title_akas_etl.sql

&gt; /* INSERT INTO STAGING TABLE */
&gt; TRUNCATE TABLE ST_IMDB_BASE_TITLE_AKAS;
&gt; 
&gt; INSERT INTO ST_IMDB_BASE_TITLE_AKAS (
&gt; 	TITLE_NK
&gt; 	,ORDERING_NK
&gt; 	,TITLE
&gt; 	,REGION
&gt; 	,LANGUAGE
&gt; 	,TYPES
&gt; 	,ATTRIBUTES
&gt; 	,ISORIGINALTITLE
&gt; 	,SRC_ID
&gt; 	,LOAD_DATE
&gt; 	,NOTE
&gt; 	)
&gt; SELECT TITLE_NK AS TITLE_NK
&gt; 	,ORDERING_NK AS ORDERING_NK
&gt; 	,TITLE AS TITLE
&gt; 	,REGION AS REGION
&gt; 	,LANGUAGE AS LANGUAGE
&gt; 	,TYPES AS TYPES
&gt; 	,ATTRIBUTES AS ATTRIBUTES
&gt; 	,ISORIGINALTITLE AS ISORIGINALTITLE
&gt; 	,SRC_ID AS SRC_ID
&gt; 	,LOAD_DATE AS LOAD_DATE
&gt; 	,NOTE AS NOTE
&gt; FROM SV_IMDB_BASE_TITLE_AKAS;
&gt; 
&gt; ANALYZE ST_IMDB_BASE_TITLE_AKAS;
&gt; 
&gt; COMMIT;

dw_imdb_base_title_akas_etl.sql

&gt; TRUNCATE TABLE X_IMDB_BASE_TITLE_AKAS_CHG;
&gt; 
&gt; /* DETECT CHANGES */
&gt; INSERT INTO X_IMDB_BASE_TITLE_AKAS_CHG (
&gt; 	TITLE_ID
&gt; 	,TITLE_NK
&gt; 	,ORDERING_NK
&gt; 	,TITLE
&gt; 	,REGION
&gt; 	,LANGUAGE
&gt; 	,TYPES
&gt; 	,ATTRIBUTES
&gt; 	,ISORIGINALTITLE
&gt; 	,SRC_ID
&gt; 	,LOAD_DATE
&gt; 	,NOTE
&gt; 	,CHG_FLG
&gt; 	)
&gt; SELECT DW.TITLE_ID
&gt; 	,ST.TITLE_NK
&gt; 	,ST.ORDERING_NK
&gt; 	,ST.TITLE
&gt; 	,ST.REGION
&gt; 	,ST.LANGUAGE
&gt; 	,ST.TYPES
&gt; 	,ST.ATTRIBUTES
&gt; 	,ST.ISORIGINALTITLE
&gt; 	,ST.SRC_ID
&gt; 	,ST.LOAD_DATE
&gt; 	,ST.NOTE
&gt; 	,CASE
&gt; 		WHEN DW.TITLE_ID IS NULL
&gt; 			THEN 'I'
&gt; 		ELSE 'X'
&gt; 		END
&gt; FROM ST_IMDB_BASE_TITLE_AKAS ST
&gt; LEFT OUTER JOIN DW_IMDB_BASE_TITLE_AKAS DW
&gt; 	ON DW.TITLE_NK = ST.TITLE_NK
&gt; 		AND DW.ORDERING_NK = ST.ORDERING_NK
&gt; WHERE DW.TITLE_ID IS NULL
&gt; 	OR DW.TITLE &lt;&gt; ST.TITLE
&gt; 	OR DW.REGION &lt;&gt; ST.REGION
&gt; 	OR DW.LANGUAGE &lt;&gt; ST.LANGUAGE
&gt; 	OR DW.TYPES &lt;&gt; ST.TYPES
&gt; 	OR DW.ATTRIBUTES &lt;&gt; ST.ATTRIBUTES
&gt; 	OR DW.ISORIGINALTITLE &lt;&gt; ST.ISORIGINALTITLE
&gt; 	OR DW.SRC_ID &lt;&gt; ST.SRC_ID
&gt; 	OR DW.NOTE &lt;&gt; ST.NOTE;
&gt; 
&gt; ANALYZE X_IMDB_BASE_TITLE_AKAS_CHG;
&gt; 
&gt; /* INSERT CHANGES */
&gt; INSERT INTO DW_IMDB_BASE_TITLE_AKAS (
&gt; 	TITLE_ID
&gt; 	,TITLE_NK
&gt; 	,ORDERING_NK
&gt; 	,TITLE
&gt; 	,REGION
&gt; 	,LANGUAGE
&gt; 	,TYPES
&gt; 	,ATTRIBUTES
&gt; 	,ISORIGINALTITLE
&gt; 	,SRC_ID
&gt; 	,LOAD_DATE
&gt; 	,LAST_UPDATE
&gt; 	,NOTE
&gt; 	)
&gt; SELECT M.MAX_ID + ROW_NUMBER() OVER (
&gt; 		ORDER BY X.TITLE_NK
&gt; 			,X.ORDERING_NK
&gt; 		)
&gt; 	,X.TITLE_NK
&gt; 	,X.ORDERING_NK
&gt; 	,X.TITLE
&gt; 	,X.REGION
&gt; 	,X.LANGUAGE
&gt; 	,X.TYPES
&gt; 	,X.ATTRIBUTES
&gt; 	,X.ISORIGINALTITLE
&gt; 	,X.SRC_ID
&gt; 	,X.LOAD_DATE
&gt; 	,X.LOAD_DATE
&gt; 	,X.NOTE
&gt; FROM X_IMDB_BASE_TITLE_AKAS_CHG X
&gt; 	,(
&gt; 		SELECT MAX(TITLE_ID) MAX_ID
&gt; 		FROM DW_IMDB_BASE_TITLE_AKAS
&gt; 		) M
&gt; WHERE X.CHG_FLG = 'I';
&gt; 
&gt; /* UPDATE CHANGES */
&gt; UPDATE DW_IMDB_BASE_TITLE_AKAS
&gt; SET TITLE_NK = X.TITLE_NK
&gt; 	,ORDERING_NK = X.ORDERING_NK
&gt; 	,TITLE = X.TITLE
&gt; 	,REGION = X.REGION
&gt; 	,LANGUAGE = X.LANGUAGE
&gt; 	,TYPES = X.TYPES
&gt; 	,ATTRIBUTES = X.ATTRIBUTES
&gt; 	,ISORIGINALTITLE = X.ISORIGINALTITLE
&gt; 	,SRC_ID = X.SRC_ID
&gt; 	,LAST_UPDATE = X.LOAD_DATE
&gt; 	,NOTE = X.NOTE
&gt; FROM X_IMDB_BASE_TITLE_AKAS_CHG X
&gt; WHERE X.TITLE_NK = DW_IMDB_BASE_TITLE_AKAS.TITLE_NK
&gt; 	AND X.ORDERING_NK = DW_IMDB_BASE_TITLE_AKAS.ORDERING_NK
&gt; 	AND X.CHG_FLG = 'X';
&gt; 
&gt; ANALYZE DW_IMDB_BASE_TITLE_AKAS;

Are there any obvious improvements to make? dw_imdb_base_title_akas_etl.sql alone takes 2.5hrs to complete on my macbook air."
4401,2020-12-08 22:33:17,1607459597.0,dataengineering,"Slack API request, limiting to 1 request per DAG failure (Airflow)",k9cdnx,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/k9cdnx/slack_api_request_limiting_to_1_request_per_dag/,1.0,6.0,0.0,22025.0,"Hello jr data engineer here!

For some strange reason my task\_fail\_slack\_alert module is running the slack request a ridiculous amount of times, which is then showing up in our Slack channel that many times and is really annoying. My module should only run and show up in in Slack channel the same amount as the number of tasks that failed.

What am I missing?

&amp;#x200B;

Ever dag has this in their default arguments:

    default_args = {
 'on_failure_callback': task_fail_slack_alert,
}

which is coming from my module below. 

    import json
import requests

    def task_fail_slack_alert(msg):
    webhook_url = 'slack url token'

    slack_data = {
       'channel': '#airflow_alerts_local',
       'text': msg
    }
    response = requests.post( webhook_url, data=json.dumps(slack_data),
        headers={'Content-Type': 'application/json'})
 
        if response.status_code != 200:
        raise ValueError(
                  'Request to slack returned an error %s, the response is:\n%s'
                   %(response.status_code, response.text)
        )

    task_fail_slack_alert('Test msg')"
4402,2020-12-09 02:34:52,1607474092.0,dataengineering,Jobs opportunities in Europe for Latin-American people,k9gz2g,mmen0202,,https://www.reddit.com/r/dataengineering/comments/k9gz2g/jobs_opportunities_in_europe_for_latinamerican/,1.0,3.0,0.0,22034.0,"Hi all, I'd like to hear some advice to get a data engineering position in Europe (it doesn't matter the country) with work permit sponsorship. I'm currently a data engineer with a couple of years working with spark, scala and aws and I'd like to migrate from my country.
Any comment will be appreciated
Cheers!"
4403,2020-12-09 03:35:11,1607477711.0,dataengineering,Best cloud solution to archive 5-10tb/25-50tb/50-100tb of photos?,k9i10r,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/k9i10r/best_cloud_solution_to_archive_510tb2550tb50100tb/,1.0,3.0,0.0,22038.0,"Have a friend who does photography for a living, always complains about having to use a physical external hard drive to archive all his client photos. Pretty sure he has multiple external hard drives just for the sake of archiving all his 10+ years worth of client photos in case of an emergency. Not sure what the size of each photo is nor the format. I know that it’s at least several terabytes of historical images. Also, they are taken by a dslr camera if they helps in anyway. Which cloud solution is the most reliable and cost efficient to store 5-10tb of historical images, 25-50tb, and 50-100tb on the cloud? The images will rarely be fetched. I was originally thinking s3 buckets. But not entirely sure"
4404,2020-12-09 03:43:52,1607478232.0,dataengineering,How do I improve my pipeline?,k9i69r,levelworm,,https://www.reddit.com/r/dataengineering/comments/k9i69r/how_do_i_improve_my_pipeline/,1.0,16.0,0.0,22038.0,"Hi experts,

Background: 
We produce mobile games and analysts want to get the game economy analyzed. After looking at some of their queries I figured it would be super useful if I can concentrate all economic information in one table and use Airflow for scheduling, with date as partition.

So the final structure of the economy table looks like this, with some sample data:

Date | Time | UserId | EconomyItem | Amount | Reason
-----|------|--------|-------------|--------|-------
12-01|09:33 | 12345  | Coins       | 4000   | Store_Purchase
12-01|09:45 | 12345  | Coins       | -1000  | Skin_Purchase
12-01|10:00 | 12345  | Coins       | 20     | Hourly_Bonus
12-01|10:00 | 12345  | Energy      | 5      | Hourly_Bonus
12-01|10:04 | 12345  | Energy      | -2     | Level_Start
12-01|10:06 | 12345  | Star        | 5      | Level_Won

So my problem is, I pretty much have to query tons of fact tables to fetch all kinds of economic information, so the query has a huge number of `UNION` of small queries. If one of those small queries got blocked (e.g. table schema changed without notifying me), the whole query gets blocked.

However, should I go the other way, and create a huge number of small queries? It would be a bit of difficult to maintain the Airflow pipeline but maybe that's the better way? 

Please share your thoughts, thank you!"
4405,2020-12-09 09:22:05,1607498525.0,dataengineering,Facebook and Google ads to lake,k9nedo,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/k9nedo/facebook_and_google_ads_to_lake/,1.0,0.0,0.0,22048.0,"Tried writing a small article on this. Help me with the feedback

[https://ibnipun10.medium.com/google-and-facebook-ads-to-lake-c6498575f97a](https://ibnipun10.medium.com/google-and-facebook-ads-to-lake-c6498575f97a)"
4406,2020-12-09 12:01:09,1607508069.0,dataengineering,Data catalog for unstructured data (and non data-engineer/science users),k9p8c7,Anxious_Reporter,,https://www.reddit.com/r/dataengineering/comments/k9p8c7/data_catalog_for_unstructured_data_and_non/,1.0,2.0,0.0,22053.0,"Just wanted to make this post to help others who may have a similar situation.

Have seen lots of data catalogs like Alation, Erwin, Collibra, Amundsen, but all of these seem to be geared more towards data engineers / scientists and only focus on handling structured / tabled data.

In my use case, we have less technical users (eg. professionals, statisticians, scientists, and others that are not as comfortable using SQL et al to do their particular analysis) working with (some relational data, but mostly) unstructured data (like image files, PDF reports, SAS files, etc).

For this, I found that [Data Cookbook](http://www.datacookbook.com/videos-2/) was a really good fit. An evaluation and explanation on how it works can be found here in an article from University of Birmingham: [https://intranet.birmingham.ac.uk/it/innovation/documents/public/Experiments/TheDataCookBook-Evaluation-v0.1.pdf](https://intranet.birmingham.ac.uk/it/innovation/documents/public/Experiments/TheDataCookBook-Evaluation-v0.1.pdf) 

Anyone else have any other good software recommendations in this area (Data Cookbook is commercial, thus anything FOSS would be nice)?"
4407,2020-12-09 14:44:41,1607517881.0,dataengineering,Solo data guy trying to orient himself,k9r7gg,thingthatgoesbump,,https://www.reddit.com/r/dataengineering/comments/k9r7gg/solo_data_guy_trying_to_orient_himself/,1.0,5.0,0.0,22059.0,"Hi,

I'm a former sysadmin promoted to data guy: I was given one VM and all the OSS I could find and install myself to extract, process and report on various data within my company. Since I'm on my own and have to figure things out as we go along, I have some questions about things that can be improved. 

The constraints in which I work are:

- limited budget
- most data sources and 1 reporting tool (Splunk) are managed by dedicated teams;while they have been and are helpful, I can't ask them to do major redesign

Questions:

- I've built up a little collection of python scripts working as daily extractors for APIs and DBs. These are triggered from cron; there are no inter-dependencies between them or specific sequences to follow. The worst that could happen is that when script errors out, I have no data for a specific domain for a day. This was accepted as risk.

Are there better schedulers which might retry a failed job?

- There are some DBs, as part of a commercial package, that only have current state information about the things they track. There is no notion of history. As a workaround, I do daily extracts of some tables which I timestamp and shove into another DB. 

Some  of these DB extraction jobs require full table scans; occasionally a scripts breaks because my SQL select was chosen as the deadlock victim.

Is there a better way to do all of this?

- At the moment I have python and R as processing and prometheus (TSDB) and MariaDB as stores. The way things are looking I might have to introduce MongoDB to easily store and process large amounts of JSON from certain APIs. 

While so far manageable, are there other ways/tools for processing or storing data I should look into?

- As front ends I have Grafana mostly for time series, Shiny Server for specialized reports. There is also Splunk as corporate SIEM, managed by a dedicated team in which I created dashboards. For some reports you have to go to Splunk, for others to Grafana mostly depending on where the data is. I could dump everything in Splunk but then I'd be tied down by the limitations of their service offering and it would introduce delays into getting things out.

While I did create a basic portal page listing and linking all the reports, I wonder what else could be done here


All feedback appreciated"
4408,2020-12-09 16:16:46,1607523406.0,dataengineering,Data Engineering Frameworks/Stack in GIS domain,k9skr7,phmark19,,https://www.reddit.com/r/dataengineering/comments/k9skr7/data_engineering_frameworksstack_in_gis_domain/,1.0,13.0,0.0,22065.0,"Hello all,

Does anyone here works with the GIS domain?


If yes, then,
What tech stack are you using?
What are the challenges that you are facing in your DE job? 
What's the most interesting project have you worked with?
Any tips for someone entering the jungle?


I'm just trying to evaluate a DE job related to this domain. I'll be thankful for your participation!"
4409,2020-12-09 16:33:52,1607524432.0,dataengineering,🎅🎄TwelveDaysOfSMT 🎄🎅 - Exploring Single Message Transforms for Kafka Connect,k9sv8h,rmoff,,https://www.reddit.com/r/dataengineering/comments/k9sv8h/twelvedaysofsmt_exploring_single_message/,1.0,0.0,0.0,22066.0,"✨ Do you use Single Message Transforms in **K****afka** **Connect**? Or maybe you've wondered what they are?

🎥 ✍️I'm doing a series of videos and blogs about them during December (**#TwelveDaysOfSMT**), with the first one out today: [https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/](https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/)

Reply here with any feedback, any particular SMT you'd like me to explore, or scenarios to solve :)"
4410,2020-12-09 20:36:37,1607538997.0,dataengineering,Monitor Spark jobs in EMR with Gigahex,k9xmcy,shad-rocks,,https://www.reddit.com/r/dataengineering/comments/k9xmcy/monitor_spark_jobs_in_emr_with_gigahex/,1.0,0.0,0.0,22074.0,
4411,2020-12-09 22:39:15,1607546355.0,dataengineering,I heard many versions of Data Mesh and decided to write my thoughts on the same. How Data Lake is writing for NYT vs. Data Mesh is writing for O'Reilly? When to adopt Data Mesh? Find out more on,ka04xy,vananth22,,https://www.reddit.com/r/dataengineering/comments/ka04xy/i_heard_many_versions_of_data_mesh_and_decided_to/,1.0,0.0,0.0,22078.0,
4412,2020-12-09 23:06:36,1607547996.0,dataengineering,Exploring the Octoverse with ClickHouse,ka0p1v,vaosinbi,,https://www.reddit.com/r/dataengineering/comments/ka0p1v/exploring_the_octoverse_with_clickhouse/,1.0,0.0,0.0,22080.0,
4413,2020-12-10 04:26:43,1607567203.0,dataengineering,Looking for advice on what my next move should be in my career,ka6ial,BrokeAmount,,https://www.reddit.com/r/dataengineering/comments/ka6ial/looking_for_advice_on_what_my_next_move_should_be/,1.0,29.0,0.0,22093.0,"I work  as a Data Engineer at a relatively big company (not tech focused). I currently with with ADF, Databricks, and Snowflake. we write custom SQL and implement stored procedures where needed. We're an azure house but I've worked with AWS and various other on-prem databases in the past. i have experience in various object oriented programming languages like java and python, but i would say I'm about 3/10. when i need to do work specific, i do a lot of googling and piecing together what i need to get it done. so definitely not my forte, but I get it done. 

I currently make 125k, and want to make much more. I have a few ideas:

1. i can do some Azure/AWS/Google certifications and get much better at what i currently do
2. i can find a path to become a cloud Engineer
3. i can completely shift and try to become a software engineer

I don't necessarily care much for the work i do. i find work to be a means to an end. i want to chase the money, meaning i want to make 200k+ (if possible). i have no issues learning new technologies and actually really enjoy it. 

I quickly googled 2020 top certifications and saw that Google Certified Professional (GCP) Cloud Architect blows the rest out of the water with an average salary of around $175k. This is something I'd be interested in and would be curious to talk to anyone who's done it and what track they took to get there. 

i have a group of brilliant software engineer friends that have already laid out a course plan for me and willing to mentor me should i choose that path. This will take at least a year to do and that's with minimum 2-3 hrs of studying every day. 

I'm not at a crossroad or anything. I'm still employed, but i want to set myself up for success either by putting all my time and effort into my current job, or branching out and learning something else. I'm open to anything really. if anyone has any feedback or direction, please let me know."
4414,2020-12-10 07:48:50,1607579330.0,dataengineering,Data engineering over data science?,ka9pc0,ConfusedData,,https://www.reddit.com/r/dataengineering/comments/ka9pc0/data_engineering_over_data_science/,1.0,7.0,0.0,22100.0,"Like a lot of people, I was drawn to data science because it sounded ""cool"". But I was recently offered a data engineering position and I've been looking into what career path would pay me better for less taxing work (my background is computer science).

Turns out that could be DE in the long run as DS got overcrowded due to the hype and a lot of companies jumped on the bandwagon because of the hype without understanding the need of having someone actually carefully design the pipeline that will feed the data scientists the data. The reason I got this opportunity is because I substantially helped on a project where data scientists were having troubles with properly getting the data from the source.


But I'm reading conflicting reports about who's salaried better, with some even bringing up experienced software engineers and saying they still get paid better than both DSs and DEs.


I'm at the beginning of my career and I've got some experience in all three, but I'm unsure what path to take. This line of work is not my burning passion, but I enjoy it enough to do the work. Guess what I'm saying is I'm looking for the best bang for my buck as far as invested time and energy goes. I still find DS more interesting, but if DE could put me on a less competitive, better paid path I'd have no doubts about choosing it. From what I observed the DS people really need to live it.

Longterm tho, I'd like to get into management or smth, I wouldn't want to do technical work forever."
4415,2020-12-10 11:42:28,1607593348.0,dataengineering,[Tutorial] Secure Azure Data Lake with Datbabricks Credential Passthrough,kackf4,scientific_problem,,https://www.reddit.com/r/dataengineering/comments/kackf4/tutorial_secure_azure_data_lake_with_datbabricks/,1.0,1.0,0.0,22101.0,"Tutorial on how to set up fine-grained control over data sets in Azure Data Lake and use it via Azure Databricks, with credential passthrough. 

[https://datapao.com/azure-databricks-credential-passthrough/](https://datapao.com/azure-databricks-credential-passthrough/)"
4416,2020-12-10 12:29:46,1607596186.0,dataengineering,Welcoming Dataform to BigQuery: Create and manage your data transformations within your data warehouse,kad2mc,popopopopopopopopoop,,https://www.reddit.com/r/dataengineering/comments/kad2mc/welcoming_dataform_to_bigquery_create_and_manage/,1.0,1.0,0.0,22102.0,
4417,2020-12-10 13:17:07,1607599027.0,dataengineering,Data Engineer vs. Software Engineer: Career Guide,kadlmf,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/kadlmf/data_engineer_vs_software_engineer_career_guide/,1.0,15.0,0.0,22103.0,
4418,2020-12-10 13:58:42,1607601522.0,dataengineering,The Correlation Between a Data Scientist and a Big Data Engineer,kae2wl,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/kae2wl/the_correlation_between_a_data_scientist_and_a/,1.0,0.0,0.0,22104.0,
4419,2020-12-10 16:10:10,1607609410.0,dataengineering,We explain our data platform for Apache Kafka in short video Q&amp;As and we are curious about which one is the best. Please help us with your insights!,kafw98,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/kafw98/we_explain_our_data_platform_for_apache_kafka_in/,1.0,0.0,0.0,22107.0,
4420,2020-12-10 18:34:54,1607618094.0,dataengineering,We explain our data platform for Apache Kafka in short video Q&amp;As and we are curious about which one is the best. Please help us with your insights!,kaijqf,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/kaijqf/we_explain_our_data_platform_for_apache_kafka_in/,1.0,0.0,0.0,22111.0,"We try to reach out to all groups working with data within an enterprise. Accordingly, we made videos that are a bit more technical and videos explaining our product in a less technical way.  
What do you think, are we on the right track to explain our product? You can reach out to the videos here: [https://www.youtube.com/playlist?list=PLcaSjO4-G0qCjWEFpz5ss\_0MBj-DS-8Bl](https://www.youtube.com/playlist?list=PLcaSjO4-G0qCjWEFpz5ss_0MBj-DS-8Bl)"
4421,2020-12-10 18:39:56,1607618396.0,dataengineering,The Rules for Data Processing Pipeline Builders,kaindf,nfrankel,,https://www.reddit.com/r/dataengineering/comments/kaindf/the_rules_for_data_processing_pipeline_builders/,1.0,1.0,0.0,22112.0,
4422,2020-12-10 20:14:35,1607624075.0,dataengineering,#idataengineer podcast 004 Zoltán C. Tóth,kakm6x,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kakm6x/idataengineer_podcast_004_zoltán_c_tóth/,1.0,0.0,0.0,22119.0,"In the fourth part of our micro-podcast, [Zoltán C. Tóth](https://twitter.com/zoltanctoth) tells how much he likes Great Expectations and why all should master SQL.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-004](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-004)"
4423,2020-12-10 20:27:27,1607624847.0,dataengineering,How Compass Prevents Broken Data Pipelines/Workflows,kakvpi,mkvor8,,https://www.reddit.com/r/dataengineering/comments/kakvpi/how_compass_prevents_broken_data/,1.0,0.0,0.0,22120.0,"Would be interested to hear how other folks are tackling the problems outlined in this article: 

[https://towardsdatascience.com/how-compass-is-reinventing-real-estate-with-data-reliability-in-the-cloud-c2d8c23f2ea0](https://towardsdatascience.com/how-compass-is-reinventing-real-estate-with-data-reliability-in-the-cloud-c2d8c23f2ea0)

Here, they monitor for key elements of data health (lineage, distribution, volume, etc.) via a central dashboard and alert/triage when issues arise."
4424,2020-12-10 23:05:15,1607634315.0,dataengineering,"Looking for advice , in which order?",kao759,sperimentale,,https://www.reddit.com/r/dataengineering/comments/kao759/looking_for_advice_in_which_order/,1.0,4.0,0.0,22128.0,"*Hi guys,  i am working for a big bank for 5 years , not US and not EU country . 
As you know in such big companies everything is already built and you don't create something new most of the time . 
**I know oracle sql , lil pl/sql,  some Datastage and Odi, and im beginner in python planning to learn pandas and matplotlib 
***I want to find a job in England or Germany but with my skillset there is nothing i can find . We are not using any cloud system in my bank so no cloud experience no tableueu .
**** i want to find a job asap so what should be my gameplan ?
 I'm thinking about learning pandas, getting aws cloud practitioner then getting aws developer certificate but without much hands on experience will those certificates be meaningless? Should i try to learn pyspark and hadoop?
????? I am very well motivated but without a roadplan my days go not so productive. Can YOU guys create me a roadplan so i can stick to it ? 
Thanks &lt;3"
4425,2020-12-11 02:28:39,1607646519.0,dataengineering,Visualizing Taxis in real time,karimo,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/karimo/visualizing_taxis_in_real_time/,1.0,0.0,0.0,22132.0,
4426,2020-12-11 02:34:01,1607646841.0,dataengineering,Is there any tool that aggregate job logs? (For monitoring purpose),karmg0,curiousguy_08,,https://www.reddit.com/r/dataengineering/comments/karmg0/is_there_any_tool_that_aggregate_job_logs_for/,1.0,4.0,0.0,22132.0,"Hey guys, in my company we have several ETL pipelines/jobs (mostly SSIS packages and C# workflows) and I’m wondering if you guys know or have you used any tool that aggregates the job logs for monitoring purpose. This would help us just look into a single place instead of going multiple places for check the status/output of them.

Any tip/advice/help would be appreciated!!

Be safe!"
4427,2020-12-11 05:32:03,1607657523.0,dataengineering,Project Ideas for ETL using python script,kaus97,TheCalamariGuy,,https://www.reddit.com/r/dataengineering/comments/kaus97/project_ideas_for_etl_using_python_script/,1.0,20.0,0.0,22139.0,"I’m new to this field and want to make a simple ETL project but I don’t know where to start... any help / guidance , or even links would be very helpful! Thanks a lot!!"
4428,2020-12-11 05:56:27,1607658987.0,dataengineering,Data pipelines and expiring dictionaries,kav6kx,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/kav6kx/data_pipelines_and_expiring_dictionaries/,1.0,0.0,0.0,22140.0,
4429,2020-12-11 09:48:21,1607672901.0,dataengineering,How to use Variables and XCom in Apache Airflow?,kaygio,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/kaygio/how_to_use_variables_and_xcom_in_apache_airflow/,1.0,0.0,0.0,22143.0,
4430,2020-12-11 11:23:07,1607678587.0,dataengineering,Apache Flink 1.12.0 is out! Check the Release Announcement for all new features!,kazkmj,Marksfik,,https://www.reddit.com/r/dataengineering/comments/kazkmj/apache_flink_1120_is_out_check_the_release/,1.0,0.0,0.0,22146.0,
4431,2020-12-11 11:51:46,1607680306.0,dataengineering,"Power BI Governance, Good Practices: Setting up Azure Purview for Power BI",kazwg6,soheileee,,https://www.reddit.com/r/dataengineering/comments/kazwg6/power_bi_governance_good_practices_setting_up/,1.0,2.0,0.0,22149.0,
4432,2020-12-11 13:21:08,1607685668.0,dataengineering,Data Hunters: The first data community for data professionals and business decision makers.,kb0yod,Bright-Palpitation33,,https://www.reddit.com/r/dataengineering/comments/kb0yod/data_hunters_the_first_data_community_for_data/,1.0,0.0,0.0,22153.0,
4433,2020-12-11 16:49:37,1607698177.0,dataengineering,How to Become a Cloud Engineer in 2021 | Skills Required for a Cloud Engineer,kb45lo,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kb45lo/how_to_become_a_cloud_engineer_in_2021_skills/,1.0,0.0,0.0,22158.0,
4434,2020-12-11 16:50:47,1607698247.0,dataengineering,Top 5 Real World Artificial Intelligence Applications,kb46cr,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kb46cr/top_5_real_world_artificial_intelligence/,1.0,0.0,0.0,22158.0,
4435,2020-12-11 16:52:56,1607698376.0,dataengineering,How to Become a Data Analyst in 2021?,kb47uq,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kb47uq/how_to_become_a_data_analyst_in_2021/,1.0,0.0,0.0,22158.0,
4436,2020-12-11 16:53:58,1607698438.0,dataengineering,The Four Flavors of Goodhart’s Law,kb48i2,katori24tumble1,,https://www.reddit.com/r/dataengineering/comments/kb48i2/the_four_flavors_of_goodharts_law/,1.0,0.0,0.0,22158.0,
4437,2020-12-11 17:00:56,1607698856.0,dataengineering,What is needed to be a full stack developer in 2021,kb4czh,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kb4czh/what_is_needed_to_be_a_full_stack_developer_in/,1.0,0.0,0.0,22159.0,
4438,2020-12-11 17:54:27,1607702067.0,dataengineering,Project Advice,kb5cc0,Mumo2020,,https://www.reddit.com/r/dataengineering/comments/kb5cc0/project_advice/,1.0,2.0,0.0,22161.0,"Hi

I am new to data engineering and would like to create a personal project

Can I please get an advice on any good project which I can solve using Azure data factory, Azure data bricks and Kafka?

Also, where I could find some good data for this work 

Thanks"
4439,2020-12-11 20:16:53,1607710613.0,dataengineering,"ArtLine, Create Amazing Line Art Portraits.Github link in comments.",kb821a,vijish_madhavan,,https://www.reddit.com/r/dataengineering/comments/kb821a/artline_create_amazing_line_art_portraitsgithub/,1.0,2.0,0.0,22164.0,
4440,2020-12-11 20:53:22,1607712802.0,dataengineering,Structuring my dataset,kb8rja,DaiBax,,https://www.reddit.com/r/dataengineering/comments/kb8rja/structuring_my_dataset/,1.0,2.0,0.0,22167.0,"Hi all,

Does anyone have any advice on working out how to structure my dataset as a relational database? 

I'm a data novice, but I am aware that the answer to ""how to structure my data"" is dependant on what I want to do with it...

If I'm honest, I'm at the start of this journey so I'm not entirely sure what I *should* be able to do with this dataset, but I know that I want to interrogate the data with a bunch of questions as I think of them.

The dataset is current an excel spreadsheet of 21 columns and 2000+ rows. It's a table of couples, dances, songs, dates and scores for the last 17 seasons of Strictly Come Dancing - I'd like to turn this into a relational database but I'm not sure how to break the table down I to smaller tables that relate to each other. 

Any articles or advice on how to view your single table of data as smaller and related tables would be appreciated. 

If it helps, below are the columns in the table. 

`Celebrity Professional Pair Dance Song Series Week Date Order Craig Motsi Shirley Bruno Alfonso Arlene Len Alesha Jennifer Donny Darcey Total`"
4441,2020-12-11 21:00:51,1607713251.0,dataengineering,Question about apache airflow,kb8wur,fffrost,,https://www.reddit.com/r/dataengineering/comments/kb8wur/question_about_apache_airflow/,1.0,12.0,0.0,22167.0,"I'm trying to learn about airflow and I'm a bit stuck. I have read that it should be used for scheduling but not actually doing the heavy lifting of executing a complex task. For example a typical machine learning workflow:

1. read in data &amp; select features
2. scale the data
3. fit a model
4. evaluate the model

I see two options. Either I could create python functions that run these tasks and then use the PythonOperator to run each function. Or I could store these steps as different scripts and use the BashOperator to run these from the terminal, outputting the transformations/results at each stage.

I initially took the first approach but now I'm starting to think I should take the second. Can anybody shed some light on this for me please? Would be good to know what a typical approach would be for this."
4442,2020-12-11 22:13:27,1607717607.0,dataengineering,Hot topics amongst Data Engineers,kbac72,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/kbac72/hot_topics_amongst_data_engineers/,1.0,31.0,0.0,22165.0,"Hi! I am curious to know what are some of the popular topics amongst data engineers these days? I read and hear a lot wrt ETL tools, but would love to hear about other topics and thoughts from this community.

Thanks!"
4443,2020-12-11 22:58:17,1607720297.0,dataengineering,Which SQL solution for Leetcode Problem 185 would be acceptable (or most preferred) in an interview?,kbb6n5,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kbb6n5/which_sql_solution_for_leetcode_problem_185_would/,1.0,5.0,0.0,22169.0,"Problem: [https://leetcode.com/problems/department-top-three-salaries/](https://leetcode.com/problems/department-top-three-salaries/)  


I've seen this post explaining 3 different solutions (using window function, using subquery, using join):  
[https://leetcode.com/problems/department-top-three-salaries/discuss/797620/Three-solutions%3A-window-function-subquery-and-Join](https://leetcode.com/problems/department-top-three-salaries/discuss/797620/Three-solutions%3A-window-function-subquery-and-Join)

  
For a DE interview, which solution would be acceptable and also most preferred for this problem? Is there another solution that would be preferred outside of these 3 solutions?"
4444,2020-12-12 00:45:24,1607726724.0,dataengineering,How separate are the datasets you work with?,kbd68o,graciousgroob,,https://www.reddit.com/r/dataengineering/comments/kbd68o/how_separate_are_the_datasets_you_work_with/,1.0,1.0,0.0,22172.0,"For example, you you often find yourself joining datasets from completely different places into one analysis model? Ive found that often engineering data is completely separate from internal metrics, which are themselves fully separate from market analytics datasets. Has anyone else had a similar experience? Do you actually find much crossover between these datasets and are they worth merging?"
4445,2020-12-12 01:12:07,1607728327.0,dataengineering,Data Teams Survey Results,kbdnpm,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/kbdnpm/data_teams_survey_results/,1.0,0.0,0.0,22174.0,
4446,2020-12-12 11:34:16,1607765656.0,dataengineering,Volunteering,kbmeea,pdiddy_flaps,,https://www.reddit.com/r/dataengineering/comments/kbmeea/volunteering/,1.0,8.0,0.0,22185.0,"Hi folks, does anybody here volunteer their services in any kind of data professional capacity? If so, how did it come about?
I’ve been looking to do a bit of pro bono work for ages but haven’t found a resource to link me with opportunities."
4447,2020-12-12 16:05:18,1607781918.0,dataengineering,Attaching meaning to digital data. Data annotation/labeling explained,kbpm3k,OnlyProggingForFun,,https://www.reddit.com/r/dataengineering/comments/kbpm3k/attaching_meaning_to_digital_data_data/,1.0,0.0,0.0,22190.0,
4448,2020-12-12 18:48:33,1607791713.0,dataengineering,Event data to update Big query table?,kbs9ur,steak_ale_piethon,,https://www.reddit.com/r/dataengineering/comments/kbs9ur/event_data_to_update_big_query_table/,1.0,6.0,0.0,22196.0,"In the past 6 months, I've started a new DE role at work, building a number of basic ETL pipelines, taking data from 3rd party APIs, and sticking this into GCP, specifically Big query.

However, now I'm looking at a much larger data source and I'm wondering the best way to handle it.

Essentially, I want to take our hubspot database, stick it into gcp and keep it updated with new contacts and updated field values. There are over 1 million contacts and I'm looking to keep around 200 columns up to date. Rather than query their API, I've set up webhooks for all the data points I'm looking to capture, and have them pointing towards a cloud function endpoint which sticks the event in BQ.

Each event contains the timestamp, contact id, and a flag to say which field was updated and the new value. There could quite easily be 100k events per day, so this table could grow quite quickly

Whats the best way to transform this event data into a pivoted table, and keep these values up to date. I'm happy to have this done daily, so it doesn't need to be more frequent than that. Would you recommend  just fully reconstructing the table each day, looking at the most recent events for each contact ? Or would it be best to update specific values using bq's DML? It would be good to get an idea of the different possible approaches you may consider."
4449,2020-12-12 23:48:18,1607809698.0,dataengineering,"The special 21st edition of @data_weekly is out, focusing on Metadata Edition. The newsletter focuses on the timeline of data discovery systems from @LinkedIn @lyft @Uber @Facebook @netflix @Spotify @ShopifyEng @Airbnb @DatakinHQ @PayPalEng",kbxqph,vananth22,,https://www.reddit.com/r/dataengineering/comments/kbxqph/the_special_21st_edition_of_data_weekly_is_out/,1.0,2.0,0.0,22206.0,
4450,2020-12-13 00:03:39,1607810619.0,dataengineering,Childrens out of primary school rate per 1000 (1970-2019),kby183,shanrap,,https://www.reddit.com/r/dataengineering/comments/kby183/childrens_out_of_primary_school_rate_per_1000/,1.0,0.0,0.0,22206.0,
4451,2020-12-13 00:39:06,1607812746.0,dataengineering,Is there a SQL set of questions equivalent to the 75 Leetcode questions for FAANG interviews?,kbynb2,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kbynb2/is_there_a_sql_set_of_questions_equivalent_to_the/,1.0,18.0,0.0,22206.0,Im specifically looking to get into FANG as a DE. I’m sure a lot of you heard of the well known LC 75 questions made by the Facebook employee which covers most of the topics for data structure and algo questions for a SWE interview at FAANG. But is there a SQL set of questions for FAANG interviews? Specifically studying for data engineering position at FAANG. Basically for DEs they test you on DS/algos as well as very advanced sql. So I’m wondering if there is a SQL version for FAANG?
4452,2020-12-13 01:45:38,1607816738.0,dataengineering,Setting Up a Personal Hadoop Environment,kbzror,RandomJaguarSquad,,https://www.reddit.com/r/dataengineering/comments/kbzror/setting_up_a_personal_hadoop_environment/,1.0,2.0,0.0,22208.0,"With my goal being to learn more about the overall process of how HDFS systems function and I can interact with them, I've found the following guides for setting up a personal/local environment.  

\-  [Tutorial: Building your Own Big Data Infrastructure for Data Science | by Ashton Sidhu | Towards Data Science](https://towardsdatascience.com/tutorial-building-your-own-big-data-infrastructure-for-data-science-579ae46880d8) 

\-  [How To Set Up a Hadoop 3.2.1 Multi-Node Cluster on Ubuntu 18.04 (2 Nodes) | by João Torres | Medium](https://medium.com/@jootorres_11979/how-to-set-up-a-hadoop-3-2-1-multi-node-cluster-on-ubuntu-18-04-2-nodes-567ca44a3b12) 

My big question right now is whether I can install this setup on another old laptop, and interact with the system from my current laptop.  

As an alternative, I can create a virtual Ubuntu machine on my current laptop and set everything up there, but again, my knowledge ends at how I'd be able to interact with it.  

Any recommendations or suggestions based on the limited info I've given so far?  

My ultimate goal would be to potentially read and store data to attempt some data science projects on, but the short term is on understanding the inner workings on HDFS and moving data around.  

Thanks!"
4453,2020-12-13 04:45:07,1607827507.0,dataengineering,Python for data engineering,kc2nkt,BrokeAmount,,https://www.reddit.com/r/dataengineering/comments/kc2nkt/python_for_data_engineering/,1.0,14.0,0.0,22212.0,"I’m currently a data engineer that has very little to no python experience. I want to learn python well, what that means exactly, I don’t know. I use ADF, Databricks, and snowflake. 

With python being such a vast and flexible language, I was hoping someone here would be able to recommend a path or courses that would help me in this endeavor. 

I found the following from another Reddit post was was wondering if anyone has an opinion on it. 

https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/htmlview#gid=0"
4454,2020-12-13 07:35:46,1607837746.0,dataengineering,Streaming Systems and Global State,kc54t6,sap1enz,,https://www.reddit.com/r/dataengineering/comments/kc54t6/streaming_systems_and_global_state/,1.0,0.0,0.0,22217.0,
4455,2020-12-13 15:05:29,1607864729.0,dataengineering,I'm getting a lot of transactions while idling (Airflow and Azure File Share),kcaahi,danbcooper,,https://www.reddit.com/r/dataengineering/comments/kcaahi/im_getting_a_lot_of_transactions_while_idling/,1.0,3.0,0.0,22223.0,"Hi, I need to load data from different files into an Azure SQL database. So I set up a VM running Airflow and two Azure File Shares, one for my dags (so that I can modify them without sshing into the VM) and another to drop the files that will be loaded.

I mounted those two fileshares to the VM and my PC and use them as normal drives.

The system is currently idling and I can see in Azure's portal that I'm getting about 24k transactions every 5 minutes, but I can't see specifically what is generating them.

Is it possible that the VM is constantly requesting a list of files or touching the fileshare to check if it's still there? How can I avoid this?

Thanks!"
4456,2020-12-13 19:11:49,1607879509.0,dataengineering,Airflow 2.0 yay or nay?,kcea64,que_wut,,https://www.reddit.com/r/dataengineering/comments/kcea64/airflow_20_yay_or_nay/,1.0,19.0,0.0,22234.0,"Curious for those folks who are planning to move to 2.0 once Airflow 2.0 stabilizes.

Are the enhancements/patches worth the effort to migrate DAGs from 1.0 to 2.0?"
4457,2020-12-14 06:44:49,1607921089.0,dataengineering,Data Lake - Prod vs. Dev,kcqt43,miskozicar,,https://www.reddit.com/r/dataengineering/comments/kcqt43/data_lake_prod_vs_dev/,1.0,4.0,0.0,22253.0,"My organization is switching from traditional relational storage to data lake storage in the cloud. We have done some pilot projects and suddenly they became ""production"". Doing analysis and changes  on Prod data work w/o problem because we had small and disciplined team.

There are some process-oriented people in the org that now want us to create Dev and QA environment, so that changes are developed and tested there and only then promoted to Prod. Argument is that some change in ELT process can corrupt DL.

First problem is that we need to feed Dev and QA even in that case from OLTP systems in Prod. There is no point with working on fake data. We need to model things with data problems that exist only on prod servers. I think that we successfully sold that approach.

Then there is also question of size. Our Prod data lake is very large. One pattern would be to have complete copies of data in Dev and QA data lake and all the processes that feed Prod DL should be done 2 more times. Cost of everything would be 3 times higher.

We could also have smaller copies (subsets) in Dev and QA. Maybe we can load just data from last 2 years or something. Unfortunately that would give us a skewed picture of data anomalies. We also change formats of underlying files over time. We would have to take that into account as well. At some point it becomes too complicated.

I was on some conference and presenter had on the screen his DL. There were folders Prod and Dev on first level. (we are using raw, curated... pattern)

What are best practices/patterns for dealing with this?"
4458,2020-12-14 08:23:38,1607927018.0,dataengineering,Data Engineer Contract role at Facebook,kcs8oz,iwillgetintofaang,,https://www.reddit.com/r/dataengineering/comments/kcs8oz/data_engineer_contract_role_at_facebook/,1.0,6.0,0.0,22256.0,"I have a 90 minute onsite(virtual) interview for Data Engineer contract role at Facebook. I gather onsite is focused on ETL, Data Modeling, SQL and Coding for Full time. Is it the same for Contract role as well? Has anybody gone through a final interview for DE contract role at FB? Appreciate if someone can provide inputs on this."
4459,2020-12-14 09:12:13,1607929933.0,dataengineering,ETL with Spark on GPUs,kcsvov,Pundoras-box,,https://www.reddit.com/r/dataengineering/comments/kcsvov/etl_with_spark_on_gpus/,1.0,0.0,0.0,22258.0,
4460,2020-12-14 09:14:13,1607930053.0,dataengineering,ETL with Spark on GPUs,kcswlu,Pundoras-box,,https://www.reddit.com/r/dataengineering/comments/kcswlu/etl_with_spark_on_gpus/,1.0,5.0,0.0,22258.0,"I'm working on a logically simple ETL (fetch from a couple tables, bunch of joins, load into DB). The whole dataset is around 20gb in size.

I wanted to know if anyone has seen improved performance during the transform phase of their ETLs with the use of GPUs.

As far as I understand the increased parallel processing efficiency of GPUs makes a difference only when there are a large number of matrix operations, so I'm a little sceptical if simply shifting to GPUs will do the pipeline any good.

Would love to hear your thoughts on this, thanks!"
4461,2020-12-14 10:50:16,1607935816.0,dataengineering,Coded Bias,kcu1lr,illhamaliyev,,https://www.reddit.com/r/dataengineering/comments/kcu1lr/coded_bias/,1.0,5.0,0.0,22261.0,"Hi! Would anyone be willing to share how they are assessing their datasets for Fairness?

What is important to you in a data? 

How do you use the context of a dataset's collection?

When you find issues in your dataset, what do you do?

Thank you so much!"
4462,2020-12-14 14:08:01,1607947681.0,dataengineering,Is Google down ..?,kcwga9,ChemEngandTripHop,,https://www.reddit.com/r/dataengineering/comments/kcwga9/is_google_down/,1.0,15.0,0.0,22269.0,Sounds daft but I can't access anything on GCP and my colleagues are having the same experience
4463,2020-12-14 17:21:49,1607959309.0,dataengineering,Does anyone believe there will be a surge of niche data storage/warehouse tools in 2021?,kczl82,techbuzzwords,,https://www.reddit.com/r/dataengineering/comments/kczl82/does_anyone_believe_there_will_be_a_surge_of/,1.0,4.0,0.0,22277.0,"With the explosion of VC interest and M&amp;A in the data storage/warehousing space, I'm curious to see if anyone has thoughts on if we will see a lot more data storage/warehouses businesses get started that will focus on niche clients or industries. Would love if people are able to share those tools and what makes them more useful than a typical storage/warehouse setup."
4464,2020-12-14 17:50:03,1607961003.0,dataengineering,What are the must read books for data engineers?,kd03ve,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/kd03ve/what_are_the_must_read_books_for_data_engineers/,1.0,19.0,0.0,22278.0,"Hey everyone,

What are the books that you would recommend to read for a data engineer?

Thanks a lot for the suggestions in advance!"
4465,2020-12-14 20:02:53,1607968973.0,dataengineering,Data Catalogs Are Dead; Long Live Data Discovery,kd2qb8,mkvor8,,https://www.reddit.com/r/dataengineering/comments/kd2qb8/data_catalogs_are_dead_long_live_data_discovery/,1.0,20.0,0.0,22277.0,"Are data catalogs dead? For decades, data teams have relied on data catalogs to power data governance and metadata management, but the authors argue that they’re falling short for 3 main reasons:  

(1) lack of automation

(2) inability to scale with the growth and diversity of your data stack

(3) their undistributed format

Agree or disagree? What should/can (if anything) replace them when it comes to metadata management &amp; data quality? 

Would love to hear your thoughts...

[https://towardsdatascience.com/data-catalogs-are-dead-long-live-data-discovery-a0dc8d02bd34](https://towardsdatascience.com/data-catalogs-are-dead-long-live-data-discovery-a0dc8d02bd34)"
4466,2020-12-14 22:27:15,1607977635.0,dataengineering,Anyone use sqlpad.io to practice? What flavor of sql is it?,kd5r7o,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kd5r7o/anyone_use_sqlpadio_to_practice_what_flavor_of/,1.0,1.0,0.0,22284.0,"I’ve recently found out from a comment about sqlpad.io

Seems like a great resource to practice sql for DE interviews. But anyone that has used this platform, which sql flavor is the platform using? I can’t find anything about that on their website? MySQL, MS SQLServer, etc?"
4467,2020-12-14 22:54:39,1607979279.0,dataengineering,"Technical skill is often brought up, but what are some soft skills that are necessary to be a good data engineer?",kd6br7,SmashPingu,,https://www.reddit.com/r/dataengineering/comments/kd6br7/technical_skill_is_often_brought_up_but_what_are/,1.0,7.0,0.0,22285.0,"From my understanding a lot of newbies (myself definitely included) tend to focus more on technical skills and toolkits, but hearing from more experienced data professionals (like Joma on YT) soft skills are actually arguably more important when it comes to company impact and career progression. So what are some soft skills that every data engineer needs?"
4468,2020-12-15 01:41:47,1607989307.0,dataengineering,TensorFlow installation on AWS EC2,kd9ncl,Iffexibility1,,https://www.reddit.com/r/dataengineering/comments/kd9ncl/tensorflow_installation_on_aws_ec2/,1.0,0.0,0.0,22292.0,"Hello all,

I have been trying to deploy my deep learning model on AWS but I have been having issues installing TensorFlow on AWS, when I try, the installation gets killed and due to that the deployment has not been successful.

Please is there any way I can make it install?"
4469,2020-12-15 09:12:45,1608016365.0,dataengineering,Attendance platform on the edge,kdh0s5,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/kdh0s5/attendance_platform_on_the_edge/,1.0,0.0,0.0,22311.0,"Hi All,

We recently did a small POC on building an attendance platform on the edge. Also wrote a small blog on the same

[https://ibnipun10.medium.com/computing-on-the-edge-4575ad39c8f4](https://ibnipun10.medium.com/computing-on-the-edge-4575ad39c8f4)

Hope you like it"
4470,2020-12-15 09:16:48,1608016608.0,dataengineering,mid-level data engineering interview,kdh2l1,lalessandrorizzo,,https://www.reddit.com/r/dataengineering/comments/kdh2l1/midlevel_data_engineering_interview/,1.0,7.0,0.0,22311.0,I'm collating interview questions for a mid-level data engineering role. What topics would you cover? what questions would you ask?
4471,2020-12-15 09:29:17,1608017357.0,dataengineering,Data Science With R,kdh894,Techbiason,,https://www.reddit.com/r/dataengineering/comments/kdh894/data_science_with_r/,1.0,0.0,0.0,22311.0,
4472,2020-12-15 10:42:02,1608021722.0,dataengineering,Setting Up and Running Apache Kafka on Windows OS,kdi3l6,pareek-narendra,,https://www.reddit.com/r/dataengineering/comments/kdi3l6/setting_up_and_running_apache_kafka_on_windows_os/,1.0,0.0,0.0,22313.0,
4473,2020-12-15 16:06:04,1608041164.0,dataengineering,Become a data engineer on a shoestring,kdmbac,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kdmbac/become_a_data_engineer_on_a_shoestring/,1.0,21.0,0.0,22320.0,"I was tinkering with the idea of finding the right way to help others to identify the resources that give you bang for the buck when it comes to upskilling yourself in data engineering… and this is what I came up with. So this is how to spend the remainder of your learning budget before the year ends.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/become-a-data-engineer-on-a-shoestring](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/become-a-data-engineer-on-a-shoestring)

I hope you will find it useful. If you feel that I missed things, please comment below. Would be happy to compile a community edition."
4474,2020-12-15 18:31:54,1608049914.0,dataengineering,HackerRank or Leetcode for SQL prep?,kdoxth,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kdoxth/hackerrank_or_leetcode_for_sql_prep/,1.0,5.0,0.0,22321.0,"Which platform is better for prepping for sql portion of DE interviews for FANG? Would it be sufficient enough to just use leetcode?

[View Poll](https://www.reddit.com/poll/kdoxth)"
4475,2020-12-15 22:47:32,1608065252.0,dataengineering,I am at a Crossroads in my Career and would appreciate your Guidance / Thoughts,kdu54y,jak131,,https://www.reddit.com/r/dataengineering/comments/kdu54y/i_am_at_a_crossroads_in_my_career_and_would/,1.0,0.0,0.0,22331.0,"Hello,

As the title notes, I'm looking for your thoughts on my current situation, and how I'm navigating it

# First Job (~1 years)

Some background about me -- I am about 3 years out of Undergrad. I started in a technical consulting role where I was able to use and build my Python skills and do things like:

* build time-series forecasting algorithms for a PoC (ARIMA-based models)
* use angularJS to develop a dashboard for a planning tool
* build and maintain a data pipeline with complex logic (in terms of munging and business rules) for a planning tool (\~40M rows) 
* gain exposure to SQL &amp; window functions 

# Second Job (~2 years) 

From there I left and joined a less technical consulting role, focusing on ""strategy"". I left with the intention that this shift in work would help me decide whether I want to pursue a more technical role or a more business-oriented one

I didn't really develop my Python skills during this time, but I was exposed to the role of Data Architect when I assisted one in designing a Software Asset Management tool, using Neo4J as the backend. Almost everything else I did involved creating slides for presentations. Working with the Data Architect was the highlight of my time here, and I basically hated everything else. I think this was due to a mixture of company culture and my role

# Dilemma

I now know that I enjoyed working with Python, building data pipelines and munging data. I also know that I *might* want to become a Data Architect in the future, because I really enjoyed the design process. I have been talking to a few people I know, and they always say to get certified on one of the cloud platforms, so I started working towards the Data Engineer track on Microsoft Azure

What's stressing me out is when I search for jobs that relate to having this certificate, they're asking for 5-7 YoE, and they're listing so many technologies I haven't worked with (Airflow, Luigi, Spark, Databricks, Snowflake, Kafka, etc.). I feel out of my depth...

# Questions

The questions I could use some input on: 

1. Will me getting certified as an Azure Data Engineer help me find an entry level Data Engineer role or a low-level Senior Data Engineer role?
2. My coding background is WAY more hack-y than what I believe a Data Engineers should be. I don't really know the ""industry standard"" to setting up clean and organized APIs and Pipelines (I do know how to code efficiently, making use of parallel processing and column-wise manipulations to speed up operations). I currently feel like an imposter, and am wondering what is expected of a Data Engineer / Senior Data Engineer?
3. If you were hiring me, what would impress you and what would detract from my profile?

# Summary

I want to pivot my career from less-technical work to a Data Engineer role, with the eventual goal of becoming a Data Architect

This is a long post, so thank you to whomever stuck with it and offered their thoughts. I truly appreciate it :)"
4476,2020-12-15 22:51:46,1608065506.0,dataengineering,I am at a Crossroads in my Career and would appreciate your Guidance / Thoughts,kdu854,hmmwhatdoyouthinkabt,,https://www.reddit.com/r/dataengineering/comments/kdu854/i_am_at_a_crossroads_in_my_career_and_would/,1.0,3.0,0.0,22332.0,"Hello,

As the title notes, I'm looking for your thoughts on my current situation, and how I'm navigating it

**First Job (~1 years)**

Some background about me -- I am about 3 years out of Undergrad. I started in a technical consulting role where I was able to use and build my Python skills and do things like:
* build time-series forecasting algorithms for a PoC (ARIMA-based models)
* use angularJS to develop a dashboard for a planning tool
* build and maintain a data pipeline with complex logic (in terms of munging and business rules) for a planning tool (~40M rows)
gain exposure to SQL &amp; window functions

**Second Job (~2 years)**

From there I left and joined a less technical consulting role, focusing on ""strategy"". I left with the intention that this shift in work would help me decide whether I want to pursue a more technical role or a more business-oriented one

I didn't really develop my Python skills during this time, but I was exposed to the role of Data Architect when I assisted one in designing a Software Asset Management tool, using Neo4J as the backend. Almost everything else I did involved creating slides for presentations. Working with the Data Architect was the highlight of my time here, and I basically hated everything else. I think this was due to a mixture of company culture and my role

**Dilemma**

I now know that I enjoyed working with Python, building data pipelines and munging data. I also know that I might want to become a Data Architect in the future, because I really enjoyed the design process. I have been talking to a few people I know, and they always say to get certified on one of the cloud platforms, so I started working towards the Data Engineer track on Microsoft Azure
What's stressing me out is when I search for jobs that relate to having this certificate, they're asking for 5-7 YoE, and they're listing so many technologies I haven't worked with (Airflow, Luigi, Spark, Databricks, Snowflake, Kafka, etc.). I feel out of my depth...


**Questions**

The questions I could use some input on:

* Will me getting certified as an Azure Data Engineer help me find an entry level Data Engineer role or a low-level Senior Data Engineer role?
* My coding background is WAY more hack-y than what I believe a Data Engineers should be. I don't really know the ""industry standard"" to setting up clean and organized APIs and Pipelines (I do know how to code efficiently, making use of parallel processing and column-wise manipulations to speed up operations). I currently feel like an imposter, and am wondering what is expected of a Data Engineer / Senior Data Engineer?
* If you were hiring me, what would impress you and what would detract from my profile?


**Summary**

I want to pivot my career from less-technical work to a Data Engineer role, with the eventual goal of becoming a Data Architect
This is a long post, so thank you to whomever stuck with it and offered their thoughts. I truly appreciate it :)"
4477,2020-12-16 05:16:35,1608088595.0,dataengineering,"Career advice: promotion offered, but requires me to move teams and downgrade on tech stack",ke14xe,frodomtbaggins,,https://www.reddit.com/r/dataengineering/comments/ke14xe/career_advice_promotion_offered_but_requires_me/,1.0,12.0,0.0,22347.0,"Was asked to apply/move teams from a junior data engineer role to a senior role, but they're requiring me to move teams for the promotion because they claim they can't just promote me in place due to caps on roles in the org. 

Pros: Senior Title and ~$15k raise. 

Cons: deprecated tech stack and new team I don't know at all with largely simple problems (lift/land data, small volume, mainly rest API ingestion, with a lot of deprecated jobs to support)...leave behind spark Scala in favor of spring boot. 

I'm torn but feel like I'm leaning towards staying where I'm at. Any advice/experience from others who have gone through similar things? Are there things I'm missing from a high level/would this be a set back?"
4478,2020-12-16 07:23:45,1608096225.0,dataengineering,BigQuery -&gt; S3 using AWS Chalice (Overkill or no?),ke33a7,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/ke33a7/bigquery_s3_using_aws_chalice_overkill_or_no/,1.0,12.0,0.0,22354.0,"So the task is:

1. Query some data in BigQuery
2. Export as CSV
3. Move CSV to S3 (which is then used to drive a d3.js application)

The previous developer/engineer uses AWS Chalice to perform the above.

Question is: do we need to use Chalice to perform the above? No-one else is involved in dropping/exporting files from BQ to S3 other than myself. What is a better/simpler alternative?"
4479,2020-12-16 09:25:12,1608103512.0,dataengineering,Airflow UI feels really slow,ke4paa,theseus905,,https://www.reddit.com/r/dataengineering/comments/ke4paa/airflow_ui_feels_really_slow/,1.0,11.0,0.0,22361.0,"Hello all! I just started using Airflow, and I'm having a hard time knowing if this is an issue with my system, or just something with Airflow. When running airflow webserver, any request takes more than 5 seconds to load, and I don't know if that's normal or if something is wrong with my system. I only have one simple DAG with only two tasks. 

&amp;#x200B;

I currently have airflow 1.10.14. 

&amp;#x200B;

Any help would be greatly appreciated!"
4480,2020-12-16 11:19:28,1608110368.0,dataengineering,Hierarchical Filters on data,ke60ya,SprinkleData,,https://www.reddit.com/r/dataengineering/comments/ke60ya/hierarchical_filters_on_data/,1.0,0.0,0.0,22368.0,"Sprinkle now supports filters based on Hierarchy, in a nested fashion where one value depends on the other. This way, the values are filtered naturally which shows just the relevant values. Access the full article in through the following link [**Hierarchical Filters**](https://www.sprinkledata.com/docs/hierarchical-filters/index.html?utm_source=reddit_161220&amp;utm_medium=hierarchical)

https://preview.redd.it/xwahjx7vni561.png?width=1920&amp;format=png&amp;auto=webp&amp;s=fbb5c16b30f60a46babb09790cd9c7cd5bdbc08a"
4481,2020-12-16 11:57:41,1608112661.0,dataengineering,Apache Flink-powered Machine Learning model serving &amp; real-time feature generation at Razorpay,ke6gn9,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ke6gn9/apache_flinkpowered_machine_learning_model/,1.0,0.0,0.0,22370.0,
4482,2020-12-16 13:43:51,1608119031.0,dataengineering,The process of machine recognition with the biometric feature face,ke7qfp,Only_Klasiks,,https://www.reddit.com/r/dataengineering/comments/ke7qfp/the_process_of_machine_recognition_with_the/,1.0,0.0,0.0,22374.0,
4483,2020-12-16 14:39:24,1608122364.0,dataengineering,#idataengineer podcast 005 Leandro Loi,ke8gfw,soobrosa,,https://www.reddit.com/r/dataengineering/comments/ke8gfw/idataengineer_podcast_005_leandro_loi/,1.0,1.0,0.0,22374.0,"Leandro from Clue by Biowink talks about that we should focus less on the tools and more on the results.  


[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-005](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-005)"
4484,2020-12-16 15:52:03,1608126723.0,dataengineering,What Is Data Engineering and Is It Right for You? – Real Python,ke9jgn,brainfuckguru,,https://www.reddit.com/r/dataengineering/comments/ke9jgn/what_is_data_engineering_and_is_it_right_for_you/,1.0,13.0,0.0,22375.0,
4485,2020-12-16 18:04:44,1608134684.0,dataengineering,Normalizing API request data,kebw4b,vinsanity1603,,https://www.reddit.com/r/dataengineering/comments/kebw4b/normalizing_api_request_data/,1.0,7.0,0.0,22379.0,"I'm doing a project where I'm fetching data from an API call using Python, export it to csv, and feed it into a Postgres db. However, I want to normalize the data first. My question is do I normalize it in Python or do I do the normalization inside Postgres? This I'm really confused about. Hope someone can help."
4486,2020-12-16 18:46:56,1608137216.0,dataengineering,Python For Data Science,kecq1a,Techbiason,,https://www.reddit.com/r/dataengineering/comments/kecq1a/python_for_data_science/,1.0,0.0,0.0,22380.0,
4487,2020-12-16 19:33:02,1608139982.0,dataengineering,Architecture to store and fetch millions of images?,kedmwc,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kedmwc/architecture_to_store_and_fetch_millions_of_images/,1.0,2.0,0.0,22381.0,AWS is not an option due to company limitations. Need to be able to call api to be able to store and save an image on the cloud and also fetch. Need to be able to fetch images with relatively low latency from the cloud from our api application (Java) with millions of images stored. Any ideas appreciated
4488,2020-12-16 20:41:22,1608144082.0,dataengineering,$500 Course for Python with Mentor,kef0pe,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/kef0pe/500_course_for_python_with_mentor/,1.0,9.0,0.0,22382.0,"Hello everyone,

I'm having 10+ years of experience in IT(helpdesk/support). I have tried to learn Python multiple times through Udemy and failed to understand and write even a small piece of logic or program. Multiple times failed to implement in a use case what I learned. I have completed functions, But where to implement them is the question and failed to implement even if I know where to implement them.

My aim is to become Data Engineer with Good Python, SQL, and Pyspark Skills which I'm failing to do multiple times.

I'm depressed about don't to know to code on a day to day basis. Unable to handle the pressure inside me.

Even I'm paying 1:1, the Course structure cant is modified. the entire duration of the course is approx &lt; 60 hours. attaching course syllabus.

[https://docdro.id/U5mytMd](https://docdro.id/U5mytMd)

[https://docdro.id/hWtYm8P](https://docdro.id/hWtYm8P)

It is like do or die situation for me.  I have received multiple advice to take Data science courses to avoid coding. But I always wanted to code in my Org as it is small in size approx 7k to 8k on the whole. . I feel embarrassed about not knowing to code. So it is about proving myself to them. 

My concern is can someone validate the syllabus and help me with modifying the syllabus for me, please. Or some examples like use case like 

\*data preprocessing in data engineering like removing duplicates,source-target md5 check

\* load the data to the table and modifying the data or filtering the data

\*removing null values from the data

\*identify bad data and handle the data. 

Please add more scenarios or how I can google or get a good amount of scenarios. 

How can I ask the scenarios where I want to teach me or made me write a production-ready coder. I have seen freshers also writing a good amount of code."
4489,2020-12-16 21:08:14,1608145694.0,dataengineering,"DEs working for FAANG, what does your day to day look like?",kefk8u,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kefk8u/des_working_for_faang_what_does_your_day_to_day/,1.0,36.0,0.0,22382.0,Specifically asking data engineers that are currently working at a top tier tech company aka FAANG or similar tier. Do you do python development (build pipelines etc) or do you just do SQL queries all day and if so who are the queries for? Is it more SQL querying or developing pipelines using python? How much python do you use?
4490,2020-12-17 01:27:42,1608161262.0,dataengineering,Data Engineering Technology Tree,kektuv,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/kektuv/data_engineering_technology_tree/,1.0,0.0,0.0,22389.0,
4491,2020-12-17 07:10:11,1608181811.0,dataengineering,ALERT to all Data Scientist and Data Analyst,keqye8,[deleted],,https://www.reddit.com/r/dataengineering/comments/keqye8/alert_to_all_data_scientist_and_data_analyst/,1.0,0.0,0.0,22404.0,
4492,2020-12-17 07:12:13,1608181933.0,dataengineering,ALERT to all Data Scientists and Data Analysts,keqzjg,sharavanan_,,https://www.reddit.com/r/dataengineering/comments/keqzjg/alert_to_all_data_scientists_and_data_analysts/,0.0,2.0,0.0,22404.0,"To every Data Analysts and Data Engineers, here a blog on the [Best Books To Read in 2021](https://www.sprinkledata.com/docs/best-books-for-data-analysts-and-data-engineers/index.html?utm_source=Reddit_171120&amp;utm_medium=Bestbook2021)"
4493,2020-12-17 09:49:32,1608191372.0,dataengineering,5 Cool Python Projects for Beginners,ket53m,SnooConfections5700,,https://www.reddit.com/r/dataengineering/comments/ket53m/5_cool_python_projects_for_beginners/,1.0,0.0,0.0,22410.0,
4494,2020-12-17 12:23:19,1608200599.0,dataengineering,"Consumer Identity Management for the CMO, CISO, and CIO",keuz50,LynnCobos,,https://www.reddit.com/r/dataengineering/comments/keuz50/consumer_identity_management_for_the_cmo_ciso_and/,1.0,0.0,0.0,22414.0,
4495,2020-12-17 15:42:08,1608212528.0,dataengineering,[Help] A few minutes to fill an Academic Research Questionnaire,kexngs,sanjulamadurapperuma,,https://www.reddit.com/r/dataengineering/comments/kexngs/help_a_few_minutes_to_fill_an_academic_research/,1.0,0.0,0.0,22416.0,
4496,2020-12-17 19:59:37,1608227977.0,dataengineering,Apache Airflow 2.0 is here!,kf2gy7,gman1023,,https://www.reddit.com/r/dataengineering/comments/kf2gy7/apache_airflow_20_is_here/,1.0,2.0,0.0,22424.0,
4497,2020-12-17 20:23:58,1608229438.0,dataengineering,New Release KafkaIDE 2020.12.4 - Where conditions are here!,kf2zam,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/kf2zam/new_release_kafkaide_2020124_where_conditions_are/,3.0,0.0,0.0,22427.0,"We are very excited to announce that [version 2020.12.4](https://kafkaide.com/download/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=release-2020.12.4&amp;position=top&amp;utm_term=dataengineering) is available from today.

Filtering records based on specific constraints is a must-have when dealing with large datasets.

**Introducing Where conditions**

Every aspect has been designed for performance and ease of use. Like SQL, you can now filter incoming records based on their type, metadata, and value.

Filtering based on date and number ranges, regular expression matching, boolean masking, null handling, and more is now possible.

As simple as pulling the field you want to use as a filter and dropping it into the new Where area. A dialog will appear where you can specify your constraints.

**Detailed release notes**

* Where conditions
* UI Forms have been greatly revamped to appear more simple and easier to use.
* Some UI widgets have been adjusted to be easier to read.
* Rendering performance greatly improved. Animations run now run much smoother.
* Fixed an issue where the connection context menu was not visible if the connection menu widget was at the bottom of the window.

**We will be happy to assist you!**

If you experience any technical issues with Kafka IDE, you can leave a message in this post or report while using the app.

As usual, you can download the latest version at [kafkaide.com/download](https://kafkaide.com/download/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=release-2020.12.4&amp;position=bottom&amp;utm_term=dataengineering)."
4498,2020-12-17 20:52:12,1608231132.0,dataengineering,Apache Airflow 2.0 is here!,kf3jrn,[deleted],,https://www.reddit.com/r/dataengineering/comments/kf3jrn/apache_airflow_20_is_here/,1.0,0.0,0.0,22429.0,
4499,2020-12-17 20:53:22,1608231202.0,dataengineering,Apache Airflow 2.0 Released,kf3kr1,kaxil_naik,,https://www.reddit.com/r/dataengineering/comments/kf3kr1/apache_airflow_20_released/,3.0,20.0,0.0,22429.0,
4500,2020-12-17 21:04:06,1608231846.0,dataengineering,Airflow 2.0 has been released,kf3ssg,make_at_night,,https://www.reddit.com/r/dataengineering/comments/kf3ssg/airflow_20_has_been_released/,2.0,0.0,0.0,22430.0,
4501,2020-12-18 00:44:36,1608245076.0,dataengineering,Goldman Sachs Data Engineering,kf86ay,sneha20393,,https://www.reddit.com/r/dataengineering/comments/kf86ay/goldman_sachs_data_engineering/,2.0,5.0,0.0,22436.0,"Hi All! 
I have an upcoming coderpad interview with Goldman Sachs for Data Engineer position.
Can anyone please share interview experience? 
They said it is going to be SQL. What difficulty level should I expect?"
4502,2020-12-18 02:25:50,1608251150.0,dataengineering,"[help] Moving DB events -&gt; Airflow, need some advice",kfa10g,12mZBmBJMy6VnzkJhjXn,,https://www.reddit.com/r/dataengineering/comments/kfa10g/help_moving_db_events_airflow_need_some_advice/,2.0,3.0,0.0,22439.0,"Morning, guys and girls. I'm in the process of moving our data warehouse events to airflow jobs and want some advice on best practice. 

## Current Setup
At the moment, we have a daily snapshot and hourly incremental tasks. The daily snapshots handle our large tables which don't need to be kept up to date every hour. At this stage I'm only looking to put the daily snapshot into airflow as a bit of a proof of concept. 

### Daily Snapshot
Our daily snapshot gets called at 1am and takes about 45 minutes to run for all facts and dimensions. The setup is we have a DB event which calls a stored procedure called `sp_daily_snapshot`. In this stored proc we have around 15 other stored procs for each fact and/or dimension. At the start of the daily snapshot sp, we write to a log table that the job has started and at the end it will update that row with a complete time. 


#### Approach
The simplest way would be to just do a basic airflow dag which calls the `sp_daily_snapshot` stored procedure but I expect there is a better way to do this. 

Any suggestions?"
4503,2020-12-18 03:26:56,1608254816.0,dataengineering,Dell Boomi for both operational system integrations and data analytics platform?,kfb3ly,EatGreatEvnLate,,https://www.reddit.com/r/dataengineering/comments/kfb3ly/dell_boomi_for_both_operational_system/,1.0,0.0,0.0,22441.0,"I have a few questions, and wondering if this is the right place to ask them.

* Is there a best practice around separating/mixing  operational system integrations from data analytics ETL processes?
* Anyone have experience with Dell Boomi I could bounce some questions off of?
* Our integrations team is looking to move on a tool but our data analytics team and strategy is in its infancy, will there be problems for our analytics team if we get locked into a do-it-all tool like Dell Boomi?
* Is Boomi worth it's price vs hiring developers and rolling our own with open source tools or making use of SaaS cloud options?

Thanks"
4504,2020-12-18 10:55:39,1608281739.0,dataengineering,Google DataFlow &amp; Apache Beam,kfhmm2,Substantial-Fudge-33,,https://www.reddit.com/r/dataengineering/comments/kfhmm2/google_dataflow_apache_beam/,1.0,1.0,0.0,22456.0,I am preparing a small demo Apache Beam and it seems that Google DataFlow was launched in 2015 whereas Apache Beam only in 2016. Is that correct? I was expecting that DataFlow was launched later
4505,2020-12-18 12:03:36,1608285816.0,dataengineering,How to do Data Visualization in Python for Data Science,kfifw7,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kfifw7/how_to_do_data_visualization_in_python_for_data/,1.0,0.0,0.0,22461.0,
4506,2020-12-18 15:58:29,1608299909.0,dataengineering,Udacity Nanodegree DE Project Files,kflkpq,vinsanity1603,,https://www.reddit.com/r/dataengineering/comments/kflkpq/udacity_nanodegree_de_project_files/,2.0,9.0,0.0,22471.0,"Hi guys, not sure if this is an acceptable post here. But, just would like to ask ppl in this sub who finished or taking the udacity nanodegree DE course if they still have the empty project file template with them. Just the project files itself (unanswered), not the course content itself. Would definitely appreciate it!!"
4507,2020-12-18 19:48:22,1608313702.0,dataengineering,The Broken Data Pipeline Before Christmas,kfpvme,mkvor8,,https://www.reddit.com/r/dataengineering/comments/kfpvme/the_broken_data_pipeline_before_christmas/,13.0,6.0,0.0,22477.0,"'Twas the night before Christmas and all through the house, 

Not a creature was stirring, not even... your data pipelines? 

[https://barrmoses.medium.com/the-broken-data-pipeline-before-christmas-4d42c2a8f054?source=friends\_link&amp;sk=f4be0d403403ba58fdee3e50428b3b75](https://barrmoses.medium.com/the-broken-data-pipeline-before-christmas-4d42c2a8f054?source=friends_link&amp;sk=f4be0d403403ba58fdee3e50428b3b75)"
4508,2020-12-18 20:03:43,1608314623.0,dataengineering,Open sourced opportunity for early stage data engineers and first time contributors,kfq6pr,JohnnyQScope,,https://www.reddit.com/r/dataengineering/comments/kfq6pr/open_sourced_opportunity_for_early_stage_data/,0.0,3.0,0.0,22478.0,"I see a lot of beginners asking how to get experience with software development. This project contains a few moving parts that I think would be fairly useful to learn when just starting out such as testing, automation, and releasing. The repository is heavily geared towards learning and all skill levels are welcome!

[SimpleLearn](https://pypi.org/project/simple-learn/) - The actual project is focused on trying to eliminate the need for math/stats knowledge when it comes to data modeling or other ML engineering tasks."
4509,2020-12-18 20:08:09,1608314889.0,dataengineering,Can you use Hadoop as a data lake and data warehouse?,kfq9xn,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kfq9xn/can_you_use_hadoop_as_a_data_lake_and_data/,2.0,10.0,0.0,22478.0,"where I currently work, we use an on prem server that archives all files structured or unstructured. Then we ingest These files into Hadoop (hive tables) which are structure. So I guess in that sense it would mean that, that on prem server is our data lake and Hadoop our data warehouse since it has structure?"
4510,2020-12-18 22:00:18,1608321618.0,dataengineering,Data Wrangling - Data Signals,kfsfu7,[deleted],,https://www.reddit.com/r/dataengineering/comments/kfsfu7/data_wrangling_data_signals/,1.0,0.0,0.0,22482.0,
4511,2020-12-18 22:03:27,1608321807.0,dataengineering,What are Data Signals?,kfsi7v,conradvilan76,,https://www.reddit.com/r/dataengineering/comments/kfsi7v/what_are_data_signals/,1.0,3.0,0.0,22482.0,"Hi guys, I've just started studying up data wrangling and I came upon these 2 terms (Data Signals and Features)

I get that **Features** are like extra inputs/columns for my dataset to make the dataset more detailed and helps remove dirty data. 

And based on what I'm reading it seems that **Data Signals** are the dataset's patterns. And that there are 2 parts to this, which are Signals and Noise, with Noise being the dirty data.

My final question is: Are there any methods/algorithms used for Data Signals when it comes to Data Wrangling? Or is it just a term?

I would really appreciate it if anyone could shed some light on this. Thank you"
4512,2020-12-18 22:13:31,1608322411.0,dataengineering,Can some data engineers take a look at my GitHub and give me some feedback?,kfsozs,Bgreg10,,https://www.reddit.com/r/dataengineering/comments/kfsozs/can_some_data_engineers_take_a_look_at_my_github/,7.0,14.0,0.0,22483.0,"I’m in the final year of my A levels and hope to get some sort of junior/intern positions as a data engineer next year. I’ve been learning SQL, Python, R and databases (SQL and NoSQL) and have made a GitHub with some of my personal projects on it. 

It’s nothing amazing, just some DAGs made with Airflow and some simple ETL scripts showing how to interact with AWS in Python. I’d really appreciate it if someone could take a look and let me know what they think - what I could improve upon and more importantly what I should learn next to help progress in this field. 

GitHub repo: https://github.com/tomwelch2/Data-Engineering-Projects"
4513,2020-12-19 02:35:22,1608338122.0,dataengineering,Career Help in Data Engineering,kfxise,ogyacub,,https://www.reddit.com/r/dataengineering/comments/kfxise/career_help_in_data_engineering/,1.0,30.0,0.0,22487.0,"My question is, how do land a job ideally entry level position in data engineering. I also can't seem to find any entry level positions on the market. 

Quick background:
I am looking to transition from a non IT/Tech field to Data Engineering. Over the past couple of months I have taken some python classes on Udemy and plan on taking the PCAP certification which I'm studying for now. 
I also took SQL, Pyspark and Hadoop classes on Udemy to further my knowledge in data engineering. Finally I also have my AWS Solutions Architect Certificate. 

Is there anything I need to do to get my first job as a data engineer? 
If anyone became a data engineer form an IT or non IT background, how did you do it? I'm looking to learn and grow and build a career in this field."
4514,2020-12-19 07:53:26,1608357206.0,dataengineering,Azure Data Factory Data Flows vs. Databricks cost - ADF costs more,kg2d9c,1010101100111,,https://www.reddit.com/r/dataengineering/comments/kg2d9c/azure_data_factory_data_flows_vs_databricks_cost/,2.0,47.0,0.0,22491.0,"We've been experimenting with both ADF Data Flows and Databricks for data transformation work. What we're finding is that the same workload in ADF costs more (1 million unordered rows, ordered alphabetically). It appears the same, even for small jobs of 1000 rows. I think ADF Dataflows, is categorically more expensive.

With that in mind, we are thinking of moving our existing pipelines to Databricks. Monthly we end up saving several thousand pounds.

Is anyone else experienced in pure Databaricks, is there something that we are not considering?"
4515,2020-12-19 08:06:05,1608357965.0,dataengineering,How much does linux certifications ( Linux+ or Red Hat) matter when it comes to getting hired as as a data engineer?,kg2jcr,mcfryme,,https://www.reddit.com/r/dataengineering/comments/kg2jcr/how_much_does_linux_certifications_linux_or_red/,3.0,17.0,0.0,22491.0,"Differently put, How qualified do you need to be in linux to be a competitive DE."
4516,2020-12-19 11:04:31,1608368671.0,dataengineering,The Most Popular Programming Languages - 1965/2020 - Statistics and Data,kg4kjg,accappatoiviola,,https://www.reddit.com/r/dataengineering/comments/kg4kjg/the_most_popular_programming_languages_19652020/,1.0,0.0,0.0,22492.0,
4517,2020-12-19 14:26:59,1608380819.0,dataengineering,FB Data Engineer Coding Interview,kg6wcq,sokoono,,https://www.reddit.com/r/dataengineering/comments/kg6wcq/fb_data_engineer_coding_interview/,2.0,20.0,0.0,22498.0,"Hey y’all! I have an interview for Facebook in January for a Data Engineer Position. Anyone else have an interview and want to do some coding prep and mock interviews? It will be in SQL and Python. I have 10 YOE with SQL and 3 YOE with Python.

I also have an Amazon Interview in January for a Business Intelligence Engineer position. I know that’s not related to this sub but if anyone else wants to prep for that, I’m down too!"
4518,2020-12-19 18:16:29,1608394589.0,dataengineering,anyone here working with an old technology stack?,kgam76,rotterdamn8,,https://www.reddit.com/r/dataengineering/comments/kgam76/anyone_here_working_with_an_old_technology_stack/,1.0,23.0,0.0,22499.0,"I recently interviewed for an ""analyst"" position, but when I talked to them I found their ETLs are primarily in Linux shell scripts. The role is 80% developing shell scripts and 20% SQL troubleshooting.

I asked the hiring manager ""why shell scripts?"", and he had only been there one year, so this is what he inherited. Also, it's a food distribution company.  Not exactly tech-savvy. 

I actually love using Linux but this just seemed to be the wrong direction to go in. Not that a company needs to be using the latest and greatest, and they seem like nice people, but....you know what I'm saying, right?"
4519,2020-12-19 20:41:49,1608403309.0,dataengineering,Niche in the intersection of Data Engineering and privacy/security?,kgdjjk,slavetoastory,,https://www.reddit.com/r/dataengineering/comments/kgdjjk/niche_in_the_intersection_of_data_engineering_and/,1.0,3.0,0.0,22503.0,"Hey all. I've been a Data Engineer for a bit over 3 years now, and am interested in starting to specialize a bit more. Specifically I'm curious about learning a lot more about Data Engineering as it relates to creating data infrastructure that is secure and is built to protect the privacy of the people that provide the data. Privacy is a subject close to my heart, and I want to use my powers for good, so to speak. I want to stay completely on the technical side of things, involving more knowledge about anonymization, encryption, networks, etc. in my work. I also see a need for this kind of knowledge at my current employer, so a transition like this could prove valuable to them as well.

So, question is: does this niche have a name at all? Are there certifications or reputable learning resources in this area? Any other advice that could help me head down this direction?

Thank you"
4520,2020-12-19 23:08:53,1608412133.0,dataengineering,Cloud Networking concepts for Data Engineer job roles,kggd0k,johnreese421,,https://www.reddit.com/r/dataengineering/comments/kggd0k/cloud_networking_concepts_for_data_engineer_job/,1.0,9.0,0.0,22505.0,"* Can someone guide me , how much depth and knowledge of networking concepts does a data engineer require ?
* Any learning material/tutorials to go through to get good at it ? Basically, something for dummies level."
4521,2020-12-20 05:16:02,1608434162.0,dataengineering,Download Linux,kgmhq9,SurendarCreme24,,https://www.reddit.com/r/dataengineering/comments/kgmhq9/download_linux/,1.0,1.0,0.0,22511.0,
4522,2020-12-20 14:20:46,1608466846.0,dataengineering,questions regarding the construction of a data warehouse,kgt4pl,THROWAWAY_de00,,https://www.reddit.com/r/dataengineering/comments/kgt4pl/questions_regarding_the_construction_of_a_data/,1.0,20.0,0.0,22526.0,"Hello everyone,

I've been looking into building dimension and fact tables from business cases I find online in order to learn more about data warehouses and the actual process used in companies, but I keep getting stuck on creating the dimension tables.

**Example** :  (I will try to keep the example as generic as possible)

Suppose we have a company selling a bunch of products, that provided us with information about the customers(id, name, nationality...) and their buying tendencies, employees(their age, salary, nationality of employees...), products and where they're being sold(physical shops in some countries, or online shops), orders, suppliers...etc.

We want to build a DWH that answers the following questions :

\-when it comes to the customers, we are only interested in the gender of a customer and his  profitability 

\- For employees, we are interested to filter employee performance by gender, age, salary

\- products that sell the most, and in what continent

My **questions** :  

**1)** Do we build the DWH using only the attributes mentioned in the specifications above, or do we add attributes that have potential for future analysis? So for example, for customers we were asked to filter them by gender and profitability, so we wouldn't want to add nationality or their buying tendencies of customers into the mix, right? even if that information might be interesting in the future.

**2)** Would access be faster/ more optimized if we built a dimension for each of the filtering criteria? for example : since we ask for employee performance by age, gender, salary... can we build a gender dimension, and reference the gender via an ID in fact table?

**3)** Can we do the same for age and salary, by determining an interval of values and then in the age / salary dimension table, we would have entries like : ID , Interval of values, value, where value belongs to Interval of values. Or should we just add age and salary to the fact table(this seems more messy), but it also seems messy to have a bunch of dimension tables with just 2 - 3 attributes. Or am I just getting confused on what dimensions to pick? Is there more to it(like an aspect of granularity that I should be considering?)

Thanks,"
4523,2020-12-20 14:54:31,1608468871.0,dataengineering,You may want to try these top classes,kgtj8q,poonddetatte,,https://www.reddit.com/r/dataengineering/comments/kgtj8q/you_may_want_to_try_these_top_classes/,1.0,0.0,0.0,22526.0,
4524,2020-12-20 18:13:22,1608480802.0,dataengineering,"Is ""97 Things Every Data Engineer Should Know"" a real thing?",kgwoe0,L1MB0_,,https://www.reddit.com/r/dataengineering/comments/kgwoe0/is_97_things_every_data_engineer_should_know_a/,1.0,7.0,0.0,22531.0,"Who knows what is this book ""97 Things Every Data Engineer Should Know""? 

It appears on [amazon](https://www.amazon.de/Things-Every-Data-Engineer-Should/dp/109811504X)/[goodreads](https://www.goodreads.com/book/show/50025727-97-things-every-data-engineer-should-know)/google books, but rather than title and cover there is no any additional information... is it a real thing??

Is it somehow related to [Tobias Macey's in-progress book](https://docs.google.com/forms/d/e/1FAIpQLSdAsJjYx6zU8SdajqKv3YiT78kS-JloDxaIopNsBtbHW36xOg/viewform) with the same name?"
4525,2020-12-20 18:27:37,1608481657.0,dataengineering,The Most Popular Programming Languages - 1965/2020 - Statistics and Data,kgwxjf,accappatoiviola,,https://www.reddit.com/r/dataengineering/comments/kgwxjf/the_most_popular_programming_languages_19652020/,1.0,0.0,0.0,22531.0,
4526,2020-12-20 22:10:32,1608495032.0,dataengineering,"The 22nd edition of @data_weekly focuses on @DatakinHQ OpenLineage, @LinkedIn metadata day, @Microsoft metadata mgmt,@alibaba_cloud real-time data warehouse, @Uber no-code workflow, @SlackHQ react logging lib,@LinkedIn Corel,@netflix ML content decision",kh19yx,vananth22,,https://www.reddit.com/r/dataengineering/comments/kh19yx/the_22nd_edition_of_data_weekly_focuses_on/,1.0,0.0,0.0,22536.0,
4527,2020-12-21 00:29:14,1608503354.0,dataengineering,What do you do to invalid data in your pipeline,kh3vid,theslay,,https://www.reddit.com/r/dataengineering/comments/kh3vid/what_do_you_do_to_invalid_data_in_your_pipeline/,1.0,12.0,0.0,22539.0,What actions do you take when you have data that fails some tests in your data pipeline. And how do you do it?
4528,2020-12-21 04:09:19,1608516559.0,dataengineering,"Cost comparisons among Kinesis, MSK and Confluent Cloud (on AWS)?",kh7qfx,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/kh7qfx/cost_comparisons_among_kinesis_msk_and_confluent/,1.0,5.0,0.0,22545.0,"I was trying to estimate the cost of build a stream pipeline using different options on the table.


On Confluent Cloud cost estimator page 
I assume I have a 2MB/s write to the stream pipeline and a 2MB/s read from the stream
 - ‘region’ —&gt; ‘US-East-1’
 - ‘cluster’ —&gt; ‘basic’ 
 - ‘availability’  —&gt; ‘multi-zone’
 - ‘storage’ —&gt;100GB
 - partitions —&gt; 3

The cost is $1365

On the Kinesis cost estimator page:
 - I assume the above 2MB/s as 1024 msg/s * 2k / msg 
 - used the default (24hr) retentions 
 - number of consumer app —&gt; 1
 - number of enhanced fan out—&gt; 1

The Kinesis stream cost is $148

Why there is such a 10x cost difference? What Confluent cloud gives me extra by charging 10x more?"
4529,2020-12-21 04:57:21,1608519441.0,dataengineering,How to Learn Python For Data Science,kh8k3g,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kh8k3g/how_to_learn_python_for_data_science/,1.0,0.0,0.0,22548.0,
4530,2020-12-21 06:10:30,1608523830.0,dataengineering,How to Learn Python For Data Science,kh9r9v,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kh9r9v/how_to_learn_python_for_data_science/,1.0,0.0,0.0,22550.0,
4531,2020-12-21 10:45:53,1608540353.0,dataengineering,First steps with AWS Batch,khdh58,commonsnook,,https://www.reddit.com/r/dataengineering/comments/khdh58/first_steps_with_aws_batch/,1.0,0.0,0.0,22556.0,
4532,2020-12-21 11:55:38,1608544538.0,dataengineering,QnA HLD round for data engineering profile.,khec83,CupGroundbreaking334,,https://www.reddit.com/r/dataengineering/comments/khec83/qna_hld_round_for_data_engineering_profile/,1.0,1.0,0.0,22556.0,"Hi Guys, I have an interview scheduled at one of the product-based companies for a data engineer profile.

I have passed my 2 coding rounds and now the 3 rd round is ""Q n A - HLD"" i am not able to understand what type of questions I can expect in this round because If it's related to pipeline building i don't have any workflow tool work experience in my current company we have one of its own etl frameworks can this be a blocker for me in getting this job and if there is something which anybody can suggest for practice or any reference where i can learn anything."
4533,2020-12-21 13:15:36,1608549336.0,dataengineering,Is data modelling (star schema etc.) no longer valid?,khffad,1010101100111,,https://www.reddit.com/r/dataengineering/comments/khffad/is_data_modelling_star_schema_etc_no_longer_valid/,1.0,40.0,0.0,22557.0,"I keep hearing that with modern platforms performance is no longer an issue and with that, creating, managing and maintaining data models is technical debt - is this true in anyones experience?"
4534,2020-12-21 18:19:29,1608567569.0,dataengineering,Processing 12 trillion rows on Black Friday,khkhuo,xlpz,,https://www.reddit.com/r/dataengineering/comments/khkhuo/processing_12_trillion_rows_on_black_friday/,1.0,0.0,0.0,22565.0,
4535,2020-12-21 18:40:12,1608568812.0,dataengineering,What is the best way to report from firebase on a regular cadence?,khkw4n,inmanenz,,https://www.reddit.com/r/dataengineering/comments/khkw4n/what_is_the_best_way_to_report_from_firebase_on_a/,1.0,0.0,0.0,22565.0,"I have an application that stores its data in firebase. Each week, I'd like to pull down current data and feed it to Python/some data analysis tool to generate a set of reports.   


Here is my current, very clunky, workflow...

1. Export full collections from firebase into google cloud storage bucket (exports in leveldb format)
2. Download leveldb from google cloud to disk
3. [Parse](https://github.com/Venryx/firestore-leveldb-tools) leveldb into json. Requires python 2.7.
4. Use python 3/pandas/etc to take json and generate reports.

This works, but each week I'm needlessly reexporting old data that I've already exported in prior weeks. And in general this feels overkill for what I imagine is a common need.

Have any of you tackled anything similar? Any better ways of doing this?

Thanks!"
4536,2020-12-21 19:23:15,1608571395.0,dataengineering,"Math/Stats degree, no engineering experience. What jobs should I apply for?",khlqba,Savings_Tonight_568,,https://www.reddit.com/r/dataengineering/comments/khlqba/mathstats_degree_no_engineering_experience_what/,1.0,0.0,0.0,22564.0,"Education:
Got a Stats degree from UOFT 8 months ago.
Basic knowledge of SQL, Python and data viz stuff.
Currently doing a Data Warehouse course on Coursera
Starting Jr data analyst boot camp at Npower in Jan.

Work experience:
No internships. (Did car sales for 3 years throughout university)
Currently working as a data analyst (Use a little bit of SQL but mostly data entry.) 

What jobs should I apply for if I want to get into Data engineering?
I feel that my current qualifications are not competitive enough to get into data engineering straight away."
4537,2020-12-21 19:30:23,1608571823.0,dataengineering,Statistics major. No internships. How to get into Data Engineering ?,khlv31,Savings_Tonight_568,,https://www.reddit.com/r/dataengineering/comments/khlv31/statistics_major_no_internships_how_to_get_into/,1.0,8.0,0.0,22564.0,"Education:

* Got a Stats degree from a university in Canada 8 months ago.
* Basic knowledge of SQL, Python and data viz stuff.
* Currently doing a Data Warehouse course on Coursera.
* Starting Jr data analyst boot camp at Npower in Jan.


Work experience:

* No internships. (Yea I realized I fked up) 
* Did car sales for 3 years throughout university
* Currently working as a data analyst (Use a little bit of SQL but mostly data entry.) 


What jobs should I apply for if I want to get into Data engineering?

* I feel that my current qualifications are not competitive enough to get into data engineering straight away."
4538,2020-12-21 19:52:39,1608573159.0,dataengineering,How to Parse dbt Artifacts,khma5o,laguitte,,https://www.reddit.com/r/dataengineering/comments/khma5o/how_to_parse_dbt_artifacts/,1.0,4.0,0.0,22567.0,
4539,2020-12-21 20:00:24,1608573624.0,dataengineering,Almost done with my DE Project. Mind Taking a look?,khmfqi,TheKoalaKeys,,https://www.reddit.com/r/dataengineering/comments/khmfqi/almost_done_with_my_de_project_mind_taking_a_look/,1.0,33.0,0.0,22567.0,"Hey, everyone! I am in the last part of finalizing my first DE project. I've been working on this for a while now trying to make it the best I possibly can. I know it is far from perfect but, I am super proud of how it turned out and I wanted to share it here so I could get some feedback. I am still new to the field so keep that in mind while you're roasting me for my code not being the work of a god haha.

&amp;#x200B;

I welcome any and all feedback!

&amp;#x200B;

[https://github.com/dylanzenner/business\_closures\_de\_pipeline](https://github.com/dylanzenner/business_closures_de_pipeline)"
4540,2020-12-21 20:03:21,1608573801.0,dataengineering,Python Set/Library of Standard ETL Operations,khmhvp,ahornic,,https://www.reddit.com/r/dataengineering/comments/khmhvp/python_setlibrary_of_standard_etl_operations/,1.0,4.0,0.0,22567.0,"Hi Community,
Coming from tradional data engineering via SQL Scripts/ODI/BODI/SSIS i would like to dive into python/airflow based data engineering.

Is there a repo with basic best practice python data operations like load from file/DB, Filtering, join/merge, write to file/DB etc. where i can start from?

Any other ressources or blogs you can recommend?


Thank you!"
4541,2020-12-21 21:00:11,1608577211.0,dataengineering,Why Your Metadata is (Probably) Useless,khno8o,mkvor8,,https://www.reddit.com/r/dataengineering/comments/khno8o/why_your_metadata_is_probably_useless/,1.0,0.0,0.0,22568.0,"Lineage = eye candy and fodder for great product demos, but it's only useful unless it's applied to a clear use case. Thoughts? 

[https://towardsdatascience.com/metadata-is-useless-535e43311cd8?source=friends\_link&amp;sk=4a6ae3575a5c6eae61ed6f153610a799](https://towardsdatascience.com/metadata-is-useless-535e43311cd8?source=friends_link&amp;sk=4a6ae3575a5c6eae61ed6f153610a799)"
4542,2020-12-21 23:55:21,1608587721.0,dataengineering,Is there any shot for someone without a degree?,khr8g6,SteezeNYC,,https://www.reddit.com/r/dataengineering/comments/khr8g6/is_there_any_shot_for_someone_without_a_degree/,1.0,14.0,0.0,22573.0,"No degree in any subject (I plan to finish my CS degree AS SOON as I land a job), but I've been working towards breaking into IT since last year and got A+ and Network+. I'm working on AWS certs now and a few days away from Solutions Architect associate, and possibly  their Data Analytics and Developer certs, and creating a rest API project in python and deploying it to AWS. I'm close to being finished with this but I CANNOT shake the feeling of despair at the fact that I'm possibly wasting my time and could end up without a job anyway. The advice is mixed. I need to do something now because it kills me that all my peers have degrees and got through the pandemic relatively unscathed, I want that security for myself. (Sorry to vent at the end, just a bit of context)"
4543,2020-12-22 00:33:14,1608589994.0,dataengineering,Metadata is Useless — Unless You Have a Use Case,khrz6b,mkvor8,,https://www.reddit.com/r/dataengineering/comments/khrz6b/metadata_is_useless_unless_you_have_a_use_case/,1.0,0.0,0.0,22574.0,
4544,2020-12-22 00:47:35,1608590855.0,dataengineering,An interview with the YipitData team about how they built a self service platform for building analytics products on alternative data sets to power investment strategies.,khs8jj,blarghmatey,,https://www.reddit.com/r/dataengineering/comments/khs8jj/an_interview_with_the_yipitdata_team_about_how/,1.0,0.0,0.0,22574.0,
4545,2020-12-22 01:15:55,1608592555.0,dataengineering,Designing a Laboratory Information Management System (LIMS) need some advice,khsrmf,kile22,,https://www.reddit.com/r/dataengineering/comments/khsrmf/designing_a_laboratory_information_management/,1.0,3.0,0.0,22576.0,"This post might be slightly outside the scope of DE, but I feel it's the best forum for my question. This question has lot's of hats involved, data science, data engineering, database admin, and webdev.

I've been tasked with basically designing a LIMS platform at work. We are small group that does automated experiments. I have pretty much full control of the tech stack, within reason. We use LabView for talking to the equipment and as much post-processing as we can do with it. The raw data side of it isn't really my concern. My question is more about consuming and displaying the data later.

We're more of a hack at it until it works kind of group, but this is going to be an ongoing project with a lot of eyes on it from upper management, so I don't want to screw it up. I don't know a lot about database design.

I'm currently using MongoDB as the central store for data and have a MERN stack dashboard built with a GraphQL layer to consume it. (That is if IT could figure out how to give me a webserver.) I love the flexibility of Mongo and a lot webdev stuff uses it. Is there any danger in going this route, especially for long-term maintenance? Am I missing out by not using SQL and designing a schema upfront? Should I use MongoDB for raw data (I need to store arrays) and process it into a SQL database?

We're also in the beginning stages so now is the time to make changes."
4546,2020-12-22 05:28:01,1608607681.0,dataengineering,Data Analytics Tools For Beginners,khx30n,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/khx30n/data_analytics_tools_for_beginners/,1.0,0.0,0.0,22584.0,
4547,2020-12-22 05:30:06,1608607806.0,dataengineering,Data Analytics Tools For Beginners,khx49w,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/khx49w/data_analytics_tools_for_beginners/,1.0,0.0,0.0,22584.0,
4548,2020-12-22 05:33:39,1608608019.0,dataengineering,Data Analytics Tools For Beginners,khx6ck,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/khx6ck/data_analytics_tools_for_beginners/,1.0,0.0,0.0,22585.0,
4549,2020-12-22 05:52:19,1608609139.0,dataengineering,Data Analytics Tools For Beginners,khxh8e,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/khxh8e/data_analytics_tools_for_beginners/,1.0,0.0,0.0,22585.0,
4550,2020-12-22 10:19:08,1608625148.0,dataengineering,salted SQL aggregations in Spark/Distributed Computing,ki1a2a,TKTheJew,,https://www.reddit.com/r/dataengineering/comments/ki1a2a/salted_sql_aggregations_in_sparkdistributed/,1.0,0.0,0.0,22586.0,
4551,2020-12-22 15:36:55,1608644215.0,dataengineering,where can i get some real training ?,ki5gms,bazdabacshtecos,,https://www.reddit.com/r/dataengineering/comments/ki5gms/where_can_i_get_some_real_training/,1.0,4.0,0.0,22594.0,"hello everyone !

My work place is gving me the oprtunity to get a training in database management . There are 3 databases that we should know at the moment : actian, posgresql and mongodb .

does anyone know where i can find online a personal trainer that can teach me this ? because of covid i think i can only do it on skype.

Thank you"
4552,2020-12-22 16:16:05,1608646565.0,dataengineering,Let's talk about trade offs — How to choose the right database for the job,ki64a1,nimrodparasol,,https://www.reddit.com/r/dataengineering/comments/ki64a1/lets_talk_about_trade_offs_how_to_choose_the/,1.0,0.0,0.0,22596.0,
4553,2020-12-22 16:39:29,1608647969.0,dataengineering,Let’s talk about trade offs — How to choose the right database for the job — Part 1,ki6jew,nimrodparasol,,https://www.reddit.com/r/dataengineering/comments/ki6jew/lets_talk_about_trade_offs_how_to_choose_the/,1.0,5.0,0.0,22596.0,
4554,2020-12-22 17:15:11,1608650111.0,dataengineering,Why Data Versioning as an Infrastructure Matters,ki775k,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/ki775k/why_data_versioning_as_an_infrastructure_matters/,1.0,1.0,0.0,22597.0,
4555,2020-12-22 19:44:17,1608659057.0,dataengineering,Proven Patterns For Building Successful Data Teams - The Data Engineering Podcast,kia405,eljefe6a,,https://www.reddit.com/r/dataengineering/comments/kia405/proven_patterns_for_building_successful_data/,1.0,0.0,0.0,22603.0,
4556,2020-12-23 00:42:09,1608676929.0,dataengineering,The Keys To Unlock TDD for Data Engineering,kify2o,david_ok,,https://www.reddit.com/r/dataengineering/comments/kify2o/the_keys_to_unlock_tdd_for_data_engineering/,1.0,0.0,0.0,22614.0,
4557,2020-12-23 01:56:57,1608681417.0,dataengineering,how the cache works for Databricks analytical SQL,kihdkd,mim722,,https://www.reddit.com/r/dataengineering/comments/kihdkd/how_the_cache_works_for_databricks_analytical_sql/,1.0,1.0,0.0,22616.0,"I don't have to the preview and the documentation is not clear,  my understanding if you run a Query the result will be cached, now does the cluster needs to be running in order to use the cache or is there a service like Snowflake that decided either to use the cache or run a new Query if the data source has changed ?"
4558,2020-12-23 03:44:52,1608687892.0,dataengineering,"[ETL] What is the best tool for the job? Database, Query, Scripts",kijbyg,lowercase00,,https://www.reddit.com/r/dataengineering/comments/kijbyg/etl_what_is_the_best_tool_for_the_job_database/,1.0,13.0,0.0,22616.0,"Hey all.

So I'm pretty new to the whole ETL world and was looking for some guidance/help to point me in the right direction.

I currently need to design a workflow if the following actions (two steps):

**STEP 1**

1. Connect to a SQL Server database (and continuously monitor the connection)
2. Make a query (has to be loaded from a SQL file)
3. Get the results of that query
4. Connect to another SQL Server database (staging), continuously monitor the connection
5. Insert the results on my staging database

**STEP 2**

1. Connect (and monitor the connection) to my staging database
2. Query the database (loaded from SQL file)
3. Receive some input from the user (can be manual, file, etc)
4. Run a Python script that runs some calculations to the query results, and compare it to the user's input, and has to return an Object
5. I want this object

A few other requirements:

\- I need to be able to monitor each step of the process to know if anything went wrong (is my source database offline? is my staging database offline? did my query raise an error?).

\- I need to be able to change the queries without much code/deployment on the fly

\- I need to be able to save a template of a workflow so that I can easily replicate it

\- A UI would be nice, but I would be ok using just the API

**Possible Solutions**

I've seen Airflow, but I couldn't understand how to ""persist"" data between tasks, so it seemed more of a task/workflow manager than an ETL tool itself.

I've seen NiFi, but I couldn't replicate a workflow with different parameters.

I've seen Prefect, but I honestly couldn't even make it work at all.

I've seen Dagster, but I also didn't quite understand how to apply my needs to it.

I was wondering if you guys could point me in the right direction in order to solve this particular challenge. I'm a bit confused on whether I'm using these tools the wrong way, and they are, indeed, the right tool for the job, or I should be looking at a different set of tools for the job.

Any input is much appreciated, thanks a lot!!"
4559,2020-12-23 05:07:21,1608692841.0,dataengineering,How likely is it for a new grad to get a data engineering position with two internships as an ETL developer and a side project in a large city?,kikqky,jana50nn,,https://www.reddit.com/r/dataengineering/comments/kikqky/how_likely_is_it_for_a_new_grad_to_get_a_data/,1.0,4.0,0.0,22616.0,
4560,2020-12-23 05:22:41,1608693761.0,dataengineering,Programming Languages For Data Science,kikzu5,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kikzu5/programming_languages_for_data_science/,1.0,0.0,0.0,22616.0,
4561,2020-12-23 05:24:20,1608693860.0,dataengineering,Programming Languages For Data Science,kil0xd,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kil0xd/programming_languages_for_data_science/,1.0,0.0,0.0,22616.0,
4562,2020-12-23 05:40:14,1608694814.0,dataengineering,Programming Languages For Data Science,kilaj3,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kilaj3/programming_languages_for_data_science/,1.0,0.0,0.0,22616.0,
4563,2020-12-23 09:18:29,1608707909.0,dataengineering,Data Analyst to Data Engineer,kioips,mmartymcfly_,,https://www.reddit.com/r/dataengineering/comments/kioips/data_analyst_to_data_engineer/,1.0,3.0,0.0,22621.0,"Hi to all,

I have a questions about possible career paths for me (data analyst).
Briefly about me: I am 26 years old (27 in January), living in Europe, with BA in Economic Analytics (field of econometrics).

I have been working as a data analyst for over 3 years, so I have already some  experience in data world. During my work for one of the employers (one of the largest companies in the world in its industry, but not technology) I even received one of five awards for whole Europe for my tool that I made in Python (improve processes, something like ETL). So there are some successes too. 
My current technology stack is mainly Python / R (I prefer Python), SQL, Tableau. However, I also dealt a bit with Docker, Talend, Tableau Prep.

In my current job, I am participating in a very interesting project of building a data warehouse, so I also learning ETL processes and stuff about data warehouses.

I wonder about my future path. On the one hand, the promotion from an analyst should probably go towards Data Science, but my bachelor's degree limits me a lot. So I AM considering position of data engineer - event more interesting position for me. In my work so far, I liked the technical side (Python/SQL) and the process of delivering, cleaning and manipulating data. For me IT was much more interesting than the analysis and presentation part. Do you think such a direction would be appropriate for me or what career options does a data analyst have according to my preferences?

From what I read, the position of a data engineer would suit me best, but I am afraid that the lack of previous experience as a software engineer may be an obstacle for me.

Tldr: 27 years old, wondering future paths for Data Analyst and if Data Engineer would be a good persepective for me."
4564,2020-12-23 12:48:05,1608720485.0,dataengineering,Bachelor final project,kiqygs,No_Potential6129,,https://www.reddit.com/r/dataengineering/comments/kiqygs/bachelor_final_project/,1.0,1.0,0.0,22628.0,"Hi, next year I'll have to present a project for the bachelor's degree in maths and computer science I'm currently pursuing. I'm looking for some ideas for this project as I'm free to choose whatever I want as long as it relates to maths and/or computer science and I'd be interested to learn more about data engineering and data streaming in particular.  


A bit of background: I also have a few years of work as a python developer dealing with web scraping and the data processing that was required (for cleaning, monitoring, exporting, etc.). I was thinking of trying a new language for this project for learning sakes so I'm currently considering go or scala. (Go because I was thinking of hooking up some IoT devices and sensors and scala because I've heard spark may be interesting).  


Do you have any suggestions/ideas regarding this?  Any interesting project ideas? (Any learning resources would be much appreciated as well, I learned a lot already from this sub!)"
4565,2020-12-23 14:39:18,1608727158.0,dataengineering,DVC - can it be used to sync 1 dataset across 2 repositories?,kiscjb,ratatouille_artist,,https://www.reddit.com/r/dataengineering/comments/kiscjb/dvc_can_it_be_used_to_sync_1_dataset_across_2/,1.0,2.0,0.0,22629.0," I am trying to keep a dataset in sync between 2 repositories can DVC be used for this?   


I create a dataset in `creation` repo and use it in `use_data` repo can I use dvc to keep the data in sync between the two repos?  


I think there are 2 options to do this based on this [SO question](https://stackoverflow.com/questions/58952962/how-to-use-different-remotes-for-different-folders):

1. Setup different remotes for different folders and sync those across `creation` and `use_data` 
2. For assets that are only consumed dvc import seems like a cleaner pattern where I would use the dvc definition from the `creation` repo and bring  


I was just wondering if DVC is the right tool for this type of problem? It seems like the most recommended one online and the simple data storage interface is a big plus as well. The DVC pipeline feature seems less good to me than Airflow so I am just planning to use it as a way to sync data and maybe for the metric tracking."
4566,2020-12-23 19:10:40,1608743440.0,dataengineering,#idataengineer podcast 006 Bence Faludi,kiwwyn,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kiwwyn/idataengineer_podcast_006_bence_faludi/,1.0,0.0,0.0,22639.0,"[Bence Faludi](https://github.com/bfaludi) @ Facebook, currently in #Singapore, my partner in crime in previous numerous adventures tells you to keep learning.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-006](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-006)"
4567,2020-12-23 22:10:01,1608754201.0,dataengineering,How to constantly query many API’s and listen to many web sockets and stream into a sql database.,kj0a7r,rcyost,,https://www.reddit.com/r/dataengineering/comments/kj0a7r/how_to_constantly_query_many_apis_and_listen_to/,1.0,12.0,0.0,22643.0,"How would you recommend going about streaming from many api and websockets into a database? 

I don’t have a machine that can be run 24/7 and would like to avoid constantly paying for cloud compute. Could I do something like a raspberry pi to a Synology NAS setup? I would use python and a sql database. 

Thanks for you time. Any resources you could point me to would be great."
4568,2020-12-24 00:17:11,1608761831.0,dataengineering,"SQL and AdventOfCode 2020, on Snowflake",kj2m0t,fhoffa,,https://www.reddit.com/r/dataengineering/comments/kj2m0t/sql_and_adventofcode_2020_on_snowflake/,1.0,0.0,0.0,22645.0,
4569,2020-12-24 02:50:00,1608771000.0,dataengineering,apache airflow handling large batch jobs,kj5azf,guru223,,https://www.reddit.com/r/dataengineering/comments/kj5azf/apache_airflow_handling_large_batch_jobs/,1.0,14.0,0.0,22654.0,"How does apache airflow scale to handle large batch jobs? For example, if we need to process updates for 25k records (take the record, make an api call for an update, then update on our system) then how would this job get distributed in a way to process many records at once?"
4570,2020-12-24 06:49:15,1608785355.0,dataengineering,Exporting Airflow Metrics to Prometheus and Visualizing them,kj90wv,Rey661199,,https://www.reddit.com/r/dataengineering/comments/kj90wv/exporting_airflow_metrics_to_prometheus_and/,1.0,1.0,0.0,22658.0,"Hello everyone : ) I have managed to connect a Prometheus exporter to Apache Airflow. When I go to admin/metrics on the web server, I could see metrics that look like (sample):

  
# TYPE task_status gauge
    task_status{dag_id=""woof"",owner=""airflow"",status=""upstream_failed"",task_id=""bar""} 2.0
task_status{dag_id=""meow"",owner=""airflow"",status=""failed"",task_id=""baz""} 2.0


Am I on the right track? If so, how can I do something with those metrics (visualize or create alerts)?"
4571,2020-12-24 07:45:50,1608788750.0,dataengineering,Google Data Studio - Now supports HIPAA Business Associate Agreements (BAAs),kj9tiw,jwfergus,,https://www.reddit.com/r/dataengineering/comments/kj9tiw/google_data_studio_now_supports_hipaa_business/,1.0,0.0,0.0,22658.0,"[https://support.google.com/datastudio/answer/10043514](https://support.google.com/datastudio/answer/10043514)

Evidently support started sometime in Nov of 2020, but I missed the email. I know this more on the front end of most data engineering, but it could be relevant to some folks. 

If you're in healthcare and your team is already using GSuite, it could be a new front end for your report developers to look into."
4572,2020-12-24 09:30:00,1608795000.0,dataengineering,apache airflow webserver domain name,kjb58b,guru223,,https://www.reddit.com/r/dataengineering/comments/kjb58b/apache_airflow_webserver_domain_name/,1.0,1.0,0.0,22662.0,"is it possible to set a domain name for airflow instead of connecting by ip address?

I was able to set an A record in Route 53, pointing [domain-airflow.com](https://domain-airflow.com) to the IP address of the airflow webserver running on port 80 (not 8080), but I'd imagine there must be a more robust way of doing this.

Thoughts?"
4573,2020-12-24 09:54:17,1608796457.0,dataengineering,Architecting a Data Lake,kjbfc4,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/kjbfc4/architecting_a_data_lake/,1.0,0.0,0.0,22662.0,"Amid COVID crisis the best machine learning models failed to perform, thus increasing the demand for data science professionals. Changes in Recommendation systems gained importance.

[https://www.dasca.org/world-of-big-data/article/data-science-driven-recommender-systems-in-the-post-covid-era](https://www.dasca.org/world-of-big-data/article/data-science-driven-recommender-systems-in-the-post-covid-era)"
4574,2020-12-24 10:00:13,1608796813.0,dataengineering,Architecting a Data Lake,kjbhn2,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/kjbhn2/architecting_a_data_lake/,1.0,0.0,0.0,22662.0,"Amid COVID crisis the best machine learning models failed to perform, thus increasing the demand for data science professionals. Changes in Recommendation systems gained importance.

[https://www.dasca.org/world-of-big-data/article/data-science-driven-recommender-systems-in-the-post-covid-era](https://www.dasca.org/world-of-big-data/article/data-science-driven-recommender-systems-in-the-post-covid-era)"
4575,2020-12-24 10:19:53,1608797993.0,dataengineering,What tools in the VMware vSphere suite should a DE need to know?,kjbq05,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/kjbq05/what_tools_in_the_vmware_vsphere_suite_should_a/,1.0,13.0,0.0,22662.0,"As I understand, virtualization is an important aspect of the DE role and VMware vSphere suite is the market leader for this. I'm currently using ESXi to virtualize my homelab. I wonder what other vSphere tools should I learn for the DE role?"
4576,2020-12-24 10:20:11,1608798011.0,dataengineering,Architecting a Data Lake,kjbq4u,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/kjbq4u/architecting_a_data_lake/,1.0,0.0,0.0,22662.0,"Amid COVID crisis the best machine learning models failed to perform, thus increasing the demand for data science professionals. Changes in Recommendation systems gained importance.

[https://www.dasca.org/world-of-big-data/article/data-science-driven-recommender-systems-in-the-post-covid-era](https://www.dasca.org/world-of-big-data/article/data-science-driven-recommender-systems-in-the-post-covid-era)"
4577,2020-12-24 12:41:12,1608806472.0,dataengineering,The Data Janitor Letters - November 2020,kjdcco,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kjdcco/the_data_janitor_letters_november_2020/,1.0,4.0,0.0,22667.0,"Adblockers, personal datawarehouse, S3 disaster recovery, time series data synthesis, Python and Parquet, Clickhouse, dbt and open source data integration in the current edition.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-november-2020](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/the-data-janitor-letters-november-2020)"
4578,2020-12-24 18:57:43,1608829063.0,dataengineering,What data warehouse architecture is like for companies with huge data?,kjino0,Curious_Guy81,,https://www.reddit.com/r/dataengineering/comments/kjino0/what_data_warehouse_architecture_is_like_for/,1.0,3.0,0.0,22672.0,"I'm beginner in data engineering field and wants to know the big picture, where all different components fit in the architecture. Can you help me understand the entire architecture with an example, maybe for user analytics or for feature engineering for Machine Learning.

Specific questions:

1. NoSQL or SQL - which database is good for analytics? How to decide which kind of database(relational vs non-relational) and what specific database to choose ?
2. What are the important considerations to have in mind when thinking about designing a data warehouse?
3. One company may have few different needs - like one team may need to fetch data for recommendations or some ML, one team may need to create dashboard for user analytics and one for real-time analytics. How does it look like on high level to design warehouse to contribute to different applications?
4. How companies save historic data, example- twitter may need to store each data point for every user that has ever logged in- how do they do it? I mean does scaling happen everyday as data grows?

Probably vague questions but any insights or recommendations to online resources will be great. Thanks!"
4579,2020-12-24 19:32:08,1608831128.0,dataengineering,"Free PDF 2nd edition of ""Infrastructure as Code: Dynamic Systems for the Cloud Age"" by Kief Morris",kjj8rs,L1MB0_,,https://www.reddit.com/r/dataengineering/comments/kjj8rs/free_pdf_2nd_edition_of_infrastructure_as_code/,1.0,4.0,0.0,22673.0,"Many of you probably know this book [https://infrastructure-as-code.com/](https://infrastructure-as-code.com/) and what's exciting is that the second edition was recently out.

Linode is giving PDF away - [https://www.linode.com/content/infrastructure-as-code-book-oreilly/get/5750a36ec491/](https://www.linode.com/content/infrastructure-as-code-book-oreilly/get/5750a36ec491/)"
4580,2020-12-24 19:43:09,1608831789.0,dataengineering,Is it possible to transition from data science to a data engineering? And how would I go about doing so?,kjjf95,Least_Curious_Crab,,https://www.reddit.com/r/dataengineering/comments/kjjf95/is_it_possible_to_transition_from_data_science_to/,1.0,10.0,0.0,22673.0,"Currently a data scientist, but finding it incredibly stressful. It's not at all a team driven career and politics are rampant. Looking at lots of options. Has anyone on this sub transitioned from data science to a data engineering? And how did you go about doing so?"
4581,2020-12-24 22:38:45,1608842325.0,dataengineering,Is it ever a good idea to directly connect Power BI to a Data warehouse like Azure Synapse?,kjmedg,1010101100111,,https://www.reddit.com/r/dataengineering/comments/kjmedg/is_it_ever_a_good_idea_to_directly_connect_power/,1.0,4.0,0.0,22679.0,"Much of Microsoft's documentation illustrates Power BI connecting directly to a DW. Ignoring the expense, does anyone have any idea why this may or may not be a good idea?"
4582,2020-12-24 23:14:57,1608844497.0,dataengineering,Data Engineering podcasts,kjmzgv,stolzen,,https://www.reddit.com/r/dataengineering/comments/kjmzgv/data_engineering_podcasts/,1.0,4.0,0.0,22680.0,Can you recommend me some data engineering podcasts? Also anything data related would be interesting as well
4583,2020-12-25 01:55:37,1608854137.0,dataengineering,Data access and architecture,kjpf6w,IamJesperrekuh,,https://www.reddit.com/r/dataengineering/comments/kjpf6w/data_access_and_architecture/,1.0,2.0,0.0,22686.0,"Question, from a data and solution architectural point of view.

We have all those different formats (csv,unstructured, json, graph db, rdbms, xml , rdf, key value  )   
Then there is MDM and Schemas, jsonSchema, Avro, Parque, XSD.  
All of this applicable in our current landscape. 

One of the biggest challenges is to have direct access to these types without refactoring it to new schemas and datatypes where it is then copied into and mapped to its (new) types (int, floats, varchar , object, blob, etc ).  It frustrates me because in order to query it I need to change, from vendor / ansi SQL, XPath, Sparql, PGQL. Why isn't there a UniformQueryLanguage.

What (architectural) solutions do you have in place:  
\- reduce data replication and transformations. However you need to copy once...   
\- still being format agnostic from an access point of view. Never loose its original form / be able to have access to.   
\- keep track of schema changes / mdm."
4584,2020-12-25 03:09:51,1608858591.0,dataengineering,airflow 2.0 clean installation,kjqgut,sinistersparrow,,https://www.reddit.com/r/dataengineering/comments/kjqgut/airflow_20_clean_installation/,1.0,7.0,0.0,22688.0,"Greetings,

I am looking for a clean install guide for airflow 2.0.  Has anyone seen any guidance for clean installation for production?   Thanks."
4585,2020-12-25 03:57:36,1608861456.0,dataengineering,Don’t let this be your project: “Poor Oversight Leads To A State Wasting $365k On A Broken Model That Archaeologists Are Told To Use”,kjr3ch,PaperImperium,,https://www.reddit.com/r/dataengineering/comments/kjr3ch/dont_let_this_be_your_project_poor_oversight/,1.0,1.0,0.0,22689.0,
4586,2020-12-25 06:41:44,1608871304.0,dataengineering,Using large unstructured (image/video) datasets? We want to interview you.,kjt7pz,DeadPukka,,https://www.reddit.com/r/dataengineering/comments/kjt7pz/using_large_unstructured_imagevideo_datasets_we/,1.0,0.0,0.0,22694.0,"I have a brand-new startup working on tools and technologies for managing large unstructured datasets (image, video, 3D, documents).

If any of you work on large-scale projects, which have built your own data pipelines for storing, indexing, querying, and integrating unstructured data - and can talk publicly about it - we are looking for folks to interview (via Zoom) and learn more about your pain points and your build/buy decisions. 

You will be compensated for your time, via Amazon gift card or similar.

Please DM me if you're interested to chat with us.

Happy holidays!"
4587,2020-12-25 11:56:30,1608890190.0,dataengineering,Why is Data Analytics So Far Behind Software Engineering?,kjwo36,duyenla257,,https://www.reddit.com/r/dataengineering/comments/kjwo36/why_is_data_analytics_so_far_behind_software/,1.0,1.0,0.0,22699.0,
4588,2020-12-25 14:01:17,1608897677.0,dataengineering,Data Engineering Advent Calendar 2020,kjxyo5,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kjxyo5/data_engineering_advent_calendar_2020/,1.0,1.0,0.0,22701.0,"Throughout December 2020 we’ve shared a daily dose of semi-esoteric data engineering wisdom on our social media channels. This post shall serve as a commemorative monolith you can always turn to when the data engineering gods are not picking up your call.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/data-engineering-advent-calendar](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/data-engineering-advent-calendar)"
4589,2020-12-25 23:38:36,1608932316.0,dataengineering,Reading Parquet Without A Distributed Cluster,kk6i1e,DataEngUncomplicated,,https://www.reddit.com/r/dataengineering/comments/kk6i1e/reading_parquet_without_a_distributed_cluster/,1.0,11.0,0.0,22715.0,"Parquet is growing in popularity as a format in the big data world as it allows for faster query run time, it is smaller in size and requires fewer data to be scanned compared to formats such as CSV. If you don't have access to a distributed cluster and still want to work with parquet files on your local machine, I put together a video walk tutorial.

[Step By Step Guide](https://youtu.be/XFO5jdGsMek)

[Code and Blog Post](https://adrianonicolucci.medium.com/how-to-read-parquet-files-in-python-without-a-distributed-cluster-4d4e8ba600e5)  
If you find this helpful, please check out my [data engineering youtube channel](https://www.youtube.com/channel/UCNbfqCkmHEyf1CVKjuhEW_A). I'm really looking forward to helping and contributing to the data engineering community."
4590,2020-12-26 02:29:49,1608942589.0,dataengineering,Python Script to Extract,kk92we,noxlv,,https://www.reddit.com/r/dataengineering/comments/kk92we/python_script_to_extract/,1.0,0.0,0.0,22718.0," Anywhere I can look up a sample python script to generate a TPT Export as a text file?

What module do I need to import?"
4591,2020-12-26 07:34:00,1608960840.0,dataengineering,I've been told you need to know Java to use Talend (well) - how much do you need to know?,kkde1k,stackhat47,,https://www.reddit.com/r/dataengineering/comments/kkde1k/ive_been_told_you_need_to_know_java_to_use_talend/,1.0,8.0,0.0,22723.0,"Hello, 

I'm fishing for a data engineer/analyst role at my current workplace, I've got some of the required skills, but we use Talend.  I don't know how to use it, and the manager has said he wants people who know Java in the role in order to run Talend well.   I can access workplace training for Talend. 

How much Java would I need to know to be competent?   

Thanks in advance..."
4592,2020-12-26 09:30:44,1608967844.0,dataengineering,Are you ready for CCPA?,kkersa,orion_governance,,https://www.reddit.com/r/dataengineering/comments/kkersa/are_you_ready_for_ccpa/,1.0,0.0,0.0,22725.0,
4593,2020-12-26 12:10:32,1608977432.0,dataengineering,Career Path for Data Related,kkgc5z,noxlv,,https://www.reddit.com/r/dataengineering/comments/kkgc5z/career_path_for_data_related/,1.0,7.0,0.0,22731.0,"Suppose  you're a DBA touch basing data warehouse/ETL  
You want to transition to a different role and while learning python, you come across that it's difficult, but feel good when you can automate stuff.   
Is Data Engineer the right fit? Any other roles you recommend whether Data or outside of Data role?"
4594,2020-12-26 15:40:13,1608990013.0,dataengineering,Data engineering conferences 2021,kkiqq2,ibnipun10,,https://www.reddit.com/r/dataengineering/comments/kkiqq2/data_engineering_conferences_2021/,1.0,3.0,0.0,22735.0,"Does anyone have a list of DE related conferences in 2021? Some of them I can point out are

1 qcon
2 cncf, kubecon
3 gartner"
4595,2020-12-26 23:53:21,1609019601.0,dataengineering,DataTalks.Club - a community for people who like data,kkqrft,stolzen,,https://www.reddit.com/r/dataengineering/comments/kkqrft/datatalksclub_a_community_for_people_who_like_data/,1.0,1.0,0.0,22759.0,"If you're looking for data communities to join, check [DataTalks.Club](https://DataTalks.Club) \- it's a community for people who like data topics, including data engineering. See you there!"
4596,2020-12-27 07:40:19,1609047619.0,dataengineering,Data engineer career path,kky6zd,miralledsw95,,https://www.reddit.com/r/dataengineering/comments/kky6zd/data_engineer_career_path/,1.0,2.0,0.0,22765.0,"Hello, 

I have a background in networking and I've dabbled in python a bit. I'm very interested in data engineering and want to focus my attention on the steps necessary to get a data engineering job. So I guess my questions are, what exactly do I need to know (I've seen python, SQL and apache spark seem to come up a lot) to get my first job and what resources do you use to learn?

Also any idea on projects I could make to have my CV taken more seriously?

&amp;#x200B;

Thank you"
4597,2020-12-27 17:12:31,1609081951.0,dataengineering,readsql - convert SQL to most human readable format,kl56ef,Azisk,,https://www.reddit.com/r/dataengineering/comments/kl56ef/readsql_convert_sql_to_most_human_readable_format/,1.0,18.0,0.0,22781.0,"Save your and your code reviewers time by automatically upper-casing SQL keywords for best code readability. I am a data engineer and I write a lot of SQL code inside Python files. I believe that this would be a very useful tool, I will propose it to my team after the holidays. It can be added as a [pre-commit](https://pre-commit.com/) git hook as well. The library is written in Python.

Go to [https://github.com/AzisK/readsql](https://github.com/AzisK/readsql) to find out more. Enjoy!"
4598,2020-12-27 18:30:11,1609086611.0,dataengineering,Understanding Common Table Expressions(CTEs),kl6gis,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/kl6gis/understanding_common_table_expressionsctes/,1.0,12.0,0.0,22778.0,"Common Table Expressions (CTEs) are an interesting feature of SQL. They are useful in splitting out a large query with multiple subqueries into ephemeral tables that last only for the duration of that query.

If you are not sure what they are and when to use them, then this post is for you. In this post, we go over what CTEs are, and its performance comparisons against subqueries, derived tables, and temp tables to help decide when to use them.

[https://www.startdataengineering.com/post/using-common-table-expression-in-redshift/](https://www.startdataengineering.com/post/using-common-table-expression-in-redshift/)

Any comments or feedback is appreciated. Hope this provides value to someone."
4599,2020-12-27 23:38:26,1609105106.0,dataengineering,"It's the yearend edition of @data_weekly !!! Back To The Future: Data Engineering Trends 2020 &amp; Beyond. We look at data engineering trends 2020 and the future of data infrastructure, data architecture &amp; data management. Comment your thoughts",klc36q,vananth22,,https://www.reddit.com/r/dataengineering/comments/klc36q/its_the_yearend_edition_of_data_weekly_back_to/,1.0,1.0,0.0,22785.0,
4600,2020-12-28 06:12:02,1609128722.0,dataengineering,How to Become a Cloud Engineer in 2021 | Skills Required for a Cloud Engineer,klixd6,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klixd6/how_to_become_a_cloud_engineer_in_2021_skills/,1.0,0.0,0.0,22798.0,
4601,2020-12-28 06:12:24,1609128744.0,dataengineering,How To Become A Software Engineer in 2021,klixm8,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klixm8/how_to_become_a_software_engineer_in_2021/,1.0,0.0,0.0,22798.0,
4602,2020-12-28 06:14:05,1609128845.0,dataengineering,How To Become A Software Engineer in 2021,kliyi2,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kliyi2/how_to_become_a_software_engineer_in_2021/,1.0,0.0,0.0,22798.0,
4603,2020-12-28 06:15:33,1609128933.0,dataengineering,Top 5 Real World Artificial Intelligence Applications,kliza5,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kliza5/top_5_real_world_artificial_intelligence/,1.0,0.0,0.0,22798.0,
4604,2020-12-28 06:16:17,1609128977.0,dataengineering,How to Become a Data Analyst in 2021?,kliznm,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kliznm/how_to_become_a_data_analyst_in_2021/,1.0,0.0,0.0,22798.0,
4605,2020-12-28 06:16:41,1609129001.0,dataengineering,What is needed to be a full stack developer in 2021,klizvj,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klizvj/what_is_needed_to_be_a_full_stack_developer_in/,1.0,0.0,0.0,22798.0,
4606,2020-12-28 06:17:02,1609129022.0,dataengineering,Artificial Intelligence vs Machine Learning vs Data Science,klj02u,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klj02u/artificial_intelligence_vs_machine_learning_vs/,1.0,0.0,0.0,22798.0,
4607,2020-12-28 06:18:04,1609129084.0,dataengineering,What do you need to become a Python developer in 2021?,klj0oh,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klj0oh/what_do_you_need_to_become_a_python_developer_in/,1.0,0.0,0.0,22798.0,
4608,2020-12-28 06:18:34,1609129114.0,dataengineering,Effect of blockchain on programming languages especially python,klj0yw,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klj0yw/effect_of_blockchain_on_programming_languages/,1.0,0.0,0.0,22798.0,
4609,2020-12-28 06:20:26,1609129226.0,dataengineering,What is Logistic Regression and how does it work?,klj208,chase2learn,,https://www.reddit.com/r/dataengineering/comments/klj208/what_is_logistic_regression_and_how_does_it_work/,1.0,0.0,0.0,22798.0,
4610,2020-12-28 06:47:01,1609130821.0,dataengineering,Data Analytics in Excel,kljgtr,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kljgtr/data_analytics_in_excel/,1.0,0.0,0.0,22798.0,
4611,2020-12-28 12:25:17,1609151117.0,dataengineering,Machine learning is going real-time [Discussion] How does your company serve ML ?,klo1k6,SuccessfulChocolate,,https://www.reddit.com/r/dataengineering/comments/klo1k6/machine_learning_is_going_realtime_discussion_how/,1.0,2.0,0.0,22803.0,
4612,2020-12-28 12:56:14,1609152974.0,dataengineering,Data Quality from First Principles,klofgl,anhthong00,,https://www.reddit.com/r/dataengineering/comments/klofgl/data_quality_from_first_principles/,1.0,2.0,0.0,22803.0,
4613,2020-12-28 20:31:16,1609180276.0,dataengineering,We’re rebranding PrestoSQL as Trino,klvzah,martintraverso,,https://www.reddit.com/r/dataengineering/comments/klvzah/were_rebranding_prestosql_as_trino/,1.0,13.0,0.0,22821.0,"We’re not going anywhere - it’s the same software, by the same people, just under a new name. 

Learn about why we’re doing it: http://trino.io/blog/2020/12/27/announcing-trino.html"
4614,2020-12-29 00:36:35,1609194995.0,dataengineering,Career Guidance - Senior ETL Engineer,km0roj,optimist26,,https://www.reddit.com/r/dataengineering/comments/km0roj/career_guidance_senior_etl_engineer/,1.0,18.0,0.0,22831.0,"Working on traditional ETL tools from past 7 years, Informatica PowerCenter and Informatica Cloud with RDBMS like Oracle, SQL Server and Unix for job scheduling etc. Started looking out for job opportunities, only to realize, good companies have more expectations like scripting in Python, a visualization tool. Can someone shed some light as to what level of Python one needs to know, also what would be a good visualization tool to learn and at what level should one be in both to be confident to apply for jobs. Please feel free to add anything else I should learn as well to get good opportunities. Appreciate the help, thanks in advance!"
4615,2020-12-29 06:32:18,1609216338.0,dataengineering,Webscraping ETL project help,km748v,scuba-steve-0,,https://www.reddit.com/r/dataengineering/comments/km748v/webscraping_etl_project_help/,1.0,2.0,0.0,22837.0,"I want to learn some data engineering skills via projects and have decided to transform a webscraping script I previously wrote into a more formal DE project. The script currently scrapes data from a site weekly, does a series of data cleaning/transformations, and then appends the new results to 2-3 previously saved CSVs of historical data. I want to automate the running of this script and load it into a SQL database likely with a couple tables. I know little to nothing about DE but see a lot of tools/services/techniques mentioned and am not really sure where to start. Anyone have recommendations on the following?

Where should I host the script for automation? I’m familiar with cron jobs but would be nice to run this not locally.

The source site I scrape from will likely change in the future, what is the best way to log errors in the scraper when this happens?

For this project I will likely have at most 5 tables in the database but any thoughts on database design/best practices?

Where should I host the database and any recommendations on what type of database? I’m most familiar with Postgres but am seeing a lot of mentions of big query lately.

Would it make sense to learn/use Airflow for something like this? I see it get mentioned a lot and would be interested in learning more about it."
4616,2020-12-29 11:41:03,1609234863.0,dataengineering,Download Guide for Feature Engineering &amp; Selection,kmbbyi,Techtter,,https://www.reddit.com/r/dataengineering/comments/kmbbyi/download_guide_for_feature_engineering_selection/,1.0,1.0,0.0,22849.0,
4617,2020-12-29 11:42:08,1609234928.0,dataengineering,How to build Data Pipeline on AWS?,kmbch1,Techtter,,https://www.reddit.com/r/dataengineering/comments/kmbch1/how_to_build_data_pipeline_on_aws/,1.0,0.0,0.0,22849.0,
4618,2020-12-29 18:04:43,1609257883.0,dataengineering,Streaming Fluentd Logs to Azure EventHubs via the Kafka Streaming Protocol,kmh3ea,philmarius,,https://www.reddit.com/r/dataengineering/comments/kmh3ea/streaming_fluentd_logs_to_azure_eventhubs_via_the/,1.0,1.0,0.0,22857.0,
4619,2020-12-29 18:08:21,1609258101.0,dataengineering,AWS Glue Environment/SDLC,kmh5ub,gringopaisa18,,https://www.reddit.com/r/dataengineering/comments/kmh5ub/aws_glue_environmentsdlc/,1.0,4.0,0.0,22857.0,"My company leverages Glue for flat file ETL and I am tasked with creating the dev and deployment pipeline for the jobs. 

Does anyone have experience with leveraging glue within dev/qa/prod environments?

Lambda has versions and aliases but I don’t see anything like that for Glue"
4620,2020-12-29 18:26:10,1609259170.0,dataengineering,"Here are some informative Apache Spark optimization articles, please follow the page if you like and gain some knowledge.",kmhhky,DataKubeSpark,,https://www.reddit.com/r/dataengineering/comments/kmhhky/here_are_some_informative_apache_spark/,1.0,0.0,0.0,22857.0,
4621,2020-12-29 19:21:26,1609262486.0,dataengineering,How do you manage Data Quality in your Data Warehouse?,kmik2x,ksubrent,,https://www.reddit.com/r/dataengineering/comments/kmik2x/how_do_you_manage_data_quality_in_your_data/,1.0,35.0,0.0,22858.0,"Hello All,

I’m planning to implement some ￼Data Quality controls to our Azure Synapse instance early this year. I’m curious what you have done to implement and monitor these control processes. What sort of metrics or KPIs do you use to establish a healthy baseline? 

Thank you!"
4622,2020-12-29 23:27:10,1609277230.0,dataengineering,Sisense - Legit or Bloat?,kmndxr,p_h_a_e_d_r_u_s,,https://www.reddit.com/r/dataengineering/comments/kmndxr/sisense_legit_or_bloat/,1.0,5.0,0.0,22867.0,"Does anyone have a success story with this company? Or a use case at least? I just got off the phone for a senior DE position and I still can't make heads or tails of what it is they do and if it's any different than what anyone else is doing? I asked the recruiter to elaborate and he read the most jargony jargon paragraph I've ever heard. 

Hard pass from me. But curious anyway on others' thoughts and experiences."
4623,2020-12-29 23:33:31,1609277611.0,dataengineering,Can someone help me decipher these tables on a Facebook glassdoor interview question comment?,kmnib9,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kmnib9/can_someone_help_me_decipher_these_tables_on_a/,1.0,4.0,0.0,22867.0,"Below is from Facebook's glassdoor data engineer interview questions page.   


I am attempting to practice the questions that the user has posted, but cannot for the life of me decipher what the hell the tables are... Can someone please try to help me decipher these hieroglyph tables lol  


link to glassdoor page (i got the below picture from this link on the comment with 68 helpful upvotes) [https://www.glassdoor.com/Interview/Facebook-Data-Engineer-Interview-Questions-EI\_IE40772.0,8\_KO9,22.htm](https://www.glassdoor.com/Interview/Facebook-Data-Engineer-Interview-Questions-EI_IE40772.0,8_KO9,22.htm)) 

https://preview.redd.it/lyk2s0zg27861.png?width=1290&amp;format=png&amp;auto=webp&amp;s=a57983dbb9449d74915269675d479b73cbf3813c"
4624,2020-12-30 00:16:26,1609280186.0,dataengineering,Trino 🐇 is trending on Github! Please consider giving us a 🌟 on https://github.com/trinodb/trino if you love being a part of the Presto and now https://Trino.io community!,kmobt2,bitsondatadev,,https://www.reddit.com/r/dataengineering/comments/kmobt2/trino_is_trending_on_github_please_consider/,1.0,0.0,0.0,22868.0,
4625,2020-12-30 00:27:58,1609280878.0,dataengineering,"How can I used the DockerOperator in Airflow, of I am already running Airflow in Docker?",kmojyc,Rey661199,,https://www.reddit.com/r/dataengineering/comments/kmojyc/how_can_i_used_the_dockeroperator_in_airflow_of_i/,1.0,6.0,0.0,22868.0,"I am running Airflow, and trying to run a proof of concept for a Docker container using Airflow's DockerOperator1. I am deploying to Kubernetes (EKS), but not using Kubernetes Executor yet. Given that I am running pods, by using the `DockerOperator` I will be running (to my understanding) Docker in Docker. 

Whenever I run my task, I am receiving the Error: `ERROR - Error while fetching server API version`. The error happens both on `docker-compose` as well as EKS (kubernetes). 


Thi is how my airflow Dockerfile looks like:

FROM apache/airflow:1.10.14-python3.8
# Use airflow user for pip installs and other things.
USER root
# Copying Airflow requirements
USER airflow
COPY requirements.txt /tmp/requirements.txt
# Installing requirements. Using airflow user (docs: https://airflow.apache.org/docs/apache-airflow/stable/production-deployment.html)
RUN pip install --no-cache-dir -r /tmp/requirements.txt


This is how the dag I am trying to run looks like:
```
with DAG(
        dag_id='task',
        default_args=dict(
            start_date=days_ago(0)
        ),
        schedule_interval='@daily'
) as dag:

    task_1 = DockerOperator(
        dag=dag,
        task_id='docker_task',
        image='centos:latest',
        api_version=""auto"",
        docker_url='unix://var/run/docker.sock',
        command='/bin/sleep 30'
    )
```

This is the stack trace of the error I am getting:

    [2020-12-29 14:18:52,601] {taskinstance.py:1150} ERROR - Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))
    Traceback (most recent call last):
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 670, in urlopen
        httplib_response = self._make_request(
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 392, in _make_request
        conn.request(method, url, **httplib_request_kw)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1255, in request
        self._send_request(method, url, body, headers, encode_chunked)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1301, in _send_request
        self.endheaders(body, encode_chunked=encode_chunked)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1250, in endheaders
        self._send_output(message_body, encode_chunked=encode_chunked)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1010, in _send_output
        self.send(msg)
      File ""/usr/local/lib/python3.8/http/client.py"", line 950, in send
        self.connect()
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/transport/unixconn.py"", line 43, in connect
        sock.connect(self.unix_socket)
    FileNotFoundError: [Errno 2] No such file or directory
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/home/airflow/.local/lib/python3.8/site-packages/requests/adapters.py"", line 439, in send
        resp = conn.urlopen(
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 726, in urlopen
        retries = retries.increment(
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/util/retry.py"", line 410, in increment
        raise six.reraise(type(error), error, _stacktrace)
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/packages/six.py"", line 734, in reraise
        raise value.with_traceback(tb)
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 670, in urlopen
        httplib_response = self._make_request(
      File ""/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 392, in _make_request
        conn.request(method, url, **httplib_request_kw)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1255, in request
        self._send_request(method, url, body, headers, encode_chunked)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1301, in _send_request
        self.endheaders(body, encode_chunked=encode_chunked)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1250, in endheaders
        self._send_output(message_body, encode_chunked=encode_chunked)
      File ""/usr/local/lib/python3.8/http/client.py"", line 1010, in _send_output
        self.send(msg)
      File ""/usr/local/lib/python3.8/http/client.py"", line 950, in send
        self.connect()
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/transport/unixconn.py"", line 43, in connect
        sock.connect(self.unix_socket)
    urllib3.exceptions.ProtocolError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py"", line 214, in _retrieve_server_version
        return self.version(api_version=False)[""ApiVersion""]
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/api/daemon.py"", line 181, in version
        return self._result(self._get(url), json=True)
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/utils/decorators.py"", line 46, in inner
        return f(self, *args, **kwargs)
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py"", line 237, in _get
        return self.get(url, **self._set_request_timeout(kwargs))
      File ""/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py"", line 543, in get
        return self.request('GET', url, **kwargs)
      File ""/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py"", line 530, in request
        resp = self.send(prep, **send_kwargs)
      File ""/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py"", line 643, in send
        r = adapter.send(request, **kwargs)
      File ""/home/airflow/.local/lib/python3.8/site-packages/requests/adapters.py"", line 498, in send
        raise ConnectionError(err, request=request)
    requests.exceptions.ConnectionError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 984, in _run_raw_task
        result = task_copy.execute(context=context)
      File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/docker_operator.py"", line 260, in execute
        self.cli = APIClient(
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py"", line 197, in __init__
        self._version = self._retrieve_server_version()
      File ""/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py"", line 221, in _retrieve_server_version
        raise DockerException(
    docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))

### What I have tried
1. Mount the socket into docker compose `/var/run/docker.sock:/var/run/docker.sock:ro` 

First, that gives me a new error to:
```
ERROR - Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))
```
Second, how will I be able to mount on Kubernetes? I would guess that complicates things

2. Install docker within my container and try to give privileges to the airflow user like:
```
FROM apache/airflow:1.10.14-python3.8
# Use airflow user for pip installs and other things.
USER root
# Docker
RUN curl -sSL https://get.docker.com/ | sh  
# Usermod
RUN usermod -aG docker airflow
# Copying Airflow requirements
USER airflow
COPY requirements.txt /tmp/requirements.txt
# Installing requirements. Using airflow user (docs: https://airflow.apache.org/docs/apache-airflow/stable/production-deployment.html)
RUN pip install --no-cache-dir -r /tmp/requirements.txt
```

But that also did not work. 

3. Mounting the socket into the DockerOperator task like:
```python
    task_1 = DockerOperator(
        dag=dag,
        task_id='docker_task',
        image='centos:latest',
        api_version=""auto"",
        docker_url='unix://var/run/docker.sock',
        command='/bin/sleep 30',
        volumes=['/var/run/docker.sock:/var/run/docker.sock:ro'],
    )
```

But that also has had no effect

  [1]: https://airflow.apache.org/docs/apache-airflow/1.10.4/_api/airflow/operators/docker_operator/index.html"
4626,2020-12-30 05:52:25,1609300345.0,dataengineering,What is the responsibilities difference between data engineers and data analysts/scientists? How do they work with each other in a data project?,kmud9p,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/kmud9p/what_is_the_responsibilities_difference_between/,1.0,11.0,0.0,22872.0,Can someone share how a data engineering team interact with data scientists team / machine learning team in real life?
4627,2020-12-30 07:20:23,1609305623.0,dataengineering,TOP 6 MACHINE LEARNING TRENDS OF 2021,kmvsqa,1987_akhil,,https://www.reddit.com/r/dataengineering/comments/kmvsqa/top_6_machine_learning_trends_of_2021/,1.0,0.0,0.0,22874.0,
4628,2020-12-30 17:07:59,1609340879.0,dataengineering,My career progression from senior data engineer to ?,kn3urs,aDigitalPunk,,https://www.reddit.com/r/dataengineering/comments/kn3urs/my_career_progression_from_senior_data_engineer_to/,1.0,12.0,0.0,22897.0,"Hi all. I'm curious what you all think. My current title is Sr data engineer. I've been building data warehousing about 10 years, I was the sole bi engineer at my last job for 6 years and build up 3b row dw in sql server using ssas cubes, using disparate data and trained up 100 business users from marketing, sem, email, ab testing, feature reporting, pm reporting, finance. I enjoyed interacting with the business and building solutions that directly help improve the business. This company has only around 200 employees.

I joined a new company 3 years ago and lead a huge migration from sql to snowflake, I prototype and developed most of the work, the team has been growing and I'm now leading the team and we've built some great patterns and processes. The rest of the company wants to onboard into snowflake and there's still lots of BI use cases that haven't been started. This company is big with over 2000 employees. 

With my current team setup I feel like first of all I'm not getting recognized as a manager since my title is still Sr data engineer. I interact with directors and vps in meetings every day and ensure their teams and my teams are making the right decisions related to our snowflake projects. My team is focusing on only 1 use case in the engineering org. Theres many other use cases for other products in engineering plus product, marketing and finance departments could use a lot of love.

Theres an opportunity for a director of bi but I dont think ill have a good chance, they want someone with good experience managing managers. I'd love to keep leading the path in building and onboarding more use cases to bi/dw. There might be opportunity for me to go to principal engineer, but I worry that might not be best, since I can't do much single handedly and would prefer to keep my team plugging  away under my direction, since we've established great experience and knowledge on these patterns.

Tldr: I'm Sr data engineer leading a team, I think I'd like to be director but might be more likely to become principal, or whatever other opportunity I can try to create for myself.

Thanks"
4629,2020-12-30 17:57:11,1609343831.0,dataengineering,"ETL with Azure Functions, Python and Pandas | QueryClick",kn4qsv,sam_maurice,,https://www.reddit.com/r/dataengineering/comments/kn4qsv/etl_with_azure_functions_python_and_pandas/,1.0,3.0,0.0,22898.0,
4630,2020-12-30 18:25:37,1609345537.0,dataengineering,Do I have what it takes to become a DE?,kn5a53,danbcooper,,https://www.reddit.com/r/dataengineering/comments/kn5a53/do_i_have_what_it_takes_to_become_a_de/,1.0,10.0,0.0,22898.0,"**First of all, sorry for the long post.**

I'm a chemical engineer and I currently work as a ""graduate analyst"" at a non-tech company. During the time I've been working at this company, I discovered that I love DE and I would like to shift my career towards it instead of chemical engineering. The problem is that since I'm the only person doing this type of job at my company, I don't have anyone to compare myself to, so I don't know if I have what I need to become a DE.

**I would say that this are my DE skills:**

\- I'm very comfortable with python, I've been doing random things as a hobbie for quite a long time

\- I know some SQL, nothing super crazy but I get the job done (I google 95% of what I do)

\- Very comfortable with Excel

\- I can create simple PowerBi reports

\- I'm not a linux expert but I get the job done

\- SQL Server/Azure SQL, again, no expert but I get the job done by googling and some trial and error

&amp;#x200B;

**Some of the DE related things that I've done so far are:**

**1-** At this company, they receive data from two different sources, the first is a lot of emails with attached files (xlsx, xml, html, csv, etc). This files need to get converted to Excel spreadsheets so that an industry-specific software can read them and load them into a database. The classification, download and transformation of the emails and attachments used to be done with Outlook + VBA scripts (the VBA scripts would interact with Excel and they were CONSTANTLY crashing and having issues). What I did is I replaced this with a python program that I built (it connects directly to the email using the O365 package and does all the transformations and classifications without interacting with excel or Outlook), you can see the program here: [https://github.com/devgio/carpincho](https://github.com/devgio/carpincho)

**2-** Replaced the industry-specific ETL software (about 30k a year for the licence, not including SQL Server) with Azure SQL + Blob storage + a VM running Airflow. I designed the database, created the dags and set up everything. This is loading \~1M records a day and it's what we use for almos all our data needs.

To do this I learnt a lot about linux (I cconfigured Airflow from scratch, I created services for Airflow and the email processing program, etc)

This change was VERY positive, and reduced the annual cost by about 80% and the reliability of the system is much much better now.

**3-** All the reports used to be done using interconnected Excel spreadsheets. I replaced all of them with either PowerBi reports or Excel spreadsheets connected only to the Azure SQL database. This reduced the number of errors and the time to build the reports.

**4-** Referencing item 1, the second data source is a data historian in a remote off-shore platform. The connection with this historian is terrible and we lost a lot of data because of it. I reconfigured the data historian server to drop the data into csv files instead, and when the connection allows, the csv files are pushed to Azure blob storage to then be loaded into the Azure SQL database.

&amp;#x200B;

Are this tasked that a DE usually does? I really like doing this and I'm thinking about looking a job at a tech company. What do you guys think?

Also, any recommendation on technologies or concepts to focus on will be highly appreciated!

Thanks!"
4631,2020-12-30 22:42:40,1609360960.0,dataengineering,Notebooks Web UI,knad1z,vpol,,https://www.reddit.com/r/dataengineering/comments/knad1z/notebooks_web_ui/,1.0,4.0,0.0,22915.0,"Couple of years ago I listened to a podcast with Netflix engineer who described how they share Jupiter notebooks internally within the company and now I’m looking to implement similar solution.

This is mainly for financial projections / visual reports (for now)

I have some requirements, maybe you can point me to the right direction. Or maybe I can even build something using some open source tools.

-	Web UI (can be cloud or self-hosted)
-	Ability to store notebooks in s3 / git / etc 
-	Version control for notebooks (in case of non-git)
-	Ability to access s3 as a data source.
-	Ability to share notebook in ro-mode.
-	Groups/collections/permission management
-	Schedule recalculations (like daily or weekly)
-	Ability to work with a shadow copy of a notebook until you are ready to push/save.
-	Resource pools (spawn runners of different type per notebook/user/collection)
-	Ability to attach own runners (!)

I’m ready to pay per user (editor ideally, as I’m going to have a more viewers) per month, but don’t want to pay for compute (at least for now)

I’ve already found some partial solutions (I did some homework), but maybe I’m missing some magic project from Apache that will solve all my problems.

Thanks."
4632,2020-12-31 02:55:18,1609376118.0,dataengineering,Learning something new with Scala...?,knf6bf,ryati,,https://www.reddit.com/r/dataengineering/comments/knf6bf/learning_something_new_with_scala/,1.0,23.0,0.0,22928.0,"Around this time of year, I always like to look into learning a bit about different programming languages. I know the best advice is to learn more about python or SQL, but doing this lets me come back to those languages with fresh eyes and some new ideas.

I wanted to try to see what I could learn about scala, but i keep seeing a lot of negative talk about it. I know kotlin is a big language these days, but doesn't have the same importance to DE as scala. 

Is scala really on its way out? 
Is there any reason a DE might want to know some kotlin?
Is something else like golang more relevent these days?"
4633,2020-12-31 07:42:50,1609393370.0,dataengineering,Spark Delta tables,knk141,mdghouse1986,,https://www.reddit.com/r/dataengineering/comments/knk141/spark_delta_tables/,1.0,4.0,0.0,22937.0,"I would be interested to know people's experience using delta tables to build Fact tables in DW. We are currently planning to move our pipelines to databricks/delta and would like to know from the people who have been there done that. examples from databricks website are pretty simple (5 to 10 columns, 2 to 3 table joins). I am talking about cases where raw tables have 30 to 40 columns each at least and the final fact tables have 30 to 40 columns with varying data types (5 to 10 billion records).

Thank you."
4634,2020-12-31 20:22:58,1609438978.0,dataengineering,Data Career Choice - Data Engineer or Machine Learning Engineer?,knv5uf,data9uy,,https://www.reddit.com/r/dataengineering/comments/knv5uf/data_career_choice_data_engineer_or_machine/,1.0,41.0,0.0,22952.0,"Hi all,

I have reached a crossroad at the beginning of my career and I would love to hear some advice from anyone with advice to give. If you aren't interested in the backstory then just skip to the third-to-last paragraph.

tl;dr Need to pick between data engineer and machine learning engineer positions at the same company. Both have full-time training. Which one should I choose and why?

First, the backstory. I am an engineering graduate with a masters degree from a top UK university. I graduated in 2019 and went into a graduate data analyst role in one of the largest FMCG companies in the world (let’s call them Company A for brevity). My contract at Company A is only for 12 months in a temporary position and I am the only data analyst in the team - even the wiper department only has a couple of people with real technical data skills and they seldom use them from my understanding. It is my understanding that this position is the beginning of an initiative to start using more cutting edge technologies in what is a large conglomerate company. As such, there is not much support and the pay is not very good. I found that within 6-8months of the role I was exhausting the possibilities of what I could do with the data and, being the only data person in the team, had very little guidance on where to go next. My manager was very supportive, but there was no formal training offered - which I feel is necessary for this position to reach its full potential. Around a month ago, my manager offered me a permanent contract in my current position, but without any pay increase. My manager did, however, ask me to source a training course that will help me pick up the skills I want. 

I looked around and found the MircoMasters in Data by MIT and showed it to my manager, who said we could discuss it further once I was permanent but that he was on-board. I decided to look around for jobs outside Company A and managed to secure a job offer as a Data Engineer from a data science consultancy (Company B) that will train me for a few months full-time before rolling me out to do client work. This training looks substantially more in-depth than anything on the MicroMasters and, in my opinion, is a much better opportunity in the long run - though I am still slightly on the fence, if I am entirely honest. Upon seeing that Company B also has a Machine Learning Engineer position, I asked if I could interview for that, too - a bit cheeky, I know. They gave me a maths test and an interview and I was offered that position, too. I now have until Monday to decide which role in Company B I would like to take so that they can open the other position back up to other applicants. 

I have already put significant time into researching both roles in Company B (Data Engineer and Machine Learning Engineer) and I am leaning more towards Data Engineer due to is versatility, but I am struggling to arrive at a final answer. I understand that the Machine Learning Engineer role is yet to reach maturity in the world of data and I would love to get a first-hand account of what it is like day-to-day. My current role is already very similar to data engineering in that I spend my time building basic pipelines and data models and then I make dashboards with interactive visualisations - I do most of this on PowerBI. I love the technical side of it at the moment, hence my desire to get formal training in data science principles and relevant coding languages. I already have experience with R and MatLab from my time studying engineering at university, but I never use these in my job as MatLab is license-only and my R knowledge is not sufficient in my opinion. Data engineering seems like the most logical path for me, but machine learning engineering is more technical and I think I would enjoy that. My hesitation with machine learning engineering is that the specialist role it occupies in the industry makes it less versatile and may give me fewer career options in the future. It also seems to me that this role is more back-of-office than the data engineering role, which is less preferable for me - this is only my speculation from what I have read, though. 

Obviously, I have no intention of leaving my manager at Company A hanging so as soon as I know what I am doing I want to let them know. In the future, I would be keen to come back to Company A when I have more experience and can take up a managerial position, so I am not trying to burn any bridges. My manger knows of my job offer from Company B and asked that I have a decision by my first week back in work - Monday coming. I have a chance to speak to my interviewer at Company B on Monday and I will ask him some questions to help me reach a decision, but I would love to hear what you lot have to say about it all first. 

I would appreciate any advice that anyone can give me which may help me make my mind up. In particular, I would be very interested in hearing from anyone currently occupying these positions. Thanks!"
4635,2021-01-01 01:36:08,1609457768.0,dataengineering,Not Sure Where to go with my Career,ko0mr2,shittyfuckdick,,https://www.reddit.com/r/dataengineering/comments/ko0mr2/not_sure_where_to_go_with_my_career/,1.0,10.0,0.0,22961.0,"Right now I’m working my first Data Engineer job for ~1.5 years. I don’t like it. 

But, I’m not sure if it’s the company or job I don’t like. At my company I’m the only DE even though I have no prior experience. I have no one to learn from and there’s pretty much zero structure to our data team. 

Here is what I do like about my job, in this order: 

1. Python 
2. Linux
3. SQL

I want to start learning some more on my own and doing side projects before I start applying. Idk if I want to keep doing DE or just be some sort of python developer. Anyone have suggestions? I figured I’d learn so machine learning, and maybe some Django and HTML to round myself out as a full stack dev."
4636,2021-01-01 23:04:41,1609535081.0,dataengineering,[Hiring] Senior Data Ops Engineer,koj2j5,nfmcclure,,https://www.reddit.com/r/dataengineering/comments/koj2j5/hiring_senior_data_ops_engineer/,1.0,0.0,0.0,22989.0,"Happy New Year DE-reddit,

I've messaged the mods if this is acceptable, they haven't responded, but they can remove this post if need be.

The company I work for, Hitch Works ([https://hitch.works/](https://hitch.works/) ) is hiring a Senior Data Ops Engineer. Job Posting link below.

**Job Summary:**

We are looking for someone to help us solve the DE tasks of data-pipelines, data-warehousing, and database syncing.

Additionally, we also need some custom help with deploying (testing, logging, AB deploys) of data science services. We are looking for someone with experience setting up DAG APIs.

The candidate should be a self-starter and be able to spot and solve problems. We are a small remote startup with about 15 developers. We are expanding. We are fully remote at the moment, even though the posting lists our headquarters (Seattle). You will be expected to work very closely with data science, engineering, and product.

We cannot sponsor candidates, but we have international employees in the EU. So we can legally hire in the US and EU currently. Salary is competitive based on location and experience.  


I can answer any other questions you have. Feel free to respond or message me. (Apply through the below link).

[https://hitchworks.bamboohr.com/jobs/view.php?id=36](https://hitchworks.bamboohr.com/jobs/view.php?id=36)"
4637,2021-01-02 02:21:03,1609546863.0,dataengineering,Where can I find part-time data engineering jobs in the US?,komnzl,engineer_of_data,,https://www.reddit.com/r/dataengineering/comments/komnzl/where_can_i_find_parttime_data_engineering_jobs/,1.0,7.0,0.0,22993.0,"I have been working as a data engineer part-time/full-time for one year and have a total of two years of experience in software engineering (if you count my internship). I currently train full-time for a sport, trying to make the national team for my country.

I could get this job to fit with my schedule, but I am currently looking for something else that can pay more and support me better. However, I'm limited in only having 20 hours a week to spare for work. Where can I find companies who will take me on part-time? Are startups the best option for this?"
4638,2021-01-02 06:33:25,1609562005.0,dataengineering,Data Science Tools,koqtm8,Techbiason,,https://www.reddit.com/r/dataengineering/comments/koqtm8/data_science_tools/,1.0,0.0,0.0,23000.0,
4639,2021-01-02 15:12:08,1609593128.0,dataengineering,Udemy new year sale - Get courses at $9.99 each,kox9hg,awsconsultant,,https://www.reddit.com/r/dataengineering/comments/kox9hg/udemy_new_year_sale_get_courses_at_999_each/,1.0,0.0,0.0,23010.0,
4640,2021-01-02 18:11:07,1609603867.0,dataengineering,How Entrepreneurs Can Use Data Aggregation to Grow Their Business,kp02bc,arminham1967,,https://www.reddit.com/r/dataengineering/comments/kp02bc/how_entrepreneurs_can_use_data_aggregation_to/,1.0,0.0,0.0,23012.0,
4641,2021-01-02 20:23:30,1609611810.0,dataengineering,Airflow DAG Schedule Graph Plugin,kp2kp4,savsr,,https://www.reddit.com/r/dataengineering/comments/kp2kp4/airflow_dag_schedule_graph_plugin/,1.0,0.0,0.0,23019.0,
4642,2021-01-02 20:54:16,1609613656.0,dataengineering,Remote Senior Data Engineer Jobs,kp36go,feelosophical,,https://www.reddit.com/r/dataengineering/comments/kp36go/remote_senior_data_engineer_jobs/,1.0,6.0,0.0,23019.0,I am based out of India with 3 and a half years of experience as a BigData engineer and on a lookout for remote job opportunities. Would appreciate any leads.
4643,2021-01-02 22:39:27,1609619967.0,dataengineering,Uncertainty in NoSQL vs SQL?,kp58u9,ChillySlacks,,https://www.reddit.com/r/dataengineering/comments/kp58u9/uncertainty_in_nosql_vs_sql/,1.0,0.0,0.0,23023.0,
4644,2021-01-02 23:28:44,1609622924.0,dataengineering,Looking for feedback - beginner DE project design (AWS),kp6666,lihingorange,,https://www.reddit.com/r/dataengineering/comments/kp6666/looking_for_feedback_beginner_de_project_design/,1.0,7.0,0.0,23024.0,"Hey all, I've been lurking this sub and am encouraged by all the constructive feedback given to beginners. I looked through past posts, but I'm still not fully confident about my intended project design. I appreciate any feedback!

**Summary/goals:**

I want grab the text of \~25,000 Japanese news articles on a particular website (this is a one-time batch job). I'll use a third-party API service to gather the news article text, which I'm guessing simply scrapes the requested URL and returns the relevant page info.

I've reviewed Japanese copyright and scraping related laws - taking data for my intended use is legal, but I just have to follow the site's ToS and robots.txt. Scraping isn't explicitly forbidden on the site, but my code will look for throttling and send a request to the third-party API every \~10 seconds to be considerate (hence a slow batch job with no concurrency). 

**Does my intended project design make sense?:**

third-party API (JSON data) -&gt; AWS EC2 (process an article's words, counts, and other info, store in a MySQL server in EBS) -&gt; copy completed DB to RDS MySQL -&gt; AWS Glue and Athena+QuickSight (word frequency visualization and prep for the next stage of my project)

It feels redundant to make a database in EBS and then copy it all to RDS. I'm using the EC2 free tier though, and I'm not sure if the t3.micro's 1 GB of storage is enough to hold everything I need.

Anyway, thanks again for any advice!"
4645,2021-01-03 02:03:18,1609632198.0,dataengineering,SLI/SLO/SLAs for pipelines?,kp92i1,swigganicks,,https://www.reddit.com/r/dataengineering/comments/kp92i1/slisloslas_for_pipelines/,1.0,8.0,0.0,23027.0,"I'm a machine learning engineer and we have many model training and evaluation pipelines as well as upstream ETL pipelines that we manage. Our team and product scope started pretty small so a mature SLO/SLA system didn't seem useful at the time, but now we're growing and expanding and it seems ripe to revisit how we incorporate SLO/SLA monitoring. I think we struggle a bit with prioritizing development velocity versus technical debt and having a clear set of SLIs and associated SLOs would help in this regard.

Most examples of SLI/SLA/SLO I've seen are in reference to traditional SWE and cover things like latency, requests served, error rates, etc. so I'm trying to understand what some key signals for data and machine learning pipelines I should be monitoring and alerting on instead.

Do you guys regularly integrate SLI/SLO/SLAs for your data and model pipelines?  What are some SLIs you use? How useful were they? What monitoring/alerting tools do you use?"
4646,2021-01-03 04:00:17,1609639217.0,dataengineering,About to attend College and can't decide on which degree will be most beneficial for a career in Date Engineering,kpb4ol,joeyarctic,,https://www.reddit.com/r/dataengineering/comments/kpb4ol/about_to_attend_college_and_cant_decide_on_which/,1.0,0.0,0.0,23031.0,
4647,2021-01-03 05:41:57,1609645317.0,dataengineering,Oracle DBA to Data Engineer,kpcup9,nc470,,https://www.reddit.com/r/dataengineering/comments/kpcup9/oracle_dba_to_data_engineer/,1.0,0.0,0.0,23033.0,
4648,2021-01-03 15:24:20,1609680260.0,dataengineering,Catching up with recent technology,kpkk75,bohoho,,https://www.reddit.com/r/dataengineering/comments/kpkk75/catching_up_with_recent_technology/,1.0,0.0,0.0,23043.0,
4649,2021-01-03 16:21:54,1609683714.0,dataengineering,Unit Testing Spark Jobs,kpldxk,T3Z0,,https://www.reddit.com/r/dataengineering/comments/kpldxk/unit_testing_spark_jobs/,1.0,1.0,0.0,23045.0,
4650,2021-01-03 20:31:06,1609698666.0,dataengineering,Do faang data engineering interviews test on dimensional modeling AND relational modeling?,kppvxh,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/kppvxh/do_faang_data_engineering_interviews_test_on/,1.0,0.0,0.0,23053.0,
4651,2021-01-03 20:42:59,1609699379.0,dataengineering,"The 24'th edition of @data_weekly focus on @netflix data warehouse storage optimization, @Adobe high throughput ingestion with Iceberg, @Uber @apachekafka disaster recovery,@ConflueraIQ @ApachePinot adoption &amp; year-in-review, @ApacheBeam data frame API",kpq4t7,vananth22,,https://www.reddit.com/r/dataengineering/comments/kpq4t7/the_24th_edition_of_data_weekly_focus_on_netflix/,1.0,0.0,0.0,23053.0,
4652,2021-01-03 21:45:03,1609703103.0,dataengineering,Serverless Webhook Design on AWS,kprevv,christolagali,,https://www.reddit.com/r/dataengineering/comments/kprevv/serverless_webhook_design_on_aws/,1.0,2.0,0.0,23058.0,"A Very Happy New year All!

In my pursuit of building a server-less webhook with cost effectiveness in mind, I stumbled upon bugs and some important todos.

I wish to help anyone who is about to embark on a similar journey so they can avoid the mistakes I did. In return I would appreciate any feedback that you can provide; this could be tips on better representation to topics I could cover to improve upon the article.

Would love to gather and work on the feedback!"
4653,2021-01-03 23:23:50,1609709030.0,dataengineering,Is the Udemy Airflow course worth it?,kptf00,montrex,,https://www.reddit.com/r/dataengineering/comments/kptf00/is_the_udemy_airflow_course_worth_it/,1.0,1.0,0.0,23060.0,"I've been interested in learning/testing Airflow for quite a while, and because these courses are on sale they look tempting.. However when looking through the details I couldn't work out if they were based on a Windows 10 machine.

I know Airflow doesn't natively work on Windows, but I was hoping for some hand holding on getting Docker or the Windows Linux App thing, working with Airflow.

Since there is almost no chance we will move away from Windows at work, I don't want to waste my time/money on this course if it doesn't really help me with a Windows implementation."
4654,2021-01-04 00:11:45,1609711905.0,dataengineering,Leetcode equivalent for Data Engineers?,kpubxl,civilsaspirant13,,https://www.reddit.com/r/dataengineering/comments/kpubxl/leetcode_equivalent_for_data_engineers/,1.0,34.0,0.0,23062.0,"Hi 

LC is mostly focused on SDE roles. Is there any such website, that serves a similar purpose specific to DE roles?"
4655,2021-01-04 07:15:14,1609737314.0,dataengineering,Can I download files in parallel?,kq1xzn,nartb,,https://www.reddit.com/r/dataengineering/comments/kq1xzn/can_i_download_files_in_parallel/,1.0,2.0,0.0,23073.0,"I'm not sure if this is a dumb question or not.

I'm trying to download and process a bunch of data but it can all be done in parallel chunks. I've got the processing parallelized using GNU Parallel but am running into issues when trying to parallelize the downloading, specifically timeout issues. I don't know much about networking so I'm not even sure if it makes sense to be downloading things in parallel.

I'm running this on a laptop with a quad-core processor.

I've noticed that GNU Parallel allows the max # of jobs to be 8. When I set this number to 4, the number of cores, the timeout issue seems to go away and the downloading chugs along just fine.

Hoping for someone with more experience to explain this.

Thanks!"
4656,2021-01-04 11:02:20,1609750940.0,dataengineering,SQL Server Hash-Partitioned Parallel Data Acquisition – How to Accelerate Your ‘E’ in ELT/ETL Using a Simple T-SQL Framework,kq521y,dingopole,,https://www.reddit.com/r/dataengineering/comments/kq521y/sql_server_hashpartitioned_parallel_data/,1.0,0.0,0.0,23080.0,
4657,2021-01-04 13:46:03,1609760763.0,dataengineering,Join the big data world,kq75kt,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/kq75kt/join_the_big_data_world/,1.0,0.0,0.0,23084.0,
4658,2021-01-04 15:07:02,1609765622.0,dataengineering,Master Data Management Market Worth $27.9 Billion by 2025,kq8eq6,pradnya123,,https://www.reddit.com/r/dataengineering/comments/kq8eq6/master_data_management_market_worth_279_billion/,1.0,0.0,0.0,23084.0,
4659,2021-01-04 15:09:26,1609765766.0,dataengineering,Master data management market will reach $27.9 billion by 2025,kq8g2r,pradnya123,,https://www.reddit.com/r/dataengineering/comments/kq8g2r/master_data_management_market_will_reach_279/,1.0,0.0,0.0,23084.0,
4660,2021-01-04 15:13:35,1609766015.0,dataengineering,Master Data Management Market Value to Reach $27.9B by 2025,kq8ih8,pradnya123,,https://www.reddit.com/r/dataengineering/comments/kq8ih8/master_data_management_market_value_to_reach_279b/,1.0,0.0,0.0,23084.0,
4661,2021-01-04 17:27:08,1609774028.0,dataengineering,"Using Python to write data from 900,000 individual files in an S3 bucket into a MySQL database, but script is taking DAYS to finish, what best practices have I missed?",kqauxl,Lostwhispers05,,https://www.reddit.com/r/dataengineering/comments/kqauxl/using_python_to_write_data_from_900000_individual/,1.0,0.0,0.0,23088.0,
4662,2021-01-04 18:00:03,1609776003.0,dataengineering,Apache Flink's stream-batch unification powers Alibaba's 2020 Double 11 Global Shopping Festival,kqbhzf,Marksfik,,https://www.reddit.com/r/dataengineering/comments/kqbhzf/apache_flinks_streambatch_unification_powers/,1.0,0.0,0.0,23088.0,
4663,2021-01-04 18:37:30,1609778250.0,dataengineering,Kafka Connect - Deep Dive into Single Message Transforms,kqc987,rmoff,,https://www.reddit.com/r/dataengineering/comments/kqc987/kafka_connect_deep_dive_into_single_message/,1.0,0.0,0.0,23090.0,
4664,2021-01-04 19:59:05,1609783145.0,dataengineering,"Operations (SQL, Python, Huginn) to Engineering (SQL, Python, ETL, Cloud)",kqdxjo,NOTHINGBUTCOMPUTERS,,https://www.reddit.com/r/dataengineering/comments/kqdxjo/operations_sql_python_huginn_to_engineering_sql/,1.0,8.0,0.0,23093.0,"Hello chaps,

**Quick summary** \- I studied mechanical engineering at uni and worked approx. 3 years doing project engineering. I worked in my spare time doing Java/Python which landed me a data ops role that I've been in since March 2020, doing basic SQL, a bit of Python and maintaining the Huginn pipelines we use (not sure if using Huginn for data pipelines is a huge red flag for the company). I hoped this would get me an ""in"" into the data industry. I've resolved to make the leap from ops to engineering this year. An internal move is likely off the cards due to language barrier between the eng team and the ops team, and they're very experienced and not recruiting junior positions. I'm London-based, if it helps anyone figure out the tech ""scene"" I'm in.

**The first meat and potatoes** \- Like I said, I want to move to eng. All over the reddit it's easy to read about ""learn Spark, learn Hadoop, learn Airflow, brush up on data structures/algorithms and polish your Python and SQL"". I've drilled these into my head that they need to be done but... **How good do you actually need to be? Does anyone have a reasonable benchmark for what a junior position is expected to know?** 

**The second meat and potatoes** \- I don't feel like I'm learning a huge amount more in my role - the extensive use of Huginn has me particularly worried that I'm stagnating. **If I'm not learning, is that reason enough to ditch my current job in favour of a (very) junior data engineering position?** Conversely, I'm worried about leaving my current role and landing a data engineering job that is just SQL queries which also pigeon-holes me into uninteresting work.

Appreciate any and all responses. Will answer all questions if anyone wants to dig further into this, in case it helps."
4665,2021-01-04 22:47:19,1609793239.0,dataengineering,Why you should be using your data warehouse to sync your SaaS tools,kqhjs4,ced_narrator,,https://www.reddit.com/r/dataengineering/comments/kqhjs4/why_you_should_be_using_your_data_warehouse_to/,1.0,0.0,0.0,23097.0,
4666,2021-01-05 00:31:30,1609799490.0,dataengineering,Nature of 'staging area' in a DWH for DBT,kqjrff,omnigoth,,https://www.reddit.com/r/dataengineering/comments/kqjrff/nature_of_staging_area_in_a_dwh_for_dbt/,1.0,9.0,0.0,23101.0,"Hi all, I'm quite a beginner in data engineering and the only 'data engineer' in my 6 person org. I've read quite extensively on DBT and prototyped some models with it. I'm creating a basic data pipeline to get data from simple APIs, store these raw datasets in an s3 bucket, then build data models out of these. I'm using redshift. So my pipeline goes like this:

**source(s) -&gt; s3 -&gt; redshift staging schema -&gt; da models in public schema(built with DBT)**

Now my question is, I've seen so much written about DBT when you have the data in the warehouse but I haven't seen any best practices of how to store the data in a staging area within the warehouse for the use of DBT. How do you generally structure your staging area in the DWH? Do you push every single record needed for the final data models in the staging schema? And for how long?"
4667,2021-01-05 00:39:50,1609799990.0,dataengineering,Data quality management platform for DWHs,kqjxe2,RaikoL,,https://www.reddit.com/r/dataengineering/comments/kqjxe2/data_quality_management_platform_for_dwhs/,1.0,4.0,0.0,23102.0,"Hello!

I’ve been developing a DQM platform with a small team for over a year now. We’ve talked to dozens of companies and tested our solution in some of them.

We would very much like to get some feedback and ideas from the community to develop it further (pain points in QA, functionalities you are lacking in test process, etc).

Our site: [https://litech.app](https://litech.app)"
4668,2021-01-05 01:38:45,1609803525.0,dataengineering,Where can I find ECS and Fargate in the AWS Pricing Calculator?,kql4qy,engineer_of_data,,https://www.reddit.com/r/dataengineering/comments/kql4qy/where_can_i_find_ecs_and_fargate_in_the_aws/,1.0,0.0,0.0,23103.0,
4669,2021-01-05 01:44:16,1609803856.0,dataengineering,Data Modeling,kql8p8,tpedar50,,https://www.reddit.com/r/dataengineering/comments/kql8p8/data_modeling/,1.0,0.0,0.0,23103.0,
4670,2021-01-05 03:43:10,1609810990.0,dataengineering,Airflow failsafe?,kqnjbo,warrenbuddgett,,https://www.reddit.com/r/dataengineering/comments/kqnjbo/airflow_failsafe/,1.0,16.0,0.0,23109.0,"Hi all. Like many of you, my organization uses AF to manage workflows and pipelines. This morning, the EC2 instance that hosts our dockerized AF went bust and took its logs with it. We eventually recovered but not without a lot work - it was indeed a great way to return to work after the holidays /s. 

What measure do you have in place to automate the resolution of this should it happen to you? Do you have a DR solution for airflow? Perhaps a standby server hosting a replicated image of your production AF running in parallel?

Any insight would be much appreciated. Thanks!"
4671,2021-01-05 08:09:49,1609826989.0,dataengineering,impact of artificial intelligence in everyday life,kqs5yk,chase2learn,,https://www.reddit.com/r/dataengineering/comments/kqs5yk/impact_of_artificial_intelligence_in_everyday_life/,1.0,0.0,0.0,23112.0,
4672,2021-01-05 09:42:28,1609832548.0,dataengineering,Why Data Versioning as an Infrastructure Matters,kqtgnx,nourishing-saliva,,https://www.reddit.com/r/dataengineering/comments/kqtgnx/why_data_versioning_as_an_infrastructure_matters/,1.0,2.0,0.0,23116.0,
4673,2021-01-05 12:31:32,1609842692.0,dataengineering,Data Virtualization,kqvv24,soujoshi,,https://www.reddit.com/r/dataengineering/comments/kqvv24/data_virtualization/,1.0,0.0,0.0,23124.0,
4674,2021-01-05 15:41:46,1609854106.0,dataengineering,What should a SaaS company's infrastructure look like?,kqyo1u,SalahAzhary,,https://www.reddit.com/r/dataengineering/comments/kqyo1u/what_should_a_saas_companys_infrastructure_look/,1.0,0.0,0.0,23132.0,
4675,2021-01-05 18:37:06,1609864626.0,dataengineering,What is the best etl/elt tool to use with a snowflake cdw?,kr1yvb,ito_integration,,https://www.reddit.com/r/dataengineering/comments/kr1yvb/what_is_the_best_etlelt_tool_to_use_with_a/,1.0,10.0,0.0,23139.0,"My org is currently standing up a snowflake warehouse. People suggest so many different cloud etl/elt tools .. I'm getting kind of lost. We need more governance than can be provided by dbt or Snowpipe, so we're looking at the short list of vendors below.

Let's all help each other out and cast our votes for the best cloud tool for etl/elt. May the best tool win.

[View Poll](https://www.reddit.com/poll/kr1yvb)"
4676,2021-01-05 19:23:31,1609867411.0,dataengineering,"APIs to continuously detect and protect sensitive data including credit cards, credentials, names, and addresses",kr2yak,alig80,,https://www.reddit.com/r/dataengineering/comments/kr2yak/apis_to_continuously_detect_and_protect_sensitive/,1.0,0.0,0.0,23141.0,
4677,2021-01-05 20:41:25,1609872085.0,dataengineering,Create a json file with env var contents,kr4oct,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/kr4oct/create_a_json_file_with_env_var_contents/,1.0,0.0,0.0,23145.0,
4678,2021-01-05 21:04:47,1609873487.0,dataengineering,What tools do you use for ER Diagrams and Data Dictionaries?,kr56hk,mac-0,,https://www.reddit.com/r/dataengineering/comments/kr56hk/what_tools_do_you_use_for_er_diagrams_and_data/,1.0,23.0,0.0,23146.0,"One of our initiatives this year is to improve our knowledge base by improving ER Diagrams, Data Dictionaries, etc.

Does anyone have any suggestions for tools that can be used at a large scale? Ones that are partially automated, e.g. they can infer database schemas to spit something out, but also let you overwrite and manually ""fix"" things? Something that is easy to maintain (meaning that if someone adds/changes a table, they don't need to repeat the entire process, a new table can easily be added to a diagram and data dictionary). 

Do tools like that exist that work well at a large scale? I'm seeing a lot of solutions like lucid chart, but the challenges is coming up with a solution that a team of engineers can easily work with. At a small scale, something like lucid chart is fine, but when you have hundreds of tables and dozens of engineers, it's tough to use and documentation gets outdated."
4679,2021-01-05 22:14:37,1609877677.0,dataengineering,Airflow XComs for Beginners in 10 mins!,kr6oty,marclamberti,,https://www.reddit.com/r/dataengineering/comments/kr6oty/airflow_xcoms_for_beginners_in_10_mins/,1.0,3.0,0.0,23149.0,
4680,2021-01-05 23:08:10,1609880890.0,dataengineering,Are GCP persistent disks too slow for doing large file IO?,kr7uaw,nartb,,https://www.reddit.com/r/dataengineering/comments/kr7uaw/are_gcp_persistent_disks_too_slow_for_doing_large/,1.0,0.0,0.0,23149.0,
4681,2021-01-06 05:25:40,1609903540.0,dataengineering,The Best CDP Solution Is Already Sitting In Your Data Warehouse,krf73d,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/krf73d/the_best_cdp_solution_is_already_sitting_in_your/,1.0,0.0,0.0,23162.0,
4682,2021-01-06 12:41:29,1609929689.0,dataengineering,Introduction to Apache Airflow,krlh54,adilkhash,,https://www.reddit.com/r/dataengineering/comments/krlh54/introduction_to_apache_airflow/,1.0,3.0,0.0,23174.0,
4683,2021-01-06 16:46:52,1609944412.0,dataengineering,[help] Data casting in Data Vault,krp7ze,Mr_Again,,https://www.reddit.com/r/dataengineering/comments/krp7ze/help_data_casting_in_data_vault/,1.0,4.0,0.0,23183.0,"I'm building a raw data vault, and I understand that data should not be modified when loading in to the raw data vault layer from the staging layer. (correct me if I'm wrong)

However, given a simple example where a date has been saved in the staging layer as an integer, and I need to cast it to a date type. I can't find any information in the Data Vault book by Dan Linstead on when this kind of transformation is supposed to happen so I was wondering if anyone knew.

My options are:

1. Just cast it to a date type when loading the satellite in raw vault.

2. Load the satellite with the original data type and add a satellite to the business vault containing the casted field.

4. Only cast the data to the correct type in the next layer, ie. the presentation layer or conformed layer.

3. Add both the original data and a casted field to the raw vault satellite ie. date_of_birth_orig INT and date_of_birth DATE.

Seems like an important point I can't find any information on, any help appreciated, thanks."
4684,2021-01-06 17:09:09,1609945749.0,dataengineering,The Most Popular Databases - 2006/2020 - Statistics and Data,krpnkn,accappatoiviola,,https://www.reddit.com/r/dataengineering/comments/krpnkn/the_most_popular_databases_20062020_statistics/,1.0,0.0,0.0,23183.0,
4685,2021-01-06 17:51:26,1609948286.0,dataengineering,Open Beta Kafka IDE 2020.12.5 available from today!,krqgpq,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/krqgpq/open_beta_kafka_ide_2020125_available_from_today/,1.0,0.0,0.0,23185.0,"[Kafka IDE](https://kafkaide.com/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=release-2020.12.5&amp;position=top&amp;utm_term=dataengineering) is a desktop client similar to Tableau or Looker that queries Apache Kafka directly. It aims to be a better alternative to kafkacat, kafka manager or similar.

\---

Confluent's Schema Registry version 5.5.0 added two new schema formats besides the already supported Avro schema. These new formats are Protobuf Buffers version 3 and JsonSchema draft 7.

&amp;#x200B;

**Introducing Avro, Protobuf, JsonSchema support**

KafkaIDE 2020.12.5 now fully supports any schema format, including Confluent Platform ones, plus a new enhanced schemaless topic inference, which smartly deduces from raw data the schema in your topic without the need of using Confluent's Schema Registry.

We are very excited to announce that [version 2020.12.5](https://kafkaide.com/download/?utm_source=Reddit&amp;utm_medium=social&amp;utm_campaign=release-2020.12.5&amp;position=bottom&amp;utm_term=dataengineering) is available from today.

&amp;#x200B;

**We want your feedback!**

If you like Kafka IDE and think we can do better, we would love to hear from you. Let us know if you are available for a quick chat.

&amp;#x200B;

**Detailed release notes**

* Protobuf and JsonSchema support
* Improved field and querying operations when using Avro Schemas and schemaless topics.
* Fixed an issue where empty topics triggered longer waits for field inference."
4686,2021-01-06 18:23:47,1609950227.0,dataengineering,Do FANG DE Interviews have a system design portion?,krr4f3,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/krr4f3/do_fang_de_interviews_have_a_system_design_portion/,1.0,0.0,0.0,23187.0,
4687,2021-01-06 20:59:57,1609959597.0,dataengineering,Why Testing Your Data Is Insufficient,krukhb,mkvor8,,https://www.reddit.com/r/dataengineering/comments/krukhb/why_testing_your_data_is_insufficient/,1.0,0.0,0.0,23188.0,
4688,2021-01-06 22:22:24,1609964544.0,dataengineering,Centralized or Decentralized Data Ingestion and DQ to Data Warehouse,krwblq,aDigitalPunk,,https://www.reddit.com/r/dataengineering/comments/krwblq/centralized_or_decentralized_data_ingestion_and/,1.0,0.0,0.0,23191.0,
4689,2021-01-07 01:38:33,1609976313.0,dataengineering,Change Data Capture(CDC) using Singer,ks09o1,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/ks09o1/change_data_capturecdc_using_singer/,1.0,0.0,0.0,23193.0,
4690,2021-01-07 03:22:01,1609982521.0,dataengineering,BI Engineering vs DE (AMAZON) Career progression,ks28cz,Aero_Lad,,https://www.reddit.com/r/dataengineering/comments/ks28cz/bi_engineering_vs_de_amazon_career_progression/,1.0,15.0,0.0,23193.0,"Hello guys, I have just received an internship offer for a **BI Engineering position at Amazon**, with a project shifted towards **DE (some web-scraping, building pipelines and probably some visualization tools on top of that)**. I think the proiect is interesting and the team is small enough for me to learn a lot of techniques and develop, however my **ideal position would be to shift towards the DE role, either at Amazon or at another tech giant** (or FinTech etc). From my limited understanding and reading some job descriptors, the 2 positions are really similar, depending on the company obv.

Do you guys have any idea is **this position (BIE) can be a good starting point for a career in DE**, and how easy would be to change roles, assuming my project is mostly on the DE part already? I would massively appreciate it if any of you guys had any inputs or advice!

Thanks in advance and all the best!"
4691,2021-01-07 10:16:52,1610007412.0,dataengineering,Distributed Locking for Micro-Batch Processing: Preventing Resource Starvation,ks8wsz,schoolgurllou,,https://www.reddit.com/r/dataengineering/comments/ks8wsz/distributed_locking_for_microbatch_processing/,1.0,0.0,0.0,23201.0,
4692,2021-01-07 11:14:30,1610010870.0,dataengineering,"Best online courses, best place to start?",ks9p5p,ViolinistFriendly,,https://www.reddit.com/r/dataengineering/comments/ks9p5p/best_online_courses_best_place_to_start/,1.0,10.0,0.0,23205.0,"I've been working as a general software engineer for the last 3 years at a larger company, I would like to move towards a data engineering position. 

What are some good places to start? 
Are there some good online courses/resources to jump start myself into the content?
Any good project ideas to throw myself into it?

Thanks for your time!"
4693,2021-01-07 13:43:04,1610019784.0,dataengineering,Why Data Versioning as an Infrastructure Matters,ksbnq0,hnikret,,https://www.reddit.com/r/dataengineering/comments/ksbnq0/why_data_versioning_as_an_infrastructure_matters/,1.0,3.0,0.0,23209.0,
4694,2021-01-07 17:23:06,1610032986.0,dataengineering,Python and Apache Parquet. Yes Please.,ksfdxk,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/ksfdxk/python_and_apache_parquet_yes_please/,1.0,0.0,0.0,23212.0,There is nothing better then Python and Apache Parquet. Parquet has taken over the Big Data world and Data Engineering. It's a beautiful thing. [https://www.confessionsofadataguy.com/python-and-apache-parquet-yes-please/](https://www.confessionsofadataguy.com/python-and-apache-parquet-yes-please/)
4695,2021-01-07 18:42:25,1610037745.0,dataengineering,DE Pipeline best practices to handle failure/errors based on your experience?,ksh1ed,phmark19,,https://www.reddit.com/r/dataengineering/comments/ksh1ed/de_pipeline_best_practices_to_handle/,1.0,0.0,0.0,23216.0,
4696,2021-01-07 19:26:52,1610040412.0,dataengineering,Data Scientist vs Data Engineer vs Data Analyst - CLOUDit-eg,kshzx1,Malika_harkati,,https://www.reddit.com/r/dataengineering/comments/kshzx1/data_scientist_vs_data_engineer_vs_data_analyst/,1.0,0.0,0.0,23219.0,
4697,2021-01-08 00:30:21,1610058621.0,dataengineering,can you authenticate with LDAPS on Adobe Airflow 2.0.0,ksojog,deve_o,,https://www.reddit.com/r/dataengineering/comments/ksojog/can_you_authenticate_with_ldaps_on_adobe_airflow/,1.0,4.0,0.0,23229.0,"I have been trying to authenticate Airflow with LDAPS. Recently have been tasked with setting up an airflow server and I have LDAP working, however in our environment our security officer would like everything to authenticate via LDAPS.  uncertain if it makes a difference, but we are running Airflow on a Windows Server 2019 build using WSL.  Not sure if LDAPS was dropped in a later version for unknown reasons, but i have seen very little documentation on this.  Just wondering if anyone has LDAPS working on Airflow 2.0.0."
4698,2021-01-08 01:21:42,1610061702.0,dataengineering,Airflow and Python Multiprocessing Module,ksplzb,sudnyank,,https://www.reddit.com/r/dataengineering/comments/ksplzb/airflow_and_python_multiprocessing_module/,1.0,0.0,0.0,23230.0,
4699,2021-01-08 02:50:22,1610067022.0,dataengineering,Masters to focus on DE?,ksr9r6,quicktypeeats,,https://www.reddit.com/r/dataengineering/comments/ksr9r6/masters_to_focus_on_de/,1.0,7.0,0.0,23235.0,I have years of experience as a dev and I’m going to pursue a masters in CS. Wondering if people think that is a good idea if I specifically want to focus on data engineering.
4700,2021-01-08 06:46:05,1610081165.0,dataengineering,Single RDBMS becoming the bottleneck.,ksvd5y,Shoddy_Researcher,,https://www.reddit.com/r/dataengineering/comments/ksvd5y/single_rdbms_becoming_the_bottleneck/,1.0,12.0,0.0,23238.0,"Hi All,

High level question.

Let's say there is a single monolithic 2TB SQLServer DB that handles all reads/writes. This is getting very close to reaching the limits of vertically scaling in a single instance - predominantly due to the heavy processing done inside MANY store procs.

&amp;#x200B;

What's the traditional options to scale here? 

I'm thinking reads are fine, so a read-replica doesn't help. However I could offload the most intensive DB calculations to external services? IE. Spark reads, does its own calcs and updates/inserts after calculation."
4701,2021-01-08 10:17:28,1610093848.0,dataengineering,Embrace The Data Schlep First,ksyagl,katori24tumble1,,https://www.reddit.com/r/dataengineering/comments/ksyagl/embrace_the_data_schlep_first/,1.0,0.0,0.0,23245.0,
4702,2021-01-08 14:09:47,1610107787.0,dataengineering,DataTalks.Club Conference - ML in production,kt19b4,stolzen,,https://www.reddit.com/r/dataengineering/comments/kt19b4/datatalksclub_conference_ml_in_production/,1.0,0.0,0.0,23253.0,
4703,2021-01-08 14:30:49,1610109049.0,dataengineering,Make Money Online,kt1k1j,Successful_Ad3199,,https://www.reddit.com/r/dataengineering/comments/kt1k1j/make_money_online/,1.0,0.0,0.0,23253.0,
4704,2021-01-08 15:10:38,1610111438.0,dataengineering,Ensure the Best Web Data Scraping Services For Enterprises With Saivi | Web scraping service providers In UK | US | India,kt25qq,Optisolsolution,,https://www.reddit.com/r/dataengineering/comments/kt25qq/ensure_the_best_web_data_scraping_services_for/,1.0,0.0,0.0,23256.0,
4705,2021-01-08 17:49:14,1610120954.0,dataengineering,Microsoft has a ton of free courses for data engineering. Here is one with 10 hours of content for DataBricks,kt5329,tmccormick92,,https://www.reddit.com/r/dataengineering/comments/kt5329/microsoft_has_a_ton_of_free_courses_for_data/,1.0,18.0,0.0,23262.0,
4706,2021-01-08 18:21:46,1610122906.0,dataengineering,AWS Redshift vs Apache Spark with Parquets? Which one to choose?,kt5r4a,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/kt5r4a/aws_redshift_vs_apache_spark_with_parquets_which/,1.0,16.0,0.0,23263.0,"I've got to decided between AWS Redshift vs Apache Spark with Parquets stored in s3 for a data warehouse. On the surface there doesn't seem to be much difference except that Spark probably gives more flexibility and transforms. Both can do analytics at scale, the same type of joins and aggregations. I need some input about the pros and cons of each as opposed to each other for data warehousing."
4707,2021-01-08 19:38:37,1610127517.0,dataengineering,Data modeling : Employees and their skills,kt7dsf,neluk,,https://www.reddit.com/r/dataengineering/comments/kt7dsf/data_modeling_employees_and_their_skills/,1.0,0.0,0.0,23268.0,
4708,2021-01-08 20:45:37,1610131537.0,dataengineering,We built a multi-column fuzzy merge tool for combining datasets. No software to install - runs right in your browser. It's free to try and we are looking for feedback! We also make it really easy to build and deploy ML models (Akkio.com),kt8t2q,Akkio-JonR,,https://www.reddit.com/r/dataengineering/comments/kt8t2q/we_built_a_multicolumn_fuzzy_merge_tool_for/,1.0,0.0,0.0,23270.0,
4709,2021-01-09 00:08:51,1610143731.0,dataengineering,NodeJS,ktd27l,Swimming_Ad1570,,https://www.reddit.com/r/dataengineering/comments/ktd27l/nodejs/,1.0,0.0,0.0,23275.0,
4710,2021-01-09 00:56:52,1610146612.0,dataengineering,How do you deal with this issue?,kte0q1,levelworm,,https://www.reddit.com/r/dataengineering/comments/kte0q1/how_do_you_deal_with_this_issue/,1.0,5.0,0.0,23278.0,"Sorry I don't even know how to call it, so I'll make up a story to describe it.

Assuming your company is using microservices, and service A and B do not fetch information from each other. This is a descision you cannot change. Both services send data into their own Kafka topics, from which you mirrorred into your side and finally into some db tables. Now business analyst wants to use data from both services to do analysis, but they do not know how to join them. How do you mitigate the issue?

A concrete example that we are experiencing: In our game, player has bankrolls (think coins), and bankroll changes very rapidly. Now I want to know, what's the bankroll of a player when he launches a raid towards his enemies. Now, the raid part comes from another service that doesn't care about bankroll, so business analysts cannot join the two tables. Or they can, by joining close timestamps, which is ugly.

How should I prepare the data that mitigates this kind of issues?

I did a bit research and looks like the technics about RCD doesn't really solve the issue (junk dimension), anything else? BTW we are using columnar database, but I'm willing to take any data modelling that solves this issue."
4711,2021-01-09 03:41:04,1610156464.0,dataengineering,Data Engineering Slack Channels?,kth30n,Dave0404,,https://www.reddit.com/r/dataengineering/comments/kth30n/data_engineering_slack_channels/,1.0,3.0,0.0,23281.0,Anyone know of some good slack channels for data engineers? I’m familiar with ones for dbt and prefect but trying to compile a more comprehensive list.
4712,2021-01-09 10:00:31,1610179231.0,dataengineering,High velocity high volume streaming data ingestion and persistence,ktmtqz,cptfoobar,,https://www.reddit.com/r/dataengineering/comments/ktmtqz/high_velocity_high_volume_streaming_data/,1.0,9.0,0.0,23292.0,"Hi. I'm wondering what you folk use for streaming large volumes of real time data at a high velocity. Ballpark numbers would be trillions of records a day, coming in at between 1M-10M records/sec. 

Our current setup is to let the source push the data into our Kafka cluster, from where different Spark Steaming jobs (structured and unstructured) consume the records, maybe do some monitor transformation and dump to HDFS in parquet. But we almost always have something or the other going wrong (issues with maintaining Kafka offsets, write failures to HDFS, skipping batches in jobs etc). 

How do you people do it? Do you there are other stacks better suited for this kind of a workload?

Thoughts?"
4713,2021-01-09 12:30:25,1610188225.0,dataengineering,Do you know a high-performance library for time series(EEG) visualization on the web? Plotly and Bokeh are great but can't handle the data amount. I am searching for a lib which on I can build something similar to edfbrowser?,ktoknl,rested_as_usual,,https://www.reddit.com/r/dataengineering/comments/ktoknl/do_you_know_a_highperformance_library_for_time/,1.0,0.0,0.0,23296.0,
4714,2021-01-09 18:30:55,1610209855.0,dataengineering,Deploying Apache Airflow on Kubernetes,kttzrz,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/kttzrz/deploying_apache_airflow_on_kubernetes/,1.0,4.0,0.0,23301.0,
4715,2021-01-09 19:38:22,1610213902.0,dataengineering,Which role requires SQL skill as their core skill-set?,ktvcl8,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/ktvcl8/which_role_requires_sql_skill_as_their_core/,1.0,0.0,0.0,23303.0,
4716,2021-01-09 20:07:56,1610215676.0,dataengineering,Building AWS Portifolio with Free Tier.,ktvxyk,mathisnot,,https://www.reddit.com/r/dataengineering/comments/ktvxyk/building_aws_portifolio_with_free_tier/,1.0,0.0,0.0,23304.0,
4717,2021-01-09 22:56:20,1610225780.0,dataengineering,Python/PySpark code and Databricks,ktz8kp,Vallack,,https://www.reddit.com/r/dataengineering/comments/ktz8kp/pythonpyspark_code_and_databricks/,1.0,0.0,0.0,23314.0,
4718,2021-01-10 01:48:52,1610236132.0,dataengineering,How much have certs helped in your career?,ku2i65,qazwsx123_1_2,,https://www.reddit.com/r/dataengineering/comments/ku2i65/how_much_have_certs_helped_in_your_career/,1.0,7.0,0.0,23328.0," I’m looking for my first data engineering job. I’m wondering if i should do AWS data analytics (formerly big data ) Cert ? Has this helped you guys land interviews? 

On the topic, what online courses have helped you guys land jobs or just help to gain more knowledge? What has people’s experience been with datacamp or coursera?

Thanks a lot for answering?"
4719,2021-01-10 02:06:57,1610237217.0,dataengineering,Designing a database from IMDB data,ku2uyy,Choose_Goose79,,https://www.reddit.com/r/dataengineering/comments/ku2uyy/designing_a_database_from_imdb_data/,1.0,10.0,0.0,23332.0,"Hi,

In attempt to put my understanding of relational database theory to work. I have decided to recreate my own relational database from IMDB data.Data can be found here ([https://datasets.imdbws.com/](https://datasets.imdbws.com/))

&amp;#x200B;

I would like to know if what I have come up with is okay, this will be my first time trying to design something from scratch and i am not sure if i have made any major mistakes or if i could do something better, if so can anyone point some of these out.

&amp;#x200B;

Below is a graphical illustration of the design i came up with made using [draw.io](https://www.draw.io)

https://preview.redd.it/5nkb8a1ecea61.png?width=1921&amp;format=png&amp;auto=webp&amp;s=ee1d9589ecf8e48eb2ce6730fc8464c9a996653a

I was mainly interested in animated / cartoon tv shows which is why everything branches off a production studio.

The thought process begin the design is as follows:

1. Production studios create a cartoon / animated tv show.
2. Cartoon has two main areas, individuals (people who worked on its production: writers, directors, storyboard writers, animators and voice actors) and episodes.
3. The individuals area holds unique individuals in one table and the unique roles each individual had in the cartoon as a whole in another table ( individuals can have more than one role e.g a writer could also be an animator or director)
4. The episode area has a table to hold the episodes aired for a particular cartoon and another table that holds casting information for episodes providing information such as who was involved in the episodes production and what their role was.

The ""Individual Roles"" table has a compound primary key  created from the individuals ID and a role ID, therefore creating a unique identification for each row. The reason why this table exists is because i expect many individuals to have more than one role depending on either how involved theyre on a particular show or if they did worked under different roles for different tv shows. This table also links to the ""Episode Cast"" table through the compound key as a way of providing information on the individual involved in a particular episode and their role for that episode.

I didn't want to repeat this data so decided  to have the ""Individual Roles"" table as a unique list of an individual and whatever role they are capable of performing, and using this as reference in the ""Episode Cast"" table as opposed to repeating this information in text form throughout the episode cast table.

The ""Cartoon Characters"" table holds a list of characters associated to a cartoon and is related to the ""Individual Character"" table which holds the information on the individual (voice actor) and the character they voice. A character can be voiced by multiple people therefore a character id can exist more than once in this table thus the one to many relationship between it and the ""Cartoon Characters"" table. This table also  doesn't have a unique / primary key as i thought one was not needed.

Last but not least, I have a table which contains details genre wise of a cartoon. I expect cartoons to have a number of categories such as comedy, action and romance. So the ""Cartoon Details"" table which holds this information will have a number of rows for each genre it is associated with. The genre name is held in the ""Genre"" table and is linked to the ""Cartoon Details"" table through a one to one relationship, this is to prevent the name being repeated in text in the ""Cartoons Details"" table.

&amp;#x200B;

I tried to make the design as efficient as possible but i am not sure if my thought process and design choices are as sound as possible, please let me know what you think.

&amp;#x200B;

Kind regards,

\-Goose"
4720,2021-01-10 03:47:03,1610243223.0,dataengineering,"Coming from DE intern (healthcare, HL7 standards) in major US city. Any feedback would be helpful. Thank you!",ku4o1i,swiss_alpines,,https://www.reddit.com/r/dataengineering/comments/ku4o1i/coming_from_de_intern_healthcare_hl7_standards_in/,1.0,0.0,0.0,23339.0,
4721,2021-01-10 04:38:07,1610246287.0,dataengineering,Advice for improving my company's data strategy,ku5k14,Guitar_player87,,https://www.reddit.com/r/dataengineering/comments/ku5k14/advice_for_improving_my_companys_data_strategy/,1.0,0.0,0.0,23344.0,
4722,2021-01-10 15:00:46,1610283646.0,dataengineering,From data engineering to cloud engineering,kue2au,dataeng_cloud,,https://www.reddit.com/r/dataengineering/comments/kue2au/from_data_engineering_to_cloud_engineering/,1.0,16.0,0.0,23373.0,"I need some advice regarding a career/job  change decision that I have to take.

I am currently working as a data engineer (5 years) and in my opinion it's a very hectic and laborious job considering you have to learn a lot of diverse tools, do scripting in multiple languages and is very complicated job in my opinion.

From what I have observed, Cloud engineering is a bit easier than data engineering. in my team DE folks work for more than 12 hours a days but cloud guys there rarely work that much. Although CE has to code too but no as much as DE.

I am getting burnt out working as a DE and this is my 2nd job , previous job was also very hectic.

P.S. - I am not in anyway saying that cloud engineers don't work as hard, I just need your opinion if my thinking is correct or not and whether I should move to cloud or do both or do only DE. I also have been working on aws/azure services relating to DE."
4723,2021-01-10 17:40:41,1610293241.0,dataengineering,How to Build a Modern Data Lake with MinIO,kuglqz,guliyo,,https://www.reddit.com/r/dataengineering/comments/kuglqz/how_to_build_a_modern_data_lake_with_minio/,1.0,0.0,0.0,23382.0,
4724,2021-01-10 20:55:29,1610304929.0,dataengineering,Looking for feedback on another DE project,kuk95n,TheKoalaKeys,,https://www.reddit.com/r/dataengineering/comments/kuk95n/looking_for_feedback_on_another_de_project/,1.0,0.0,0.0,23387.0,
4725,2021-01-10 21:15:34,1610306134.0,dataengineering,Azure data certification for backend application developer,kukohv,devsujit,,https://www.reddit.com/r/dataengineering/comments/kukohv/azure_data_certification_for_backend_application/,1.0,0.0,0.0,23388.0,
4726,2021-01-10 22:12:33,1610309553.0,dataengineering,"The 25'th edition of @data_weekly focus on @kleinerperkins future of data infra, @Intuit data journey, @AlibabaGroup Flink 4B events per sec, @LinkedIn Gobblin journey, @databricks handling late-arriving dimension, @ExpediaGroup ML deployment pattern",kulw8r,vananth22,,https://www.reddit.com/r/dataengineering/comments/kulw8r/the_25th_edition_of_data_weekly_focus_on/,1.0,0.0,0.0,23391.0,
4727,2021-01-11 01:04:59,1610319899.0,dataengineering,Resume Review,kupgb7,citizenofacceptance2,,https://www.reddit.com/r/dataengineering/comments/kupgb7/resume_review/,1.0,0.0,0.0,23395.0,
4728,2021-01-11 01:23:28,1610321008.0,dataengineering,Questions About Data Engineering,kupsvc,rilan270,,https://www.reddit.com/r/dataengineering/comments/kupsvc/questions_about_data_engineering/,1.0,0.0,0.0,23395.0,
4729,2021-01-11 03:39:58,1610329198.0,dataengineering,Finished my first project,kusak7,mugshotjoshy,,https://www.reddit.com/r/dataengineering/comments/kusak7/finished_my_first_project/,1.0,0.0,0.0,23397.0,
4730,2021-01-11 05:11:26,1610334686.0,dataengineering,How to create HA API consumers?,kutvrr,Shoddy_Researcher,,https://www.reddit.com/r/dataengineering/comments/kutvrr/how_to_create_ha_api_consumers/,1.0,1.0,0.0,23399.0,"Hi,

Let's say I have a stream of data coming in via repeated polling of a REST/RPC connection.

I can error handle and pretty confidently get 99.X% uptime on the consumer deployed to a single machine.

&amp;#x200B;

Now, what if I can't tolerate data loss/downtime..

My thinking to create a highly available API consumer would be to run them in multiple docker containers and store the state (index, timestamp) in an external service and structure the API calls to query based on the last\_updated value.

Does this make sense?

I guess analogous to this is a Kafka Connect source connector - I know they can run in distributed mode and store the metadata/offset/state inside the brokers. Do they just round-robin the API calls? Would it do this automatically if I wrote a custom connector?

&amp;#x200B;

Not sure how/if other people deal with this. Thanks for any help."
4731,2021-01-11 05:40:17,1610336417.0,dataengineering,How to Calculate Variance In Excel,kuuda5,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuuda5/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23400.0,
4732,2021-01-11 05:47:19,1610336839.0,dataengineering,How to Calculate Variance In Excel,kuuhkt,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuuhkt/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23401.0,
4733,2021-01-11 05:51:40,1610337100.0,dataengineering,How to Calculate Variance In Excel,kuuk6h,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuuk6h/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23401.0,
4734,2021-01-11 05:52:01,1610337121.0,dataengineering,How to Calculate Variance In Excel,kuukdx,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuukdx/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23401.0,
4735,2021-01-11 05:56:41,1610337401.0,dataengineering,How to Calculate Variance In Excel,kuun3w,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuun3w/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23401.0,
4736,2021-01-11 07:58:39,1610344719.0,dataengineering,How to Calculate Variance In Excel,kuwle4,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuwle4/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23405.0,
4737,2021-01-11 08:05:48,1610345148.0,dataengineering,How to Calculate Variance In Excel,kuwpmh,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuwpmh/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23405.0,
4738,2021-01-11 08:33:01,1610346781.0,dataengineering,How to Calculate Variance In Excel,kux3w6,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kux3w6/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23405.0,
4739,2021-01-11 08:37:54,1610347074.0,dataengineering,How to Calculate Variance In Excel,kux6in,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kux6in/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23406.0,
4740,2021-01-11 09:24:27,1610349867.0,dataengineering,How to Calculate Variance In Excel,kuxtki,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/kuxtki/how_to_calculate_variance_in_excel/,1.0,0.0,0.0,23407.0,
4741,2021-01-11 11:59:37,1610359177.0,dataengineering,Baleen Analytics - Pat Helland,kuzqrt,rmoff,,https://www.reddit.com/r/dataengineering/comments/kuzqrt/baleen_analytics_pat_helland/,1.0,0.0,0.0,23415.0,
4742,2021-01-11 14:41:00,1610368860.0,dataengineering,How To Become a Data Engineer in 2021,kv1vc5,adilkhash,,https://www.reddit.com/r/dataengineering/comments/kv1vc5/how_to_become_a_data_engineer_in_2021/,1.0,18.0,0.0,23416.0,
4743,2021-01-11 15:17:25,1610371045.0,dataengineering,Running Serverless Spark ETL Jobs without having to write code,kv2g33,DataEngUncomplicated,,https://www.reddit.com/r/dataengineering/comments/kv2g33/running_serverless_spark_etl_jobs_without_having/,1.0,0.0,0.0,23416.0,"On September 23rd, 2020, AWS released AWS Glue Studio which provides a graphical interface to publish Spark-based ETL jobs on top of the Glue Platform in AWS.  Glue Studio also gives the ability through custom transforms to add your custom spark code when the graphical interface isn't enough.  Check out my [AWS Glue Studio Overview Video](https://www.youtube.com/watch?v=NuGqN3Aj07M).  


For more technical step by step guides with AWS Glue Studio, See my [AWS Glue Studio Technical Playlist](https://www.youtube.com/playlist?list=PL7bE4nSzLSWfYAc3q1vEYFi145Mt_DLcF).   


In my opinion, this hybrid approach of leveraging a graphical interface but still being able to add your own custom spark code to the script really lifts the barrier to getting started with Spark for large ETL jobs."
4744,2021-01-11 15:50:57,1610373057.0,dataengineering,Red flags in new company shortly after starting,kv30ds,goobdin,,https://www.reddit.com/r/dataengineering/comments/kv30ds/red_flags_in_new_company_shortly_after_starting/,1.0,0.0,0.0,23417.0,
4745,2021-01-11 17:02:49,1610377369.0,dataengineering,Intro to Spark ML Pipelines for Data Engineers,kv4bnv,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/kv4bnv/intro_to_spark_ml_pipelines_for_data_engineers/,1.0,0.0,0.0,23419.0,
4746,2021-01-11 18:02:18,1610380938.0,dataengineering,Thinking to create data engineering related content!,kv5iaq,chanduparmar33,,https://www.reddit.com/r/dataengineering/comments/kv5iaq/thinking_to_create_data_engineering_related/,1.0,0.0,0.0,23421.0,
4747,2021-01-11 20:05:00,1610388300.0,dataengineering,Designing a data warehouse from scratch — Advice?,kv857e,nangaparbat1,,https://www.reddit.com/r/dataengineering/comments/kv857e/designing_a_data_warehouse_from_scratch_advice/,1.0,10.0,0.0,23427.0,"Data scientist with some limited DE experience here, just joined a new company that's seeking to overhaul their current DWH setup. I have a good amount of experience in actually using/querying cloud data warehouses, but I'm not so sure on how to go about designing one from scratch -- including selecting a platform, orchestrating ETL pipelines into the DWH, estimating costs etc.

Our current setup is pretty rudimentary, and doesn't suit our analytics needs well. Essentially, what we have is an OLTP MariaDB database on RDS, which we fetch tables from and load into another MariaDB database which serves as our data warehouse. All of this is orchestrated via Airflow with Pandas and SQLAlchemy. Obviously MariaDB/RDS isn't designed for doing complex analytics queries and powering dashboards etc, so the current setup isn't ideal.

&amp;#x200B;

The main requirements we have are:

* Daily incremental load of a \~dozen tables from a MariaDB database on RDS. We don't need to stream data.
* Primary use will be to power a suite of Metabase dashboards. Other uses include fielding ad-hoc queries and providing a consumption layer for the data scientists.
* Must be cloud-based.
* Must serve a small team of 3-5 data scientists/analysts.

In terms of platform, I'm leaning towards Redshift (it just seems to make sense, given everything is on AWS). I've also considered Snowflake, but I'm not sure how it stacks up against Redshift in terms of cost, which is our number 1 priority.

I'm also hitting a wall when it comes integration and ETL. Currently we orchestrate our ETL pipelines in Airflow, but given that all we really need to do is replicate a bunch of tables from RDS to the DWH, I'm wondering if a managed service would be a better choice -- something like AWS Database Migration Service for instance. We're also considering something like Fivetran, but we're not sure how the costs stack up.

Any words of advice?"
4748,2021-01-11 20:33:02,1610389982.0,dataengineering,Data Observability with Barr Moses and Lior Gavish,kv8qut,mkvor8,,https://www.reddit.com/r/dataengineering/comments/kv8qut/data_observability_with_barr_moses_and_lior_gavish/,1.0,0.0,0.0,23427.0,
4749,2021-01-12 00:54:35,1610405675.0,dataengineering,Any good DE course? I am planning to start with Udacity Nanodegree. Please let me know if you guys know better option,kveckc,barca_iniesta,,https://www.reddit.com/r/dataengineering/comments/kveckc/any_good_de_course_i_am_planning_to_start_with/,1.0,0.0,0.0,23446.0,
4750,2021-01-12 01:05:18,1610406318.0,dataengineering,Reasonable solutions for this recurring problem?,kvekaq,PocketHobbit,,https://www.reddit.com/r/dataengineering/comments/kvekaq/reasonable_solutions_for_this_recurring_problem/,1.0,0.0,0.0,23446.0,
4751,2021-01-12 01:28:26,1610407706.0,dataengineering,backfill a specific dag task from the command line,kvf0av,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/kvf0av/backfill_a_specific_dag_task_from_the_command_line/,1.0,0.0,0.0,23447.0,
4752,2021-01-12 02:34:06,1610411646.0,dataengineering,Kimball: How to model game item purchases,kvg8c0,levelworm,,https://www.reddit.com/r/dataengineering/comments/kvg8c0/kimball_how_to_model_game_item_purchases/,1.0,8.0,0.0,23448.0,"Hi experts, I'm trying to do some exercises here.

Consider a game that sells two type of items:

- Type I: Bundles that can directly exchanged to a fixed rewards. For example by purchasing a $4.99 bundle you get 5,000 coins and 5 stars. Everyone gets the same reward. Period.

- Type II: Game items that can be exachanged to a variable reward. For example you accumulate your ""Piggy"" by putting 5% of each play into it, and you can break it at any time at a fixed price. For example you spend $2.99 to break it and get 4,000 coins, while the other guy spends the same amount of money and get say 5,600 coins and 1 energy as a bonus.

So I'm thinking the first one can go into a `sku` table with `skuID`, `Price`. For the second type I'll have to build their own fact tables because I have to track the coins. 

But how do I link the transaction fact table with the, say, piggy fact table? Piggy does have a skuID but it's the same across the board, so we also need a PiggyID. But it's a bit weird to keep the Piggy ID in transaction fact table, because Piggy is not the only thing of Type II (we have a lot of those).

How can I do it properly? It's equally weird to keep a transaction ID in the Piggy table as well."
4753,2021-01-12 08:56:30,1610434590.0,dataengineering,Can anyone provide 'Construction and waste dataset' for running multiple regression?,kvml02,numbnumbnumbnumb,,https://www.reddit.com/r/dataengineering/comments/kvml02/can_anyone_provide_construction_and_waste_dataset/,1.0,0.0,0.0,23456.0,
4754,2021-01-12 09:01:12,1610434872.0,dataengineering,What tools do you use to visualize data and publish data sets?,kvmn8f,enginerd298,,https://www.reddit.com/r/dataengineering/comments/kvmn8f/what_tools_do_you_use_to_visualize_data_and/,1.0,0.0,0.0,23457.0,
4755,2021-01-12 09:02:11,1610434931.0,dataengineering,How to apply for a data engineering job,kvmnr2,Aggressive-Pup-28,,https://www.reddit.com/r/dataengineering/comments/kvmnr2/how_to_apply_for_a_data_engineering_job/,1.0,0.0,0.0,23457.0,
4756,2021-01-12 09:29:52,1610436592.0,dataengineering,Can anyone provide 'Construction and waste dataset' for running multiple regression?,kvn0j9,numbnumbnumbnumb,,https://www.reddit.com/r/dataengineering/comments/kvn0j9/can_anyone_provide_construction_and_waste_dataset/,1.0,0.0,0.0,23458.0,
4757,2021-01-12 10:14:25,1610439265.0,dataengineering,[P] [R] Automatic and Self-aware Anomaly Detection at Zillow Using Luminaire,kvnkkd,sayan341,,https://www.reddit.com/r/dataengineering/comments/kvnkkd/p_r_automatic_and_selfaware_anomaly_detection_at/,1.0,1.0,0.0,23461.0,
4758,2021-01-12 11:34:22,1610444062.0,dataengineering,How to hire data engineers,kvoipp,Engineering-Design,,https://www.reddit.com/r/dataengineering/comments/kvoipp/how_to_hire_data_engineers/,1.0,0.0,0.0,23464.0,
4759,2021-01-12 13:57:08,1610452628.0,dataengineering,Introduction to Kubeflow Free Mini-Course by Google Data Engineer,kvqb6k,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/kvqb6k/introduction_to_kubeflow_free_minicourse_by/,1.0,0.0,0.0,23470.0,
4760,2021-01-12 15:08:47,1610456927.0,dataengineering,Enable Accelerated Business Data Analytics with Snowflake and Matillion,kvrcjs,suemethen,,https://www.reddit.com/r/dataengineering/comments/kvrcjs/enable_accelerated_business_data_analytics_with/,1.0,0.0,0.0,23472.0,
4761,2021-01-12 15:36:44,1610458604.0,dataengineering,Does your data lake resemble a Used Book Store or a Public Library. Thoughts from Intuit as we invest in Data Lake Management Tools,kvrrxw,SeshuAd,,https://www.reddit.com/r/dataengineering/comments/kvrrxw/does_your_data_lake_resemble_a_used_book_store_or/,1.0,0.0,0.0,23473.0,
4762,2021-01-12 19:46:58,1610473618.0,dataengineering,#idataengineer podcast 007 Adrian Brudaru,kvwjzs,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kvwjzs/idataengineer_podcast_007_adrian_brudaru/,1.0,0.0,0.0,23482.0,
4763,2021-01-12 21:07:02,1610478422.0,dataengineering,Ditching the blueprint for API management,kvy99m,lensesio,,https://www.reddit.com/r/dataengineering/comments/kvy99m/ditching_the_blueprint_for_api_management/,1.0,0.0,0.0,23486.0,
4764,2021-01-12 21:21:23,1610479283.0,dataengineering,🔐 Connecting Auth0 to MongoDB,kvyjwj,robertinoc,,https://www.reddit.com/r/dataengineering/comments/kvyjwj/connecting_auth0_to_mongodb/,1.0,0.0,0.0,23486.0,Hey folks! I’d like to share with you a very interesting article about [**Connecting Auth0 to MongoDB**](https://auth0.com/blog/connecting-auth0-to-mongodb/?utm_source=reddit&amp;utm_medium=sc&amp;utm_campaign=auth0-mongo). It’s awesome to simplify `user migration`. How did you like this post? Please share any comments or feedback here.
4765,2021-01-12 23:24:37,1610486677.0,dataengineering,Need help ingesting data from IMBD into PostgreSQL with Curl,kw13j3,_work_redditor_,,https://www.reddit.com/r/dataengineering/comments/kw13j3/need_help_ingesting_data_from_imbd_into/,1.0,0.0,0.0,23489.0,
4766,2021-01-13 00:36:11,1610490971.0,dataengineering,What roles can I apply for that will help set me up for a data engineering role?,kw2kqt,Manyreason,,https://www.reddit.com/r/dataengineering/comments/kw2kqt/what_roles_can_i_apply_for_that_will_help_set_me/,1.0,0.0,0.0,23490.0,
4767,2021-01-13 00:38:30,1610491110.0,dataengineering,Open-source Workflow Management Tools: A Survey,kw2md4,ploomber-io,,https://www.reddit.com/r/dataengineering/comments/kw2md4/opensource_workflow_management_tools_a_survey/,1.0,0.0,0.0,23490.0,
4768,2021-01-13 02:59:34,1610499574.0,dataengineering,What do you think of talend cloud?,kw5ahx,no_homogeneity,,https://www.reddit.com/r/dataengineering/comments/kw5ahx/what_do_you_think_of_talend_cloud/,1.0,2.0,0.0,23491.0,"I always think of talend as an on-prem gui based java code generator. Though I've recently been hearing about their cloud product and was told that it is often the etl tool of choice when standing up a snowflake dw.

What do you all think of it? Have you heard of it? Is Talend getting a second-life from its cloud product or not.

I'm hesitant to use it because I don't want to be stuck in something that will become less relevant in the future.

[View Poll](https://www.reddit.com/poll/kw5ahx)"
4769,2021-01-13 03:34:34,1610501674.0,dataengineering,[CODE REVIEW] I created my first data pipeline - would love some feedback and tips!,kw5xfl,jc-de,,https://www.reddit.com/r/dataengineering/comments/kw5xfl/code_review_i_created_my_first_data_pipeline/,1.0,0.0,0.0,23492.0,
4770,2021-01-13 05:44:29,1610509469.0,dataengineering,Data Observability: How to Fix Data Quality at Scale,kw87c1,mkvor8,,https://www.reddit.com/r/dataengineering/comments/kw87c1/data_observability_how_to_fix_data_quality_at/,1.0,0.0,0.0,23497.0,
4771,2021-01-13 05:57:20,1610510240.0,dataengineering,Data Observability: How Yotpo Fixes Data Quality (Case Study),kw8fcd,mkvor8,,https://www.reddit.com/r/dataengineering/comments/kw8fcd/data_observability_how_yotpo_fixes_data_quality/,1.0,0.0,0.0,23497.0,
4772,2021-01-13 10:48:37,1610527717.0,dataengineering,Microsoft Defender for Linux now has endpoint detection and response security,kwcm0w,glowsplash,,https://www.reddit.com/r/dataengineering/comments/kwcm0w/microsoft_defender_for_linux_now_has_endpoint/,1.0,0.0,0.0,23500.0,
4773,2021-01-13 12:23:46,1610533426.0,dataengineering,Data Engineering Bootcamp financed by the German government,kwdroy,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kwdroy/data_engineering_bootcamp_financed_by_the_german/,1.0,0.0,0.0,23504.0,
4774,2021-01-13 14:27:19,1610540839.0,dataengineering,The Airflow BranchPythonOperator for beginners in 10 mins! Choose your tasks to execute,kwfgdl,marclamberti,,https://www.reddit.com/r/dataengineering/comments/kwfgdl/the_airflow_branchpythonoperator_for_beginners_in/,1.0,6.0,0.0,23508.0,
4775,2021-01-13 15:20:21,1610544021.0,dataengineering,Are there any tools out there to version control database permissions as code?,kwgahw,EggShellBuddyPal,,https://www.reddit.com/r/dataengineering/comments/kwgahw/are_there_any_tools_out_there_to_version_control/,1.0,0.0,0.0,23510.0,
4776,2021-01-13 18:46:41,1610556401.0,dataengineering,Open Beta Kafka IDE 2021.1.1 being released today! 🎉,kwkbs7,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/kwkbs7/open_beta_kafka_ide_202111_being_released_today/,1.0,0.0,0.0,23517.0,
4777,2021-01-13 19:08:14,1610557694.0,dataengineering,Considerations when moving your Apache Kafka to the cloud,kwks9e,lensesio,,https://www.reddit.com/r/dataengineering/comments/kwks9e/considerations_when_moving_your_apache_kafka_to/,1.0,0.0,0.0,23517.0,
4778,2021-01-13 19:56:23,1610560583.0,dataengineering,Snowflake vs Redshift: Just Choose Snowflake,kwlv8u,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/kwlv8u/snowflake_vs_redshift_just_choose_snowflake/,1.0,1.0,0.0,23518.0,
4779,2021-01-13 20:03:39,1610561019.0,dataengineering,Open Beta Kafka IDE 2021.1.1 being released today! 🎉,kwm13o,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/kwm13o/open_beta_kafka_ide_202111_being_released_today/,1.0,0.0,0.0,23518.0,
4780,2021-01-13 20:57:22,1610564242.0,dataengineering,Azure Data Platform Landscape inspired by Linux Foundation App,kwn6jy,valdasm,,https://www.reddit.com/r/dataengineering/comments/kwn6jy/azure_data_platform_landscape_inspired_by_linux/,1.0,0.0,0.0,23518.0,
4781,2021-01-13 20:59:31,1610564371.0,dataengineering,Looking for advice on my Data Engineer Resume,kwn86r,andresg3,,https://www.reddit.com/r/dataengineering/comments/kwn86r/looking_for_advice_on_my_data_engineer_resume/,1.0,36.0,0.0,23519.0,
4782,2021-01-13 21:06:23,1610564783.0,dataengineering,Using CI/CD to generate pyspark env/lib/bin for executor distribution,kwndp0,ColdPorridge,,https://www.reddit.com/r/dataengineering/comments/kwndp0/using_cicd_to_generate_pyspark_envlibbin_for/,1.0,5.0,0.0,23520.0,"I will start off by saying I am not at all a dev-ops master, and I probably am doing some things wrong. I also have a lot to learn about how spark development should flow at a more advanced level. Constructive feedback is welcome.

In order to make packages and custom modules available to a pyspark job, we need to point to a python zip/egg distribution for both the driver and executors. As far as I can tell, the executors require this file to be on HDFS. I'm trying to make the pyspark development and requirements maintenance process as painless as possible, as it's mostly data scientists writing these jobs with minimal engineering background. My current workflow is this:

1. Write pyspark jobs and airflow dags in some repo
2. Manage python dependencies with pip-env, with different requirements files for the driver, executor, and airflow separately
3. When we update any spark requirements, we manually run some make commands to build new python distributions for driver/executor, and copy these to a location on HDFS
4. When committing code, it builds a docker image with the repo on it for the driver's use
5. Run any jobs/airflow tasks in this docker container, with references for the executors to this constant location on HDFS to get the most up-to-date distribution

The issues I'd like to solve:

1. Having the executor python be generated outside of CI/CD process has led to the environment getting out of sync with the spark jobs. Someone may update the requirements, but if they forget to manually rerun the make commands it will cause issues. Also you could update the environment before updating the code, which could also cause issues.
2. I'm thinking I could use my CI/CD to automatically build the environments and push to HDFS? Getting things to talk to HDFS has historically been an issue at every step of the way with kerberos/auth/etc, so I'm not super enthusiastic about this. However, this also still has issues of version control.
3. I could have CI/CD build the python distributions and push these to Artifactory? And somehow pull these down to HDFS via some other process? That would facilitate versioning better but I'm not sure how I'd go about getting these onto HDFS via that route.

Am I on the complete wrong course? All I want is to make this straightforward for our data scientists. They shouldn't need to jump through hoops to add a package, or worry about breaking something that exists. However, they also shouldn't need to go through any nexus to update their requirements, as that is a horrible dev experience when you can't even manage your own environment. Any thoughts are welcome."
4783,2021-01-13 23:04:16,1610571856.0,dataengineering,How do you run an airflow backfill if the DAG ran successfully?,kwpvhj,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/kwpvhj/how_do_you_run_an_airflow_backfill_if_the_dag_ran/,1.0,0.0,0.0,23521.0,
4784,2021-01-14 02:35:24,1610584524.0,dataengineering,"Those of you who use a linux based OS, what distribution do you use and why?",kwu764,Sleepy_Trees,,https://www.reddit.com/r/dataengineering/comments/kwu764/those_of_you_who_use_a_linux_based_os_what/,1.0,0.0,0.0,23531.0,
4785,2021-01-14 04:05:15,1610589915.0,dataengineering,Consulting Tips and Rates,kwvsw8,gringopaisa18,,https://www.reddit.com/r/dataengineering/comments/kwvsw8/consulting_tips_and_rates/,1.0,0.0,0.0,23532.0,
4786,2021-01-14 04:29:40,1610591380.0,dataengineering,Anyone user their background in data engineering / possibly analytics to get them into a graduate program or a new role that was well complimented by having a background in DE ?,kww8jm,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/kww8jm/anyone_user_their_background_in_data_engineering/,1.0,0.0,0.0,23533.0,
4787,2021-01-14 05:29:09,1610594949.0,dataengineering,Ingest Salesforce Data Into Amazon S3 Data Lake,kwxahg,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/kwxahg/ingest_salesforce_data_into_amazon_s3_data_lake/,1.0,0.0,0.0,23534.0,
4788,2021-01-14 05:30:16,1610595016.0,dataengineering,Spark ETL To Derive Sales Insights on Azure HDInsight And Power BI,kwxb69,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/kwxb69/spark_etl_to_derive_sales_insights_on_azure/,1.0,0.0,0.0,23534.0,
4789,2021-01-14 12:39:51,1610620791.0,dataengineering,I need some guidance for a school project.,kx3a3p,Curious-Capital-2553,,https://www.reddit.com/r/dataengineering/comments/kx3a3p/i_need_some_guidance_for_a_school_project/,1.0,2.0,0.0,23549.0,"Hello guys! I am a software engineer student, i'm currently working on a data science school project and i need some guidance please. So i'm working with a Spotify song raking data set from kaggle ([https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking](https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking)), i'm struggling with these 3 questions :

* Do continents share same top ranking artists or songs?
* Are people listening to the very same top ranking songs on countries far away from each other?
* How long time does a top ranking song takes to get into the ranking of neighbor countries?

I really have no idea where to start, I would appreciate any help.

Thanks a lot for any help, it's much appreciated."
4790,2021-01-14 12:59:55,1610621995.0,dataengineering,Udemy free online courses with certifications to upgrade your skills,kx3j0e,Fit-Independence-786,,https://www.reddit.com/r/dataengineering/comments/kx3j0e/udemy_free_online_courses_with_certifications_to/,1.0,0.0,0.0,23551.0,
4791,2021-01-14 13:07:25,1610622445.0,dataengineering,#idataengineer podcast 008 Oleg Soroka,kx3n08,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kx3n08/idataengineer_podcast_008_oleg_soroka/,1.0,0.0,0.0,23551.0,
4792,2021-01-14 15:44:39,1610631879.0,dataengineering,DW as a source to ETL,kx5vw0,fleite87,,https://www.reddit.com/r/dataengineering/comments/kx5vw0/dw_as_a_source_to_etl/,1.0,5.0,0.0,23559.0,"Hi everyone,

Is it a good practice extract data from a DW what it has been using as OLAP, transform these data then loading the transformed data to a DW again?

I'm reviewing some pipelines and felt into this question."
4793,2021-01-14 16:08:38,1610633318.0,dataengineering,Data catalog / discovery,kx6alh,Mafixo,,https://www.reddit.com/r/dataengineering/comments/kx6alh/data_catalog_discovery/,1.0,0.0,0.0,23569.0,
4794,2021-01-14 16:17:19,1610633839.0,dataengineering,"Uhm, inquiry on starting as data architect...",kx6g0t,tinkylala,,https://www.reddit.com/r/dataengineering/comments/kx6g0t/uhm_inquiry_on_starting_as_data_architect/,1.0,0.0,0.0,23583.0,
4795,2021-01-14 17:30:16,1610638216.0,dataengineering,"We don't need data scientists, we need data engineers",kx7tul,joshdick,,https://www.reddit.com/r/dataengineering/comments/kx7tul/we_dont_need_data_scientists_we_need_data/,2.0,34.0,0.0,23680.0,
4796,2021-01-14 17:37:06,1610638626.0,dataengineering,What We Know and Don't Know About Analytics Engineering,kx7yka,duyenla257,,https://www.reddit.com/r/dataengineering/comments/kx7yka/what_we_know_and_dont_know_about_analytics/,1.0,0.0,0.0,23681.0,
4797,2021-01-14 18:56:24,1610643384.0,dataengineering,"""EMR required too much setup and maintenance work. Databricks' steep pricing ruled them out."" Customer Story of using Data Mechanics (YC S'19), a Cloud-Native Spark Platform for Data Engineers!",kx9kd9,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/kx9kd9/emr_required_too_much_setup_and_maintenance_work/,1.0,0.0,0.0,23713.0,
4798,2021-01-14 19:11:05,1610644265.0,dataengineering,The Data Janitor Letters - December 2020,kx9vow,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kx9vow/the_data_janitor_letters_december_2020/,1.0,0.0,0.0,23714.0,
4799,2021-01-14 22:59:03,1610657943.0,dataengineering,"The Great Expectations team is hosting hackathons for Students and Professional Data Engineers! Great opportunity to gain some experience, to contribute to an awesome open source project and win some prizes.",kxemro,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/kxemro/the_great_expectations_team_is_hosting_hackathons/,1.0,1.0,0.0,23755.0,"Hey Everyone,

The [Great Expectations](https://github.com/great-expectations/great_expectations) Team is hosting a series of hackathons!  
With the most recent release including Modular Expectations, contributing has become much easier. This is the perfect opportunity to join us in hacking on some Expectations and contribute to an awesome open-source project! There will be three different event times and two of them are for current university students. Expect swag, doordash credit and cash prizes. You will be joined by the core team to help you contribute!

* Student Hackathon 1/23 5-9pm PST (students only, must be currently attending a university)
* Data Professionals Hackathon 1/28 5-9pm PST
* Student Hackathon 2/6 2-6pm PST (students only, must be currently attending a university)

Sign up here: [https://www.surveymonkey.com/r/great-expectations-hackathon-3](https://www.surveymonkey.com/r/great-expectations-hackathon-3)  
We blogged about it here: [https://greatexpectations.io/blog/great-expectations-hackathons/](https://greatexpectations.io/blog/great-expectations-hackathons/)

If you are interested in the hackathon then you may want to attend the Modular Expectations Webinar next week.

**Thursday, Jan 21 @ 2:00pm Eastern Time**, We will be hosting a Webinar featuring the new Modular Expectations. The new Expectation class now includes all the logic for creating an Expectation and integrating it with the other moving parts of Great Expectations: execution on multiple platforms, documentation, rendering, profiling, etc. It’s actually quite a lot. Gathering all these parts into a single class makes the experience of developing Expectations much better, which will in turn make the Great Expectations ecosystem much richer and more expressive.

**Sign up for the webinar here:** [**https://greatexpectations.io/blog/modular-expectations/**](https://greatexpectations.io/blog/modular-expectations/)

Sign up if you can't make it and I'll send you the video for the event once it is posted."
4800,2021-01-14 23:00:40,1610658040.0,dataengineering,"Interview prep: ""big data patterns?"" What does this mean to you?",kxeo3g,fail_to_reject_null,,https://www.reddit.com/r/dataengineering/comments/kxeo3g/interview_prep_big_data_patterns_what_does_this/,1.0,7.0,0.0,23756.0,"I have to do some interview prep and learn more about ""big data patterns."" I'm not 100% sure what this means, or what I should necessarily be reading, but my plan is to work through a Kafka book, and a Spark book, and then re-read Kleppmann's Designing Data-Intensive Applications.

Can anybody suggest anything else I should look into, given what the phrase ""big data patterns"" means to them?"
4801,2021-01-14 23:05:12,1610658312.0,dataengineering,Detecting labels in graphs using Label Propagation Algorithm,kxeri9,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/kxeri9/detecting_labels_in_graphs_using_label/,1.0,0.0,0.0,23756.0,
4802,2021-01-14 23:30:22,1610659822.0,dataengineering,"Lambda architecture speed layer, visualizing taxi data",kxf9xa,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/kxf9xa/lambda_architecture_speed_layer_visualizing_taxi/,1.0,0.0,0.0,23762.0,
4803,2021-01-14 23:32:22,1610659942.0,dataengineering,Lambda architecture batch layer visualizing taxi data,kxfbbb,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/kxfbbb/lambda_architecture_batch_layer_visualizing_taxi/,1.0,0.0,0.0,23765.0,
4804,2021-01-14 23:33:29,1610660009.0,dataengineering,Lambda architecture speed layer visualizing taxi data,kxfc4t,thatsadsid,,https://www.reddit.com/r/dataengineering/comments/kxfc4t/lambda_architecture_speed_layer_visualizing_taxi/,1.0,0.0,0.0,23765.0,
4805,2021-01-15 00:04:27,1610661867.0,dataengineering,Best Airflow Design Patterns for extracting 200 table DB to S3 parquet files?,kxfydi,OkieDaddy,,https://www.reddit.com/r/dataengineering/comments/kxfydi/best_airflow_design_patterns_for_extracting_200/,2.0,5.0,0.0,23767.0,"Greetings fellow data engineers. I have a problem I'd like some input on. In past projects the data extraction/ETL/ELT processes I've built have all been very targeted ELT/ETL pipelines. Get data from these 5-10 tables, these 3-4 API's, store them in the dw/datalake, then transform them for final reporting/dimensional querying.

However, the requirements for this new project is, Stage the entire DB in parquet, and have it capture all changes every two hours. Then suck in what's necessary from there into the DW/transform and load into data marts/etc. The DB is 200+ tables.

I've got a python script built that takes a select statement/table name and will convert said table output to parquet, so that's not an issue. My question is how to best do this in Airflow? I don't want to build 200 tasks in airflow, repeating myself, right? And I don't want a single task to loop through and do them one table at a time, as then I lose the parallel processing. I've seen a few posts about dynamic dags, but then I always see more saying that dynamic dags are an anti-pattern and should be avoided.

I already pitched the idea of AWS Database Migration Service, and the continuous replication they have available, but I was told we'd rather keep it batch processed, rather than streaming/continuous replication.

I know I'm not the first person to do this, so please, what is your preferred way of accomplishing this?"
4806,2021-01-15 01:13:08,1610665988.0,dataengineering,Add data engineer roadmap · Pull Request #216 · kamranahmedse/roadmap.sh,kxhedv,ASK_ME_ABOUT_MMT,,https://www.reddit.com/r/dataengineering/comments/kxhedv/add_data_engineer_roadmap_pull_request_216/,1.0,0.0,0.0,23775.0,
4807,2021-01-15 02:01:03,1610668863.0,dataengineering,Your experience with following the guided projects on Coursera to learn new skills,kxia91,msv5450,,https://www.reddit.com/r/dataengineering/comments/kxia91/your_experience_with_following_the_guided/,1.0,3.0,0.0,23779.0,"I am working for a company as an analyst and I would like to learn cloud engineering because that's the way thw future is headed. I have not been successful learning the basics of AWS and Hadoop by watching relevant courses on Coursera. I feel like they contain a lot of jargon and it is hard for me to digest and make the material solidify in my head.

Do you think that doing the guided projects on coursera step by step will be more helpful than just watching their course videos?"
4808,2021-01-15 02:17:26,1610669846.0,dataengineering,Skills for an entry level data engineer position,kxil8g,PuTooT90,,https://www.reddit.com/r/dataengineering/comments/kxil8g/skills_for_an_entry_level_data_engineer_position/,1.0,0.0,0.0,23781.0,
4809,2021-01-15 04:20:18,1610677218.0,dataengineering,Beginner projects to gain experience,kxksc9,TonyRomeN,,https://www.reddit.com/r/dataengineering/comments/kxksc9/beginner_projects_to_gain_experience/,1.0,0.0,0.0,23787.0,
4810,2021-01-15 11:56:12,1610704572.0,dataengineering,What are some solid books and resources on starting and implementing Data Governance in a 10k staff size company?,kxrilk,TheDataGentleman,,https://www.reddit.com/r/dataengineering/comments/kxrilk/what_are_some_solid_books_and_resources_on/,1.0,0.0,0.0,23814.0,
4811,2021-01-15 14:53:02,1610715182.0,dataengineering,Real life data streaming,kxtxym,Mumo2020,,https://www.reddit.com/r/dataengineering/comments/kxtxym/real_life_data_streaming/,1.0,19.0,0.0,23818.0,"Hi

I am just in the process of changing my career to a an azure data engineer

Is there a place I can have access to real life data streaming to work on

I do not mind volunteering to assist anyone

Thank you"
4812,2021-01-15 14:55:09,1610715309.0,dataengineering,"Catboost, too large to deploy to AWS lambda server, only need a small part of the catboost functionality (predict)",kxtz33,aspiringdatascience,,https://www.reddit.com/r/dataengineering/comments/kxtz33/catboost_too_large_to_deploy_to_aws_lambda_server/,1.0,1.0,0.0,23818.0,"Hi all,
I am using catboost to predict. I was trying to upload to aws lambda, however catboost and all its dependecies are too big. I only need the predict functionality of catboost. How do I go about deleting all the unnecessary stuff, and rebuilding? This feels like a very obvious problem to come across; yet I find very little information on it. It's suprising there isn't a small production version of catboost predict available, for production environments. Does anyone have any ideas?"
4813,2021-01-15 17:23:39,1610724219.0,dataengineering,Modern Data Engineer Roadmap 2021,kxwndu,alexandraabbas,,https://www.reddit.com/r/dataengineering/comments/kxwndu/modern_data_engineer_roadmap_2021/,1.0,0.0,0.0,23821.0,
4814,2021-01-15 20:12:11,1610734331.0,dataengineering,How to Prevent Data Downtime,ky07ir,mkvor8,,https://www.reddit.com/r/dataengineering/comments/ky07ir/how_to_prevent_data_downtime/,1.0,0.0,0.0,23830.0,
4815,2021-01-15 20:19:00,1610734740.0,dataengineering,Better alternatives to Zeppelin?,ky0crv,ColdPorridge,,https://www.reddit.com/r/dataengineering/comments/ky0crv/better_alternatives_to_zeppelin/,1.0,8.0,0.0,23830.0,"Looking for some insight for alternatives to Apache Zeppelin for scala/pyspark/sql, as it's critically buggy and a constant headache. JupyterLab seems to generally work, but I was wondering what other alternatives exist, and how enjoyable they are to set up and use. Ease of use and flexibility for end users is the top priority."
4816,2021-01-15 22:11:05,1610741465.0,dataengineering,What options do I have for career growth.,ky2pxo,bob7913,,https://www.reddit.com/r/dataengineering/comments/ky2pxo/what_options_do_i_have_for_career_growth/,2.0,35.0,0.0,23836.0,"Hi there,

I am wanting to learn new skills/technologies to expand my career opportunities in the future but really stuck on where to start and what kind of career path would suit me so looking for some options.

A bit about me:

* Mid 20's
* BSc Mathematics
* Data Analyst 3yr experience
* Tools I am at least somewhat familiar with: Python + (OOP concepts), DAX, Power BI, SQL, Excel, Data structures &amp; algo's.

My career has basically stalled over the past 1.5yrs, I am not learning new skills and working from home for the past year due to covid has just meant I have been doing what I need to do to get by.

I am wanting to increase my employability, salary, and also to work with some cool new technologies, I spend a good chunk of my day on excel and I have had enough.

I am looking towards the future and am not opposed to getting some new qualifications.

The biggest problem is that I am really not too sure what is out there and what would suit me as a career path. I see so much stuff out there to learn in terms of technologies and frameworks am in need of some advice in terms of what is going to be in demand, and is interesting to learn.

Has anyone that is maybe a bit older and more experienced been in my shoes before? 

Or simply got any advice to give me on what options I could go for and what you would recommend me based on what I have said?

PS: I am looking for something more technical as I think that is what I would enjoy more (rather than working with customers etc. but open to all suggestions!)"
4817,2021-01-16 03:34:39,1610760879.0,dataengineering,I don’t know what to do...,ky91b2,thewizarddan,,https://www.reddit.com/r/dataengineering/comments/ky91b2/i_dont_know_what_to_do/,1.0,35.0,0.0,23857.0,"Long story short, I don’t know if I should continue pursuing data engineering or drop data engineering and pour myself 110% into my current job.

I’m turning 30-years old in about two weeks. I’m married and happy, but I just got an unexpected promotion. The thing is, my promotion is to be a business consultant for my department, but I just went back to school to get a bachelor’s in Data Management/Data Analytics with the goal of transitioning into data engineering. So the promotion catapults me in a completely different direction: business.

The second aspect is that this promotion is going to put me at about a 110k compensation package. That’s more than I ever dreamed of making. But the compensation has more to do with the company and the situation I’m in; I happen to be at the right place in the time, so the compensation is not so much because of my skill, and much more because of the opportunity, people, and company. That’s why I’m a bit uneasy.

You see, I can ditch data engineering, pour myself into my job and start accumulating a tidy sum of money. If I manage to do that for several years, I’ll be very well off - so much so that I anticipate paying off my house in the next four years. BUT, if for whatever reason I lose my job at anytime, I am 100% positive my next job will be somewhere around 65k. Again, the 110k compensation package cannot be sustained outside my current job.

I have an obsession with systems, really enjoy python and SQL, and I do long for the opportunity of working in data engineering; this desire is constantly in front of me. But achieving financial independence in several years with my current job is also very appealing.

So if I went with data engineering, I’d be doing something I enjoy even more, and I’d be able to slowly increase my salary and also keep it or increase as I transition jobs, but it would probably take me around 3-years to make something close to where I’m at right now.

So guys and gals, this is my dilemma. Any input would be massively appreciated. Also, these are very polarized times in the US, so regardless of my beliefs or yours, I’m sending you a hug and some love your way in genuine appreciation of you.

God bless."
4818,2021-01-16 07:01:39,1610773299.0,dataengineering,Springboard’s Data Engineering career track worthwhile?,kycfxk,lessonslearnedaboutr,,https://www.reddit.com/r/dataengineering/comments/kycfxk/springboards_data_engineering_career_track/,1.0,0.0,0.0,23862.0,
4819,2021-01-16 18:06:02,1610813162.0,dataengineering,Scalable Tableau Extracts,kylj9v,LiquidSynopsis,,https://www.reddit.com/r/dataengineering/comments/kylj9v/scalable_tableau_extracts/,1.0,0.0,0.0,23877.0,
4820,2021-01-16 19:15:12,1610817312.0,dataengineering,CS knowledge required for data engineering,kymv8v,avclipavclip,,https://www.reddit.com/r/dataengineering/comments/kymv8v/cs_knowledge_required_for_data_engineering/,3.0,25.0,0.0,23885.0,"How much computer science “theory” knowledge is required to work and excel in data engineering? I’m not referring to more general, applied knowledge on how to program or understanding of relational db’s. What I mean is “core” CS knowledge like OS, comp architecture, DS&amp;A, etc. I’m a developer without much of a CS background and I’m trying to determine if brushing up on the core CS components will help me in my pursuit of data engineering."
4821,2021-01-16 23:35:03,1610832903.0,dataengineering,Mid Life Tech Crisis - Suggestions required?,kyryzq,jta_11,,https://www.reddit.com/r/dataengineering/comments/kyryzq/mid_life_tech_crisis_suggestions_required/,1.0,28.0,0.0,23891.0,"Working as a traditional ETL engineer for 8 years. Engineering non CS background. Have worked on only drag and drop tools like Informatica and ODI. Have good experience with SQL specially on Oracle, data warehousing. Have been recently looking to move out from my current company only to realize there are very few companies in my country hiring for proprietary ETL tools for a respectable salary. Started looking out what are respectable paying companies requiring for custom ETL developer role, to find out that they actually are expecting SWE who can work with data. Tried looking out for roadmaps as to how can I become ""hireable"" data engineer and have been overwhelmed by the sheer amount of stuff I need to learn. Not sure what all should I prioritize. I'm thinking about at least an year worth of effort would go towards learning new stuff but want to maximize my learning with the effort I'm planning to put in. Turning to the community for guidance and help as to how should I go about it

Thanks in advance!"
4822,2021-01-16 23:51:03,1610833863.0,dataengineering,An open-source local-first database to build collaborative and end-to-end secured applications (and so much more),kys90b,[deleted],,https://www.reddit.com/r/dataengineering/comments/kys90b/an_opensource_localfirst_database_to_build/,1.0,0.0,0.0,23891.0,
4823,2021-01-17 00:53:06,1610837586.0,dataengineering,An open-source local-first database to build collaborative and end-to-end secured applications (and so much more),kytf27,Malexik_T,,https://www.reddit.com/r/dataengineering/comments/kytf27/an_opensource_localfirst_database_to_build/,1.0,0.0,0.0,23892.0,
4824,2021-01-17 02:04:25,1610841865.0,dataengineering,Should I be looking for another job?,kyur0v,BigTyminandRymin,,https://www.reddit.com/r/dataengineering/comments/kyur0v/should_i_be_looking_for_another_job/,1.0,0.0,0.0,23897.0,
4825,2021-01-17 07:30:04,1610861404.0,dataengineering,"Current DE, anyone ever done a ML 20% project applicable to DE?",kz0bs2,PotentialVast5352,,https://www.reddit.com/r/dataengineering/comments/kz0bs2/current_de_anyone_ever_done_a_ml_20_project/,1.0,9.0,0.0,23911.0,Basically I have the option to take some time to do a side project with ML if it somehow connects to us as data engineers.  Was wondering if anyone has done anything like this has any suggestions?
4826,2021-01-17 14:39:44,1610887184.0,dataengineering,Introduction to Databases for Data Engineers,kz5r9b,oleg_agapov,,https://www.reddit.com/r/dataengineering/comments/kz5r9b/introduction_to_databases_for_data_engineers/,1.0,0.0,0.0,23924.0,
4827,2021-01-17 15:00:23,1610888423.0,dataengineering,A lightweight alternative to Amundsen for your dbt project,kz61f1,laguitte,,https://www.reddit.com/r/dataengineering/comments/kz61f1/a_lightweight_alternative_to_amundsen_for_your/,1.0,0.0,0.0,23925.0,
4828,2021-01-17 15:47:37,1610891257.0,dataengineering,I want to change my career from Academia (Computer Science) to Data Engineering and I am kind of lost,kz6pr8,b13_git2,,https://www.reddit.com/r/dataengineering/comments/kz6pr8/i_want_to_change_my_career_from_academia_computer/,1.0,13.0,0.0,23925.0,"Hello all. First a little career background about myself: Currently, I am in academia, as a Lecturer in Computer Science in one of the private universities in Bangladesh. I have taught courses such as Intro to Database, Intro to Web Technologies, C++, Data Structures, Algorithms, Compiler Design, etc. I also worked as a web developer for 5 months after completing my BSc and before pursuing my MSc. I also did an internship before that in one of the leading tech companies in my home country. However, honestly, I did not learn very much from that internship. I remember tinkering with Unity Game Engine a bit to create a small game.

&amp;#x200B;

My educational background: I completed my BSc in Computer Science &amp; Engineering at the end of 2014 from my home country, Bangladesh (from the same University I am a lecturer currently), and went to the University of Gothenburg in Sweden with a scholarship to pursue my MSc in Computer Science. It was, however, not a data related MSc (Now, I wish it really were!). 

&amp;#x200B;

Anyway, for some personal reasons after finishing my MSc right away I had to come back to my home country. Then I took my current job (A University Lecturer in Bangladesh) starting from 2018. I never really liked academia, but due to some priorities in my life as well as personal reasons, I took the job. After that, I got married to the love of my life. She is a doctor and was in the early stages of her career. She got an opportunity to move to the UK to work with the NHS and she took it. That was actually always the plan. Currently, she is working in NHS starting this month and I am here in my home country. I plan to join her there in 3-4 months leaving academia and pursuing a new career in Tech. My desire is to be in Data Engineering.

&amp;#x200B;

Currently, I am in my late 20s (27.5 years to be exact). I always wanted to be working with Data but was never sure which role to pursue. A few months back I decided to learn the skills required to become a Data Analyst. I was pretty sure that when I move to the UK leaving my current job, I would pursue a Data Analyst career path. However, as time passed I became more and more confused about whether I should go for being a Data Analyst or Data Engineer position. The reason I initially was prepping for a Data Analyst position was that I knew preliminary Excel, Tableau, intermediate SQL, Python (again, not advanced), Web Development (JS, PHP, HTML, AJAX, etc) \[intermediate\]. But as I was learning to be a Data Analyst, I was also trying to understand what Data Engineering is and whether that is something I would like to do because frankly speaking, my understanding is that you have to deal with LOTS of people and be an extrovert with domain knowledge of the business in your job to be a good Data Analyst and unfortunately I don't have much of these. And I started to have less and less interest in the Data Analyst role the more I learned about it. I would rather work which involves solving programming and more technical problems with my skills in a job. So, the more and more I learn about what Data Engineers do as opposed to Analysts, I'm attracted to go for Data Engineering positions.

&amp;#x200B;

BUT, I am really afraid as I am switching careers and I don't have that graduate scheme to take advantage of to start as a Data Engineer. From my understanding, if I go for a Jr. Data Analyst position, probably I will get it if I learn a bit more of the skills required but I will not enjoy my work I think. But, to change my career now and be a Data Engineer the learning curve is pretty steep! At least steeper than being a Data Analyst, I presume. And that scares me because when will I learn and when will I apply that knowledge? How much time will it take? There's no clear way! Also, Data Engineering seems to be such a career path where to start with it only the theoretical knowledge is not enough.

&amp;#x200B;

But after watching [this video](https://www.youtube.com/watch?v=q59rbUyhKCg), I got a little bit of hope and courage to probably go for Data Engineering. Honestly, the things so far that I have learned about being a Data Engineer attracts me more than to be a Data Analyst. I feel happy when I imagine I am working as a Data Engineer with all of the required skills. But I don't have those skills under my belt yet. I saw this [article](https://www.datacamp.com/community/blog/the-path-to-becoming-a-data-engineer) from [Datacamp](https://www.datacamp.com/) and there are so many skills listed here and not gonna lie, it scares me. Probably another way to get into the Data Engineering career is to go through a Data Consultancy (example: Kubrick Group, FDM Group. I only know these 2 in the UK) where they train successful candidates roughly in 4 months and place them in one of their clients for 2 years with relatively low compensation. Then again, I don't know if this kind of Data Consultancy will even take me in as a trainee (because I am not a fresh graduate anymore). However, I think if given the opportunity, that would be the best way to progress for me because I will learn the Data Engineering tools hands-on with real-life projects and exposure to such things is really important I feel. I really don't care about the salary/ compensation at the beginning. All I care about is learning to be a Data Engineer with real-life exposure to real-life Data and solving real-life problems.

&amp;#x200B;

But, before I apply for entry-level jobs or even for training in a Data Consultancy, I wanted to learn Data Engineering as much as possible and it is not really clear how to learn ALL THESE tools hands-on solving real problems. I wonder if there are projects I can take to learn most of the skills and tools required for being a Data Engineer if not all. First, I thought Datacamp could be a place to start. But, people have advised against it as things are not as hands-on as it should be there. I also learned about [DataQuest](https://www.dataquest.io/), but not sure if the materials that they have there are good for learning Data Engineering skills.

&amp;#x200B;

As you can understand, I am kind of lost and hungry for learning Data Engineering skills as much as possible. In these 3-4 months that I have in my hand before moving to the UK, I want to learn and level up my skills as much as possible for being a Data Engineer. The best-case scenario could be to land an entry-level job there in the first 1-2 months or get the opportunity to be enrolled as a trainee in one of the Data Consultancies there.

&amp;#x200B;

Thank you for reading and I would REALLY appreciate ANY kind of direction on this."
4829,2021-01-17 17:01:46,1610895706.0,dataengineering,How to make best of a 6 month DE opportunity?,kz7vot,Lukasek97,,https://www.reddit.com/r/dataengineering/comments/kz7vot/how_to_make_best_of_a_6_month_de_opportunity/,1.0,4.0,0.0,23928.0,"Currently in a graduate scheme in the UK where you are allowed to do rotations throughout the business, and one area that caught my eye has been data engineering. The more I have been reading about it, the more I am interested in the potential benefits. My background is in mechanical engineering, but I have created a web app for a large uni project using JavaScript / CSS / HTML / firebase with Ionic and Angular, though I am aware that this is quite different. I have also used Matlab for data analysis in my current role.

I have already made a start on learning python and SQL properly which were the two main recommended languages to get a feel for, and will try to get some experience with some data engineering tools too. I don't want to go into much detail but the company uses GCP. 

Any recommendations for other pre-learning? They will teach me lots of what is required, but I want to hit the ground running.

What would be some achievable goals of what I can get competent at in terms of data engineering in a 6 month time frame? Any general or particular DE skills / SWE skills? Stacks used with GCP?"
4830,2021-01-17 21:25:23,1610911523.0,dataengineering,Moving career to Data Engineering,kzczf3,pawan_it17,,https://www.reddit.com/r/dataengineering/comments/kzczf3/moving_career_to_data_engineering/,1.0,0.0,0.0,23937.0,
4831,2021-01-18 00:57:52,1610924272.0,dataengineering,Looking for recommendations for books or courses for managing data engineering teams,kzh77g,aDigitalPunk,,https://www.reddit.com/r/dataengineering/comments/kzh77g/looking_for_recommendations_for_books_or_courses/,1.0,0.0,0.0,23946.0,
4832,2021-01-18 02:42:14,1610930534.0,dataengineering,Regression testing against hoarded data over 10 years,kzj4qr,asdfzxcvfdsa,,https://www.reddit.com/r/dataengineering/comments/kzj4qr/regression_testing_against_hoarded_data_over_10/,1.0,0.0,0.0,23947.0,
4833,2021-01-18 07:30:39,1610947839.0,dataengineering,Help understanding difference between virtual environments and packages with poetry for python?,kzo46o,pbj800100,,https://www.reddit.com/r/dataengineering/comments/kzo46o/help_understanding_difference_between_virtual/,1.0,3.0,0.0,23954.0,"Hello!  Previously I have never used virtual environments, but i understand that the benefit is that you can install packages within a specific environment and then use that environment to run specific scripts.  This way the main environment (base environment?) does not get cluttered with every single package used for every project (please correct me if i’m missing something here).

My question is that I’m a bit lost understanding how this relates to packages that I am developing (specifically in regards to Poetry) and what the general process should look like.  It sort of sounds like first I want to set up a new poetry package, which automatically creates a new virtual environment.  Then within that environment I can install whatever python package dependencies i need (for example, boto3).  From there, i need to also add those dependencies to the package?  Is it possible to have multiple virtual environments within a single poetry package?  Does it make sense to use something like venv along with poetry, or would that just be redundant?  can anyone give me a brief overview of how this all works?  Thanks!"
4834,2021-01-18 09:25:45,1610954745.0,dataengineering,Data Analyst to Data Engineer,kzpttp,cuhristophet,,https://www.reddit.com/r/dataengineering/comments/kzpttp/data_analyst_to_data_engineer/,1.0,0.0,0.0,23955.0,
4835,2021-01-18 14:39:53,1610973593.0,dataengineering,Need Help,kztwwo,Creative_Stuff3940,,https://www.reddit.com/r/dataengineering/comments/kztwwo/need_help/,1.0,0.0,0.0,23963.0,Can anybody guide me what path should I follow to become a Data Engineer. I am currently pursuing  BE in Computer Science.
4836,2021-01-18 16:04:49,1610978689.0,dataengineering,Introduction to Databases for Data Engineers,kzv8lj,oleg_agapov,,https://www.reddit.com/r/dataengineering/comments/kzv8lj/introduction_to_databases_for_data_engineers/,1.0,0.0,0.0,23964.0,
4837,2021-01-18 18:09:17,1610986157.0,dataengineering,"Are there any examples of lightweight Airflow projects on Github? I work with Airflow at work, but have no idea how a mini project should be structured.",kzxklr,mac-0,,https://www.reddit.com/r/dataengineering/comments/kzxklr/are_there_any_examples_of_lightweight_airflow/,1.0,6.0,0.0,23965.0,"I've got a fun data-related project I'm working on, and am using Airflow to orchestrate it. It's the scheduler I'm most familiar with, and I figure at the very least it's something to add to a portfolio since I don't really have one. 

What I'm struggling with is project layout. Ideally it'd be a repo someone could clone, run a few terminal commands, and actually be able to deploy the DAGs locally on their machine and see them being scheduled. 

Does anyone have any examples of how a repo should be structured? Because right now I am just using the `/airflow/` folder that is created by default on the [Airflow Quickstart](https://airflow.apache.org/docs/apache-airflow/stable/start.html) guide, and I just manually put my dags in the `airflow/dags/` folder and that's it. Is that the way to go? Just upload my top level airflow folder?"
4838,2021-01-18 19:14:31,1610990071.0,dataengineering,Running the Cloudera Quickstart Docker Image with WSL 2 and Powershell,kzywxz,glenmccallumcan,,https://www.reddit.com/r/dataengineering/comments/kzywxz/running_the_cloudera_quickstart_docker_image_with/,1.0,0.0,0.0,23966.0,
4839,2021-01-18 19:15:52,1610990152.0,dataengineering,Sustainable data engineering,kzyxvm,soobrosa,,https://www.reddit.com/r/dataengineering/comments/kzyxvm/sustainable_data_engineering/,1.0,0.0,0.0,23966.0,
4840,2021-01-18 20:54:08,1610996048.0,dataengineering,New chapter of my Data Engineering book,l010k7,oleg_agapov,,https://www.reddit.com/r/dataengineering/comments/l010k7/new_chapter_of_my_data_engineering_book/,1.0,0.0,0.0,23969.0,
4841,2021-01-18 21:56:54,1610999814.0,dataengineering,Trying to break into data engineering? Offering some help as a hiring manager,l02cim,samdjohnson52,,https://www.reddit.com/r/dataengineering/comments/l02cim/trying_to_break_into_data_engineering_offering/,1.0,18.0,0.0,23971.0,"Hi everyone!

I am a Senior Data Engineering Manager (10+ years in the field and have hired 50+ data folks) and I want to help people who are trying to break into data engineering. I'm offering a 30 minute call to give advice on building skills, tweaking your resume to get noticed, prepping for job interviews, or negotiating job offers.

If you are interested, please send me a message and we can set up some time to talk."
4842,2021-01-18 22:04:26,1611000266.0,dataengineering,Ideas for Data Science App,l02iff,devawesomee,,https://www.reddit.com/r/dataengineering/comments/l02iff/ideas_for_data_science_app/,1.0,0.0,0.0,23972.0,"Lets see where this goes.

* I am looking for good ideas to build an application that can be commercialized. 
* Good developers who are interested in working on the idea part time/ full time

My background - Data Science, Java, Python developer with lot of experience in Consulting."
4843,2021-01-18 22:10:21,1611000621.0,dataengineering,Crucial differences in MLOps for deep learning,l02n1m,abaschkim,,https://www.reddit.com/r/dataengineering/comments/l02n1m/crucial_differences_in_mlops_for_deep_learning/,1.0,2.0,0.0,23972.0,"I am new to the field of MLOps and about to setup a pipeline for a deep learning project based on TensorFlow. I am looking for differences when comparing deep learning to other machine learning approaches in the context of MLOps. So for I have only found resources that introduce the general MLOps principles. Does anybody know what pipeline components may differ specifically?

What tools &amp; resources would you suggest to start with?

Thanks!"
4844,2021-01-18 22:13:23,1611000803.0,dataengineering,What are all the existing approaches to consume an Apache Kafka topic?,l02pfd,kafkaide-com,,https://www.reddit.com/r/dataengineering/comments/l02pfd/what_are_all_the_existing_approaches_to_consume/,1.0,0.0,0.0,23972.0,
4845,2021-01-19 00:00:37,1611007237.0,dataengineering,Can we setup a serverless data pipeline orchestration solution to replace airflow?,l04xi7,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/l04xi7/can_we_setup_a_serverless_data_pipeline/,1.0,0.0,0.0,23976.0,
4846,2021-01-19 02:33:24,1611016404.0,dataengineering,Any tips on design patterns to follow when refactoring old code?,l07wzb,BionicleGarden,,https://www.reddit.com/r/dataengineering/comments/l07wzb/any_tips_on_design_patterns_to_follow_when/,1.0,4.0,0.0,23980.0,"At work we have a behemoth of a solution that regularly consumes an entire week of whichever poor sap's turn it is to run the load and do validation. Years ago the company hired a consulting group to write this thing and it is absolutely massive.

The end product is simple enough: A report that has a few different tabs, showing various metrics broken out by various dimensions. But the way we get to that end product is anything but simple.

Each month when we need to load new data into this solution, we cross our fingers and hope everything works. If something fails, or one of our QA testers finds a discrepancy in a metric somewhere, then we can basically kiss our week goodbye.

We'll start by recreating the issue or discrepancy in the report. Then we'll trace the data back to the SSAS tabular model that the report sits on. If we still haven't found the culprit then we'll go further back to the SQL Server views from which the tabular model reads. We'll find that those views read from various fact tables. Those fact tables are loaded by stored procedures that build their final result set from reading from other views into temp tables and then joining up all the temp tables and doing an insert. Oh yeah, I forgot to mention that multiple stored procedures might insert to the same table, so how do you know which stored procedure your problem data even came from?

Needless to say, this gives us all a generous serving of spaghetti code to consume each and every month. We've been talking about refactoring this whole solution, but with so many different types of objects involved (SSAS tabular, views, tables, stored procedures, temp tables, SSIS, etc) where do you even start? And what types of design patterns would you follow to ensure you don't just produce another solution that's also spaghetti?

At a high level I've learned about some design patterns to follow in order to produce CLEAN code - code that is Cohesive, Loosely Coupled, Encapsulated, Assertive, and Nonredundant. However most examples I read are based on object oriented programming and talk about separating your object definitions from object creation, or something like that.

While some of these patterns can be applied at a high level, I'm not sure exactly what that looks like in actual practice. What kind of design patterns do you follow when it comes to databases and ETL?"
4847,2021-01-19 07:58:07,1611035887.0,dataengineering,Need help,l0dlm8,Creative_Stuff3940,,https://www.reddit.com/r/dataengineering/comments/l0dlm8/need_help/,1.0,0.0,0.0,23990.0,What should be a learning parth and best source of learning for CS student to become a Data Engineer.
4848,2021-01-19 15:44:04,1611063844.0,dataengineering,What python library (or libraries) can I use to process geo-spatial data and bulk upload from csv to sql table?,l0k2ty,boris-mtdv1,,https://www.reddit.com/r/dataengineering/comments/l0k2ty/what_python_library_or_libraries_can_i_use_to/,1.0,12.0,0.0,24000.0,"I have a csv with approximately 10,000 rows, and among the columns are ones containing longitude, latitude, and an srid (id for a spatial referencing coordinate system), as well as an id with a foreign key constraint pointing at a different table. There is also a third table containg coordinates to a series of polygons. I need to put together a python app that processes the data with the following steps:

1. read csv
2. check to make sure the id field exists in the separate table, if not, add id to separate table
3. Check if the coordinates (longitude and latitude) lie within one of the polygons in the ploygon table, and if so, add the id of the polygon to the in the corresponding column of the first table.
4. bulk insert the processed data.

I tried using flas-sqlalchemy for steps 1,2, and 4 (haven't attempted step 3 yet), but processing the rows one by one takes way too long. As for the geo-spatial verification, I've heard you can do this directly with sql in some databases, or you can use geopandas, but I'm not sure how I would go about this and I'm not sure which verwion isw faster. Any help would be much appreciated."
4849,2021-01-19 17:17:57,1611069477.0,dataengineering,Project structure and best practices for ingesting JSON with Python?,l0lu4t,[deleted],,https://www.reddit.com/r/dataengineering/comments/l0lu4t/project_structure_and_best_practices_for/,1.0,0.0,0.0,24002.0,
4850,2021-01-19 17:19:30,1611069570.0,dataengineering,Transitioning from Traditional ETL to modern Data Engineer,l0lv75,larn123,,https://www.reddit.com/r/dataengineering/comments/l0lv75/transitioning_from_traditional_etl_to_modern_data/,1.0,0.0,0.0,24002.0,
4851,2021-01-19 17:35:17,1611070517.0,dataengineering,Project structure and best practices for ingesting JSON with Python?,l0m6j9,boggle_thy_mind,,https://www.reddit.com/r/dataengineering/comments/l0m6j9/project_structure_and_best_practices_for/,1.0,3.0,0.0,24002.0,"Hi,

Looking for feedback. First time working on a project like this and no one to bounce ideas from. Been lurking around this channel for some time, first time poster.

I'm working on a project where I have to pull json data from different API providers and store the results in a database for reporting and analysis. I have experience working with straight forward elt - moving data from source and transforming it, but json is new to me, as such I'm unsure about the best practices, project structure, naming conventions, etc.

In this project I'm building Python CLI tool for all integrations and transformations and SQL Server for storage.

The project structure looks something like this:
```
src:
|   aggregator.py
|   main.py
|   queries.py
|   uploader.py
|
+---readers
|       base.py
|       api_1.py
|		...
+---parsers
|       base.py
|       api_1.py
|		...
\---utils
        utils.py
```
Every API, get's a module in /readers (getting and storing raw data as is from API) and /parsers (parsing the json into flat csvs) packages and share a common folder structure, that looks something like this:

```
data:
+---aggregated
|   +---api_1
|   +---...
+---parsed
|   +---api_1
|   +---...
\---raw
    +---api_1
    +---...
```

Each API .py creates a dedicated folder in raw/parsed/aggregated folders, the folder name is used as a schema identifier in the database. Raw contains the raw JSON received from an API, parsed contains CSVs generated from JSON and aggregated contains (you guessed it) aggregated cvs based on name in parsed (speeds up upload). Every run, creates a unique folder name, which propagates across raw/parsed/aggregated and is used as a batch  identifier in the database. Some APIs can generate hundreds/thousands of files in raw, as I need to pass different parameters, each creating their own json response. The parameter values passed with an associated batch/request are stored in a database at run time.

Both the aggregator.py and uploder.py modules know how to interact with the folder structure, so they are generalizable, uploader.py uses the api folder name and the csv file name to identify into which schema and table it needs to be inserted. queries.py contains all the table definitions, insert statements, etc..

My goal of course is that the project would be maintainable and scalable.

Is this a reasonable approach? Do you envision any problems? What would you have done differently?

Other Questions:
 - where and how do you define your target tables? In the project I store the SQL statements for all target tables in a dedicated queries.py file, would using SqlAlchemy be a better approach?
 - Is there a standard naming convention for such projects? for example, should /readers be called adapters/connectors/plugins/datagetters?
 - are there any github repos I could take a look for inspiration?
 
 I'd appreciate any input and feedback I can get."
4852,2021-01-19 17:45:02,1611071102.0,dataengineering,Transitioning from Traditional ETL to modern Data Engineering,l0mdj7,larn123,,https://www.reddit.com/r/dataengineering/comments/l0mdj7/transitioning_from_traditional_etl_to_modern_data/,1.0,25.0,0.0,24003.0,"Hello everyone,

I'm making this post as I've been trying to learn about modern ETL technologies, in order to transition from my current job as traditional ETL (SSIS, Datastage, a lot of PL/SQL and T-SQL, etc..) to a more modern Data Engineering(Spark, Airflow, Hive, etc...) . I have a masters in Computer Engineering so even though i haven't coded that much in my current work, most CS concepts are at least familiar to me.

My biggest concerns are the following:

\- I'm 27 years old right now (4 years of experience), is it too old to make this transition? Will i be able to land a job and should i aim for entry level position?

\- I'm currently doing the following courses:

\- Taming-Big-Data-with-Apache-spark: Udemy

\- TeamDataScience Academy

Is there any other course/book/resource that anyone strongly recommends?

How should i go about doing small projects to build up my portfolio, is there any place where I can get some ideas or templates?

Thank you for reading my concerns, I have just been feeling very overwhelmed by the sheer amount of information there exists, and feeling a little bit discouraged as I feel I might be a bit too old and can't leverage my existing experience into a new job.

&amp;#x200B;

(my last post got removed, but I don't know why)"
4853,2021-01-19 17:57:24,1611071844.0,dataengineering,Ensuring Data Quality in a Data Lake Environment,l0mm95,ozzyboy,,https://www.reddit.com/r/dataengineering/comments/l0mm95/ensuring_data_quality_in_a_data_lake_environment/,1.0,2.0,0.0,24003.0,
4854,2021-01-19 18:50:58,1611075058.0,dataengineering,New in Apache Kafka 2.5 - viewing the topics used by Kafka Connect,l0nqwp,rmoff,,https://www.reddit.com/r/dataengineering/comments/l0nqwp/new_in_apache_kafka_25_viewing_the_topics_used_by/,1.0,0.0,0.0,24006.0,
4855,2021-01-19 19:28:45,1611077325.0,dataengineering,Making legacy program &amp; DB stable,l0okql,asdfzxcvfdsa,,https://www.reddit.com/r/dataengineering/comments/l0okql/making_legacy_program_db_stable/,1.0,0.0,0.0,24006.0,
4856,2021-01-19 20:35:00,1611081300.0,dataengineering,Career advice?,l0q0bh,LilyTmrw,,https://www.reddit.com/r/dataengineering/comments/l0q0bh/career_advice/,1.0,0.0,0.0,24022.0,
4857,2021-01-19 20:54:53,1611082493.0,dataengineering,Introduction to Databases for Data Engineers,l0qg2l,oleg_agapov,,https://www.reddit.com/r/dataengineering/comments/l0qg2l/introduction_to_databases_for_data_engineers/,1.0,33.0,0.0,24023.0,
4858,2021-01-19 21:40:48,1611085248.0,dataengineering,Where to practice hard SQL problems?,l0rfj8,FoxySatyr,,https://www.reddit.com/r/dataengineering/comments/l0rfj8/where_to_practice_hard_sql_problems/,1.0,4.0,0.0,24025.0,"Thinking of starting to interview soon and I'm a little rusty on SQL.

Any sites for difficult problems? 

I've used LeetCode for Python but their SQL material seems pretty bad."
4859,2021-01-19 22:38:25,1611088705.0,dataengineering,Streaming Annotated Monthly – January 2021,l0snhb,antonmry,,https://www.reddit.com/r/dataengineering/comments/l0snhb/streaming_annotated_monthly_january_2021/,1.0,0.0,0.0,24028.0,
4860,2021-01-19 23:25:35,1611091535.0,dataengineering,ETL Projects on AWS for Practice,l0tnuh,vsmatcha,,https://www.reddit.com/r/dataengineering/comments/l0tnuh/etl_projects_on_aws_for_practice/,1.0,0.0,0.0,24030.0,
4861,2021-01-19 23:55:21,1611093321.0,dataengineering,Stitch vs. FiveTran vs. AWS AppFlow,l0uata,DKSoftDev,,https://www.reddit.com/r/dataengineering/comments/l0uata/stitch_vs_fivetran_vs_aws_appflow/,1.0,0.0,0.0,24030.0,
4862,2021-01-20 01:13:34,1611098014.0,dataengineering,Transitioning from Azure to Another cloud platform,l0vwtm,Omar_88,,https://www.reddit.com/r/dataengineering/comments/l0vwtm/transitioning_from_azure_to_another_cloud_platform/,1.0,0.0,0.0,24037.0,
4863,2021-01-20 01:25:06,1611098706.0,dataengineering,"Columnar database BI developers/DE, how do you setup your development database environment?",l0w4w9,levelworm,,https://www.reddit.com/r/dataengineering/comments/l0w4w9/columnar_database_bi_developersde_how_do_you/,1.0,7.0,0.0,24037.0,"Hi friends, I read a post today (written in 200X when msot people are using RDMS) which says that it's a bad practice to use a shared development database.

*Q1*:

I'm wondering what's your setup of development database, if you are using Columnar databases such as Vertica? 

With the amount of data, I guess it's impossible to put a seperate copy locally, but does each one in the team has a separate dev enviornment (i.e. on different nodes), or everyone shares the same dev enviornment and name their tables differently (i.e. I have a fact_login_markus_sprint_20.0 and you have a fact_login_joe_sprint_20.0)?

*Q2*:

I'm also worried about sync schema changes because our tables do change fairly often. I'm thinking about pushing the production schemas into a table and writing some scripts to sync development databases, but it definitely takes some time to get it right and fully automated.

*Q3*:

How do you sync data from production to development, if it happens fairly frequently, and each sync needs different scale of data. For example let's say you need to sync every two weeks at least, and each sync can take maybe 3-8 tables, each up to 30~60 million rows of data, so that's a fair large amount of data. Is the best thing I have (for free) is `vsql` COPY command?"
4864,2021-01-20 01:32:33,1611099153.0,dataengineering,Stitch Enterprise Pricing,l0wa7m,DKSoftDev,,https://www.reddit.com/r/dataengineering/comments/l0wa7m/stitch_enterprise_pricing/,1.0,0.0,0.0,24039.0,
4865,2021-01-20 01:44:16,1611099856.0,dataengineering,"Two courses I'm taking this year offered by Carnegie Mellon for free: Database Systems and Advanced Database Systems. Both with full lectures, slides, and assignments",l0wioj,tmccormick92,,https://www.reddit.com/r/dataengineering/comments/l0wioj/two_courses_im_taking_this_year_offered_by/,1.0,0.0,0.0,24039.0,
4866,2021-01-20 02:48:15,1611103695.0,dataengineering,Currently Working Data Engineers; Help!,l0xqs6,Uttasarga,,https://www.reddit.com/r/dataengineering/comments/l0xqs6/currently_working_data_engineers_help/,1.0,0.0,0.0,24041.0,
4867,2021-01-20 04:18:52,1611109132.0,dataengineering,Data Engineering in IoT and Edge scenarios,l0zfco,s4hc,,https://www.reddit.com/r/dataengineering/comments/l0zfco/data_engineering_in_iot_and_edge_scenarios/,1.0,0.0,0.0,24043.0,
4868,2021-01-20 07:32:51,1611120771.0,dataengineering,Docker base image?,l12psq,pbj800100,,https://www.reddit.com/r/dataengineering/comments/l12psq/docker_base_image/,1.0,0.0,0.0,24047.0,"I'm new to docker so hoping someone can explain this like I'm 5.  I write python on my Mac. I've just started using virtual environments as well to install all my dependencies. My next step is to understand how to incorporate docker. I'm confused about selecting the base image. How do I determine what base image/OS to select? I would think I want to set everything the same as I'm running locally, but I'm obviously on macos and there are only Linux distributions to choose from?"
4869,2021-01-20 07:35:51,1611120951.0,dataengineering,A Quick Way To Get On Top Of Your Company’s Data: Create a Spreadsheet-Based Data Dictionary!,l12rcl,anhthong00,,https://www.reddit.com/r/dataengineering/comments/l12rcl/a_quick_way_to_get_on_top_of_your_companys_data/,1.0,0.0,0.0,24047.0,
4870,2021-01-20 07:36:28,1611120988.0,dataengineering,How to Create a Spreadsheet-Based Data Dictionary,l12rok,anhthong00,,https://www.reddit.com/r/dataengineering/comments/l12rok/how_to_create_a_spreadsheetbased_data_dictionary/,1.0,3.0,0.0,24047.0,
4871,2021-01-20 16:50:39,1611154239.0,dataengineering,An easy way to collect crash reports in our Android libraries,l1amtw,kelseyfecho,,https://www.reddit.com/r/dataengineering/comments/l1amtw/an_easy_way_to_collect_crash_reports_in_our/,1.0,0.0,0.0,24057.0,
4872,2021-01-20 17:44:26,1611157466.0,dataengineering,Landing Remote Data Engineering Job,l1bobg,penac00,,https://www.reddit.com/r/dataengineering/comments/l1bobg/landing_remote_data_engineering_job/,1.0,0.0,0.0,24058.0,
4873,2021-01-20 18:06:42,1611158802.0,dataengineering,The 4 Steps of Building Recommender Systems — Data Science,l1c4aa,lucascrafter,,https://www.reddit.com/r/dataengineering/comments/l1c4aa/the_4_steps_of_building_recommender_systems_data/,1.0,0.0,0.0,24060.0,
4874,2021-01-20 18:34:34,1611160474.0,dataengineering,The End of ETL As We Know It,l1co4w,twopairisgood,,https://www.reddit.com/r/dataengineering/comments/l1co4w/the_end_of_etl_as_we_know_it/,1.0,6.0,0.0,24059.0,
4875,2021-01-20 21:21:18,1611170478.0,dataengineering,"For those folks working with Apache Kafka, this app is pretty cool for querying Kafka topics!",l1gaue,javioverflow,,https://www.reddit.com/r/dataengineering/comments/l1gaue/for_those_folks_working_with_apache_kafka_this/,1.0,0.0,0.0,24065.0,
4876,2021-01-20 22:18:55,1611173935.0,dataengineering,🧩 Making a CRUD API using Azure Functions and Azure Cosmos DB,l1hjn8,robertinoc,,https://www.reddit.com/r/dataengineering/comments/l1hjn8/making_a_crud_api_using_azure_functions_and_azure/,1.0,0.0,0.0,24065.0,Hey folks! I’d like to share with you a very interesting article about [**Making a CRUD API using Azure Functions and Azure Cosmos DB**](https://auth0.com/blog/making-crud-api-with-azure-functions/?utm_source=reddit&amp;utm_medium=sc&amp;utm_campaign=api-azure). How did you like this post? I’m excited to read your comments or feedback here.
4877,2021-01-21 03:31:46,1611192706.0,dataengineering,Can you become a data engineer without a CS degree?,l1nuox,[deleted],,https://www.reddit.com/r/dataengineering/comments/l1nuox/can_you_become_a_data_engineer_without_a_cs_degree/,1.0,0.0,0.0,24077.0,
4878,2021-01-21 08:01:18,1611208878.0,dataengineering,My company is willing to pay someone to hang with me for a day and chat about our data needs and what we should be doing now to prepare for data scaling in the future. I’d rather ask here than post a job to UpWork.,l1se0x,BillyE53,,https://www.reddit.com/r/dataengineering/comments/l1se0x/my_company_is_willing_to_pay_someone_to_hang_with/,1.0,12.0,0.0,24084.0,"Title basically explains it. I work for a small-medium sized SaaS company that serves a network of around 15K disconnected member sites. The current extent of our data collection/analytics setup is Google Analytics for our member network and a mix of GA, Mixpanel and Segmetrics for our sales site and product metrics. 

I started with this company about 8 months ago in a purely A/B split testing role but have grown to be the point person for company-wide data needs. 

The entire selling point of our product is that our web marketing platform converts more leads for a certain industry than our competitors. I heavily utilize VWO and Google Analytics to do Conv rate testing and run queries on our network of sites and the general web data they are generating. 

But I know for a fact we could be doing a lot more with how we collect, move, and analyze data. 

Even though I’m the point person in the company on data, I kind of stumbled into that role. My background is in paid digital ads and optimization with a moderate level of data/analytics knowledge. I asked my team lead if I could go out, find a really knowledgeable DE/data scientist, and just spend a day walking them through our platform, our needs, and our options in the interest of charting the roadmap for our data buildout. 

Ideally would love to chat with...

- someone who has experience working in the tech/SaaS space 

- someone who somewhat understands A/B testing and Conv rate optimization 

Do you fit that mold? Want to spend a day with me shooting the shit and brainstorming how to build a really cool, really unique data system from scratch?

We’re happy to pay you for your time. Don’t even need you to do any actual work. I just need someone to chat with to better understand how to turn a bunch of ingredients into a recipe. 

PM me if you think you might be a good fit!"
4879,2021-01-21 08:30:26,1611210626.0,dataengineering,Is DE a job you can do in your old age?,l1ssk1,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/l1ssk1/is_de_a_job_you_can_do_in_your_old_age/,1.0,0.0,0.0,24083.0,
4880,2021-01-21 17:36:45,1611243405.0,dataengineering,Apache Airflow: Adios SubDAGs! Welcome TaskGroups!,l20r42,marclamberti,,https://www.reddit.com/r/dataengineering/comments/l20r42/apache_airflow_adios_subdags_welcome_taskgroups/,1.0,0.0,0.0,24107.0,
4881,2021-01-21 18:39:34,1611247174.0,dataengineering,Top 10 Data Engineering Blogs,l221d8,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/l221d8/top_10_data_engineering_blogs/,1.0,2.0,0.0,24107.0,
4882,2021-01-21 20:28:07,1611253687.0,dataengineering,Big Data Engineering Certifications,l24c72,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/l24c72/big_data_engineering_certifications/,1.0,2.0,0.0,24111.0,
4883,2021-01-21 22:29:37,1611260977.0,dataengineering,Can someone recommend me a book or a course on Hive that explains everything in detail?,l270ag,bakaneko_musume,,https://www.reddit.com/r/dataengineering/comments/l270ag/can_someone_recommend_me_a_book_or_a_course_on/,1.0,0.0,0.0,24117.0,
4884,2021-01-22 00:07:50,1611266870.0,dataengineering,Need help on what to study next.,l291zn,pacojosedelvino,,https://www.reddit.com/r/dataengineering/comments/l291zn/need_help_on_what_to_study_next/,1.0,0.0,0.0,24118.0,
4885,2021-01-22 05:03:46,1611284626.0,dataengineering,Open-Source No-Code Chart Maker from Microsoft Research Team,l2eklf,anhthong00,,https://www.reddit.com/r/dataengineering/comments/l2eklf/opensource_nocode_chart_maker_from_microsoft/,1.0,2.0,0.0,24121.0,
4886,2021-01-22 11:47:59,1611308879.0,dataengineering,Is anybody on their own road to become a data engineer? Am pretty much at the beginning of my journey and would like to pass as an Azure data engineer.,l2khmf,SecretLoquat3,,https://www.reddit.com/r/dataengineering/comments/l2khmf/is_anybody_on_their_own_road_to_become_a_data/,1.0,3.0,0.0,24138.0,I hope I can exchange ideas or possibly even create a study hub with like minded people.
4887,2021-01-22 15:25:39,1611321939.0,dataengineering,Data Engineer Interview at Amazon,l2noa4,thisiskcl,,https://www.reddit.com/r/dataengineering/comments/l2noa4/data_engineer_interview_at_amazon/,1.0,14.0,0.0,24144.0,Hi everyone! Who has had an interview at Amazon for this role? Can you share with me your experience and what to expect and some sample questions?
4888,2021-01-22 18:23:30,1611332610.0,dataengineering,Data Processes Help,l2r3ej,Filmerandeditorguy,,https://www.reddit.com/r/dataengineering/comments/l2r3ej/data_processes_help/,1.0,0.0,0.0,24147.0,
4889,2021-01-22 19:14:21,1611335661.0,dataengineering,Data Engineer Interview Qs (Hiring side),l2s67g,montyxander,,https://www.reddit.com/r/dataengineering/comments/l2s67g/data_engineer_interview_qs_hiring_side/,1.0,0.0,0.0,24147.0,
4890,2021-01-22 19:35:19,1611336919.0,dataengineering,WHAT IS THE LEARNING PATH TO BECOME DATA ENGINEER?,l2sm6c,Creative_Stuff3940,,https://www.reddit.com/r/dataengineering/comments/l2sm6c/what_is_the_learning_path_to_become_data_engineer/,1.0,0.0,0.0,24147.0,"Hi there! I am 2nd year CS undergraduate and I want to be a Data Engineer. So, can anyone suggest what would be a learning path for me to start my journey to become a Data Engineer?"
4891,2021-01-22 22:30:30,1611347430.0,dataengineering,IDE to query presto?,l2wbl1,blabmight,,https://www.reddit.com/r/dataengineering/comments/l2wbl1/ide_to_query_presto/,1.0,0.0,0.0,24153.0,
4892,2021-01-22 23:17:54,1611350274.0,dataengineering,Python Multithreading: fatal Python error: _enter_buffered_busy: could not aquire lock for &lt;_io.BufferedWriter name='&lt;stdout&gt;'&gt; at interpreter shutdown,l2xaar,engineer_of_data,,https://www.reddit.com/r/dataengineering/comments/l2xaar/python_multithreading_fatal_python_error_enter/,1.0,0.0,0.0,24154.0,
4893,2021-01-23 00:28:38,1611354518.0,dataengineering,Why do companies use Snowflake + BI solutions over native analytics?,l2yqge,DKSoftDev,,https://www.reddit.com/r/dataengineering/comments/l2yqge/why_do_companies_use_snowflake_bi_solutions_over/,1.0,0.0,0.0,24155.0,
4894,2021-01-23 03:20:50,1611364850.0,dataengineering,Should you get paid more for using more languages at work?,l31zpm,The_Alpacas,,https://www.reddit.com/r/dataengineering/comments/l31zpm/should_you_get_paid_more_for_using_more_languages/,1.0,7.0,0.0,24158.0,"Let’s say a job requires C++ and Python on their description. 

A few months later they give you a task that requires another programming language such as JavaScript. 

Would this warrant asking for more pay since more languages/value is being used?

Thanks"
4895,2021-01-23 04:56:15,1611370575.0,dataengineering,Python for Data Science,l33mhm,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33mhm/python_for_data_science/,1.0,0.0,0.0,24159.0,
4896,2021-01-23 04:56:32,1611370592.0,dataengineering,Python for Data Science,l33mnb,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33mnb/python_for_data_science/,1.0,0.0,0.0,24159.0,
4897,2021-01-23 04:57:16,1611370636.0,dataengineering,Python for Data Science,l33n2p,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33n2p/python_for_data_science/,1.0,0.0,0.0,24159.0,
4898,2021-01-23 04:58:42,1611370722.0,dataengineering,Python for Data Science,l33nx7,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33nx7/python_for_data_science/,1.0,0.0,0.0,24159.0,
4899,2021-01-23 05:00:34,1611370834.0,dataengineering,Python for Data Science,l33p5d,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33p5d/python_for_data_science/,1.0,0.0,0.0,24159.0,
4900,2021-01-23 05:03:54,1611371034.0,dataengineering,Python for Data Science,l33r5b,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33r5b/python_for_data_science/,1.0,0.0,0.0,24159.0,
4901,2021-01-23 05:18:45,1611371925.0,dataengineering,Python for Data Science,l33zyy,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l33zyy/python_for_data_science/,1.0,0.0,0.0,24159.0,
4902,2021-01-23 09:56:16,1611388576.0,dataengineering,Databricks Raising Funds At $27B Valuation: Report,l380jj,The-Techie,,https://www.reddit.com/r/dataengineering/comments/l380jj/databricks_raising_funds_at_27b_valuation_report/,1.0,0.0,0.0,24163.0,
4903,2021-01-23 10:42:57,1611391377.0,dataengineering,"Help choosing a data pipeline framework/product (batch, not big-data)",l38kyp,techtonico,,https://www.reddit.com/r/dataengineering/comments/l38kyp/help_choosing_a_data_pipeline_frameworkproduct/,1.0,0.0,0.0,24165.0,
4904,2021-01-23 15:24:41,1611408281.0,dataengineering,Big Data File Showdown – Avro vs Parquet with Python.,l3c12g,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/l3c12g/big_data_file_showdown_avro_vs_parquet_with_python/,1.0,16.0,0.0,24178.0,
4905,2021-01-23 19:39:21,1611423561.0,dataengineering,Senior Data QA Engineer Interview preparation,l3gme1,ThatFilm,,https://www.reddit.com/r/dataengineering/comments/l3gme1/senior_data_qa_engineer_interview_preparation/,1.0,0.0,0.0,24179.0,
4906,2021-01-23 22:13:29,1611432809.0,dataengineering,Looking for CERT recommendations,l3jgf3,Dazed_Monkey44,,https://www.reddit.com/r/dataengineering/comments/l3jgf3/looking_for_cert_recommendations/,1.0,0.0,0.0,24182.0,
4907,2021-01-24 00:08:12,1611439692.0,dataengineering,Python libraries for Data Science and machine learning you should know,l3lsqz,mooha790,,https://www.reddit.com/r/dataengineering/comments/l3lsqz/python_libraries_for_data_science_and_machine/,1.0,0.0,0.0,24187.0,
4908,2021-01-24 02:04:38,1611446678.0,dataengineering,"Can a graduate with a data engineering internship (3 month) qualify for junior roles, considering DE is usually a mid level job?",l3nze4,CroatianCrystalline,,https://www.reddit.com/r/dataengineering/comments/l3nze4/can_a_graduate_with_a_data_engineering_internship/,1.0,13.0,0.0,24187.0,I have a 3 month internship with AWS and Spark using Java as part of my masters in SE. I will also try and do some personal projects. DE seems to be for former software engineering and analysts and not for graduates... Thoughts?
4909,2021-01-24 13:48:23,1611488903.0,dataengineering,Data as a Service,l3y1bu,sk2977,,https://www.reddit.com/r/dataengineering/comments/l3y1bu/data_as_a_service/,1.0,5.0,0.0,24198.0,"Problem

You don’t know how to make better decisions.

Solution

Data as a Service (DaaS) bridges the gap between your predictions and reality. The lower your spread, the better your decisions.

Data is a means to an end, if consumed as a product. However, insights are closer to the ""job to be done"", if consumed as a service. In this article I'll cover aspects of both.

In competitive environments, the more data is shared, the less valuable it becomes. For example, shared lead lists for marketers have low close rates, publicly shared transactions are already priced in to the value of stocks. However, in cooperative environments, like public health, data sharing is extremely valuable. Consider the case of covid at the moment. Keep this distinction in mind for the rest of the article.

Motivation

So what is powering DaaS? Very few companies had the ability to make use of raw data in the past. An order of magnitude more companies are buying data today than did 5 years ago. That's because a good engineer with a tool like Snowflake can be as productive as a great engineer 5 years ago.

We are seeing a commoditisation of data-warehousing with Snowflake, data lakes with AWS S3, and even Lakehouses with Databricks. The end user has ease of data ingestion with Fivetran, aggregation and monitoring with dbt.

These solutions are driving a rise to the market of alternative data, where companies are not only interested in internal (private) data sources but also external (public) data sources.

One example is hedge funds. Less than 3 years ago, just ten hedge funds were buying alternative data. Today there are 500-700 funds currently making investments to ingest large amounts of data.

Data as a Product (DaaP) vs Data as a Service (DaaS)

It is the difference between providing ""data"" and providing ""insights""

There are two broad mandates (I'm being overly simplistic on purpose) that data teams are formed with:

Provide data to a company
Provide insights to a company

In a DaaS model, the data team partners with stakeholder groups to tackle specific problems using data. Team members have more functional experience, and are responsible for providing insights as opposed to raw data.

Challenge

There are hundreds of companies that sell software and tools (traditional SaaS) to data scientists and machine learning teams. But there aren't many that specialise in selling data to teams that require it, in whatever shape or form (DaaP or DaaS).

Being a data-only business is less exciting because data is a supporting role.

Data companies support the true innovators.

It's like selling high quality butter to pastry chefs. The end consumer of the pastry may not even know there is butter in it, but the chef knows how important the ingredient was. Of course, the most important player in making a tasty pastry is the chef. A data company is just one of the important ingredients that goes into its creation.

Prediction

The last 15 years have been about how companies get insights from their own data -- Tableau, PowerBI, Snowflake, Palantir have played into that trend.

Companies that are far along the curve of getting insights from their own data need external data if they want to continue getting value from their data teams. Because even the largest companies only knows about 0.1% of the world from their internal data alone.

Nonetheless, there are still very few data buyers today. Most companies want applications (answers), not data (collection of facts). The only reason to start a data business today is if you believe the number of data buyers will go up an order of magnitude in the next five to ten years.

It's a great time to build a pure-play DaaS business if you’re bullish on this trend.

Selling intelligent data to teams is starting to be a big business."
4910,2021-01-24 18:24:04,1611505444.0,dataengineering,Creating a Docker Container for running Pyspark Unit Tests/ Debugging using an IDE,l42fxk,Omar_88,,https://www.reddit.com/r/dataengineering/comments/l42fxk/creating_a_docker_container_for_running_pyspark/,1.0,0.0,0.0,24203.0,
4911,2021-01-24 18:46:20,1611506780.0,dataengineering,Apache spark on a one weak virtual machine,l42wfo,omrixomri,,https://www.reddit.com/r/dataengineering/comments/l42wfo/apache_spark_on_a_one_weak_virtual_machine/,1.0,0.0,0.0,24204.0,
4912,2021-01-24 23:23:34,1611523414.0,dataengineering,Anyone use this course for interview preparation for FAANG?,l47ztt,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/l47ztt/anyone_use_this_course_for_interview_preparation/,1.0,0.0,0.0,24220.0,
4913,2021-01-25 00:22:29,1611526949.0,dataengineering,"Developers Who Have or Are Wanting to Become Data Engineers, What's Your Reason?",l4976k,_ByteMeh,,https://www.reddit.com/r/dataengineering/comments/l4976k/developers_who_have_or_are_wanting_to_become_data/,1.0,5.0,0.0,24220.0,"Hi All,  


I am a newly minted Data Engineer that is trying to decide where I want to do with my career.  
Originally I was aiming to become a Full Stack Engineer, but I had an opportunity to become a Data Engineer instead. I hadn't really heard of Data Engineering until then, and now I am starting to get a feel for what the job entails.   


That said, I like what I've seen so far, but I am unsure if staying as a Data Engineer might hurt my long term prospects, if I decide I'd like to do Full Stack Engineering instead in the future.

  
For any developers who have become or want to become data engineers, what's your reason?

Also for those who have become data engineers,  have you had any difficulty with switching back and forth between data engineering and other software engineering roles?  


Thank you for your time!"
4914,2021-01-25 02:08:35,1611533315.0,dataengineering,Is Hadoop (map reduce) seeing a decline recently?,l4b9ly,YaswanthBangaru,,https://www.reddit.com/r/dataengineering/comments/l4b9ly/is_hadoop_map_reduce_seeing_a_decline_recently/,1.0,0.0,0.0,24224.0,
4915,2021-01-25 11:13:50,1611566030.0,dataengineering,Sending Emails from Apache airflow,l4k7kn,RoughWorking,,https://www.reddit.com/r/dataengineering/comments/l4k7kn/sending_emails_from_apache_airflow/,1.0,0.0,0.0,24235.0,
4916,2021-01-25 16:05:04,1611583504.0,dataengineering,Big Data File Showdown – Part 2 – ORC with Python.,l4oh2m,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/l4oh2m/big_data_file_showdown_part_2_orc_with_python/,1.0,0.0,0.0,24243.0,
4917,2021-01-25 21:46:54,1611604014.0,dataengineering,Data Pipelines Resiliency,l4vsn7,Original_Bend,,https://www.reddit.com/r/dataengineering/comments/l4vsn7/data_pipelines_resiliency/,1.0,0.0,0.0,24249.0,"Hi,

I would like to open this thread to discuss the strategies you guys use to make your pipelines “antifragile”, resilient. 

I’m doing stuff like:

-	checking table schema
-	checking proper type
-	checking for unique primary keys

I would like to know if there are more advanced methods to catch bad input / calculations, like statistics ones that are practical to use. I’ve heard about the tool greatexpectations.

Thanks"
4918,2021-01-25 22:15:52,1611605752.0,dataengineering,I'm 1.5 years into DE and I still feel like I know very little,l4wg0n,hipsterrobot,,https://www.reddit.com/r/dataengineering/comments/l4wg0n/im_15_years_into_de_and_i_still_feel_like_i_know/,1.0,1.0,0.0,24254.0,"Hello, I've actually started as a Junior DE (as an internal transfer) at my job, I was very lucky to get this position and was mainly hired to maintain and learn the existing data pipelines and our Looker BI tools. During this time, I've learned quite a lot, I now have great SQL skills, LookML and I've even had some opportunity to work with Linux cli, python and Glue. 

However I still don't feel like I have much understanding of databases, servers, various aws tools that are available (like ec2, emr, dynamo etc some other products my company uses), or would have any idea where to start if I were to build something from scratch. I also don't have any knowledge in tech like mongodb, hadoop, airflow, spark etc things many people mentioned in this sub, mainly because i'm not familiar with the use case of any of these technologies. 

I'm just a little lost as to how I should proceed in this world, I've always believed in improving myself and adding skills to my portfolio, but there's just so much out there, and I feel like I'm still missing alot of base knowledge, and don't know where to go. Thanks for the help!"
4919,2021-01-25 23:36:20,1611610580.0,dataengineering,Tech Stack,l4y83m,coreytrevorlahey69,,https://www.reddit.com/r/dataengineering/comments/l4y83m/tech_stack/,1.0,3.0,0.0,24259.0,"How's it going!

I'm a recent grad with a B.S. Statistics (Data Science), or in other words a mix between stats / cs, and I'd eventually like to get into data engineering after an entry level analyst role. 

**What does your tech stack look like, and what tools / libraries / languages would you recommend to a beginner?** 

I've got a decent grasp on Python, SQL, database design, OOP, etc, however I lack the knowledge on how / when to put it all together.

**Can you recommend a comprehensive project that would encapsulate as many core data engineering tasks as possible?**

Thank you!"
4920,2021-01-26 00:27:29,1611613649.0,dataengineering,Blogging to showcase projects,l4zbjj,infiniteAggression-,,https://www.reddit.com/r/dataengineering/comments/l4zbjj/blogging_to_showcase_projects/,1.0,2.0,0.0,24262.0,"Apologies if this is the wrong place to ask.  

I was told here that as a student and someone looking towards getting into DE, blogging about projects that I do is a good way to showcase my skills. While I understand that, I have an amateur's understanding of web development and don't consider myself skilled enough to develop a blog from **scratch**.  

Will using a template from websites such as HTML5UP to write and display my portfolio be considered a bad thing? I don't want to spend time learning web development and would like to concentrate my efforts towards DE stuff, but at the same time I'd like to have something presentable. So, would using templates send the wrong message or is it understandable?

Thanks!"
4921,2021-01-26 00:32:05,1611613925.0,dataengineering,Help deciding where to go from here (Entry-level),l4zey5,skydive_dallas,,https://www.reddit.com/r/dataengineering/comments/l4zey5/help_deciding_where_to_go_from_here_entrylevel/,1.0,0.0,0.0,24262.0,
4922,2021-01-26 04:20:57,1611627657.0,dataengineering,Hot take: You (probably) don't need high availability,l53wql,sciencewarrior,,https://www.reddit.com/r/dataengineering/comments/l53wql/hot_take_you_probably_dont_need_high_availability/,1.0,1.0,0.0,24264.0,"Seriously, I know you love writing your Kubernetes YAML and configuring  net topologies for highly resilient clusters, but before you embark into a months-long quest for high availability, ask yourself, what happens if your jobs don't run for fifteen minutes, or one hour, or even four! What's the real impact for the business? Nine out of ten times, one hour of downtime will be a minor nuisance, and you can recover a simple system from a backup in ten minutes or less, specially if you document and automate properly. I have, in the past, and with even basic monitoring, your infrastructure team can handle it and only warn you after the fact.

But you didn't listen to some random guy on the web and built your robust, high-availability system with a Kafka queue for your jobs and dynamically allocated Kubernetes instances. Great. It will probably hum along for much longer without failures, but when it does fail, you won't fix it in fifteen minutes. It may take a day. Maybe three. You will have to search the web trying to match this cryptic error spit out five levels deep with an incomplete Stack Overflow answer. Again, I've been there. It wasn't fun.*

I'm not saying you should *never* go for a spiffy, high-availability setup, but consider the cost, make sure you are comfortable with the tech, and have detailed plans to recover from failure, because as Douglas Adams wisely said, ""The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repair.""

* It was HA Hadoop, though; that was a few years before Kubernetes became a thing, and we were still on-premise."
4923,2021-01-26 06:39:19,1611635959.0,dataengineering,Searchable Interface for SQL Query Repo,l56cf9,wandersquash,,https://www.reddit.com/r/dataengineering/comments/l56cf9/searchable_interface_for_sql_query_repo/,1.0,0.0,0.0,24268.0,
4924,2021-01-26 07:25:33,1611638733.0,dataengineering,Statistics for Data Science,l573k6,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l573k6/statistics_for_data_science/,1.0,0.0,0.0,24268.0,
4925,2021-01-26 07:26:27,1611638787.0,dataengineering,Statistics for Data Science,l5741p,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l5741p/statistics_for_data_science/,1.0,0.0,0.0,24268.0,
4926,2021-01-26 07:29:41,1611638981.0,dataengineering,Statistics for Data Science,l575tj,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l575tj/statistics_for_data_science/,1.0,1.0,0.0,24268.0,
4927,2021-01-26 08:26:07,1611642367.0,dataengineering,Statistics for Data Science,l580xo,Matlabguru,,https://www.reddit.com/r/dataengineering/comments/l580xo/statistics_for_data_science/,1.0,0.0,0.0,24271.0,
4928,2021-01-26 15:37:45,1611668265.0,dataengineering,AWS python lambda csv cleanup,l5e4zt,randomepsilon,,https://www.reddit.com/r/dataengineering/comments/l5e4zt/aws_python_lambda_csv_cleanup/,1.0,0.0,0.0,24285.0,
4929,2021-01-26 15:57:21,1611669441.0,dataengineering,What is the difference between those two firms,l5ehit,Serious-Antelope4536,,https://www.reddit.com/r/dataengineering/comments/l5ehit/what_is_the_difference_between_those_two_firms/,1.0,18.0,0.0,24285.0,"Hi, 

I am struggling to get my head around the difference between these two firms: 

1.[https://www.montecarlodata.com/](https://www.montecarlodata.com/)

2. [https://databand.ai/platform/](https://databand.ai/platform/)

&amp;#x200B;

Are they both trying to solve the issue of data quality? But monte carlo does it by looking at the data, hence being a data reliability platform and databand does it by looking at the data pipelines and their overall health. 

&amp;#x200B;

Great, if you could help me!"
4930,2021-01-26 16:50:16,1611672616.0,dataengineering,Install DBT on Windows and use Azure SQL and Synapse,l5fh9i,valdasm,,https://www.reddit.com/r/dataengineering/comments/l5fh9i/install_dbt_on_windows_and_use_azure_sql_and/,1.0,0.0,0.0,24286.0,
4931,2021-01-27 17:09:23,1611760163.0,dataengineering,7 Data Engineering Evangelists to Follow,l66ek4,an_tonova,,https://www.reddit.com/r/dataengineering/comments/l66ek4/7_data_engineering_evangelists_to_follow/,1.0,0.0,0.0,24363.0,"Hi folks, I want to share my list of scientists/business leaders who I follow to stay up-to-date about what is going on on the data engineering scene.

Feel free to share your bookmarks in the comments. 

👉  [John Lafleur](https://medium.com/@jeanlafleur)  \- Co-Founder of Airbyte - writes about ETL /  ELT and his startup journey

👉  [Tristan Handy](https://medium.com/@jthandy) \- Co-founder of Fishtown, created by dbt -  about startups, trends in data analytics and a little bit about Fishtown

👉  [Connor Shorten](https://connorshorten300.medium.com/) \- Computer Science Ph.D. at FAU - about Computer Vision, Natural Language Processing, Graph Embeddings, Generative Adversarial Networks, Reinforcement Learning, and more

👉  [Jürgen Schmidhuber](http://people.idsia.ch/~juergen/) \- computer scientist, researcher, keynote speaker, co-director of the Dalle Molle Institute for Artificial Intelligence Research - about the science of AI [Sébastien Derivaux](https://dataintoresults.com/post/category/thoughts/) \- shareholder and board member of many startups - about data science and startups

👉 [Jesse Anderson](https://www.jesse-anderson.com/category/blog/) \- data engineering evangelist - all-around data engineering in a simple way

👉 [David Layton](https://medium.com/@dmlayton) \- former CERN physicists, data engineer &amp; scientist - about agile, data management, tools

👉 [George Fraser](https://twitter.com/frasergeorgew) \- CEO of Fivetran - about trends in data preparation and management

&amp;#x200B;

https://preview.redd.it/g91uhyjg4wd61.png?width=220&amp;format=png&amp;auto=webp&amp;s=88213a6aba64a70ae6fbceee88593946449805c5"
4932,2021-01-27 17:11:14,1611760274.0,dataengineering,8 Data Engineering Evangelists to Follow,l66fyz,an_tonova,,https://www.reddit.com/r/dataengineering/comments/l66fyz/8_data_engineering_evangelists_to_follow/,1.0,25.0,0.0,24363.0,"Hi folks, I want to share my list of scientists/business leaders who I follow to stay up-to-date about what is going on on the data engineering scene.

Feel free to share your bookmarks in the comments.

👉  [John Lafleur](https://medium.com/@jeanlafleur)  \- Co-Founder of Airbyte - writes about ETL /  ELT and his startup journey

👉  [Tristan Handy](https://medium.com/@jthandy) \- Co-founder of Fishtown, created by dbt -  about startups, trends in data analytics and a little bit about Fishtown

👉  [Connor Shorten](https://connorshorten300.medium.com/) \- Computer Science Ph.D. at FAU - about Computer Vision, Natural Language Processing, Graph Embeddings, Generative Adversarial Networks, Reinforcement Learning, and more

👉  [Jürgen Schmidhuber](http://people.idsia.ch/~juergen/) \- computer scientist, researcher, keynote speaker, co-director of the Dalle Molle Institute for Artificial Intelligence Research - about the science of AI

👉 [Sébastien Derivaux](https://dataintoresults.com/post/category/thoughts/) \- shareholder and board member of many startups - about data science and startups

👉 [Jesse Anderson](https://www.jesse-anderson.com/category/blog/) \- data engineering evangelist - all-around data engineering in a simple way

👉 [David Layton](https://medium.com/@dmlayton) \- former CERN physicists, data engineer &amp; scientist - about agile, data management, tools

👉 [George Fraser](https://twitter.com/frasergeorgew) \- CEO of Fivetran - about trends in data preparation and management"
4933,2021-01-27 17:33:46,1611761626.0,dataengineering,Great Expectations Hackathon Tomorrow @ 5pm PT,l66xlg,superconductiveKyle,,https://www.reddit.com/r/dataengineering/comments/l66xlg/great_expectations_hackathon_tomorrow_5pm_pt/,1.0,1.0,0.0,24363.0,"In case you missed me mentioning it a couple of weeks ago...

A reminder that the Great Expectations ([https://github.com/great-expectations/great\_expectations](https://github.com/great-expectations/great_expectations)) Data Professionals Hackathon is tomorrow at 5pm PT! 

It is a great opportunity to learn how to create custom expectations, become a contributor to the project, network with the community, get some awesome swag and win some cash (up to $750)!

In order to participate, you need to sign up here: [https://www.surveymonkey.com/r/great-expectations-hackathon-3](https://www.surveymonkey.com/r/great-expectations-hackathon-3)

In the last hackathon with college students, we were able to get contributions across the line from all but one team and it was most if not all of their first times using Great Expectations! So no worries if you are just getting started. Also, you'll notice in the sign up there's another student hackathon on Feb 6! 

It's a great opportunity to pad your portfolio! Hope to see some of you there."
4934,2021-01-27 18:20:33,1611764433.0,dataengineering,Spark interview prep: what to focus on?,l680eu,fail_to_reject_null,,https://www.reddit.com/r/dataengineering/comments/l680eu/spark_interview_prep_what_to_focus_on/,1.0,7.0,0.0,24367.0,"I have an interview for an engineering job that, among other things, uses Spark. I have not used Spark professionally, and having worked through a PySpark book, feel like I have a reasonable grasp of how it works and some common syntax. 

If you were preparing for a 1 hour interview where you were going to pair code Spark to demonstrate skills, what would you focus on?"
4935,2021-01-27 18:28:04,1611764884.0,dataengineering,Become a data engineer with a Bildungsgutschein!,l68701,soobrosa,,https://www.reddit.com/r/dataengineering/comments/l68701/become_a_data_engineer_with_a_bildungsgutschein/,1.0,0.0,0.0,24367.0,
4936,2021-01-27 18:43:16,1611765796.0,dataengineering,How Intuit Built a Self-serve Stream Processing Platform with Flink,l68kcb,Marksfik,,https://www.reddit.com/r/dataengineering/comments/l68kcb/how_intuit_built_a_selfserve_stream_processing/,1.0,0.0,0.0,24368.0,
4937,2021-01-27 19:22:14,1611768134.0,dataengineering,Practicing SSIS/SSIS Project,l69j33,ziyabikram,,https://www.reddit.com/r/dataengineering/comments/l69j33/practicing_ssisssis_project/,1.0,3.0,0.0,24370.0,"Hi,
I have just finished a SSIS course on Udemy that had no project at the end of the course. I learnt creating data flows, control flows, Windows Management Instrumentation, deploying packages, Configuration types, dynamic packages,Message queuing, CDC components, Incremental Data Loading, Maintenance Planning Tasks and some other things. Now, I want to build a project or something else in order to show it to my potential employer in the future and hone my skills. So, how can I do it with SSIS? 
Thanks!"
4938,2021-01-27 22:24:08,1611779048.0,dataengineering,Data engineering books for beginers,l6dj9t,desynher,,https://www.reddit.com/r/dataengineering/comments/l6dj9t/data_engineering_books_for_beginers/,1.0,4.0,0.0,24382.0,
4939,2021-01-28 00:47:06,1611787626.0,dataengineering,Building a data lake: from batch to real-time using Kafka,l6gnu3,rmoff,,https://www.reddit.com/r/dataengineering/comments/l6gnu3/building_a_data_lake_from_batch_to_realtime_using/,1.0,0.0,0.0,24392.0,
4940,2021-01-28 02:57:02,1611795422.0,dataengineering,unit test your sql transforms in dbt,l6jnvw,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/l6jnvw/unit_test_your_sql_transforms_in_dbt/,1.0,0.0,0.0,24397.0,
4941,2021-01-28 03:36:37,1611797797.0,dataengineering,How to set up a cloud database?,l6kjff,Mauliklm10,,https://www.reddit.com/r/dataengineering/comments/l6kjff/how_to_set_up_a_cloud_database/,1.0,10.0,0.0,24400.0,
4942,2021-01-28 03:59:17,1611799157.0,dataengineering,Exploratory Project (Looking For Input),l6l0lm,coreytrevorlahey69,,https://www.reddit.com/r/dataengineering/comments/l6l0lm/exploratory_project_looking_for_input/,1.0,2.0,0.0,24402.0,"Hey! Thanks in advance for taking the time to read / help.

I'm going to start an exploratory data engineering project (for fun), and could use some advice. 

The project (in my mind) will look like this:

1. access web API to source data
2. transfer data into cloud / local storage
3. transfer data into either cloud data warehouse / sql server database /  local sql database
4. perform some basic analysis
5. publish results to web page

Tools I'm considering using:

* Python
* Postgres
* Cloud storage 
* Cloud warehouse
* airflow
* spark

My main issue is that I don't know in which order I should develop these layers. Intuition tells me to set up local postgres and create python script to access API first, yet I'm unsure.

Any input regarding process of development, feasibility / usefulness of components / frameworks, or anything else that comes to mind would be greatly appreciated. Thank you!"
4943,2021-01-28 08:23:35,1611815015.0,dataengineering,DataBrick Community Edition Sign-Up Failing,l6pxus,Dazed_Monkey44,,https://www.reddit.com/r/dataengineering/comments/l6pxus/databrick_community_edition_signup_failing/,1.0,0.0,0.0,24410.0,
4944,2021-01-28 08:56:50,1611817010.0,dataengineering,What to read now instead of Kimball/Inmon as a more modern intro to different modeling approaches?,l6qfkm,dondraper36,,https://www.reddit.com/r/dataengineering/comments/l6qfkm/what_to_read_now_instead_of_kimballinmon_as_a/,1.0,0.0,0.0,24410.0,
4945,2021-01-28 10:01:38,1611820898.0,dataengineering,Learning about data governance,l6rdz1,ElectricalFilm2,,https://www.reddit.com/r/dataengineering/comments/l6rdz1/learning_about_data_governance/,1.0,2.0,0.0,24412.0,
4946,2021-01-28 10:30:40,1611822640.0,dataengineering,From Hadoop Data Engineer to Data Scientist,l6rtlf,GreekYogurtt,,https://www.reddit.com/r/dataengineering/comments/l6rtlf/from_hadoop_data_engineer_to_data_scientist/,1.0,4.0,0.0,24412.0,
4947,2021-01-28 14:41:26,1611837686.0,dataengineering,[Question] Airflow SubDag,l6vn6e,Fredbull,,https://www.reddit.com/r/dataengineering/comments/l6vn6e/question_airflow_subdag/,1.0,10.0,0.0,24427.0,"Hi everyone,

so I'm using Airflow to orchestrate my ETLs, and I have a question on best practices.

Basically I have an ETL composed of, let's say, 5 sequential SQL queries. I am going to be running this ETL independently every once in a while, but it will also run as a piece in a larger ETL, in parallel with other streams.

Basically, I would like to be able to:

* create a DAG for the basic ETL and store it in one module;
* somehow import this DAG into the ""big"" ETL and use it as a piece in it, the same way as I would use a task.

I came across the SubDag Airflow module, but the usage looks pretty confusing to me (having to use a DAG factory, having to set the DAG Id to ""&lt;parent&gt;.&lt;child&gt;"", etc.).

Has anyone faced this conceptual problem before? Any tips would be much appreciated!"
4948,2021-01-28 15:53:59,1611842039.0,dataengineering,BIG DAY! The Astronomer Certified for Apache Airflow Core is COMING! Test your skills and show them to the world,l6x536,marclamberti,,https://www.reddit.com/r/dataengineering/comments/l6x536/big_day_the_astronomer_certified_for_apache/,1.0,0.0,0.0,24427.0,
4949,2021-01-28 18:30:53,1611851453.0,dataengineering,Has anyone migrated from a system like WhereScape to dbt?,l71g0n,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/l71g0n/has_anyone_migrated_from_a_system_like_wherescape/,1.0,0.0,0.0,24431.0,"Evaluating if my company should move from WhereScape to dbt due to ease, cost, etc. Our environment is simple and our db is Snowflake."
4950,2021-01-28 19:15:26,1611854126.0,dataengineering,"Simplest way to ingest multiple types of large files, process them, and send data in chunks to services in AWS?",l72rmb,joehfb,,https://www.reddit.com/r/dataengineering/comments/l72rmb/simplest_way_to_ingest_multiple_types_of_large/,1.0,10.0,0.0,24436.0,"Hi everyone,

What would you guys suggest for building something that:

* Takes in several different input file types (ex: csv, json, jsonl, xml, .gz, ...)
* That can be large (as of right now we know these can be multiple GBs)
* Maybe does some transformations / processing (requirements not quite as clear on this one yet)
* And sends all this data to our web services for further processing

Not asking for a solution design to this :)  We have a pretty simple node lambdas based design right now, but I'm just looking for thoughts in terms of technologies we could leverage to make this less custom code, more flexible, and simpler that I can research into myself if you guys could help with any specific directions for me to explore. 

**Basic requirements known so far**

* Be able to handle large files - at least multiple gigabytes
* This file would be in S3 - where this capability would be reading from
* In a reasonably performant manner - obviously everyone wants the multiple GB file to be processed and the data in our services within seconds, but that's not going to happen. AFAIK an hour or two may be OK here.
* This thing will NOT be writing to files or some DBs - it will be hitting our existing web services to send data to other parts of the platform

**Possible but unknown requirements**

* This thing may need to transform data
* And if possible, the transformation might need to be dynamic without an engineer going in and hard coding a transformation for each and every customer request. 
Ex: Customer A wants all 123s -&gt; 456s
But customer B wants all ABCs -&gt; XYZs

**My goals**

* If possible, reduce custom code for the file ingestion / file transformation parts

I haven't dealt with files often in my career, and same for most of us here.  I'd like to avoid running into a problem where someone not used to processing large files writes entirely custom code from scratch and unintentionally creates something not very maintainable / extensible / scalable / etc.

* Keep it simple. Only take on additional complexity if needed.

I'm not too familiar with the data tools out there like Spark, Hadoop,  Airflow, ... From what I read so far (ex: first comment thread this thread https://www.reddit.com/r/dataengineering/comments/j4xhfk/processing_big_json_file_30gb/) maybe some of these tools will be like trying to kill a fly with a nuclear bomb overkill. 


If anyone has any thoughts around what would be good thing for me to poke around at, if there's something critical I missed, etc, happy to hear your thoughts."
4951,2021-01-28 20:45:08,1611859508.0,dataengineering,Data Observability: Building Your Own Data Quality Monitors Using SQL,l74wvm,monacodev,,https://www.reddit.com/r/dataengineering/comments/l74wvm/data_observability_building_your_own_data_quality/,5.0,3.0,0.0,24440.0,
4952,2021-01-28 21:09:29,1611860969.0,dataengineering,Simple project - looking for ideas,l75lhh,Gawgba,,https://www.reddit.com/r/dataengineering/comments/l75lhh/simple_project_looking_for_ideas/,1.0,0.0,0.0,24440.0,"We have a MySQL database that records 'checkins' of users into a location, and we also have the log file of a completely separate system that monitors user locations.  I have a need to join these two data sources and perform an action under certain conditions (e.g. user hasn't checked in according to mysql but the other system is starting they are in the location) - the action might be possibly emailing, but at the very least recording an entry in a different database indicating the discrepancy.

Typically I would approach this would some crude python script that's cronned to run every 5 minutes, and gathers the data from both sources, but are there tools or frameworks that might be better suited to this task?"
4953,2021-01-28 23:59:13,1611871153.0,dataengineering,I accidentally got an technical interview,l7a790,jolllof,,https://www.reddit.com/r/dataengineering/comments/l7a790/i_accidentally_got_an_technical_interview/,1.0,9.0,0.0,24443.0,"I am currently a data analyst (python and sql). I decided to start getting into data engineering a month or two ago and accidentally got an interview with Facebook? What should i learn to prepare?

I don’t even remember applying but apparently i did in October!"
4954,2021-01-29 01:51:25,1611877885.0,dataengineering,I need a resource/repo/template on creating an datalake ingestion api,l7cyom,fysp11,,https://www.reddit.com/r/dataengineering/comments/l7cyom/i_need_a_resourcerepotemplate_on_creating_an/,1.0,1.0,0.0,24454.0,"I need to decide on how to implement an api + authentication + kinesis to ingest data into a datalake.

This endpoint will be available for multiple tenants and with different possible data models."
4955,2021-01-29 08:47:06,1611902826.0,dataengineering,Is data engineering work less rewarding than Data Science ?,l7lsd1,GreekYogurtt,,https://www.reddit.com/r/dataengineering/comments/l7lsd1/is_data_engineering_work_less_rewarding_than_data/,1.0,48.0,0.0,24472.0,"I have just started working with Hadoop Scala + Spark + Hive.   


Right now I'm learning quite a lot, but it feels like after a point , Data Engineering work will start to feel repetitive ,working in the background and serving the Humongous data.  


How is the pay &amp; demand for Data engineers going to be in future , compared to software engineers &amp; Data Scientists"
4956,2021-01-29 14:46:09,1611924369.0,dataengineering,"The NEW TriggerDagRunOperator, Create DAG dependencies at ease in Apache Airflow",l7rnri,marclamberti,,https://www.reddit.com/r/dataengineering/comments/l7rnri/the_new_triggerdagrunoperator_create_dag/,1.0,7.0,0.0,24485.0,
4957,2021-01-29 15:20:25,1611926425.0,dataengineering,7 concepts of databases every data engineer should know,l7sck1,Prestigious_Candle61,,https://www.reddit.com/r/dataengineering/comments/l7sck1/7_concepts_of_databases_every_data_engineer/,1.0,3.0,0.0,24487.0,
4958,2021-01-29 18:06:14,1611936374.0,dataengineering,Become a Data Science expert Develop the critical skills you need for a new job in data science or fast-track your career advancement with the most comprehensive program:,l7wafe,wsm38112,,https://www.reddit.com/r/dataengineering/comments/l7wafe/become_a_data_science_expert_develop_the_critical/,1.0,0.0,0.0,24499.0,
4959,2021-01-29 19:47:20,1611942440.0,dataengineering,How to be a Data Engineer?,l7yxp7,Flimsy_Piece2760,,https://www.reddit.com/r/dataengineering/comments/l7yxp7/how_to_be_a_data_engineer/,1.0,4.0,0.0,24503.0,"Now I am working as a AI developer. But feeling very interested in Data Engineering. 

How to be a Data Engineer along side AI developer?"
4960,2021-01-30 00:22:39,1611958959.0,dataengineering,"Meltano: ELT for the DevOps era — Open source, self-hosted, CLI-first, debuggable, and extensible",l863az,MeltanoDouwe,,https://www.reddit.com/r/dataengineering/comments/l863az/meltano_elt_for_the_devops_era_open_source/,1.0,28.0,0.0,24515.0,
4961,2021-01-30 00:58:27,1611961107.0,dataengineering,Is there an easy environment for me to test my SQL Queries?,l86zta,hmmwhatdoyouthinkabt,,https://www.reddit.com/r/dataengineering/comments/l86zta/is_there_an_easy_environment_for_me_to_test_my/,1.0,7.0,0.0,24515.0,"Hello friends,

This is most likely a pretty basic question. I've tried googling it, but I am not wording my search correctly, and getting poor results. 

I have primarily used Python to transform and clean data, usually with pandas

I am trying to pivot my career to get into data engineering, and one of the interviews for a company I'm really interested in has a case study at the beginning to pass

The case study tests my SQL skills (I've used SQL before, and am familiar with basics up to windowing functions). Now, had this been in Python I would have just loaded the dataset in and been able to test with Jupyter Notebook. But I'm wondering how you guys test SQL queries? Do I need to create a local instance of a Postgres DB, and test my queries like that? Or, is there an easier way to set up your environment for exploratory analysis

Thank you"
4962,2021-01-30 07:34:18,1611984858.0,dataengineering,What's the difference between a Software engineer and a Data engineer?,l8eviq,Whencowsgetsick,,https://www.reddit.com/r/dataengineering/comments/l8eviq/whats_the_difference_between_a_software_engineer/,1.0,12.0,0.0,24527.0,I'm working as a SDE rn and I would say a large part of my job is writing data pipelines and improving data we contain (we're an applied ML team). We do own a service and I sometimes work on it although it's not very frequent. I don't really see the different between a DE and SDE then. Are those very team dependent?
4963,2021-01-30 09:08:43,1611990523.0,dataengineering,Using Great Expectations?,l8gb1v,inbox0,,https://www.reddit.com/r/dataengineering/comments/l8gb1v/using_great_expectations/,1.0,5.0,0.0,24530.0,"I thought it would be simple.  
I'm fetching a dataset from an API, I just want to pass the dataset items through some validation functions. And do `logger.error('Invalid', item)` when an item fails.  
But I'm stuck in the Getting Started guide, I can't get my head around it. Has anyone used GE in simple projects?"
4964,2021-01-30 13:54:32,1612007672.0,dataengineering,Is a masters in data engineering required ?,l8k65b,GreekYogurtt,,https://www.reddit.com/r/dataengineering/comments/l8k65b/is_a_masters_in_data_engineering_required/,1.0,7.0,0.0,24540.0,"I work in one of the fortune top 10, on Big data Hadoop Stack on scala + spark.

My salary is bit on the higher side (Being from a tier-2 college undergrad). Will doing a masters help my long time prospects ? ( Program fee is 2x my annual salary)"
4965,2021-01-30 14:53:56,1612011236.0,dataengineering,How to become a data engineer,l8l3or,70sechoes,,https://www.reddit.com/r/dataengineering/comments/l8l3or/how_to_become_a_data_engineer/,1.0,0.0,0.0,24543.0,"I think the most asked question about data engineering, is how to become a data engineer.

I created a video [here](https://www.youtube.com/watch?v=0nMzntY3dik) sharing a list of subjects that I think are necessary to enter the field.

Based on your experience would you have done it differently? Maybe include more cloud-oriented subjects? Would like to hear different opinions."
4966,2021-01-30 15:04:43,1612011883.0,dataengineering,NoSql vs sql,l8lah1,zeckk89,,https://www.reddit.com/r/dataengineering/comments/l8lah1/nosql_vs_sql/,1.0,11.0,0.0,24543.0,"Hello data engineers,
I am currently trying to design a solution for my team that involves multiple schemas. I am thinking that because we have multiple writers to our systems with different requirements that a NoSql solution would be great. I want to creat a collection per publisher of data and have the NoSql collection determine the schema for me on read. Is this a good pattern? I have a peer suggesting that we should save all the records in json documents in the NoSql collection and have the user parse the data on their end any suggestions?"
4967,2021-01-30 17:44:31,1612021471.0,dataengineering,"For those using Airflow for your ELT/Orchestration, How are you perfroming your EL?",l8oa1w,OkieDaddy,,https://www.reddit.com/r/dataengineering/comments/l8oa1w/for_those_using_airflow_for_your_eltorchestration/,4.0,22.0,0.0,24551.0,"Title says it all. We have been evaluating tools, and are landing pretty squarely on Airflow, as it's opensource, has a great community, the 2.0 release has solved a lot of our concerns around the scheduler other items, and we like the aspect of code first etl, not the gui hell of more ops based tools(We're all former software engineers/database devs). I used it back a year ago for some data science specific ML automation, but not a productionized/full etl/elt orchestrator for our data warehouse. 

Having said that I've seen a lot of things that you are not supposed to do, like embedding your actual ETL logic within dags, but with that abstracted, how many of you are going the docker/k8s direction of putting your ETL logic in containers and just having airflow schedule the containers, and how many of you are going the route of creating custom hooks/operators for you etl logic? 

Or is it more of creating the python scripts outside of the dag, but within the dag directory and calling that? Or is it really just up to my team how we do it? I'm not afraid of the coding aspect, I just want to make sure I'm doing it right, and get the engineering guidelines in place so we don't set ourselves up for pain down the road once complexity and an increasing number of dags comes into play. Because the slightly unfortunate thing with an opensource tool is that the onus is on us, and we can't point fingers at the etl vendor and say ""Your product sucks. fix it!"" :) (Which usually makes for a better tool, if we're invested in its success, rather than leaning on a vendor)

Last place I was at used a local Airflow install on a linux machine, so it was as easy as having the data science team use venv with their code, then run a bash script to call python from within their environment that contained all their logic/dependencies. However, we are probably going to go with MWAA if we go with airflow(yes, they are still only on 1.12, but 2.0 is in their pipeline and will be released soon, according to support, and they have the backfills to allow for 2.0 compatible dag development). Unfortunately that means that approach is no longer an option, as you don't have access to the MWAA instance. 

We will be using DBT for our transformations, so that is covered.   


I did see the current top post about Meltano, it sounds like it could be the missing link here, though I have just read over it a bit, not looked into it too much.   


Thoughts? Sorry for the wall of text."
4968,2021-01-30 20:01:46,1612029706.0,dataengineering,Survey on 3D Image Generation using GANs Improving Quality,l8rg7b,robinhood-girl-12,,https://www.reddit.com/r/dataengineering/comments/l8rg7b/survey_on_3d_image_generation_using_gans/,1.0,0.0,0.0,24555.0,"**Would you please fill this survey of mine and help my Final Year Project?**

[**https://forms.gle/hW8MXYLLSepVHXEHA**](https://forms.gle/hW8MXYLLSepVHXEHA)

*Data scientists or someone with knowledge in Machine learning or Generative Adversarial networks - GANs are highly appreciated.*"
4969,2021-01-30 22:13:50,1612037630.0,dataengineering,Apache Superset - a Data Visualization and Data Exploration Platform,l8ukgz,binaryfor,,https://www.reddit.com/r/dataengineering/comments/l8ukgz/apache_superset_a_data_visualization_and_data/,1.0,2.0,0.0,24559.0,
4970,2021-01-30 22:46:04,1612039564.0,dataengineering,"Salary for Business Intelligence Engineer, Amazon, Vancouver",l8v9vx,runawayanimated,,https://www.reddit.com/r/dataengineering/comments/l8v9vx/salary_for_business_intelligence_engineer_amazon/,1.0,8.0,0.0,24560.0,"Does anyone know how much Business Intelligence Engineers for Amazon are getting paid in Vancouver, Canada, roughly?

I have an interview coming up and I can’t find the information anywhere."
4971,2021-01-31 00:28:17,1612045697.0,dataengineering,Open Source BI Platform Apache Superset Just Released Version 1.0,l8xgnk,semicausal,,https://www.reddit.com/r/dataengineering/comments/l8xgnk/open_source_bi_platform_apache_superset_just/,1.0,4.0,0.0,24565.0,
4972,2021-01-31 01:34:16,1612049656.0,dataengineering,"The Future of Data Engineering, and the Tale Of Two Data Warehouses.",l8yv8t,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/l8yv8t/the_future_of_data_engineering_and_the_tale_of/,1.0,0.0,0.0,24565.0,
4973,2021-01-31 02:09:48,1612051788.0,dataengineering,Partner Up for Learning,l8zlgy,heisenbug403,,https://www.reddit.com/r/dataengineering/comments/l8zlgy/partner_up_for_learning/,1.0,0.0,0.0,24569.0,"Hello everyone, hope you doing well. I just wanted share the discord server for the people who search for learning partners. You can join server to find a partner for learning different programming languages or any topics you are interested in.
Here is the link for the server:

https://discord.gg/ayeGrsaSG2"
4974,2021-01-31 11:35:20,1612085720.0,dataengineering,Change Data Capture — Convert your database into a stream with Debezium,l99394,mszymczyk,,https://www.reddit.com/r/dataengineering/comments/l99394/change_data_capture_convert_your_database_into_a/,3.0,0.0,0.0,24805.0,
4975,2021-01-31 12:23:01,1612088581.0,dataengineering,Python for data science,l99pnk,[deleted],,https://www.reddit.com/r/dataengineering/comments/l99pnk/python_for_data_science/,0.0,2.0,0.0,24807.0,
4976,2021-01-31 13:35:27,1612092927.0,dataengineering,Building a personal data warehouse in Snowflake for fun and no profit.,l9anov,thomasdziedzic0,,https://www.reddit.com/r/dataengineering/comments/l9anov/building_a_personal_data_warehouse_in_snowflake/,88.0,18.0,0.0,24813.0,
4977,2021-01-31 16:01:17,1612101677.0,dataengineering,What Is Azure Data Factory (ADF)?,l9ctpi,NilotpalS-1,,https://www.reddit.com/r/dataengineering/comments/l9ctpi/what_is_azure_data_factory_adf/,1.0,0.0,0.0,24818.0,
4978,2021-01-31 18:44:53,1612111493.0,dataengineering,What are your thoughts on the future of Snowflake?,l9g2sb,jana50nn,,https://www.reddit.com/r/dataengineering/comments/l9g2sb/what_are_your_thoughts_on_the_future_of_snowflake/,8.0,13.0,0.0,24822.0,Do you think more companies will use Snowflake?
4979,2021-01-31 18:55:58,1612112158.0,dataengineering,Facebook Onsite Interview,l9gbfx,Suitable-Chemistry-9,,https://www.reddit.com/r/dataengineering/comments/l9gbfx/facebook_onsite_interview/,1.0,12.0,0.0,24822.0,"Hello.

I have a an onsite scheduled in three weeks for a DE position in the Product Analytics team.

Anyone else studying for it at the moment? Would you like to partner up for studying?

Any Facebook DE’s or someone who’s done that interview in here? If so, please share some advice. I’m confused about the ETL part. My recruiter said that I would build the ETL on Python for both streaming and static data. But I’ve always used SSIS. Does this basically mean that I should build it on Airflow and call from APIs for the streaming part, and csv for the static data part?

Thanks all"
4980,2021-01-31 20:34:35,1612118075.0,dataengineering,Personal Jobs Database Question?,l9ikyn,Excelisorr,,https://www.reddit.com/r/dataengineering/comments/l9ikyn/personal_jobs_database_question/,2.0,3.0,0.0,24826.0,"Apologies if this is wrong subreddit but hoping you can answer my questions/point me to the right place. 

I am trying to create a web app to view current available jobs/track what jobs I have applied too. 

I have a daily web scraper(cron - debating moving over to airflow) which grabs available jobs data from four websites and consolidate into one csv. I am able to upload into a mysql database using sqlalchmey. 

I am now running into issues on the next steps. Does anyone have a good guide/tutorial on the best practices/examples of updating daily data?

I am having the below issues:

* Should I have one table in mysql that stores all data( incremental grows each day) and if so, then create a view that would show current data?
* Should I be using sql or python to determine what is new/removed data?
* Are there any good tutorial on how to create a view from unique columns so that i can see jobs without duplicates?

Happy to provide more context/answer questions"
4981,2021-01-31 21:55:32,1612122932.0,dataengineering,Data Preprocessing with NumPy This course will guide you through one of Python’s most notable packages – NumPy. We’ll explain why it’s so popular and discuss the numerous applications of its crown jewel – the ndarray class.,l9kf2n,readerswritersj,,https://www.reddit.com/r/dataengineering/comments/l9kf2n/data_preprocessing_with_numpy_this_course_will/,0.0,0.0,0.0,24829.0,
4982,2021-02-01 01:30:22,1612135822.0,dataengineering,Selecting Azure/Synapse over AWS,l9p2ef,HansSlinger,,https://www.reddit.com/r/dataengineering/comments/l9p2ef/selecting_azuresynapse_over_aws/,3.0,6.0,0.0,24835.0,"Hi all, my organisation is looking at Azure Synapse vs AWS for a Datalake -&gt; Modern Data Warehouse platform. One question I had is:

If you've chosen Azure over AWS, why did you? What does Azure do that AWS cannot (or does better).

Many thanks in anticipation of your responses."
4983,2021-02-01 02:04:04,1612137844.0,dataengineering,Can you recommend good data engineering projects?,l9pqle,pacojosedelvino,,https://www.reddit.com/r/dataengineering/comments/l9pqle/can_you_recommend_good_data_engineering_projects/,5.0,13.0,0.0,24836.0,"Hi! I am fairly new in the data engineer world. Can you recommend any books, projects, github or videos of ETL's projects? I got a technical interview in which I had to do a complete ETL and I failed miserably .

&amp;#x200B;

Thanks!"
4984,2021-02-01 07:22:50,1612156970.0,dataengineering,Facebook DE onsite interview,l9vp4m,Affectionate-Race588,,https://www.reddit.com/r/dataengineering/comments/l9vp4m/facebook_de_onsite_interview/,55.0,56.0,0.0,24846.0,"Hello everyone, 

I have an onsite interview with Facebook in 3 days for a DE position in the Product Analytics team.

My recruiter told me that there would be some questions related to the batch and streaming ETL. I would be expected to write some code for the same. I am not sure about the streaming part yet. Are they expecting you to remember the syntax of streaming Python(Flink and Kafka) modules and the different functions? 

Anybody from Facebook or anyone who has recently interviewed at FB for DE role, can you please share your experience of the onsite round? 

Thanks in advance."
4985,2021-02-01 10:23:44,1612167824.0,dataengineering,Help on project,l9yfik,Mumo2020,,https://www.reddit.com/r/dataengineering/comments/l9yfik/help_on_project/,1.0,8.0,0.0,24850.0,"Hi

Please is there anyone who can advice me on the best approach for a work I am doing

I have a device position data below

{'id': 15126, 'name': 'device-BE-17', 'lat': 35.7721, 'lon': -78.63861}  

{'id': 12526, 'name': 'device-US-11', 'lat': 36.7721, 'lon': -71.1561}  

...

I have a weather hourly weather data consisting of station\_id column .

I also have a weather station data which has the column StationId.

The station data has the format below

station\_id,lat,lon,source,reports,country,measurement\_reliability

0011W82.82,15.16,madis,subhourly,SJ,0.5

00000,17.03,-42.97,madis,subhourly,GF,0.56

0001W,30.436,-84.122,madis,subhourly,US,0.93

0002W,30.538,-84.224,madis,subhourly,US,0.85

&amp;#x200B;

The station data is collecting hourly and sub hourly data or the station but the weather data is a hourly report. 

We would like to gather all the relevant information in a single table and provide it to the device expert to enable them to find some insight on what the problem with the devices could be.

Summarize the results to have daily granularity, use average to aggregate the temperature measurement and reliability. This table would have the following schema:

device\_id, device\_name, lat, lon, date, avg\_temp, measurement\_reliability\_score

Take into consideration that each weather measurement might come from multiple stations (source field). When a measurement comes from multiple stations, use the Harmonic mean ([https://en.wikipedia.org/wiki/Harmonic\_mean](https://en.wikipedia.org/wiki/Harmonic_mean)) to have a singular measurement\_reliability value per hour before averaging. It has the following formula:

&amp;#x200B;

https://preview.redd.it/zi5fvtezste61.png?width=366&amp;format=png&amp;auto=webp&amp;s=b48711a59525edbd33d720243ff337739b4e0d63

📷"
4986,2021-02-01 14:00:45,1612180845.0,dataengineering,A distributed and extensible workflow scheduler platform with powerful DAG visual interfaces - Apache DolphinScheduler,la1jol,dolphinscheduler,,https://www.reddit.com/r/dataengineering/comments/la1jol/a_distributed_and_extensible_workflow_scheduler/,1.0,0.0,0.0,24864.0,
4987,2021-02-01 15:22:31,1612185751.0,dataengineering,AWS Data Wrangler Overview (Python library),la2xgt,DataEngUncomplicated,,https://www.reddit.com/r/dataengineering/comments/la2xgt/aws_data_wrangler_overview_python_library/,6.0,0.0,0.0,24867.0,
4988,2021-02-01 15:50:13,1612187413.0,dataengineering,Spark Query Plans for Dummies,la3ghy,anjsudh,,https://www.reddit.com/r/dataengineering/comments/la3ghy/spark_query_plans_for_dummies/,7.0,0.0,0.0,24868.0,
4989,2021-02-01 17:09:43,1612192183.0,dataengineering,"After replicating multiple data sources into Snowflake (Sql, NoSql), is there a way to detect “relationships” across the hundreds of Tables, nested Jsons, other data?",la571q,brrdprrsn,,https://www.reddit.com/r/dataengineering/comments/la571q/after_replicating_multiple_data_sources_into/,2.0,14.0,0.0,24870.0,"Hello,

We have moved data from multiple transactional, operational, and event sources have successfully been ingested into a data warehouse (i.e. Snowflake). This data comes from different parts of the organization - with each having its own data model and schema. 

Our use case now involves: 

* denormalizing the data, and 
* building queryable models that span across multiple structured and semi-structured sources i.e. we need to understand how the data ""is joined"" within and across sources, especially where there is no consistent naming convention naming convention across sources

Are you aware of any way (via paid tools or free, open source tools) that we can automatically detect relationships across the ocean of data? 

If not: 

\- How do you presently go about making transforming raw replicas of tens of source systems into a composite form that is more usable - i.e. more easily understandable and exploration friendly (e.g. for data scientists) and modeled (e.g. for data analysts)? 

\- How long should we typically set aside for this exercise?

\- What skill sets do we need on our team?

Thank you in advance for any help you can offer..."
4990,2021-02-01 17:39:22,1612193962.0,dataengineering,Tips for developing production pipelines in airflow,la5x9y,JonnyBarda,,https://www.reddit.com/r/dataengineering/comments/la5x9y/tips_for_developing_production_pipelines_in/,8.0,2.0,0.0,24872.0,
4991,2021-02-01 18:22:00,1612196520.0,dataengineering,How can I learn Talend quick for my new job?,la709g,spongebob4272,,https://www.reddit.com/r/dataengineering/comments/la709g/how_can_i_learn_talend_quick_for_my_new_job/,3.0,10.0,0.0,24874.0,"Recently I got an opportunity to make a career switch into an ETL role. I need to work on Talend and I have no prior experience with it.

I'm hoping someone here can tell me the fastest amd best way to learn enough that Im able to function in my role as soon as possible."
4992,2021-02-01 20:14:40,1612203280.0,dataengineering,Are data engineers allowed to use pgAdmin and basic DB questions!,la9uz9,YaswanthBangaru,,https://www.reddit.com/r/dataengineering/comments/la9uz9/are_data_engineers_allowed_to_use_pgadmin_and/,0.0,9.0,0.0,24877.0,"The question says it all, as I find it intimidating to do everything from the CLI, is it okay to use pdAdmin for the database related tasks? Also, it's open source.

Also, in general, do people use it? I found about it for the first time after setting up a data base using RDS of AWS and struggling to do some basic stuff. One of my friends who is into backend told me that I need to use a combination of sqlalchemy and alembic. Now this is getting confusing af.

I can't even figure out what's for what among these options. I managed to create a DB server so far. Would appreciate if someone could through some light on what would I need to create a table inside the DB I created and populate it with csv data that I already have in a compressed format in AWS S3 bucket."
4993,2021-02-01 21:05:48,1612206348.0,dataengineering,Data Engineering Subfields,lab5gi,broomstick_jockey,,https://www.reddit.com/r/dataengineering/comments/lab5gi/data_engineering_subfields/,3.0,4.0,0.0,24879.0,"Hi All,

I'm hoping to get a feel for what the community thinks are the core competencies of a well-rounded (aka widely-employable) cloud data engineer with a few years of experience.

I was recently approached for a DE position with a few job responsibilities I hadn't seen in prior job searches. Normally I'd chalk this up to the organization not knowing what they're looking for (I've passed on a few positions looking to consolidate a DE, DS, ML engineer into one position), but after reading [this article](https://towardsdatascience.com/introduction-to-data-engineering-e16c9942dc2c), I'm now re-evaluating what I believed I should be proficient in. 

Previously, I was under the impression (and where I've focused my career) that a DE's responsibilities fall within the article's **Data Warehouse &amp; Pipelines** subfield description: 

&gt;Data warehouse engineers build batched and/or real-time data pipelines to integrate data between systems and also support the data warehouse. Since the data warehouse is meant for tackling business problems, data warehouse engineers usually work closely with data analysts, scientists, or business teams that serve a specific business function.        

However, this job posting had responsibilities in building out customer-facing APIs and assisting with the build-out of the OLTP backend for customer-facing applications (in addition to responsibilities listed out in the previous description). This seems to align with the article's description of the **Data Applications** subfield:

&gt;Data application engineers are software engineers building internal data tools and APIs. Sometimes, a great internal tool may later become an open-source product of the company. For example, one of the data product teams at Lyft built a data discovery tool called Amundsen which was open-sourced in 2019.     

&amp;#x200B;

Would you consider a well-rounded DE someone who has experience in both subfields, or are these specializations that (ideally) would be two distinct positions?"
4994,2021-02-01 22:46:20,1612212380.0,dataengineering,Data Observability: The Next Monitoring Frontier,ladn49,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/ladn49/data_observability_the_next_monitoring_frontier/,14.0,2.0,0.0,24884.0,
4995,2021-02-01 22:46:53,1612212413.0,dataengineering,Data Warehousing Advice,ladnr2,dontsaybye,,https://www.reddit.com/r/dataengineering/comments/ladnr2/data_warehousing_advice/,1.0,16.0,0.0,24884.0,"Disclaimer: I am not a data engineer. I'm not trying to be a data engineer. I'm also not the CEO of my company and I don't have any hiring authority.  


Redditors, I work at a small, non-technology company and am looking for a solution to our data drama. This is the first time I'm working at a company that doesn't already have databases set up - the entire business is run using flat files. The weekly processing is getting annoying and I have asked our one developer to set up a database for the past 6 months. Seeing that it's probably not going to happen since sales and marketing gets priority on his time for website tasks, I've decided to find another way. \*I'm the only person in the company with any real SQL or Python skills.  


I previously set up a database on AWS about a year ago, but I do not have the requisite knowledge to limit access to only our employees (it seems to be based on IP address but we can work from anywhere). I started looking into Snowflake because it's cloud hosted and it seems that might take care of creating the database part - without requiring a list of IP addresses. Our files are all hosted in Google Drive so it looks like I also need to pay for a connector service to automatically migrate the files (or create Python code to get the files out of GDrive and into the Snowflake database). Then, I need to look at some services like Airflow, DBT, or Dataform to automatically do the required transformations on the datasets.  


Basically, my question is: is this a good plan or not? If not, are there any easier pipeline services for automating ETL for less technical teams?"
4996,2021-02-01 22:58:01,1612213081.0,dataengineering,"The 27th edition of @data_weekly focuses on decentralized content moderation, Kafka as a database,@SnowflakeDB External Table, @dagsterio 0.10.0, @UberEng real-time data intelligence platform, @Dropbox Superset usage, @Cloudflare ops using Airflow",ladxas,vananth22,,https://www.reddit.com/r/dataengineering/comments/ladxas/the_27th_edition_of_data_weekly_focuses_on/,1.0,0.0,0.0,24884.0,
4997,2021-02-01 23:14:25,1612214065.0,dataengineering,"Analytics on GCP, website on AWS - how painful to manage across the two?",laecmd,NappyChord,,https://www.reddit.com/r/dataengineering/comments/laecmd/analytics_on_gcp_website_on_aws_how_painful_to/,2.0,2.0,0.0,24885.0,"Our organization is about to make the move from an on-prem/self managed tech stack (for our website/ecomm and other tech infrastructure) to one of the major cloud providers. On the analytics/data-warehouse side, we've begun our migration to GCP (BigQuery, Composer, AI Platform) already. While I believe Google is probably the slight winner across the major platforms for DW/analytics, it might make sense to shift alongside the rest of our tech infrastructure to take advantages of being on a single cloud provider (AWS or Azure). For DW, we'd likely go for Snowflake on top of whichever cloud provider is chosen.

In your experience: Should we (the DW/analytics organization) go through the cost of moving to and learning a new suite of services to take advantage of our entire tech stack being on a single cloud provider, or is it not a big deal for an organization to span multiple services across multiple cloud providers/platforms? Does your organization span across more than Azure/AWS/GCP, and how much extra effort has that required to maintain?"
4998,2021-02-02 00:39:10,1612219150.0,dataengineering,"Dynamic SQL, Schema Changes and Development Around Both",lagdto,timfcrn,,https://www.reddit.com/r/dataengineering/comments/lagdto/dynamic_sql_schema_changes_and_development_around/,1.0,0.0,0.0,24885.0,
4999,2021-02-02 01:42:25,1612222945.0,dataengineering,Metaplane | DataDog for Data,lahsxp,[deleted],,https://www.reddit.com/r/dataengineering/comments/lahsxp/metaplane_datadog_for_data/,1.0,0.0,0.0,24889.0,
5000,2021-02-02 01:43:29,1612223009.0,dataengineering,Metaplane | Datadog for Data,lahts8,curvature_propulsion,,https://www.reddit.com/r/dataengineering/comments/lahts8/metaplane_datadog_for_data/,1.0,4.0,0.0,24889.0,
5001,2021-02-02 02:06:17,1612224377.0,dataengineering,Databricks raises $1B at $28B valuation as it reaches $425M ARR,laibru,Bazencourt,,https://www.reddit.com/r/dataengineering/comments/laibru/databricks_raises_1b_at_28b_valuation_as_it/,86.0,44.0,0.0,24891.0,
5002,2021-02-02 02:20:37,1612225237.0,dataengineering,ETL tool recommendation for health tech startup?,laimb0,rods2292,,https://www.reddit.com/r/dataengineering/comments/laimb0/etl_tool_recommendation_for_health_tech_startup/,2.0,12.0,0.0,24892.0,"I joined a health tech insurance startup and currently we are in the process of preparing all the data for pricing, actuarial reserving and accountants. I am working the intersection of the data department and the finance department since I am the only data member who knows something of accounting and actuarial reserving. 

I am skilled in SQL so I could do all the data preparation using it but I am looking for an open-source ETL tool to help me on that. I was thinking about KNIME but I am not sure about it. 

Does anyone have a recommendation? A tool that could handle a big amount of data is preferable since medical records are massive"
5003,2021-02-02 04:00:29,1612231229.0,dataengineering,Data Engineering beginner project ideas,lako1z,nomatterhow202,,https://www.reddit.com/r/dataengineering/comments/lako1z/data_engineering_beginner_project_ideas/,4.0,4.0,0.0,24898.0,"Hi everyone. I'm a final year CS student interested in a career in data engineering. A bit about my background... I've taken several statistics and ML courses. I'm now currently taking a course in Database Systems and this semester has been quite light in terms of workload. 

So I'm looking for suggestions for data engineering related project ideas... Something that I could use to learn all the necessary skills which will tick the checkboxes and then put on my resume to help with my job search.

I have some experience with web dev, so I can potentially develop a website to showcase the projects. But I'm also having a hard time trying to find a data source that could replicate a real-world application of a data engineering project.

Please advice on some good data engineering project ideas. Thank you in advance!"
5004,2021-02-02 05:09:21,1612235361.0,dataengineering,Where to start,lalyjn,veeeerain,,https://www.reddit.com/r/dataengineering/comments/lalyjn/where_to_start/,4.0,18.0,0.0,24901.0,"Hello, I’m a statistics undergraduate at my university whose had prior experience in data science/ML, specifically working in R markdowns/google colab notebooks and writing data analysis scripts with building some ML/DL models. I’ve only worked with R and python specifically for data cleaning/data viz and machine learning, and only very minimal web scraping. 

Anyways, the main point of this is that data engineering seemed like an interesting side of data science I wanted to learn more about. To get started, would practicing working with api calls and fetching data be a good start? With api tokens and stuff? I haven’t done a lot of data extraction aside from scraping, mainly only kaggle datasets. I want some kind of roadmap to follow where I can learn stuff sequentially to work my way up to hopefully building a data pipeline project of my own. Does anyone have any sort of  recommendations on where to start? Where to go? I’m also working on sharpening up general python programming skills/data structures and algorithms since, as a stats student we don’t get exposed to a lot of intense software stuff.

Any help would be great"
5005,2021-02-02 05:27:48,1612236468.0,dataengineering,Clarification on Assessment Question,lamam1,hmmwhatdoyouthinkabt,,https://www.reddit.com/r/dataengineering/comments/lamam1/clarification_on_assessment_question/,1.0,0.0,0.0,24901.0,"Hello,

I'm in the midst of completing a pre-interview assignment, and I'd appreciate clarification on one of the questions -- 

&gt; Now assume that the encounter_id column is a sequentially loaded field from a single source system. How would you change the queries above to have the target tables reflect only the latest information for each entity?""

The encounter_id column refers to a hospital-encounter dataset. I just want to clarify that ""encounter_id column is a sequentially loaded field from a single source system"" means that there's a counter for every row? Why would this have an impact on upserts, would it make things easier because this way you know what data to trust (the newer value)?"
5006,2021-02-02 10:36:33,1612254993.0,dataengineering,Everyday Data Engineering: Best Practices on Data Replication to Snowflake with Python,lar650,JonnyBarda,,https://www.reddit.com/r/dataengineering/comments/lar650/everyday_data_engineering_best_practices_on_data/,3.0,0.0,0.0,24916.0,
5007,2021-02-02 11:08:43,1612256923.0,dataengineering,Book about Managing Data Engineering Projects,larm8i,lig8eia,,https://www.reddit.com/r/dataengineering/comments/larm8i/book_about_managing_data_engineering_projects/,11.0,22.0,0.0,24916.0,"Hello,

I'm looking for book suggestions related to managing data engineering projects or teams.

I have already read ""Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems"", and I'm looking to improve my project management skills in that area.

Thanks for your help!"
5008,2021-02-02 15:39:04,1612273144.0,dataengineering,Should I use my company's Kubernetes cluster for my data processing?,lavmz3,Pop-Huge,,https://www.reddit.com/r/dataengineering/comments/lavmz3/should_i_use_my_companys_kubernetes_cluster_for/,2.0,18.0,0.0,24922.0,"The company I work for has a huge Kubernetes cluster where they run most of our current operations (we are a SaaS company). I'm starting to build our data infrastructure from scratch and was wondering if it's considered best practice to run my processes and jobs there. Nothing too fancy, a regular workflow management, probably Airflow, and maybe a couple of spark batch jobs (no real-time).

&amp;#x200B;

I wonder if there's a risk of running the cluster out of resources. I know that I should take the size of the cluster and the number of nodes into consideration, but I'd like to understand if this is the right approach to start.  


Thank you!"
5009,2021-02-02 16:18:32,1612275512.0,dataengineering,Engineering Radiology AI for National Scale in the US,lawg9p,saucysassy,,https://www.reddit.com/r/dataengineering/comments/lawg9p/engineering_radiology_ai_for_national_scale_in/,3.0,0.0,0.0,24922.0,
5010,2021-02-02 16:25:15,1612275915.0,dataengineering,Getting started with an emerging data term: Analytics Engineering,lawltx,anhthong00,,https://www.reddit.com/r/dataengineering/comments/lawltx/getting_started_with_an_emerging_data_term/,0.0,0.0,0.0,24922.0,
5011,2021-02-02 17:19:05,1612279145.0,dataengineering,Implementing Argo in K8s Data Pipeline,laxxui,jtshaw_,,https://www.reddit.com/r/dataengineering/comments/laxxui/implementing_argo_in_k8s_data_pipeline/,2.0,6.0,0.0,24923.0,"Hi All!

Looking for some advice/thoughts on the best approach to adopting Argo as an orchestration tool for our data pipeline.  Currently our pipeline is written as containerized Python jobs that are run in Kubernetes as cron jobs.  We are looking to implement Argo to manage the orchestration/scheduling side of things, as well as to leverage the API to call Argo Workflows from other Microservices.

What has been your experience moving to Argo?  Any tips or advice especially about calling workflows through the API based on events?

&amp;#x200B;

Thanks!"
5012,2021-02-02 17:23:17,1612279397.0,dataengineering,"Blog post on how we used Apache Kafka, ksqlDB and Quarkus to build an application that tracks the scores of our table football games",lay1pn,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/lay1pn/blog_post_on_how_we_used_apache_kafka_ksqldb_and/,15.0,0.0,0.0,24923.0,"[https://www.confluent.io/blog/using-kafka-ksqldb-quarkus-for-real-time-sports-tracking/](https://www.confluent.io/blog/using-kafka-ksqldb-quarkus-for-real-time-sports-tracking/) 

Read our blog post if you like:

* elegant, fancy-looking solutions🧛🏻‍♂️ 
* Kafka, ksqlDB and Quarkus 🖖🏼
* table soccer ⚽ 
* Swiss quality🇨🇭"
5013,2021-02-02 18:36:08,1612283768.0,dataengineering,Running Airflow 2.0 with the CeleryExecutor and Docker in 5 mins,lazw6z,marclamberti,,https://www.reddit.com/r/dataengineering/comments/lazw6z/running_airflow_20_with_the_celeryexecutor_and/,3.0,0.0,0.0,24923.0,
5014,2021-02-02 20:25:20,1612290320.0,dataengineering,How would you process large data?,lb2lrh,sillysally09,,https://www.reddit.com/r/dataengineering/comments/lb2lrh/how_would_you_process_large_data/,3.0,5.0,0.0,24928.0,"Hi all,

I have a data engineering interview coming up and am wondering how you might go about responding to a question of this nature. It seems so open ended that I'm not sure how to approach it, what considerations I should make, and what suggestions I would make from an architectural perspective in response to the considerations. I'm not asking for a fully fledged answer but a template or any thoughts you guys would use to respond to this would be very helpful. Some things I can think of off the top that I would want to clarify (though not sure how I would respond accordingly):

\- Are we ingesting the data through a stream? Or is it sitting in a relational data warehouse?

\- If the data is streaming in how quickly is it flowing in? 100's of events per second?

\- How will this data be consumed? Is it for a monthly report? Daily report? Live report? Is it intended to be queried?

\- Will we be performing analytics on this data or is it intended for look up purposes? (touching on SQL vs NoSQL here)"
5015,2021-02-02 21:44:12,1612295052.0,dataengineering,Is Databricks still relevant,lb4hs3,yoelbenyossef,,https://www.reddit.com/r/dataengineering/comments/lb4hs3/is_databricks_still_relevant/,1.0,26.0,0.0,24937.0,"My boss wants me to start developping with Databricks but is asking is it still a good tool?  
Or is it like Hadoop, a tool that still has it's uses, but no longer leading the pack.

And if it isn't the best tool, what is?"
5016,2021-02-02 23:10:38,1612300238.0,dataengineering,Why Data Engineers Should Care about DataBricks IPO,lb6iow,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/lb6iow/why_data_engineers_should_care_about_databricks/,2.0,7.0,0.0,24939.0,
5017,2021-02-02 23:41:52,1612302112.0,dataengineering,Anyone know how to query Kudu/Impala metadata to find long running queries or high I/O (i.e. problem users),lb7ce1,PaulSandwich,,https://www.reddit.com/r/dataengineering/comments/lb7ce1/anyone_know_how_to_query_kuduimpala_metadata_to/,1.0,0.0,0.0,24941.0,"Might be off topic, but my google-fu is failing to find me anything on how to pull metadata on queries in flight.  
  
Some explanation: I don't have access to cloudera manager and, more importantly, I'd like to build some automated alerts/interventions around this monitoring query, so I'd like a SQL-based solution if at all possible. I know how to do this in other DB flavors, so I'm curious why it seems so difficult in Kudu."
5018,2021-02-03 01:52:41,1612309961.0,dataengineering,Ask Jesse Anderson anything - the author of Data Teams,lba9m7,stolzen,,https://www.reddit.com/r/dataengineering/comments/lba9m7/ask_jesse_anderson_anything_the_author_of_data/,6.0,2.0,0.0,24945.0,"We've already asked:

* Centralized data team vs distributed data team
* Encouraging business people to do analysis themselves
* Balancing in helping others vs doing your own work

And lots more!

More information here: [https://datatalks.club/books/20210201-data-teams.html](https://datatalks.club/books/20210201-data-teams.html)"
5019,2021-02-03 04:43:18,1612320198.0,dataengineering,How to develop data pipeline in Airflow through TDD (test-driven development),lbdl7l,[deleted],,https://www.reddit.com/r/dataengineering/comments/lbdl7l/how_to_develop_data_pipeline_in_airflow_through/,1.0,0.0,0.0,24949.0,
5020,2021-02-03 04:46:47,1612320407.0,dataengineering,Article/Tutorial: How to develop data pipeline in Airflow through TDD (test-driven development),lbdnh4,marcosmarxm,,https://www.reddit.com/r/dataengineering/comments/lbdnh4/articletutorial_how_to_develop_data_pipeline_in/,74.0,9.0,0.0,24949.0,"Hey folks I wrote an article/tutorial about how to develop DAGs using TDD (test-driven development) and how to setup a CI with Github Actions, you can read [here](https://blog.magrathealabs.com/how-to-develop-data-pipeline-in-airflow-through-tdd-test-driven-development-c3333439f358).

All the code is here: [https://github.com/marcosmarxm/airflow-testing-ci-workflow](https://github.com/marcosmarxm/airflow-testing-ci-workflow)

and for those just starting with Airflow/Data Engineering. I created a detailed step-by-step of the project: [https://github.com/marcosmarxm/airflow-testing-ci-workflow/blob/master/assets/how-to/create-dag-using-tdd.md](https://github.com/marcosmarxm/airflow-testing-ci-workflow/blob/master/assets/how-to/create-dag-using-tdd.md) 

Hope you enjoy! any suggestions are welcome"
5021,2021-02-03 05:13:36,1612322016.0,dataengineering,common or separate landing zone for data lake and warehouse,lbe5py,aj_112_lin,,https://www.reddit.com/r/dataengineering/comments/lbe5py/common_or_separate_landing_zone_for_data_lake_and/,2.0,2.0,0.0,24950.0,"How are you architecting staging area if you have a data lake and warehouse.
A common landing in data lake or
2 separate zones for your warehouse and data lake"
5022,2021-02-03 05:50:37,1612324237.0,dataengineering,Anyone know how to query Kudu/Impala metadata to find long running queries or high I/O (i.e. problem users),lbeu4y,PaulSandwich,,https://www.reddit.com/r/dataengineering/comments/lbeu4y/anyone_know_how_to_query_kuduimpala_metadata_to/,1.0,0.0,0.0,24951.0,"Might be off topic, but my google-fu is failing to find me anything on how to pull metadata on queries in flight.  
  
Some explanation: I don't have access to cloudera manager and, more importantly, I'd like to build some automated alerts/interventions around this monitoring query, so I'd like a SQL-based solution if at all possible. I know how to do this in other DB flavors, so I'm curious why it seems so difficult in Kudu."
5023,2021-02-03 08:38:16,1612334296.0,dataengineering,Sensitive data in data lake,lbhlza,binarytree_,,https://www.reddit.com/r/dataengineering/comments/lbhlza/sensitive_data_in_data_lake/,3.0,1.0,0.0,24958.0,"How do I flow sensitive data through a pipeline to my data lake. The data can grow so I can't use any DBMS, it needs to be scalable. Secondary, the data is sensitive as in even the developer should not be able to read it right from raw data landing in a S3 bucket to ultimately the querying layer (Athena/presto or trinio as we call it now).
Is landing on S3 even a good idea to begin with. I ve read about encryption, redaction, masking concepts. Just gathering few thoughts from u good folks here.much thanks."
5024,2021-02-03 09:19:34,1612336774.0,dataengineering,"What tools, software, programming languages, and etc. does a data engineer need to have in 2021",lbi6ul,Aggressive-Pup-28,,https://www.reddit.com/r/dataengineering/comments/lbi6ul/what_tools_software_programming_languages_and_etc/,3.0,19.0,0.0,24959.0,"I'm thinking of applying for a data engineering job. I know tech like spark, airflow, python, scala, hadoop are the need to have assets for a data engineer through online searches and datacamp.  


However, I'm quite overwhelmed in the technologies that I need to learn. I know a bunch about python pandas, and all the things for data manipulation but that's about it.  


I'm planning on learning about airflow, scala, databricks, pyspark, and hadoop but I don't know what to prioritize. What do you guys think I should prioritize on? Also, are power bi and tableau necessary to learn as well?"
5025,2021-02-03 09:48:34,1612338514.0,dataengineering,Is the Azure Data Engineer Associate Certification Worth It?,lbil7d,_ByteMeh,,https://www.reddit.com/r/dataengineering/comments/lbil7d/is_the_azure_data_engineer_associate/,7.0,4.0,0.0,24959.0,"Hi all,  


I'm a newly minted DE and I learned that my company is moving over to the cloud this year. I am curious what your thoughts are on this certification and/or any cloud certifications (AWS, GCP) in general.  


Do you think these cloud certifications are helpful from a learning and career boosting standpoint? How important is cloud infrastructure in Data Engineering?  


Thank you for your time!"
5026,2021-02-03 11:52:53,1612345973.0,dataengineering,What does the person who hires the most data engineers in Berlin advise?,lbk8w3,soobrosa,,https://www.reddit.com/r/dataengineering/comments/lbk8w3/what_does_the_person_who_hires_the_most_data/,3.0,2.0,0.0,24962.0,"Martin Loetzsch from Project A Ventures, our dear advisor at the Academy, talks about robotics, hiring, dodging #Hadoop and #Spark, focus music and comfort food. He also thinks that if you're starting out in #dataengineering you should join #pipelineacademy!

\#codingbootcamp #careeradvice #idataengineer

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-0010](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/idataengineer-confessions-interview-0010)"
5027,2021-02-03 14:09:28,1612354168.0,dataengineering,What data related issues have you faced when working with very long time series data sets?,lbm74a,stigmatic666,,https://www.reddit.com/r/dataengineering/comments/lbm74a/what_data_related_issues_have_you_faced_when/,7.0,7.0,0.0,24967.0,"About to start on a project where we have timeline data starting from year 2000, which will be used for forecasting. Looking to get ahead by learning some of the challenges they might be facing. Any help is appreciated."
5028,2021-02-03 22:53:37,1612385617.0,dataengineering,How important is learning distributed systems for aspiring data engineers?,lbxyke,nomatterhow202,,https://www.reddit.com/r/dataengineering/comments/lbxyke/how_important_is_learning_distributed_systems_for/,2.0,9.0,0.0,24984.0,"Hello. I'm a final year college student who is interested in going down the path to become a data engineer. Currently, I have to decide whether I should take a class on Distributed Systems or not. On the one hand, I've heard that the class is very heavy, and given my already busy semester (cause I'm in a rush to graduate), I would rather not take it or else my overall GPA might suffer. On the other hand, I've heard that knowledge of distributed systems is helpful to becoming a good data engineer.

I've taken a class on database systems and know my basics around SQL/No-SQL databases. My question is mainly on how important/relevant is the knowledge of distributed systems to data engineering in general. Relatively speaking, how important is it to additionally know distributed systems compared to just knowing SQL/No-SQL databases? Does listing a distributed systems course in your resume help employability in the data engineering field in any significant way?

Thanks for your advice."
5029,2021-02-03 23:06:43,1612386403.0,dataengineering,Datacamp Data Engineering track - worth the time?,lbyafw,ivantf15,,https://www.reddit.com/r/dataengineering/comments/lbyafw/datacamp_data_engineering_track_worth_the_time/,30.0,27.0,0.0,24983.0,"Datacamp has a 95-hour career track for data engineering with Python and I'm wondering if anyone has done it or parts of it and can comment on if it's worth the time. I've been a data analyst with some data engineering for about 2 years now and spent multiple internships in undergrad working as a software engineering intern, so I have a decent programming background. More or less fluent with SQL as well and a bit of familiarity with Hadoop, Hive. I'm wondering if these courses are worth trying to build a better DE background or if I should be starting somewhere else."
5030,2021-02-04 00:26:01,1612391161.0,dataengineering,⚡NEW!⚡ The Confluent Community Forum ✨,lc057d,rmoff,,https://www.reddit.com/r/dataengineering/comments/lc057d/new_the_confluent_community_forum/,0.0,2.0,0.0,24984.0,
5031,2021-02-04 03:35:13,1612402513.0,dataengineering,Google cloud registry to Google cloud compute engine,lc45cf,[deleted],,https://www.reddit.com/r/dataengineering/comments/lc45cf/google_cloud_registry_to_google_cloud_compute/,1.0,0.0,0.0,24991.0,
5032,2021-02-04 05:20:14,1612408814.0,dataengineering,Help Request On How To Build Materialized View in SQL Server,lc68ik,Cli4ordtheBRD,,https://www.reddit.com/r/dataengineering/comments/lc68ik/help_request_on_how_to_build_materialized_view_in/,1.0,2.0,0.0,24996.0,"I'm not sure this is the best place for this, but I'm lost and could use some help. I saw some relevant info on stackoverflow, but not this exact scenario and I'm somewhat new to Data Engineering (and evidently suck at it).

I'm trying to summarize tansaction data (counts and amounts) from Azure SQL Server using a Materialized View (so it doesn't have to be recalculated each time a user accesses the view, which will be 90% via Alteryx).

Requirements:

For each day and site, I need to aggregate the data to get the Count of Transactions, Total Amount, Category A Amount, Category A Quantity, and Category B Amount.

Products are either Category A or Category B. Transactions can have one or more product, so therefore I want to categorize the transactions into the following:

- Transactions where only Category A products were purchased
- Transactions where only Category B products were purchased
- Transactions that contained a mix of Category A and Category B products

This is mostly to ensure that double-counting doesn't happen as it gets aggregated.

Solution So Far:

I'm creating 5 temp tables, each just a list of the transaction IDs, and using the last 3 as each of those 3 categories above:

- Transactions involving Category A
- Transactions involving Category B
- Transactions involving both (A&amp;B)
- Transactions only involving Category A (A!B)
- Transactions only involving Category B (B!A)

I then take the latter three and calculate the aggregations correctly (producing 3 more temp tables).

I then union those 3 together (into another temp table).

I then join that unioned table to 2 dimension tables to get the final result I would expect the users to query.

Problem:

So when I tried to use the last select in the Materialized View, I got an error, which I know understand is because the view can't access any of those temp tables I've learned on so heavily.

One thing I saw online was to use CTE's instead of temp tables and just add those to the view. Is that a good idea?

Any help anybody could provide about what approach to take or where else I could research this would be much appreciated."
5033,2021-02-04 08:22:21,1612419741.0,dataengineering,Project for Advanced Database Management course,lc9f5n,[deleted],,https://www.reddit.com/r/dataengineering/comments/lc9f5n/project_for_advanced_database_management_course/,1.0,3.0,0.0,25005.0,
5034,2021-02-04 10:21:34,1612426894.0,dataengineering,[VIDEO] - Streaming Concepts &amp; Introduction to Apache Flink - Event Time and Watermarks,lcb5gu,Marksfik,,https://www.reddit.com/r/dataengineering/comments/lcb5gu/video_streaming_concepts_introduction_to_apache/,3.0,0.0,0.0,25011.0,
5035,2021-02-04 13:11:39,1612437099.0,dataengineering,Using jupyter notebook in day to day DE job?,lcdhjs,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/lcdhjs/using_jupyter_notebook_in_day_to_day_de_job/,8.0,14.0,0.0,25019.0,"I did Udacity Data Engineering nanodegree and they have exclusively used jupyter notebook in that course for all projects. Do we as a DE would be using jupyter notebook in real job? Say, I use Erwin data modeler for schema design, should I abandon it and learn to do the same in jupyter notebook?"
5036,2021-02-04 17:31:12,1612452672.0,dataengineering,Data Platform (Snowflake and Databricks ecosystem),lci4l5,timewarp80,,https://www.reddit.com/r/dataengineering/comments/lci4l5/data_platform_snowflake_and_databricks_ecosystem/,15.0,9.0,0.0,25022.0,"Hi all, just wanted to see if anyone out there has experience with a data stack that includes both Databricks and Snowflake and what the general architecture ended up looking like.  My company has recently procured Snowflake(target enterprise datawarehouse)  and Databricks (driven by the Data Science team).  I've been working on our logical architecture which is pretty basic right now, but includes a data lake (to be migrated from hdfs), a data warehouse (the decision was to go with data vault), then information/data marts as well as self-service functionality for DataSci.

Obviously, DB is going to push for use of DeltaLake for all storage and the ""DeltaLakeHouse"" paradigm, and Snowflake is going to push for using them as the data lake all the way up to the data marts (which is what I would prefer) and just use DB for the compute.  Snowflake's security model is simpler and easier to setup and maintain in my opinion, which would accelerate the deployment of the platform and migration efforts.

I wanted to see if anyone here has implemented either of those approaches and how it turned out.

&amp;#x200B;

Thanks!"
5037,2021-02-04 19:58:14,1612461494.0,dataengineering,Lecture Notes on Data Engineering Basics - Stanford CS 329S,lclm3s,neuromantik8086,,https://www.reddit.com/r/dataengineering/comments/lclm3s/lecture_notes_on_data_engineering_basics_stanford/,180.0,20.0,1.0,25026.0,
5038,2021-02-04 21:59:43,1612468783.0,dataengineering,Advice on choosing between a message queue (google pub/sub) vs directly inserting into google cloud storage?,lcoj5o,third_dude,,https://www.reddit.com/r/dataengineering/comments/lcoj5o/advice_on_choosing_between_a_message_queue_google/,2.0,8.0,0.0,25029.0,"I am making a data pipeline where several apis are integrated into a single database schema. This is for retail purposes. Every hour or so the api's are queried and say 1000-2000 records come back. Maybe a quarter are duplicate records because the api doesn't allow you to exclude already fetched items. 

The general plan is to have airflow run singer taps from these apis. I need to make a target for them and I am debating between using pub/sub or just directly putting them into google cloud storage objects. Then a different task would pull the data from the bucket or be a subscriber and process a chunk at a time (\~1000). It transforms the data into the central schema, groups the objects into updates vs creates by querying the ids, and then processes each part seperately as a bulk db update or bulk create. So option A has the unprocessed records stored in pub/sub and option B has them in Google cloud storage. 

I have read a lot about when to use a message queue and when not to and most of the advice seems to be for real time applications where you don't know when the events will take place, not batch ingest like this. Additionally the workload for this application is relatively small so distributed computing is not that necessary.. yet. However the concept of the unprocessed record seems to lend itself to a message queue? 

Has anyone had experience with a data integration pipeline like this and what would you recommend?"
5039,2021-02-04 22:42:14,1612471334.0,dataengineering,"How repetitive is data engineering on a scale of 1-10, 10 being most repetitive?",lcpk52,[deleted],,https://www.reddit.com/r/dataengineering/comments/lcpk52/how_repetitive_is_data_engineering_on_a_scale_of/,1.0,1.0,0.0,25031.0,
5040,2021-02-08 03:32:09,1612747929.0,dataengineering,Google cloud service account key question,lf0us7,pbj800100,,https://www.reddit.com/r/dataengineering/comments/lf0us7/google_cloud_service_account_key_question/,1.0,0.0,0.0,25096.0,"Can someone please explain how to set this up? I want to use the setup-gcloud GH action and one of the inputs is the ""service_account_key"" that can be passed into GH secrets. It says it should be encoded as a base64 string. I'm not clear what exact is the value of the service account key. I've used gcloud to get my .JSON file that contains the private key id, private key, client id, etc. etc. Is the service account key literally this entire file passed as a string or just one part of it? It says the privatekeydata in this file is already base64 encoded so I'm confused how I can encode the service account key. Thanks"
5041,2021-02-08 21:36:46,1612813006.0,dataengineering,Has coding made you lack empathy?,lfjsf0,The_Alpacas,,https://www.reddit.com/r/dataengineering/comments/lfjsf0/has_coding_made_you_lack_empathy/,1.0,7.0,0.0,25146.0,"I sometimes have a coding mentality - I don’t care about anything, just to get the job done irl or in code terms - just get the code to run. I lack empathy because of this I believe"
5042,2021-02-08 21:54:34,1612814074.0,dataengineering,[video] The things I wish I knew before I started my first Data Vault Project,lfk75o,fhoffa,,https://www.reddit.com/r/dataengineering/comments/lfk75o/video_the_things_i_wish_i_knew_before_i_started/,1.0,0.0,0.0,25146.0,
5043,2021-02-08 22:35:52,1612816552.0,dataengineering,Looking For Guidance In My Data Engineering Role,lfl5iu,Sensitive_Algae7284,,https://www.reddit.com/r/dataengineering/comments/lfl5iu/looking_for_guidance_in_my_data_engineering_role/,1.0,11.0,0.0,25147.0,"The company I work for is migrating from an on-premises data warehouse to a cloud provider and hired a bunch of consultants to do the architecting, modeling and some data engineering work for the transition. 

I’m a female currently in an entry level data engineering role and I admittedly don’t have much knowledge about the business rules surrounding our data. I come from a data analysis background and wanted to explore data engineering as a possible long term career. I’m a shy introvert and I figured data engineering would fit with my personality but I’m learning that I miss the creative aspects of being a data analyst (I just never enjoyed the presentation part of the job). I feel like I don’t get to be as creative in my current DE position and I’m doing a lot of manual, non-ELT related work which has left me feeling lost and unfulfilled. 

My manager hasn’t made it clear on what my responsibilities or goals are and I feel like the consultants know that I’m fairly new so they don’t include me in any conversation beyond our daily status calls. Honestly, there are days where I tell myself to stick it out for 6 months and see how it goes and there are other days where I feel like quitting to learn something more creative like web development. 

I’m looking for any advise on how to approach this situation and make this experience as a data engineer more valuable for me and my career."
5044,2021-02-08 23:25:08,1612819508.0,dataengineering,can I us REST API to upload data residing in an SFTP server?,lfmagc,saveriogzz,,https://www.reddit.com/r/dataengineering/comments/lfmagc/can_i_us_rest_api_to_upload_data_residing_in_an/,1.0,2.0,0.0,25150.0,"Hello, do you guys know if it's possible to upload data that are stored on an SFTP server into another location that supports REST API?"
5045,2021-02-08 23:56:49,1612821409.0,dataengineering,Dynamic PIVOTs in SQL with Snowflake,lfmzwf,fhoffa,,https://www.reddit.com/r/dataengineering/comments/lfmzwf/dynamic_pivots_in_sql_with_snowflake/,1.0,0.0,0.0,25151.0,
5046,2021-02-09 01:49:55,1612828195.0,dataengineering,Data Engineer - Englewood CO - Contract to Hire - Media,lfpe38,BrentRecruitz,,https://www.reddit.com/r/dataengineering/comments/lfpe38/data_engineer_englewood_co_contract_to_hire_media/,1.0,2.0,0.0,25156.0,"Hey Everyone, 

I'm looking for a Data Engineer in the Denver CO Metropolitan area that's in the market for a Data Engineering position with a Fortune 500 company! 

This is role that is focused primarily on the coding aspect of being a Data Engineer. Code slingers with Java, Spark, Python and AWS are encouraged to apply!"
5047,2021-02-09 03:43:33,1612835013.0,dataengineering,MongoDB prod to dev automation,lfrkw3,srdeabo,,https://www.reddit.com/r/dataengineering/comments/lfrkw3/mongodb_prod_to_dev_automation/,1.0,1.0,0.0,25162.0,"Hi all. I was looking for a way to automate copying a prod db to a non prod environment and masking its data. I've found a way of doing that for AWS databases using maskopy, but our application also uses Atlas MongoDB. Is there any way or tool to help achieving it?"
5048,2021-02-09 04:01:03,1612836063.0,dataengineering,Advanced SQL,lfrwkh,SecretLoquat3,,https://www.reddit.com/r/dataengineering/comments/lfrwkh/advanced_sql/,1.0,30.0,0.0,25163.0,"Hi,

I need to learn how to write better long queries, like with several nested queries or subqueries. When I search ""advanced SQL"" ebooks or Udemy courses, they aren't really all that advanced! lol. They still start from the beginning.

But for work purposes, sometimes I have to write long queries with lots of joins, aggregation, etc. And even if the query works, I want to make it cleaner, easier to read and debug. Programming languages like Python or anything else have distinct best practices, but I don't see that so much for SQL.

For example, common table expressions starting with ""with some_alias as (select…)"" are super useful. Where do I learn more stuff like that?"
5049,2021-02-09 05:16:46,1612840606.0,dataengineering,Transition from a Quality engineer to Data engineer,lftb96,Initial_Squirrel2693,,https://www.reddit.com/r/dataengineering/comments/lftb96/transition_from_a_quality_engineer_to_data/,1.0,3.0,0.0,25169.0,"Hello,

I am a Quality engineer and looking to transition into data engineering role. I have 1 year of internship experience as a data engineer. I have recently completed the Udacity data engineering nano degree program as well. 

I am puzzled on how do I approach my job search? (I am getting rejected due to my current profile). What all things should I prepare in order to stand out? Any resources or a strategy would be very helpful. Thank you in advance."
5050,2021-02-09 15:13:39,1612876419.0,dataengineering,First Data Lake Choices,lg32jt,Wahoopokie,,https://www.reddit.com/r/dataengineering/comments/lg32jt/first_data_lake_choices/,1.0,11.0,0.0,25195.0,"Want to build a small data lake to collect and analyze web traffic, email stats, survey stats, search results and text discussion, text/json/RDB/spreadsheets (for a marketing/small business project) - combo of structured and semi-structured - what's a good first step to try this out? Have used AWS data pipeline (i.e. Kinesis/S3/Lambda/Athena), it's pretty good, but the setup/scripting is a bit involved. Azure seems more practical, easier to deal with. Snowflake seems more focused on larger projects, more data warehouse-like vs. lake."
5051,2021-02-09 15:44:16,1612878256.0,dataengineering,Soda Empowers Data Engineers with Soda SQL Tools for Data Testing and Monitoring,lg3li8,smbale,,https://www.reddit.com/r/dataengineering/comments/lg3li8/soda_empowers_data_engineers_with_soda_sql_tools/,1.0,0.0,0.0,25193.0,
5052,2021-02-09 15:56:43,1612879003.0,dataengineering,Please Help me to Find...,lg3tkq,SlightCredit2443,,https://www.reddit.com/r/dataengineering/comments/lg3tkq/please_help_me_to_find/,1.0,0.0,0.0,25194.0," 

I am preparing for GCP Data Engineer Exam. Please help me to find the relevant Docs that has been highlighted in the attached ScreenShot i.e :-

=&gt; Pipeline Monitoring (e.g stackdriver)  
=&gt; Choosing b/w ACID, idempotent, eventually consistent requirements

Thanks &amp; Regards!"
5053,2021-02-09 17:50:02,1612885802.0,dataengineering,data_check - simple data validation,lg6618,_andrjas,,https://www.reddit.com/r/dataengineering/comments/lg6618/data_check_simple_data_validation/,1.0,0.0,0.0,25202.0,"Hi everyone!  
I'd like to share a small project with you that I build during the last few weeks:  
[https://github.com/andrjas/data\_check](https://github.com/andrjas/data_check)  
data\_check tries to solve a simple problem: during development you often write SQL queries to validate the data manually. With data\_check you can store the queries and CSV files with the expected result in a folder structure and let data\_check validate the data.  
data\_check tries to fit somewhere between DbFit and Great Expectations with a usage similar to unit test tools like pytest.  
Hope you'll find it useful a I look forward to your feedback,  
Andreas"
5054,2021-02-09 18:52:47,1612889567.0,dataengineering,Subscribe to the #idataengineer podcast,lg7l41,soobrosa,,https://www.reddit.com/r/dataengineering/comments/lg7l41/subscribe_to_the_idataengineer_podcast/,1.0,0.0,0.0,25204.0,"Finally you can subscribe to the #idataengineer podcast on Spotify, Apple Podcasts and Google Podcasts and all if you're interested in our stories.  
[https://anchor.fm/pipelinedataber](https://anchor.fm/pipelinedataber)"
5055,2021-02-09 19:02:47,1612890167.0,dataengineering,You can use GPT-3 to write SQL for you,lg7tsc,ajmonty21,,https://www.reddit.com/r/dataengineering/comments/lg7tsc/you_can_use_gpt3_to_write_sql_for_you/,1.0,2.0,0.0,25205.0,
5056,2021-02-09 21:30:28,1612899028.0,dataengineering,Does your organization combine operational and analytical data within a single database?,lgbany,importpandaaspd,,https://www.reddit.com/r/dataengineering/comments/lgbany/does_your_organization_combine_operational_and/,1.0,6.0,0.0,25217.0,"Hi all, I'm starting to question what I know and am hoping I can get some clarity from you guys.

As the title states, does your large enterprise company combine operational and analytical data into a single database??

When I worked for a small company I understood that this can be normal since resources are low, however I'm struggling to understand why a large company would do this. 

Some background, I work for a nontech company that's been around forever. The enterprise data warehouse is used for both operational and analytical purposes. Often times the operational side will conflict with analytical and there's a constant battle between the two, understandably since both have differing and sometimes opposite uses.

Anyone have experience with this? Everytime I recommend a different approach, people seem to not understand."
5057,2021-02-10 10:26:04,1612945564.0,dataengineering,Best way to log inference data after deployment?,lgpb0b,Integral_humanist,,https://www.reddit.com/r/dataengineering/comments/lgpb0b/best_way_to_log_inference_data_after_deployment/,1.0,0.0,0.0,25249.0,"I’d like to capture input and model prediction and some relevant metadata and store it for later analysis and visualisation and check for feature drift etc. What tools would really do the job?

This isn’t stuff like sensitivity etc mostly just a way to collect all that data."
5058,2021-02-10 12:43:58,1612953838.0,dataengineering,Data mesh learning community,lgr44j,robertsahlin,,https://www.reddit.com/r/dataengineering/comments/lgr44j/data_mesh_learning_community/,1.0,0.0,0.0,25254.0,"[Zhamak Dehghani](https://www.linkedin.com/in/ACoAAABUwgIBXL1UskOCe6sM-7QogZIG13-_sDo) (and friends) just launched a vendor independent Slack channel + companion website for people learning about data mesh, from complete beginners to the experts like Zhamak (she created the concept).

The objective is to create a healthy community that focuses on sharing and helping each other get better and do more with the data mesh concept, driving ROI for your companies. Data engineers especially should check it out. Link to join the community on slack [https://launchpass.com/data-mesh-learning](https://launchpass.com/data-mesh-learning)"
5059,2021-02-10 15:39:08,1612964348.0,dataengineering,MLOps and Data Quality: How to Deploy Reliable ML Models in Production [WEBINAR],lgtt0q,Andrey_Khakhariev,,https://www.reddit.com/r/dataengineering/comments/lgtt0q/mlops_and_data_quality_how_to_deploy_reliable_ml/,1.0,3.0,0.0,25257.0," Hey folks,

Join Provectus and AWS February 24 11 AM PT | 2 PM ET for a live webinar:  
[https://provectus.com/webinar-mlops-and-data-quality-deploying-reliable-ml-models-feb-2021/](https://provectus.com/webinar-mlops-and-data-quality-deploying-reliable-ml-models-feb-2021/)

At the webinar, we will discuss what goes into building such fundamental components of machine learning infrastructure:

1. Reusable Feature Store with reproducible data preparation pipelines
2. Reproducible experimentation &amp; model training pipelines
3. Continuous Integration and Delivery for ML (MLOps)
4. Production monitoring and model re-training
5. Data Quality checks and Data monitoring

We will also explore why data quality and metadata management are crucial to standardize and streamline machine learning life cycle management. Waiting for you at the webinar!"
5060,2021-02-10 16:52:51,1612968771.0,dataengineering,An introduction to Change Data Capture(CDC),lgv8ns,duischen_,,https://www.reddit.com/r/dataengineering/comments/lgv8ns/an_introduction_to_change_data_capturecdc/,2.0,13.0,0.0,25268.0,"How to do you keep the data in your systems in sync and consistent? 

[https://medium.com/event-driven-utopia/a-gentle-introduction-to-event-driven-change-data-capture-683297625f9b](https://medium.com/event-driven-utopia/a-gentle-introduction-to-event-driven-change-data-capture-683297625f9b)"
5061,2021-02-10 17:33:23,1612971203.0,dataengineering,Am I really a data engineer?,lgw4ol,The_Alpacas,,https://www.reddit.com/r/dataengineering/comments/lgw4ol/am_i_really_a_data_engineer/,1.0,18.0,0.0,25274.0,"My job has 1 data engineer (me) it is my first position with this title. 

I primarily perform web scrapes using Python and present this to the company. I don’t use Hadoop, ML, spark etc. We are also implementing a DB using JavaScript.

Due to my lack of familiarity in the field, would this make me a data engineer?"
5062,2021-02-10 17:34:33,1612971273.0,dataengineering,Apache Airflow In Python: Getting Started As A Data Engineer,lgw5jl,Pragyanbo,,https://www.reddit.com/r/dataengineering/comments/lgw5jl/apache_airflow_in_python_getting_started_as_a/,12.0,3.0,0.0,25274.0,
5063,2021-02-10 20:16:44,1612981004.0,dataengineering,Data Streaming Pipelines with Confluent and Databricks in Azure,lgzvnz,gnatali,,https://www.reddit.com/r/dataengineering/comments/lgzvnz/data_streaming_pipelines_with_confluent_and/,1.0,0.0,0.0,25283.0,
5064,2021-02-10 21:35:14,1612985714.0,dataengineering,how many of your organizations have already invested into Palantir tool for Data mining/integration ? My organozation recently signed a multi yr contract and so I was getting curious.,lh1pdq,satz3,,https://www.reddit.com/r/dataengineering/comments/lh1pdq/how_many_of_your_organizations_have_already/,2.0,3.0,0.0,25289.0,Regarding palantir for data integration
5065,2021-02-10 21:37:51,1612985871.0,dataengineering,Anyone have experience with creating Airflow processes that parallelize processing of many files.,lh1rl0,Puggravy,,https://www.reddit.com/r/dataengineering/comments/lh1rl0/anyone_have_experience_with_creating_airflow/,1.0,10.0,0.0,25289.0,"I'm evaluating Airflow as a potential addition to my companies tech stack, and the one thing I'm really struggling with is how people implement parallelism in practice with airflow.  


Airflow obviously has a lot of support of executing different processes in parallel with the celery executor. However since DAGs are static after parse time that functionality can't be used to scale up reactively. For instance we have a certain job that sometimes needs to parse a large number of files. We're talking about between 1 and several hundred for any given run. In practice how are people handling situations like this using the Airflow framework? Is there something obvious that I am missing or am I expecting too much from Airflow?"
5066,2021-02-10 22:50:53,1612990253.0,dataengineering,"Data Science Fundamentals: Basic Concepts, Data Wrangling, and Databases with Python",lh3ikj,lwilson747,,https://www.reddit.com/r/dataengineering/comments/lh3ikj/data_science_fundamentals_basic_concepts_data/,1.0,0.0,0.0,25296.0,
5067,2021-02-11 00:01:06,1612994466.0,dataengineering,23andMe Engineering blog post about genetic data storage on S3 using parquet and arrow,lh55fi,[deleted],,https://www.reddit.com/r/dataengineering/comments/lh55fi/23andme_engineering_blog_post_about_genetic_data/,1.0,0.0,0.0,25301.0,
5068,2021-02-11 00:02:35,1612994555.0,dataengineering,High-performance genetic datastore on AWS S3 using Parquet and Arrow,lh56ns,[deleted],,https://www.reddit.com/r/dataengineering/comments/lh56ns/highperformance_genetic_datastore_on_aws_s3_using/,1.0,0.0,0.0,25301.0,
5069,2021-02-11 00:05:00,1612994700.0,dataengineering,High-performance genetic datastore on AWS S3 using Parquet and Arrow,lh58jc,pkpenton,,https://www.reddit.com/r/dataengineering/comments/lh58jc/highperformance_genetic_datastore_on_aws_s3_using/,1.0,0.0,0.0,25301.0,
5070,2021-02-11 00:51:35,1612997495.0,dataengineering,The Data Observability Universe,lh6a65,curvature_propulsion,,https://www.reddit.com/r/dataengineering/comments/lh6a65/the_data_observability_universe/,1.0,0.0,0.0,25304.0,
5071,2021-02-11 04:20:37,1613010037.0,dataengineering,Best way to sync a MySQL database with an API once daily?,lhajnq,byebybuy,,https://www.reddit.com/r/dataengineering/comments/lhajnq/best_way_to_sync_a_mysql_database_with_an_api/,2.0,15.0,0.0,25310.0,"Let's say I have a MySQL database. There's an API out there somewhere that I need to populate this database with. There will be an initial pull to bring in most of the data in the API, and then I'll need to schedule an update daily of only the previous 24 hrs of data.

Solutions I've considered:

1. Creating a Python script in the Workbench scripting shell and then creating an event that runs it daily. There seems to be a complete dearth of tutorials on this online, which is surprising to me. Seems like it would be nice to have the whole solution contained within MySQL. However, the grt Python module is a pain to work with, I'm getting weird exceptions to my very simple sample script.
2. Creating a batch script that runs an independent Python script that uses requests and mysqlconnector to grab the data and then push it to the database. I'm not a huge fan of this because it seems like there are multiple potential points of failure, particularly the task scheduler which I just don't trust.
3. AWS Lambda? I haven't really explored this one, but you can create Python scripts that run on a schedule using AWS Lambda, right?
4. ?? Open to suggestions!

Thanks! Sorry if I left out any important details. Happy to clarify anything."
5072,2021-02-11 05:38:38,1613014718.0,dataengineering,What are the Top 7 Big Data Security Changes for 2021?,lhbzvh,abe_dearmer,,https://www.reddit.com/r/dataengineering/comments/lhbzvh/what_are_the_top_7_big_data_security_changes_for/,1.0,0.0,0.0,25313.0,"Recently, I was able to analyze the [findings](https://www.getapp.com/resources/annual-data-security-report/?utm_source=xp&amp;utm_medium=blog&amp;utm_campaign=content) of GetApp's 2020 State of Data Security Report. 

80+ IT security leaders gave their feedback in a report. It’s one that predicts, among other things, the biggest big data security challenges for the next calendar year. 

Here are the[ top 7 big data security changes for 2021](https://www.xplenty.com/blog/big-data-security-changes/?utm_source=content&amp;utm_medium=referral&amp;utm_campaign=big-data-security-changes%2F):

1. **Real-time compliance**: businesses are needing to be alerted for various compliance-related issues such as data over-sharing or other regulatory violations.
2. **Data categorization alternatives**: Recommended use of authentication methods and data access controls as more effective weapons in the fight against data breaches. 
3. **Job critical data access**: Companies who allow employees full access to big data are more likely to report a data breach thus employees should only access data critical to their job role. 
4. **Data encryption**: Expect an uptake to increase further as organizations prepare for data compliance in 2021.
5. **Industry specific data security changes**: expect at risk sectors like Banking and financial services, IT services, Digital marketing, and Education to make necessary security platform updates.
6. **Authentication**: 83% of businesses used it in 2020, expect the trend to only continue.
7. **Wider data security changes**: with businesses focusing more and more on things like *protecting customer data and company data, customer expectations, data privacy regulations and the impact of Covid-19,* expect a wide net to be thrown to find solutions to these critical issues."
5073,2021-02-11 11:03:44,1613034224.0,dataengineering,Top 10 Companies Hiring Data Engineering Professionals,lhgsl8,analyticsinsight_AI,,https://www.reddit.com/r/dataengineering/comments/lhgsl8/top_10_companies_hiring_data_engineering/,1.0,1.0,0.0,25327.0,
5074,2021-02-11 14:13:32,1613045612.0,dataengineering,Join the Whizlabs Valentine day Giveaway! Win Exciting Prizes upto 5 years Premium Subscription Free ($999). Participate Now,lhjh5v,baladba,,https://www.reddit.com/r/dataengineering/comments/lhjh5v/join_the_whizlabs_valentine_day_giveaway_win/,1.0,0.0,0.0,25334.0,
5075,2021-02-11 15:45:23,1613051123.0,dataengineering,Medium,lhkxuj,mayuri55,,https://www.reddit.com/r/dataengineering/comments/lhkxuj/medium/,1.0,0.0,0.0,25340.0,
5076,2021-02-11 16:30:24,1613053824.0,dataengineering,Adding Data Versioning to Data Labeling,lhlsxj,jimmywhit,,https://www.reddit.com/r/dataengineering/comments/lhlsxj/adding_data_versioning_to_data_labeling/,1.0,2.0,0.0,25342.0,"I’ve been working on an open source integration to combine data versioning with data labeling, using [Label Studio](https://github.com/heartexlabs/label-studio) and [Pachyderm](https://github.com/pachyderm/pachyderm). 

**Some context**: Labeling and versioning data are critical for production ML models (especially deep learning models). When I worked on ASR for financial institutions, we would constantly need to incorporate new accents, expand our models' vocabularies, etc. Scraping data was always a good start, but we needed teams of people to correct the transcripts (sometimes multiple times and even after we had trained our models), and the managing the dataset versions became one of our biggest bottlenecks. Ideally, we would be able have teams of [data labelers iterating on the data and not block the ML researchers](https://jimmymwhitaker.medium.com/completing-the-machine-learning-loop-e03c784eaab4). 

**How it works**:  I want to combine two things that are very good at what they do, to solve the bigger problem, and it went pretty well. Label Studio has the ability to write data to an S3 backend, and Pachyderm can emulate S3 storage (their S3 gateway), while adding version control. Check out the [repo](https://github.com/JimmyWhitaker/label-studio-pach) for the integration or the [blog post](https://towardsdatascience.com/versioning-and-labeling-better-together-2dd7d4fe8bd9) about the project. 

Hopefully this is useful, and let me know how I can make it better :-) 

Note: I initially dropped a different post in r/MachineLearning, but thought it might be a better fit here."
5077,2021-02-11 16:51:43,1613055103.0,dataengineering,"Guys, what conferences to visit in 20/21",lhm931,an_tonova,,https://www.reddit.com/r/dataengineering/comments/lhm931/guys_what_conferences_to_visit_in_2021/,1.0,3.0,0.0,25345.0,"Topics:

\- Data Engineering

\- ML"
5078,2021-02-11 17:14:43,1613056483.0,dataengineering,Big Data Engineering Certification - 2021,lhmqn7,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/lhmqn7/big_data_engineering_certification_2021/,1.0,0.0,0.0,25345.0,
5079,2021-02-11 17:31:01,1613057461.0,dataengineering,Big Data Engineering Certification - 2021,lhn323,sharmaniti437,,https://www.reddit.com/r/dataengineering/comments/lhn323/big_data_engineering_certification_2021/,1.0,0.0,0.0,25345.0,
5080,2021-02-11 18:41:40,1613061700.0,dataengineering,"Data Observability in Practice Using SQL, Part II: Schema &amp; Lineage",lhomwb,monacodev,,https://www.reddit.com/r/dataengineering/comments/lhomwb/data_observability_in_practice_using_sql_part_ii/,1.0,1.0,0.0,25351.0,
5081,2021-02-11 18:50:00,1613062200.0,dataengineering,covid compared- a data engineering project,lhotid,AAaction23,,https://www.reddit.com/r/dataengineering/comments/lhotid/covid_compared_a_data_engineering_project/,1.0,5.0,0.0,25351.0,"Hello,
In my first data engineering project, I created an automated data pipeline using data from John Hopkin's covid repository. The pipeline used AWS S3, AWS RDS, and Airflow.

To showcase my work, I also created a covid comparison [interactive dashboard](https://dashboard.covid19compared.com/), which allows the user to compare different location levels against each other on a per capita basis, i.e Beijing vs Montana, or Los Angeles County against Dallas County.

I'm hoping for critiques from both a coding and data engineering perspective- this is my first pipeline and I know I have a lot to learn. Furthermore, I'm hoping this project can serve as reference to others starting out.

Repo is [here](https://github.com/jjjchens235/covid-compared). 
If you're curious about how the pipeline was built, [here](https://github.com/jjjchens235/covid-compared#pipeline-steps) is a small section in the README on that.

Lastly, I just wanted to say that I learned a lot from r/dataengineering, especially these 3 resources that I found on this sub: 
1. https://josephwibowo.github.io/Meetup_Analytics/
2. https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/
3. https://github.com/damklis/DataEngineeringProject

Thank you all!"
5082,2021-02-11 19:17:32,1613063852.0,dataengineering,How are fact and dimension tables loaded?,lhpgeq,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/lhpgeq/how_are_fact_and_dimension_tables_loaded/,1.0,5.0,0.0,25352.0,"Are fact and dimension tables loaded at the same time? Separately? Looking for a very ELI5 example with real fact and dimension example. 


For example: if we have a prescription as a fact table with the following dimension tables: order, time, pharmacy, patient 

Do they all get loaded at the same time (both the fact table and dimension tables)? Or does the fact get loaded first or dimension get loaded first? Need help understanding !"
5083,2021-02-11 19:42:16,1613065336.0,dataengineering,Help me understand XCOM push limit ?,lhq0n8,corporatededmeat,,https://www.reddit.com/r/dataengineering/comments/lhq0n8/help_me_understand_xcom_push_limit/,1.0,3.0,0.0,25355.0,"Hi all ,

&amp;#x200B;

I was working on automate some SQL queries ( from BigQuery ) using airflow , I found that when I hit the Queries through airflow and push the json response trough Xcom pull and push , Sometime it fails if the query return more data say ( 600 \* 6 shaped files ) , I am new with the airflow , I dunno how to proceed , i tried wrapping the json in dataframe and then storing as a variable , but that does not seems works .  


I would really appreciate some help on this one"
5084,2021-02-11 23:35:51,1613079351.0,dataengineering,Advanced SQL Course to prepare for FAANG interview,lhvgbc,tuanavu,,https://www.reddit.com/r/dataengineering/comments/lhvgbc/advanced_sql_course_to_prepare_for_faang_interview/,1.0,0.0,0.0,25369.0,
5085,2021-02-12 00:15:12,1613081712.0,dataengineering,What Is Operational Analytics?,lhwch9,Drkpwn,,https://www.reddit.com/r/dataengineering/comments/lhwch9/what_is_operational_analytics/,1.0,0.0,0.0,25370.0,
5086,2021-02-12 02:47:48,1613090868.0,dataengineering,Senior Data Engineer - U.S Remote,lhzju9,reoccuringpayments,,https://www.reddit.com/r/dataengineering/comments/lhzju9/senior_data_engineer_us_remote/,1.0,0.0,0.0,25377.0,
5087,2021-02-12 07:26:04,1613107564.0,dataengineering,"Github Repo with All Data tranformation,Cleaning,Validation",li4ikz,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/li4ikz/github_repo_with_all_data/,1.0,4.0,0.0,25395.0,"Hello Guys,

I have seen a post either in Linked In or Medium where I clicked the link and it opened its own browser I have navigated and found that very useful for me, thought to explore later as I was occupied in something else but forget to bookmark/save. However, it is not recorded anywhere in the browser history as well as in the viewed posts. It happened a week back.  


I'm trying to recover the page somehow, What I can recollect from my memory is   


It has 20 - 25 folders --&gt; consists of topic names, Data Cleaning, Data Validation, etc.. Can't recollect more.

Never mind the story above. Can you please throw the best git repos for Data engineering from beginner to advanced"
5088,2021-02-12 16:24:28,1613139868.0,dataengineering,Thoughts on the future of data integration space?,lic0n7,hsyyid,,https://www.reddit.com/r/dataengineering/comments/lic0n7/thoughts_on_the_future_of_data_integration_space/,2.0,14.0,0.0,25419.0,"Hi everyone,

I recently wrote an [article that was published on TowardsDataScience](https://towardsdatascience.com/the-future-of-data-integration-7694ebacfe92?source=friends_link&amp;sk=f2b45b34fb8f291e39fa5336a3f109e7) about the new(er) players in the data integration space and how the older players (Talend, Informatica) are reacting. I also mentioned one of the gaps I've observed in the market which I've personally been working to address.

I'd love to start a discussion on this topic and see what you guys see as the popular trends in the data engineering space. You can read the article on Medium [here](https://towardsdatascience.com/the-future-of-data-integration-7694ebacfe92?source=friends_link&amp;sk=f2b45b34fb8f291e39fa5336a3f109e7) (this link shouldn't have any paywall).

  
Cheers!"
5089,2021-02-12 17:08:08,1613142488.0,dataengineering,New Project Architecture Suggestion,licvhu,stringman520,,https://www.reddit.com/r/dataengineering/comments/licvhu/new_project_architecture_suggestion/,1.0,5.0,0.0,25421.0,"So at my job I just had a pretty simple project assigned to me.  We'll be receiving a CSV file Monthly from a 3rd party and need a system to pull the file automatically  into a DB and deliver a few key reports off of it.  Please note, although I have never seen the data they are going to be delivering, it is likely to be quite small (100k records per month).  Delivery specifications are yet to be determined, but having dealt with this company in the past they'll likely deliver to a SFTP server.  The information is considered pretty sensitive from a business perspective although from a data security perspective, it isn't.

I work for a small company where I am the only data person on staff so I have a considerable amount of leeway in how I set this up with one key caveat.  My bosses do not want this sitting in the cloud because ""someone could steal this"" and despite many protestations, I lost the fight.  I agree, Snowflake/BigQuery/Redshift would be MUCH preferable for this project, it's just not an option. I have a server in house for my own use that will handle this project.

My preference is to set this up with entirely open source tools just to get a better sense of how they will perform on a smaller data set as well as use this project to get better acquainted with them.  Additionally, politically speaking having this setup be low cost is a good thing because the other option on the table is for me to manually process the file and deliver reports monthly to the executives.  I obviously would prefer to have this be fully automated.

While this data set is small, this is a trial run which could potentially expand in the future to other partners of this sort so I'd like to architect it in a way that it would expand easily as well as easily transition to the cloud if I make any inroads with the powers that be when they complain their reports are taking too long.

After a decent amount of research what I am thinking right now is using Airflow for orchestration, Singer for Extract/Load, and DBT for Transformation through Meltano.  For BI I'm between Superset and Metabase, although I'm leaning towards Superset since it seems like it would integrate easily with all of the DW options I'm considering.

Where I am stuck is on the Data Warehouse.  Here are the options I've come up with:

&amp;#x200B;

1. Postgres. Postgres would be fine for a data set this size, but my biases of doing OLAP on Postgres is making this a bit difficult to allow it mentally.  Also, using a columnar DW would be preferable if this does expand in the future.  I looked into cstore\_fdw as an option, but from what I've read the FDW might be problematic if I move this to the cloud.
2. Clickhouse. I've looked into Clickhouse as it sort of fits the bill nicely (columnar/on-prem).  That being said, there is no option for Clickhouse with DBT at the moment and I've used DBT in the past and I like the idea of  using version control for my data models.  Additionally, DBT makes it easy if I move to a cloud DW to just change my target. Additionally, I thought about using clickhousedb\_fdw through Postgres but my hesitation about using FDW remains.
3. Presto/Dremio. The over-engineered option I've landed on recently is setting up a ""S3 data lake"" on MinIO storing Parquet files which then uses Presto or Dremio to query.  It would satisfy all the requirements I have in my head as well as makes it easy to transition to the cloud (point it to S3 instead of MinIO) but clearly, so much infrastructure for a simple project seems overkill.

What would you do in a situation like this?  Are there tools I should look at? What level of crazy am I for considering option 3 as the best?

TLDR; I need an on-premise open source columnar DW setup.  What are you using?"
5090,2021-02-12 17:33:34,1613144014.0,dataengineering,How do you handle PII and CCPA requests?,lideni,b-e-n-j,,https://www.reddit.com/r/dataengineering/comments/lideni/how_do_you_handle_pii_and_ccpa_requests/,1.0,4.0,0.0,25423.0,"I’ve set this up a few times and it works ok but feels janky. 

CCPA: Usually I’ll get the core platform team to handle CCPA requests with a tombstone value like ‘CCPA REDACTED’ for strings and 0/null for ID numbers. I’ll use the existence of that tombstone to kick off history cleansing jobs in the data warehouse. I’ve proposed building a CCPA redaction micro service but since this tombstone setup usually gets us compliant, that’s never happened. 

PII: I use a very similar hashing technique to the one found in the gitlab data team DBT project. I store the salt in the data warehouse though to avoid security issues and accidentally using the wrong salt for a given environment. 

I love the get some fresh ideas!"
5091,2021-02-12 17:36:21,1613144181.0,dataengineering,Applying to new grad jobs,lidgpz,AggravatedSugar,,https://www.reddit.com/r/dataengineering/comments/lidgpz/applying_to_new_grad_jobs/,1.0,3.0,0.0,25424.0,"Hey, hope that you're doing well!

I plan on applying to new grad jobs for data engineering over the fall, and I was just wondering what I can do to add the most value to my resume before then? Also, how should I be preparing for data engineering interviews?

I've done 3 data engineering internships, and I've accepted an offer to do one over the spring as well. In addition to this, would it be more valuable if I contributed to open source projects such as Spark or Kafka, or if I worked on a personal project? Also, when it comes to contributing to open source projects, do recruiters/interviewers care more about how you contributed (MRs vs. code reviews) or about the fact that you took the initiative?

Also, what's the best way to go on about preparing for interviews? So far, for my internships, all I've been asked are behavioural and easy/medium level LeetCode questions. I'm sure that for the full-time positions I'm expected to answer other questions as well, relating to system design, modelling, etc. What are some good/useful resources to learn more about these things?

Thank you!"
5092,2021-02-12 18:14:47,1613146487.0,dataengineering,What is the low hanging fruit for a brand new GCP data engineer to learn?,lie9vp,Fatal_Conceit,,https://www.reddit.com/r/dataengineering/comments/lie9vp/what_is_the_low_hanging_fruit_for_a_brand_new_gcp/,1.0,13.0,0.0,25430.0,"Hi everyone, I just got a job as a data engineer after working mostly in Big Query writing SQL for 2 years. I've used cloud storage a bit, and used ODBC to pipeline out tables to excel, but other than that I feel like a total fraud to have this title. 

What are the easiest low hanging fruit in terms of tech like Airflow, PubSub, etc that I can learn in the next few months and shake my imposter syndrome. A lot of this will probably need to be self taught as most of my team are closer to analysts or data scientists.

FYI: i code in python and sql"
5093,2021-02-12 19:45:02,1613151902.0,dataengineering,Grad Courses to Prepare for DE Positions,ligaqv,NonExistentDub,,https://www.reddit.com/r/dataengineering/comments/ligaqv/grad_courses_to_prepare_for_de_positions/,1.0,2.0,0.0,25434.0,"Hi all,

I am looking for advice on which skills/courses to consider if I want to be sure I have the necessary background to apply for DE positions post-grad.

Currently, I am a first semester grad student enrolled in an M.S. in Data Science program taking Intro to Data Science, Data Mining, and Data Warehousing. I plan to take a few ML courses to cover the DS side of things, but I am unsure which courses to consider for the DE side. 

Any advice is appreciated. Thank you!"
5094,2021-02-12 20:18:35,1613153915.0,dataengineering,“Data Engineering is the new Data Science” DS Interviews: -15% DE Interviews: +40% 🚀🚀🚀,lih2xg,Suitable-Chemistry-9,,https://www.reddit.com/r/dataengineering/comments/lih2xg/data_engineering_is_the_new_data_science_ds/,1.0,30.0,0.0,25435.0,
5095,2021-02-12 22:47:08,1613162828.0,dataengineering,Any experience with Glue?,likcp8,Crolle,,https://www.reddit.com/r/dataengineering/comments/likcp8/any_experience_with_glue/,1.0,0.0,0.0,25443.0,"We are currently thinking about moving our ETL workflow from a proprietary drag-and-drop solution to a more code oriented one. Since the remainder of our stack for storing data relies on AWS (S3 &amp; Redshift), we think that Glue might be a good solution. Do you have experience to share about it? Our main concerns are :

* Cost effectiveness (is it easy to oversight spendings?)
* Ease for development (is it easy to prototype stuff?)
* CI/CD availability (is it easy to go from dev to prod?)
* Any horror stories?"
5096,2021-02-13 01:48:50,1613173730.0,dataengineering,"How I feel in the coding interview when I get asked about BSTs and I damn well know all I’m going to do is call apis, parse json, and copy to Redshift.",lio4nh,pawtherhood89,,https://www.reddit.com/r/dataengineering/comments/lio4nh/how_i_feel_in_the_coding_interview_when_i_get/,1.0,19.0,0.0,25449.0,
5097,2021-02-13 05:46:42,1613188002.0,dataengineering,Just curious how would you guys design your database tables from a collection of micro-services to a data warehouse,lisgn0,greenee111,,https://www.reddit.com/r/dataengineering/comments/lisgn0/just_curious_how_would_you_guys_design_your/,1.0,8.0,0.0,25457.0,"How would your design change if the scale was in the thousands of tables?

What would you do if the latency allowed was only 24 hours vs to just a few minutes?"
5098,2021-02-13 07:01:37,1613192497.0,dataengineering,“Cloud computing” job listings...why does it seem like they require the skills of a DE with way less pay and even more requirements?,litou7,flailing_acc,,https://www.reddit.com/r/dataengineering/comments/litou7/cloud_computing_job_listingswhy_does_it_seem_like/,1.0,2.0,0.0,25460.0,"I might just be conflating the two roles and see lots of the same terms from DE being used for cloud computing-types of careers, but it really does seem like the latter explicitly requires years of experience and all the same skills recommended to those who want to become DEs. Except also only at about $70k yearly, where DEs are at least in the ballpark of a $110k salary here. Am I missing something?"
5099,2021-02-13 09:46:09,1613202369.0,dataengineering,Aws managed Airflow - using different python environments,liw434,lppier2,,https://www.reddit.com/r/dataengineering/comments/liw434/aws_managed_airflow_using_different_python/,1.0,5.0,0.0,25465.0,"How can different projects which require different python libraries use aws’s managed airflow together? 
For example, if one requires an early version of pytorch to do create word embeddings vs another project using pytorch 1.6 ? 
What’s the recommended way to handle this?
Should we even share the airflow instance? 
New to airflow - be gentle!"
5100,2021-02-13 11:58:05,1613210285.0,dataengineering,Datumaro Dataset Management Framework for Computer Vision,lixrpj,maxra35,,https://www.reddit.com/r/dataengineering/comments/lixrpj/datumaro_dataset_management_framework_for/,1.0,0.0,0.0,25470.0,"Hi everyone! We would like to share our open-source project [Datumaro](https://github.com/openvinotoolkit/datumaro) with the DE community. It is a Python library and CLI tool to create, maintain and analyze Computer Vision datasets. The tool can be interesting for dataset maintainers, application developers, and researchers in Computer Vision domain.

One of our tools is [CVAT](https://github.com/openvinotoolkit/cvat), which we use for annotation of datasets internally. Having a number of datasets to maintain, we started to look for a tool to simplify our tasks on dataset maintenance such as comparison, tracking of changes, verification, versioning, annotation transformations, data filtering, merging, format conversions, etc. and have found few tools around this, but we wanted some kind of an integrated solution for such tasks. As a result, we have come up with our own tool, which we are presenting now. It already covers many of the topics above and will be extended with new functionality in future.

Currently, the project is in active development. We are open for pull requests, bug reports and fresh ideas on improvements!

Thank you!"
5101,2021-02-13 15:59:30,1613224770.0,dataengineering,Unsure about overall process of data engineering,lj10qw,veeeerain,,https://www.reddit.com/r/dataengineering/comments/lj10qw/unsure_about_overall_process_of_data_engineering/,1.0,28.0,0.0,25483.0,"Hello, I’m a student who is wanting to learn data engineering this summer. One question I had was that there are all these technologies like, big query, Hadoop, spark, hive, airflow, but I just don’t really know how they all fit together or what consists of an actual “pipeline”.  I have quite a bit of experience with python, but not much with SQL, so I’m trying to learn that first. 

But can someone explain to me how an ETL pipeline would work, or an example of a project? I will be taking courses relative to the above but I want to build something which uses some of these technologies. I want to build a streamlit app but I just don’t understand how I’d put those technologies together.

My idea of a pipeline was web scraping data with selenium from a website, loading it into MySQL with python. Then doing queries for some analysis, and then using that in a streamlit web app. But then again I haven’t even used any of those Hadoop, spark, big query, or airflow so it doesn’t feel like a “real” ETL pipeline.

So could someone explain, in the context of say a streamlit app how those technologies would be used and in what part of the steps they would be useful for? 

Also any general learning advice on my path would be greatly appreciated, thanks."
5102,2021-02-13 20:17:33,1613240253.0,dataengineering,"I started a podcast, Alexa's Input, just to share my conversations as I learn .. Check out the link to see my episode on databases. I have another with Kelsey Hightower talking about cloud computing!",lj5v6u,schoolgurllou,,https://www.reddit.com/r/dataengineering/comments/lj5v6u/i_started_a_podcast_alexas_input_just_to_share_my/,1.0,3.0,0.0,25493.0,[Databases and Danger](https://anchor.fm/alexagriffith/episodes/Databases-and-Danger-Part-1-ep3rvn/a-a1pm7n)
5103,2021-02-13 21:57:27,1613246247.0,dataengineering,How will we remove special characters in csv file data while reading it in Spark Scala ?,lj7v98,One-Ad6816,,https://www.reddit.com/r/dataengineering/comments/lj7v98/how_will_we_remove_special_characters_in_csv_file/,1.0,9.0,0.0,25497.0,Prior to reading it in Spark. Used to have a python process cleaning data and writing it into a new file. When trying to automate the process unable to resolve the special characters.
5104,2021-02-13 23:55:10,1613253310.0,dataengineering,As a data engineer is it sufficient to know Scala for Spark only and not vanilla Scala?,lja9om,theoriginalmantooth,,https://www.reddit.com/r/dataengineering/comments/lja9om/as_a_data_engineer_is_it_sufficient_to_know_scala/,1.0,9.0,0.0,25502.0,"Wondering if you need to know the ins and outs of Scala programming to be able to code effectively using Spark? Or can you know the basics of Scala and jump right into learning Spark?

Writing this on the go so apologies for weird wording"
5105,2021-02-14 03:05:54,1613264754.0,dataengineering,Advice on preparing for a data engineering career?,ljdu4n,Doublepotter,,https://www.reddit.com/r/dataengineering/comments/ljdu4n/advice_on_preparing_for_a_data_engineering_career/,1.0,22.0,0.0,25508.0,"Hi all. I’ve managed to get a data engineering job that’s an amazing opportunity. I start in October and will begin a 6 month training program, before going to consult for a company long term. 

I’ve been told that the training is comprehensive and no prior knowledge is assumed. However, I’m in lockdown and have lots of free time. I want to learn as much as I can now so I can get as much as I can out of the training and (hopefully) secure a better consulting placement.

Can anyone recommend where I should start? I’m new to coding, I used basic Matlab at university but that’s it. I’ve just started an introductory 2 month computer science course for a general coding foundation. I’m unsure where to go next or what I should try to learn. Any advice, resources, or stories of your experiences would be helpful. Thank you!"
5106,2021-02-14 17:13:57,1613315637.0,dataengineering,"The 29th edition of @data_weekly focus on Google research paper on Data Cascades in High-Stakes AI, montecarlodata Data Observability Using SQL, AirbnbEng apache superset adoption, SpotifyEng Sorted Merge Bucket implementation",ljq4k1,vananth22,,https://www.reddit.com/r/dataengineering/comments/ljq4k1/the_29th_edition_of_data_weekly_focus_on_google/,1.0,0.0,0.0,25532.0,
5107,2021-02-14 18:37:14,1613320634.0,dataengineering,Has anyone successfully gotten DBT to run on apple silicon?,ljrql8,Resili3nce,,https://www.reddit.com/r/dataengineering/comments/ljrql8/has_anyone_successfully_gotten_dbt_to_run_on/,1.0,4.0,0.0,25536.0,"After significant tinkering I haven't been able to get dbt to run on my m1 air.  


It seems there have been a few issues since m1 launch and I'm not sure what is currently possible and what isn't as it doesn't seem to be discussed anymore. If you have a link to a DBT m1 discussion I would love to read more.  


Initially I found that Python 3.9 that works with m1 doesn't work with DBT  and that BigSur and homebrew have issues so pip is the way forward: [https://discourse.getdbt.com/t/help-i-cant-install-dbt-nov-dec-2020/1847](https://discourse.getdbt.com/t/help-i-cant-install-dbt-nov-dec-2020/1847)  
 3.9 doesn't work mainly because of some pyarrow / snowflake-connector issue [https://github.com/snowflakedb/snowflake-connector-python/issues/562](https://github.com/snowflakedb/snowflake-connector-python/issues/562)  
Since my initial tests there has been movement on the connector issue : [https://github.com/snowflakedb/snowflake-connector-python/pull/565](https://github.com/snowflakedb/snowflake-connector-python/pull/565)  


However I haven't been able to successfully get a Python 3.9, 3.7.x, 3.8.2, 3.8.6 or 3.8.7 to successfully build DBT on m1. I've recently reinstalled BigSur and I'm going to re-approach the problem - but it would be great to hear if someone else has successfully managed.  
I haven't found any newer information that supersede the above, but there might be some updates that I have missed."
5108,2021-02-15 04:53:59,1613357639.0,dataengineering,How do I launch the Airflow UI on an EC2?,lk401u,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/lk401u/how_do_i_launch_the_airflow_ui_on_an_ec2/,1.0,0.0,0.0,25572.0,"I forgot how to connect to the Airflow UI on my local machine with an EC2 that has Airflow running. For example, when I have Airflow running locally, I enter `localhost:8080` in the address bar of my browser. 

How do I connect to Airflow that is running on an EC2? What do I type into the address bar of the browser on my local machine?"
5109,2021-02-15 11:32:39,1613381559.0,dataengineering,How do you handle raw + clean data?,lka7jl,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/lka7jl/how_do_you_handle_raw_clean_data/,1.0,17.0,0.0,25591.0,"I'm learning Python to become a Data Analyst. I'm working with a relatively large dataset (about 5MM+ rows). Here's where I'm at: 

1. I retrieved data from an API which needed to be cleaned in Python. I stored this raw dataset in SQL locally. 

2. I performed the required operations and now I have a clean DataFrame. I have also stored this in SQL locally. 

3. I want to access this clean DataFrame to perform visualizations in Matplotlib. 

Here's my predicament: My laptop only has a 1TB SSD. As I was making the API calls, I could see the storage on my machine fall shrink as the script was running.

What do you Data People do on the job? Do you store both the raw and clean data? I'm thinking that I need to store the raw dataset because I may have made a mistake and I don't want to have to run the lengthy script again. Or perhaps the full dataset may not be available in the future. I'm thinking I can store everything on AWS but [those damn AWS charges!](https://www.youtube.com/watch?v=uyIlAO390v4)  

I want to mimic what actually happens on the job so can you folks advise me on what to do?"
5110,2021-02-15 11:45:45,1613382345.0,dataengineering,A blog post alongside some notebooks on how to get started with CatBoost classification models 🐈🚀,lkadsu,t3chflicks,,https://www.reddit.com/r/dataengineering/comments/lkadsu/a_blog_post_alongside_some_notebooks_on_how_to/,1.0,2.0,0.0,25592.0,
5111,2021-02-15 11:56:12,1613382972.0,dataengineering,[OC] Get Your FREE Cleaning Data eBook,lkaizs,ezzeddinabdallah,,https://www.reddit.com/r/dataengineering/comments/lkaizs/oc_get_your_free_cleaning_data_ebook/,1.0,0.0,0.0,25593.0,"Hello guys,

As someone who cleans data almost every day

I've created an ebook to make the command line as seamless and easy as possible to do that task.

Be one of the first 10 who gets this ebook for free: [How to Clean Data at the Command Line](https://gumroad.com/l/clean-data-cmd/free)

Would love to see your feedback, Thanks!"
5112,2021-02-15 15:09:17,1613394557.0,dataengineering,Webinar invite: The new data stack is already broken. Where to next?,lkdcu3,mmanja,,https://www.reddit.com/r/dataengineering/comments/lkdcu3/webinar_invite_the_new_data_stack_is_already/,1.0,0.0,0.0,25604.0,
5113,2021-02-15 16:12:27,1613398347.0,dataengineering,Data lineage with Airflow data pipelines,lkefsk,fumi99,,https://www.reddit.com/r/dataengineering/comments/lkefsk/data_lineage_with_airflow_data_pipelines/,1.0,7.0,0.0,25606.0,"Hi fellow data engineers, I was hoping to get some suggestions for this data lineage project I've recently been given at work. I haven't done anything similar in the past, so I'm a bit at a a loss.

I've been tasked with implementing some form of Data Lineage system for data pipelines, that allow users to visualize and understand the flow of data from its source to its destination, all the way to each individual row in the database tables. The data pipelines basically go like this:

1. Read some files from S3.
2. Perform some numerical calculations on the data in Spark (EMR).
3. Save the results back to S3.
4. Python scripts load the data, transform it, and load it into a relational database with EAV table design (entity-attribute-value). I generally don't love EAV tables, but this design decision was made before my time.
5. All the above is orchestrated and scheduled via Airflow.  

My first idea was to simply add some new columns to the database tables, to be able to save some lineage information, for example, the source file in S3 where each row comes from, the Airflow dag\_run id, etc. However, I'm wary of adding many new columns to the database tables, because they already have billions of rows as it is, and are expected to maybe triple their size in the coming year; so database size is kind of a constraint. This solution would also not be easy to visualize or understand I think, and might become overly complex. 

Other things I've has a brief look at include:

* Airflow Lineage: this project still seems to be in its infancy, going by the barebones documentation and inexistent stackoverflow questions. 
* Apache Atlas: had a brief look, it does seem to integrate with Airflow lineage, but I don't understand how I could use it to manage data lineage al the way to individual database table rows.
* I came across this product called [Atlan](https://github.com/atlanhq/atlan-lineage-airflow), that also has some integration with Airflow Lineage, but its a paid product so I'm not too sure about it.

How would you tackle this task? Have you done something like this before? If not, how would you do it?"
5114,2021-02-15 16:43:46,1613400226.0,dataengineering,Trouble connecting to MySQL instance on AWS RDS,lkezyo,IamWarmduscher,,https://www.reddit.com/r/dataengineering/comments/lkezyo/trouble_connecting_to_mysql_instance_on_aws_rds/,1.0,3.0,0.0,25610.0,"I created a MySQL instance on AWS RDS. I make it publicly available. I used a prior security group that I know works. I used the default username (admin) and I wrote down my password. 

In Jupyter, I'm using the following to try to connect as I usually do: 

    engine = create_engine('mysql+pymysql://admin:password@blahblah.blahblah.us-west-1.rds.amazonaws.com:3306/')

When I run the following: 

    engine.execute(""show tables;"")

I get the following error:

    OperationalError: (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'blahblah.blahblah.us-west-1.rds.amazonaws.com' (timed out)"")
    (Background on this error at: http://sqlalche.me/e/13/e3q8)

I'm at a loss for what is happening. I've created Postgres and MySQL instances and they worked fine. I can't figure out what I'm doing wrong."
5115,2021-02-15 17:15:38,1613402138.0,dataengineering,Question about data engineering technologies,lkfmew,AspiringOfficeWorker,,https://www.reddit.com/r/dataengineering/comments/lkfmew/question_about_data_engineering_technologies/,1.0,8.0,0.0,25615.0,"I've been reading about data engineering, and I'm overwhelmed with all the terms, buzzwords and technologies. 

Hadoop, spark, kafka, hive, aws, azure, airflow, kubernets, docker, devops,  scala, linux, all the sql stuff and way more I'm forgetting.

Which of these are interchangeable? My understanding is that azure and aws is? What about the hadoop, spark, kafka, hive thing? 

What technologies would you expect an entry level guy to know? Which of the things above are interchangeable? How do people learn all this stuff, do they just learn it on the job?

Can you enter data engineering with no work experience, but a CS degree? Or is it the case that data engineering isn't an entry level role at all and you need to work x years as a software engineer? If that is the case, what type of software engineer would be best? Java? Doesn't matter? Devops (whatever that is?). Do most regular software engineers learn and pick up on this technology from just a regular software engineering job, or do most regulars never even touch most of the technology mentioned?"
5116,2021-02-15 21:42:17,1613418137.0,dataengineering,Raw log fileto practice data extraction,lklbg9,SafeStandard,,https://www.reddit.com/r/dataengineering/comments/lklbg9/raw_log_fileto_practice_data_extraction/,1.0,0.0,0.0,25623.0,I am trying to practice log extract and analysis using Python. Where can I find log file to do so?
5117,2021-02-16 01:20:26,1613431226.0,dataengineering,Python &amp; SQL knowledge needed for ETL?,lkpzwc,veeeerain,,https://www.reddit.com/r/dataengineering/comments/lkpzwc/python_sql_knowledge_needed_for_etl/,1.0,14.0,0.0,25633.0,"Hello, I was on this sub a few days ago asking general questions about DE and how to get started. I did some research myself and aggregated some tools needed to learn and wanted to just run them by some of you to know if this is a good place to start.


Python

- I have been using python for almost a year now, I have worked with pandas/matplotlib/sklearn/seaborn/tf 

- now I’m trying to just get better at general purpose progeamming. Oop design, unit testing, general software development skills

- of the things needed to do DE in python, what packages are relevant to building pipelines. I’m sure the most relevant out of the ones I’ve used is Pandas, but are there any others that would be needed?

-any other general python skills needed?


SQL

- this is where most of the focus is going to be, I have zero knowledge in SQL and to learn DE this is a huge thing to know, so up till what knowledge do I need to know SQL ?

- any specific concepts I should have mastered? 



Apache Airflow

- I was going to learn this after I had working knowledge of SQL and stronger python knowledge 

- is this the right thing to learn after mastering the above two? Or should I be learning something else before this?

-any thing else that I suggest learn?



Nanodegrees/courses

- I was going to try and learn the more advanced DE tools through a course of some kind online

- I wanted to do a certification program so it would be high quality and I’d learn the right things

- I’m a sophomore in college, and so I wanted to learn this for myself and for making my resume stand out a bit more

- what are your opinions on some of these courses?

- Udacity Nanodegree program for data engineering 

- DataQuest Data Engineering track

- Data Camp Engineering Track


Is the udacity course worth paying for? Should I spend the money on any courses to learn this stuff or is there an alternative? I wanted a guided course style so I knew what I was doing. 

Any other concepts/languages/tools/packages you think worth mentioning and learning to do a simple ETL please list! I want to make sure I allocate my time efficiently and not learn things I don’t need to."
5118,2021-02-16 03:10:26,1613437826.0,dataengineering,Just bombed a technical timed assessment..,lks4og,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/lks4og/just_bombed_a_technical_timed_assessment/,1.0,29.0,0.0,25640.0,"I’m writing this mainly to vent but maybe others can take away sinething useful about performing under pressure.

I was given a timed assessment to do online in under 60 min. It was all sql. Which is something I’m pretty comfortable with. A lot more than algo and ds type questions. So I went in pretty confident.

Format was 5 questions and you can only move onto next after you submit. You can run tests to see if any fail before you submit.

First 2 questions I did under 10 min. And I’m thinking great I have tons of time. 

Third question I can only get 4 out of 5 to pass. And I get stuck on debugging. I even rewrite my solution and still no luck. Can’t get one test to pass.  By the time I live on I only have 20 min left for question 4 and 5. 

Again same issue for 4. All tests pass except 1. I think there’s some edge case in the data which is causing that final test to fail. And I suspect its the same reason as the 3rd question.

I look at the time and I have less than 5 min left for the last question.

I get to it and it’s an open ended, read the problem and tell us how you think type of question. At this point I was stressing out and had to reread the question and before I could even think to answer it time ran out. 

So end result is I got 2 to pass. Attempted 2 and didn’t even attempt one.

Kind of bummed out. I definitely didn’t pace myself properly. I should not have been stubborn about solving q3 and tht way I would have at least attempted all questions.

I’m also disappointed in myself because I considered my sql to be strong. At my work I’ve never had an issue with it and people usually ask me for help in tuning their sql. So that fact that o bombed an initial assessment screen is definitely hurting my ego.

I have faang interview in a week. That has twice the amount of questions in the same amount of time and being both sql and ds&amp;a. So I’m not feeling great about my prospects.

TLDR: go into a timed interview with a plan and pace yourself. Don’t spend more than the time you allocate for each question in advance. Attempt them all and then go back to fix issues if time permits. Basic test strategy advice but I’ve always been bad at timed tests/exams."
5119,2021-02-16 04:50:51,1613443851.0,dataengineering,Career path for a Data Engineer,lktz66,dra_9624,,https://www.reddit.com/r/dataengineering/comments/lktz66/career_path_for_a_data_engineer/,1.0,2.0,0.0,25641.0,"I’m curious about what a career path to becoming a data engineer could look like. Through my current role, I do a lot of work that touches Data Engineering and Data Science duties, but would like to solidify my role in the data engineering side. I’ve got experience with Python, REST apis, Sql, Azure Data Warehouse and other tools and frameworks. Is this enough to make the jump? Is there an intermediary role that I should consider looking for? If/when becoming a data engineer, where do I go from there?

Any advise for career development or skill development is appreciated!"
5120,2021-02-16 08:47:14,1613458034.0,dataengineering,https://www.sanfoundry.com/contests/,lky3sj,Thin-Pop3028,,https://www.reddit.com/r/dataengineering/comments/lky3sj/httpswwwsanfoundrycomcontests/,1.0,0.0,0.0,25647.0,If anyone in this helpful.
5121,2021-02-16 09:17:21,1613459841.0,dataengineering,Data Manipulation in R Using the dplyr Package (Tutorial),lkyjub,JoachimSchork,,https://www.reddit.com/r/dataengineering/comments/lkyjub/data_manipulation_in_r_using_the_dplyr_package/,1.0,3.0,0.0,25649.0,"Hey, I've created an introduction to the dplyr package in R: [https://statisticsglobe.com/dplyr-r-package](https://statisticsglobe.com/dplyr-r-package)"
5122,2021-02-16 13:59:29,1613476769.0,dataengineering,A deep dive into Autoscaling Apache Flink with Ververica Platform Autopilot,ll2cqo,Marksfik,,https://www.reddit.com/r/dataengineering/comments/ll2cqo/a_deep_dive_into_autoscaling_apache_flink_with/,1.0,0.0,0.0,25654.0,
5123,2021-02-17 00:03:39,1613513019.0,dataengineering,Using pyspark.sql for word count and getting a blank as my top word - how to fix?,llevjf,jc-de,,https://www.reddit.com/r/dataengineering/comments/llevjf/using_pysparksql_for_word_count_and_getting_a/,1.0,1.0,0.0,25684.0,"    # my code 
notes = df.select(""data_id"", ""notes"")
    notes = notes.withColumn(""notes"", f.regexp_replace(""notes"", ""\'"", """"))
    notes = RegexTokenizer
    tokenizer = Tokenizer(outputCol=""words_from_notes_column"")
    tokenizer.setInputCol(""notes"")
    tokenized = tokenizer.transform(notes)
    sw = StopWordsRemover(inputCol=""words_from_notes_column"", outputCol=""cleaned_words_array"", stopWords=stop_words_list)
    removed = sw.transform(tokenized)
    
    # Word Count 
    word_count = removed.withColumn(""word"", f.explode(f.col(""cleaned_words_array""))) \
    .groupBy(""word"") \
    .count() \
    .sort(""count"", ascending=False) 

It's returning the table below: 

https://preview.redd.it/0uvnyz50xwh61.png?width=179&amp;format=png&amp;auto=webp&amp;s=dfad5c439496ddf16e820d5312b4a0f630d7ceab"
5124,2021-02-17 10:29:14,1613550554.0,dataengineering,Spatial Data Science Conference #SDSC21: 3 Events for 2021,llpqjg,CARTO_com,,https://www.reddit.com/r/dataengineering/comments/llpqjg/spatial_data_science_conference_sdsc21_3_events/,1.0,0.0,0.0,25707.0,
5125,2021-02-17 11:34:02,1613554442.0,dataengineering,Airflow: Optimal configuration when running hundreds of tasks (Help needed),llqo7d,CryptKeepersBrother,,https://www.reddit.com/r/dataengineering/comments/llqo7d/airflow_optimal_configuration_when_running/,1.0,4.0,0.0,25708.0,"  
When running DAGs that contain over hundred quite heavy tasks that are run every 5 minutes I encountered some problems.  
1. Task runs fail since they were all pushed to the default\_pool with size 128. This is mentioned in the documentation [https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#pools](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#pools)  
To fix this I added several pools where I distribute my tasks and it seemed to work to some extent.  
a) What would be a good way to estimate the number of slots in each pool?  
b) Is there any downside of having lets say 50 different pools?

2. I ran backfill from the cmd-line, but some of the dags stay in the running state forever, since they contain tasks that stay in the queued state. Since the dag runs are not completed, the backfill doesn't progress further(max\_active\_dag\_runs is set to 4 in the airflow.cfg). 

I guess the main question is, how should I configure my setup so that I can run these dags smoothly in the future? My test environment has plenty of resources and I can always ask more.

Setup:  
I'm are testing out Airflow v. 1.10.14 with local executor and psql. The plan is to move to 2.0 with Celery+Redis+PSQL and several processing servers in the near future."
5126,2021-02-17 20:40:23,1613587223.0,dataengineering,It's a shame that companies like Telsa do this,lm1esh,smccaffrey,,https://www.reddit.com/r/dataengineering/comments/lm1esh/its_a_shame_that_companies_like_telsa_do_this/,1.0,7.0,0.0,25737.0,"&amp;#x200B;

https://preview.redd.it/l8r0s8zi13i61.png?width=1351&amp;format=png&amp;auto=webp&amp;s=f7d9d6b061dac7ade6e92349c3b600daa12d2021"
5127,2021-02-17 21:31:12,1613590272.0,dataengineering,"OpenTelemetry Specification v1.0.0, Tracing Edition",lm2jxz,sucker03,,https://www.reddit.com/r/dataengineering/comments/lm2jxz/opentelemetry_specification_v100_tracing_edition/,1.0,0.0,0.0,25738.0,"OpenTelemetry Specification v1.0.0, Tracing Edition"
5128,2021-02-17 22:28:11,1613593691.0,dataengineering,Schema migrations on Redshift,lm3te6,JoaoVasques,,https://www.reddit.com/r/dataengineering/comments/lm3te6/schema_migrations_on_redshift/,1.0,4.0,0.0,25740.0,"Hello everyone 🙌

At my company we're going to start using Redshift. I used it a couple of years ago but I don't remember if it has something to perform schema migrations? Would something like Sqlalchmey work for instance (guess no).. I'm not finding anything which means there must be something regarding how Redshift works that makes it hard to have such things. 

Any hints? Thanks!"
5129,2021-02-18 00:46:11,1613601971.0,dataengineering,Google Cloud Data Engineer (Professional Certification),lm6wv2,lwilson747,,https://www.reddit.com/r/dataengineering/comments/lm6wv2/google_cloud_data_engineer_professional/,1.0,0.0,0.0,25749.0,
5130,2021-02-18 01:04:38,1613603078.0,dataengineering,Intuit's Data Mesh Strategy,lm7bna,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/lm7bna/intuits_data_mesh_strategy/,1.0,5.0,0.0,25749.0,"Interesting article from Intuit on how they build their data mesh. Would be curious to hear if data mesh (domain-oriented design) is something that others have applied outside of larger organizations. 

[https://medium.com/intuit-engineering/intuits-data-mesh-strategy-778e3edaa017](https://medium.com/intuit-engineering/intuits-data-mesh-strategy-778e3edaa017)"
5131,2021-02-18 02:37:25,1613608645.0,dataengineering,Hello! 👋 Sharing my idea in making data storytelling easy for businesses. Hoping to get some feedback. Thank you!,lm99sz,joeyologyph,,https://www.reddit.com/r/dataengineering/comments/lm99sz/hello_sharing_my_idea_in_making_data_storytelling/,1.0,1.0,0.0,25753.0,
5132,2021-02-18 03:16:57,1613611017.0,dataengineering,"Airflow repo template - run Airflow locally by cloning and running ""make start-airflow"", comes with DAG &amp; plugin unit tests, linting, and docker-compose. Now supports Airflow 2.0",lma2ji,youngstunna24,,https://www.reddit.com/r/dataengineering/comments/lma2ji/airflow_repo_template_run_airflow_locally_by/,1.0,8.0,0.0,25753.0,
5133,2021-02-18 03:48:05,1613612885.0,dataengineering,Redis vs Aerospike,lmaodx,fake_actor,,https://www.reddit.com/r/dataengineering/comments/lmaodx/redis_vs_aerospike/,1.0,3.0,0.0,25754.0,Anyone have experience using both? Seems like redis is a lot more popular than aerospike and I was wondering if there was a reason for that.
5134,2021-02-18 04:55:51,1613616951.0,dataengineering,More depressed as an employed data engineer than when I was unemployed,lmbz4n,DSWannaboy,,https://www.reddit.com/r/dataengineering/comments/lmbz4n/more_depressed_as_an_employed_data_engineer_than/,1.0,20.0,0.0,25755.0,"First off, I have no dependent, I'm working from my parent's house. Maybe that is why.

They said data engineering grew by 40% while data science only grew by 10%. It wasn't like I had a choice anyway, so I took a data engineering job.

I'm basically doing a bitchwork for people who are less qualified than me. They are nice, but it is also quite depressing, when none of them has a graduate degree while I have a master's and have been trying to get into data science so hard. 

I don't think it gets better, and manager won't change my job because data engineers are harder to find. Can anyone help me and to be blunt so that I can change my attitude?"
5135,2021-02-18 05:06:05,1613617565.0,dataengineering,"Exploring Online Analytical Processing Databases plus Extract, Transform and, Load in PostgreSQL",lmc663,amcquistan,,https://www.reddit.com/r/dataengineering/comments/lmc663/exploring_online_analytical_processing_databases/,1.0,0.0,0.0,25758.0,
5136,2021-02-18 06:45:26,1613623526.0,dataengineering,ELI5 Apache beam Vs Apache spark,lmdzrm,pbj800100,,https://www.reddit.com/r/dataengineering/comments/lmdzrm/eli5_apache_beam_vs_apache_spark/,1.0,3.0,0.0,25765.0,I'm new to data engineering so I've been googling this but I'm still very confused. The simplest explanation would be greatly appreciated. All I can understand is that they both allow you to process big data using parallel processing. Thank you
5137,2021-02-18 07:45:59,1613627159.0,dataengineering,Spark vs Pandas,lmf2kb,veeeerain,,https://www.reddit.com/r/dataengineering/comments/lmf2kb/spark_vs_pandas/,1.0,12.0,0.0,25767.0,"So as I’ve started learning data engineering I wanted to get the basics down of web scraping (html parsing as well as working with APIs) and was writing some scrapers for practice. As I was working I thought about how I wanted to store the data, whether it be in a pandas data frame or something I found online called spark.

I looked up spark cause I had no idea what it was and realized that it had almost the same exact type of functions as pandas, ie. It was a type of pandas dataframe, but called spark data frames.

My question is, what is the difference between spark and pandas, and why would I want to use spark? I even noticed spark has its own ml modeling functions emulating sklearn, and was wondering why I would use that?

My guess is that spark handles Big data better than pandas? Ie. If I was scraping something that say had 500,000+ rows, it would be better for me to store it in a spark dataframe than a pandas dataframe?

I just want to wrap my head around over why I’d want to use spark. To add to my confusion I also found out that there is a library called  “koalas” which is built for spark dataframe and has the same pandas like syntax.

Any explanation would be appreciated."
5138,2021-02-18 12:12:58,1613643178.0,dataengineering,Ideas to build a Regression Test Suite for Data Quality,lmjhv1,brendersplide,,https://www.reddit.com/r/dataengineering/comments/lmjhv1/ideas_to_build_a_regression_test_suite_for_data/,1.0,24.0,0.0,25779.0,"Hello Engineers,

We (the DE Team) at Nike are trying to build a framework to ensure data quality. Immediate problems that we are trying to solve are,

1. Functional Unit Testing - Ensuring a data match between Target &amp; Source systems
2. Data Quality Tests - Use something like Amazon Deequ to define data quality checks
3. Performance Testing - Test job completion time, memory utilization for high volume and spike in data, etc.
4. Automation - UT/Perf Test/Integration Tests should be made part of PR processes.

# Question:

If anyone in the past has worked on/created such a framework, can you please share your inputs on how to get started? We plan to build one from scratch.

Any suggestions or inputs one might want to add are more than welcome. Thanks in advance!"
5139,2021-02-18 13:57:52,1613649472.0,dataengineering,FAAnG resume,lml5fs,powok,,https://www.reddit.com/r/dataengineering/comments/lml5fs/faang_resume/,1.0,7.0,0.0,25783.0,"Hi guys , 

I feel a little weird and ashamed but does someone have a sample resume of someone working in any FAAnG as a DE.

I would like to take a look of how a good DE resume should be tailored.

I want to apply for the same.

:D"
5140,2021-02-18 16:10:29,1613657429.0,dataengineering,Databricks &amp; Google,lmnlj6,txtechzit,,https://www.reddit.com/r/dataengineering/comments/lmnlj6/databricks_google/,1.0,18.0,0.0,25791.0,"Very interesting press release yesterday. Curious if any of you have worked with both Databricks and Googles analytic tools and how you might see this working in future?

&amp;#x200B;

[https://cloud.google.com/blog/products/data-analytics/databricks-on-google-cloud?utm\_source=twitter&amp;utm\_medium=unpaidsoc&amp;utm\_campaign=FY21-Q1-GC-Blog&amp;utm\_content=-&amp;utm\_term=-&amp;linkId=111545718](https://cloud.google.com/blog/products/data-analytics/databricks-on-google-cloud?utm_source=twitter&amp;utm_medium=unpaidsoc&amp;utm_campaign=FY21-Q1-GC-Blog&amp;utm_content=-&amp;utm_term=-&amp;linkId=111545718)"
5141,2021-02-18 16:30:25,1613658625.0,dataengineering,Looking for feedback - Build your own data engineering platform with Blacksmith,lmo093,loicsaintroch,,https://www.reddit.com/r/dataengineering/comments/lmo093/looking_for_feedback_build_your_own_data/,1.0,1.0,0.0,25791.0,
5142,2021-02-18 16:48:05,1613659685.0,dataengineering,Curated Github repository on how organizations around the world use dbt,lmod2e,smomni,,https://www.reddit.com/r/dataengineering/comments/lmod2e/curated_github_repository_on_how_organizations/,1.0,2.0,0.0,25792.0,"I've set up a knowledge repository of dbt best practices. The idea is to gather dispersed online resources and curate them in a single repository.

Contributions are more than welcome!

https://github.com/smomni/howtheydbt"
5143,2021-02-18 18:59:14,1613667554.0,dataengineering,Getting started with the Data Mesh,lmrd3g,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/lmrd3g/getting_started_with_the_data_mesh/,1.0,1.0,0.0,25802.0,"Found this article on some high-level best practices / primer of sorts, but curious if anyone has actually successfully implemented a data mesh architecture in practice. Inquiring minds want to know...

[https://towardsdatascience.com/data-mesh-101-everything-you-need-to-know-to-get-started-72087f5a7d91?source=friends\_link&amp;sk=50d707157c6087a8a95e0e92ee1b0789](https://towardsdatascience.com/data-mesh-101-everything-you-need-to-know-to-get-started-72087f5a7d91?source=friends_link&amp;sk=50d707157c6087a8a95e0e92ee1b0789)"
5144,2021-02-18 19:03:36,1613667816.0,dataengineering,Data Mesh 101: Everything You Need to Know to Get Started,lmrhqo,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/lmrhqo/data_mesh_101_everything_you_need_to_know_to_get/,1.0,0.0,0.0,25802.0,
5145,2021-02-18 20:04:39,1613671479.0,dataengineering,Up skill to Data Engineering from BI,lmsx70,sri2014,,https://www.reddit.com/r/dataengineering/comments/lmsx70/up_skill_to_data_engineering_from_bi/,1.0,4.0,0.0,25807.0," I need small guidance. I am into BI from past 10 years cognos reporting and find it difficult to keep updated to the dynamics of the tool popularity, now cognos is not used much. I am thinking if i can upskill to Data Engineering. I know python. Can you give some practice example, how python can be used in data engineering. I am confused. I see people do data massaging with python but where does it fit in. there are tools like airflow for that. I am really confused. Please help."
5146,2021-02-18 20:22:39,1613672559.0,dataengineering,Batch Data Ingestion Pipeline Metrics,lmtdhi,BigBooledHead,,https://www.reddit.com/r/dataengineering/comments/lmtdhi/batch_data_ingestion_pipeline_metrics/,1.0,1.0,0.0,25810.0,"What are metrics I should be collecting in batch data pipelines? 

Should I be collecting things like Rows Ingested, Count of Nulls etc within the data ingestion pipelines?

what are considerations I should be making in regards to pipelines outside of the actual take the data and put it in something like Bigquery. We follow an elt methodology, so my main concern is the EL portion and collecting any information around that.

&amp;#x200B;

Let me know if I should elaborate a little more."
5147,2021-02-18 21:30:11,1613676611.0,dataengineering,Census - Copy Data from Snowflake to SaaS Apps,lmv00e,bradleybuda,,https://www.reddit.com/r/dataengineering/comments/lmv00e/census_copy_data_from_snowflake_to_saas_apps/,1.0,0.0,0.0,25817.0,
5148,2021-02-18 23:07:29,1613682449.0,dataengineering,Extract data from a website - Webcrawling,lmxa05,akeebismail,,https://www.reddit.com/r/dataengineering/comments/lmxa05/extract_data_from_a_website_webcrawling/,1.0,1.0,0.0,25822.0,"I’ve this tasks I’m working on to extract data from a website, save the data in the database.
There’s a search box on the website where one can put a name of an item, and it return the list of items that match the name input. I want to: 
- build an alphabet permutator
- build the scrapper 
- save the items in the dB 
The major challenge is this website can be updated anytime, so I created a cron to do the scrapping every weekend 
I don’t know if there’s an algorithm or any idea or a process while the scrapping is going on  to detect if I’ve some of the items in my dB so it can skip it and scrap the new one added."
5149,2021-02-19 00:16:27,1613686587.0,dataengineering,Learn how to work with pandas and s3,lmywv4,shawemuc,,https://www.reddit.com/r/dataengineering/comments/lmywv4/learn_how_to_work_with_pandas_and_s3/,1.0,0.0,0.0,25827.0,
5150,2021-02-19 02:16:00,1613693760.0,dataengineering,UiPath (RPA) and Data Engineering,ln1kxs,throwback772,,https://www.reddit.com/r/dataengineering/comments/ln1kxs/uipath_rpa_and_data_engineering/,1.0,5.0,0.0,25838.0,"Hi all,

I wanted to ask if I should invest my time learning UiPath, if I want to get into data engineering,

I’m a beginner and wanted to find out if there’s any use?

Thank you"
5151,2021-02-19 02:22:13,1613694133.0,dataengineering,Help.,ln1pry,Natural-Canary-3146,,https://www.reddit.com/r/dataengineering/comments/ln1pry/help/,1.0,4.0,0.0,25838.0,"Hi,

I'm a new graduate thrown into a data engineering role at a startup with little guidance/idea wtf is happening. It is not going well, and my question will likely illustrate the tedium which I have been delegated: Does anyone know how to replicate the DATEDIFF(week, date1, date2) function in python/pyspark/sparkSQL? I am migrating code our founder wrote ages ago from SQL Server to Spark SQL and have found numerous disparities between these sql flavors' built-in functions, including that SQL Server's DATEDIFF(week ) counts weeks by how they fall on a calendar \[ex: saturday and monday are 1 weeks apart, since they are on opposite sides of sunday, when the week begins to SQL Server\] whereas Spark SQL counts proportion-of-a-seven-day-period \[ex: saturday and monday are 2 days apart, thus floor(2/7) = 0 weeks apart\]. For god knows why, my boss cares about the off-by-one-week errors that this discrepancy sometimes causes.

Or, more generally, any advice on the highlights/key takeaways/best things I should learn about data engineering while I am in this role? I am learning way too much about SQL and not enough about the discipline. This job is actually interesting, right?"
5152,2021-02-19 03:40:33,1613698833.0,dataengineering,DB to DB realtime E/L,ln3bti,redder_ph,,https://www.reddit.com/r/dataengineering/comments/ln3bti/db_to_db_realtime_el/,1.0,0.0,0.0,25841.0,"I am working on setting up a data pipeline to move data from DB to DB. Basically need to move data when a specific data condition is triggered. The source data is an OLTP db, and target is a DW. The pipeline has to be built on top of an open-source tech stack. The data load is not very high, probably around 500K records/day. 

The data load could increase over time. I am thinking of a Nifi orchestrated pipeline with connectors for source and target DB, with kafka as the middleware.

Since there is no heavy transform needed here, I am thinking an airflow DAG would be an overkill here.

Looking for any suggestions or things I might be missing."
5153,2021-02-19 04:35:12,1613702112.0,dataengineering,Data-Ops-ish Design and Workflow Walkthrough,ln4fi7,Jwelch25,,https://www.reddit.com/r/dataengineering/comments/ln4fi7/dataopsish_design_and_workflow_walkthrough/,1.0,36.0,0.0,25845.0,"In [another thread](https://www.reddit.com/r/dataengineering/comments/lmjhv1/ideas_to_build_a_regression_test_suite_for_data/), [u/brendersplide](https://www.reddit.com/user/brendersplide/) asked about building a regression test suite. In response to my comment, there were a few individuals who wanted to know more about the system I am currently running. ( u/isleepbad and u/htrp\*\*)\*\* It's more than what should be in a reply so I thought I would do a post for anyone else interested.

WARNING: This is a long post.

DISCLAIMER: This is, by no means, the best way to do things. We are constantly evolving and tweaking.

&amp;#x200B;

As a little context, I run data and analytics at a start-up-ish financial company and have a very small team. In order to combat that constraint, we have done a lot to automate our workflow. We also rely on open-source as much as possible. We've tried to infuse this thought process into everything we do - from onboarding new hires to reproducing the entire infrastructure.

I'll break it down into the following sections:

1. Storage
2. ELT
3. Testing and QA
4. Infrastructure
5. Data Workflow Automation
6. Development
   1. Analytics
   2. Engineering
7. CI

**Storage**

Tools Used:

* [AWS S3](https://aws.amazon.com/s3/)
* [Snowflake](https://www.snowflake.com/)

The storage is quite simple. We use S3 as a landing zone from the vast majority of sources. We then pick up those files and load them into Snowflake. I call this the **Lake House** as it serves as both our Data Lake and Data Warehouse. What is really beneficial about Snowflake is how it treats 'databases'. They aren't DBs in the traditional sense but more akin to workspaces. Because of that, you can have as many as you would like. This comes in handy when you want to practices [Gitlab's concept of Infinite Data Warehouses](https://www.youtube.com/watch?v=eu623QBwakc).

&amp;#x200B;

**ELT**

Tools Used:

* [Fivetran](https://fivetran.com/)
* [dbt](https://www.getdbt.com/)
* python

We use Fivetran for third-party vendors. dbt is used in the Lake House. Once data arrives in the raw databases, dbt takes over the management. If you aren't familiar with this tool, make it your best friend. It allows you to write SQL and YAML to define your data models. It also comes with super handy features like test definition, lineage, and document generation.

Because of Snowflake's awesome ability to scale, we bring all data in its raw format and then do our transformations with dbt.

For data from source systems, we keep it easy with CRON jobs running python scripts. This is very manageable because we are essentially just replicating the source data into the S3 Landing Zones. The only transformation that should be taking place at this step is the removal of PII.  

&amp;#x200B;

**Linting, Testing, and QA**

Tools Used:

* [dbt](https://www.getdbt.com/)
* [SQLFluff](https://github.com/sqlfluff/sqlfluff)
* [pylint](https://pylint.org/)
* [black](https://pypi.org/project/black/)
* [Great Expectations](https://greatexpectations.io/)

For linting, things are pretty straightforward on the python front. We use pylint and black to enforce a standard. From a SQL point of view, [SQLFluff is a new player to the field filling a much-needed space](https://www.youtube.com/watch?v=veYB9uh0RCM).

For testing, we use Great Expectations for data coming from source and arriving in the Lake House. Once in the Lake House we leverage a couple of dbt's packages to keep the workflow standardized. Firstly is a new port of Great Expectations syntax to dbt in [dbt\_expectations](https://github.com/calogica/dbt-expectations). This allows you to bake in tests as easily as

    models:
        - name:table_a
          description: Slightly modified copy of table_123
          tests:
              - dbt_expectations.expect_table_column_count_to_equal:
                  value: 27
              - dbt_expectations.expect_table_row_count_to_equal_other_table:
                  compare_model: source('raw_schema_a','table_123')
    
    - name: month_value
      description: Month number [01 -&gt; Jan, 02 -&gt; Feb, etc.]
      tests:
          - dbt_expectations.expect_column_to_exist    

We also use [dbt\_meta\_testing](https://github.com/tnightengale/dbt-meta-testing). This checks all your models to ensure all tables and columns have descriptions and that all required tests are defined. This is important because we then leverage `dbt docs generate &amp;&amp; dbt docs serve`  which auto-generates our documentation and data lineage.

&amp;#x200B;

**Infrastructure**

Tools Used:

* [Terraform](https://www.terraform.io/)

We use Terraform of Infrastructure as Code (IaC). All of our infra is terraform'd, including our Snowflake instance. Terraform offers a [Snowflake provider](https://github.com/chanzuckerberg/terraform-provider-snowflake) which allows you to create tables, users, roles, schemas, etc. Given all of our managed infrastructure resides in AWS and Snowflake, everything is Terraformed.

&amp;#x200B;

**Data Workflow Automation**

Tools Used:

* [Prefect](https://www.prefect.io/)

There is a litany of tools out there for this (Airflow, Luigi, etc.) but we settled on Prefect for its simplicity. It focuses on data orchestration and does it very well. It also operates on what they call a hybrid model. This means that, if you are using their cloud UI, all they see is meta-data. None of your actual data is processed outside of your infrastructure. 

You still need to have an agent running somewhere in-house though. For that, we are using [AWS ECS](https://aws.amazon.com/ecs/). Again this is Terraformed. 

Prefect comes with tons of integrations, including Great Expectations and AWS. It is written in python so it works with our choice of analytics language. And it's Jupyter notebook plugin allows you to leverage the power of [papermill](https://github.com/nteract/papermill) to orchestrate, schedule, and run parameterized notebooks. Very handy for deploying models with all the benefits that come with notebooks.

 

**Development**

&gt;!I give all credit for this idea to the GitLab data team!&lt;

All of our actual data modeling, analysis, ml work is containerized in [Docker](https://www.docker.com/). Depending on the work you are doing you spin up the appropriate image.  This has the benefit of not only being able to jump into an environment that has everything you need to do your work but also ensures that everyone and everything is running on the same baseline.

We also take advantage of Docker's volume bind so that changes made to the directory outside of the container are immediately reflected inside the container. This is important because it allows you to use your code editor ([VSCode](https://code.visualstudio.com/) in our case) and not have to operate in something like vim.

**- Analytics -**

We have an in-house `data-image` that comes pre-loaded with the familiar cast. Pandas, scypi, Jupyter, they are all installed and ready to be used. We also built our own helper libraries that make connecting to and extracting data easier. The goal here is to reduce all friction that exists when you want to start an analysis. Get it all out of the way so you can just do your work.

**- Engineering -** 

Likewise, we have `dbt_image` and `prefect-image` which act in much the same way. They are all wired up so you can focus on the actual work instead of the technology. 

&amp;#x200B;

**CI**

Tools Used:

* [Github Actions](https://github.com/features/actions)

Great so we have all of these tools - what about the dev-op-y stuff?

For that, we use GitHub Actions (but Gitlab is super powerful as well and I use it for my personal projects). As you may have noticed, everything we do has been codified. That allows us to create Actions that can handle:

* All of our testing and linting by defining steps accordingly
* Updating and provisioning our infrastructure with Terraform. Need to add a user? Cut a branch, add then to the `users.yml` file with the access they need and open a Merge Request. 
* Need to create a replicated environment to try something new in AWS? Easy as changing the name.

But the real power comes in the use of the Docker images mentioned earlier. Because all of our workflows is containerize, we can then use those exact containers in both CI pipelines and production deployments. I know that if it works locally it should work all the way through. 

Finally, using the concept of infinite data warehouses, you can have a Github action that goes up and replicates Lake House prod into a dev database for you. You can do whatever you want and it won't harm production. And then when you are done we can tear it down. 

&amp;#x200B;

You've made it this far. I am happy to answer questions. I am sure I have forgotten something. And I am positive that this will evolve and there are better solutions waiting to be uncovered."
5154,2021-02-19 04:48:41,1613702921.0,dataengineering,pqrs: A parquet-tools replacement in Rust using Apache Arrow,ln4p67,pragmaticPythonista,,https://www.reddit.com/r/dataengineering/comments/ln4p67/pqrs_a_parquettools_replacement_in_rust_using/,1.0,1.0,0.0,25846.0,
5155,2021-02-19 05:11:09,1613704269.0,dataengineering,What to Learn for Health Care Industry,ln55kb,ambiguouslyforeign,,https://www.reddit.com/r/dataengineering/comments/ln55kb/what_to_learn_for_health_care_industry/,1.0,3.0,0.0,25848.0,"To anyone with healthcare experience:

What technology should I focus on if I am interesting in working with things like:
- Health insurance claims data 
- Working with different providers in a managed care network 
- Research on utilization, finances using CMS data

What type of DBMS? What cloud technology? If I were to do a cloud certification, which kind? 

I have about 250 hours I can realistically put towards this but not a lot of money. And I’m more looking to bolster my skills for analyst/health economist type roles than become a data engineer. (Unless I really like learning this stuff, and it looks like I might)

I can do all kinds of fancy queries in SQL, but mostly because I’ve done a lot of leetcode/hackerrank challenges. I

Any guidance would be really helpful, I am spending too much time trying to figure this out and not enough time learning. Thanks!"
5156,2021-02-19 07:49:53,1613713793.0,dataengineering,How to combine fact tables?,ln88mq,Memorabilia21,,https://www.reddit.com/r/dataengineering/comments/ln88mq/how_to_combine_fact_tables/,1.0,1.0,0.0,25854.0,"I was tasked with creating some proofs of concepts for some BI solutions at my company. I want to start with creating OLAP cubes in a data warehouse, of sorts. A project I want to focus on will use a Produced Quantity and a Returned Quantity given a date range. Both of these, I thought, would be two different fact tables; however, I want to show them both in the same table along with a Return Rate (Returned / Produced Quantities). Does anyone have any suggestions of how to go about doing this? Do I need to create one fact table that shows a Produced Quantity and Returned Quantity along with the Returned Rate, or do I need to create two separate fact tables and somehow relate them to one another and do the calculation in a BI tool (If so, how?)?"
5157,2021-02-19 08:53:07,1613717587.0,dataengineering,confused about hadoop tutorials,ln9cp1,data-eng,,https://www.reddit.com/r/dataengineering/comments/ln9cp1/confused_about_hadoop_tutorials/,1.0,3.0,0.0,25856.0,"I'm very confused about Hadoop. I understand that a Hadoop cluster is a collection of computers/nodes that allow a user to use the HDFS to store data across all these computers and use something like Mapreduce or Spark to process data across all these computers. However I'm confused about how one actually sets up a Spark job with Hadoop. I've watched a couple tutorials and I don't know if it's just been simplified for demonstration or if I'm missing something. In these tutorials, someone has set up a virtual machine and then installed Hadoop on just that machine and used the command line to bring data into the HDFS. This is not actually a Hadoop cluster because it's literally just Hadoop installed on a single computer/node, right? So when they demonstrate running a Spark job on this VM, there's not actually any benefit as nothing is processing in parallel - everything is just running on a single node? So how does one go about creating a cluster? In theory, say I had multiple physical computers, I would need to install Hadoop on each one and then *do something* to connect them all so that they're clustered together?"
5158,2021-02-19 09:13:48,1613718828.0,dataengineering,If my data/BI team uses SQLAlchemy/PySpark for doing ETLs does dbt (data build tool) add additional value? (apart from some of dbt's documentation features),ln9pio,adyslexic,,https://www.reddit.com/r/dataengineering/comments/ln9pio/if_my_databi_team_uses_sqlalchemypyspark_for/,1.0,8.0,0.0,25856.0,"Our team uses SQL based ETL processes using SQLAlchemy for mid sized data and Spark SQL on PySpark for relatively big data. dbt seems to be creating some buzz in the industry for ETL creation and ETL management using just SQL, so was wondering does it add any more value?"
5159,2021-02-19 11:23:51,1613726631.0,dataengineering,Writing Production Code,lnbq5v,Omar_88,,https://www.reddit.com/r/dataengineering/comments/lnbq5v/writing_production_code/,1.0,0.0,0.0,25861.0,"Hello All,

For those of us increasingly using databricks in engineering pipelines may feel the pain of not having their favorite IDE at their fingertips for writing code. 

I've thought about what would be a good alternative to writing code in the actual databricks environment and thought perhaps spinning up a docker container with a pyspark image, however these seem aimed at being used in Juypter."
5160,2021-02-19 11:51:06,1613728266.0,dataengineering,"Help wanted: Interview strategy approach, new grad",lnc57s,swiss_alpines,,https://www.reddit.com/r/dataengineering/comments/lnc57s/help_wanted_interview_strategy_approach_new_grad/,1.0,0.0,0.0,25861.0,"Hello! Sorry this is long. Looking for some feedback on what to focus on for a DE position interview I have coming up. I previously met with the person who would hypothetically be my boss, had an informational &amp; he invited me to formally apply. Me: completed undergrad in stem field 2 months ago from a top 3 university, female, 6m experience as DE intern @ startup; currently a FT client-facing data analyst. As an intern I helped with a number of projects like migrating a key lambda function to serverless, reviewed/tested a rest api boilerplate &amp; a custom data ingestion processor. I can speak to each of these projects &amp; my role in them but I’d say I’m a beginner Py/Js and intermediate SQL/AWS level. I’ve been honest with them about my skills &amp; experience. 

I didn’t take DS in school and only transitioned into CS barely a year ago. I can follow along in a pair programming or PR review sesh, can identify dependencies, test locally, tell you how it relates to your data warehousing &amp; cloud service provider but struggle with granular language nuances/syntax and pushing questions further than high level follow up.

I have a broad understanding of database, cloud, basic servers, and analytic stuff. I can give you a definition, some pros and cons and maybe 1 alternative method/approach to consider but nothing too much beyond that. I frequently help clients with their data questions, am highly motivated, and can speak to multiple examples of where I’ve succeed jumping into projects I literally knew nothing about/had very little support with developing. 

This particular company wants data integration, management, and modeling fundamentals; SQL, Py, AWS and a couple data warehouses. Based on the team this role is in, I get the impression they want a more analytics-focused hire. The interview process is 3 rounds of interviews (1 with same person I met previously, 1 with the team I’d be working on &amp; 1 with a cross functional team). There’s a take home assignment writing a script to connect to one of their APIs. I feel confident I would be successful in this part &amp; am planning to prepare throughly. 

I am wondering what people here think I should focus on realistically and what to expect in these interviews? I’m assuming 1 round is going to be primarily covering that take home project &amp; my background, 1 will be walking through some industry experience I have where I need to talk about how I contributed to 1-2 projects in depth (STAR method) and 1 more behavioral/culture fit on the cross functional team where I can speak to client experience, communication, data’s relationship across teams, etc). 

So far I just created a leetcode account today (lol) and am focusing on easy Python/SQL, am going to review their tech stack (pros/cons, make sure to speak to any experience I have with it throughly) and their fundamental “asks” (ie broadly integration, management, modeling) in context of said stack/tools they use at least at a high level, and have created a document listing interview questions I think I may be asked with my rough answers. I’m planning on just being honest if I don’t know something but explaining how I’d go about finding the answer and that I’d document the process for other coworkers who have same problem in the future. 

Literally any insight would be super helpful if I’m on the right track, what types of stuff I should prepare for, etc. thank you!! Sorry this is long just wanted to make sure I was complete. :)"
5161,2021-02-19 12:25:59,1613730359.0,dataengineering,📼 Stream Processing with Apache Kafka - A mini video series 📼,lncp2y,rmoff,,https://www.reddit.com/r/dataengineering/comments/lncp2y/stream_processing_with_apache_kafka_a_mini_video/,1.0,0.0,0.0,25868.0,
5162,2021-02-19 15:45:09,1613742309.0,dataengineering,"Example of When to Use SQL, NoSQL and Both",lng6vr,timfcrn,,https://www.reddit.com/r/dataengineering/comments/lng6vr/example_of_when_to_use_sql_nosql_and_both/,1.0,0.0,0.0,25873.0,
5163,2021-02-19 16:14:21,1613744061.0,dataengineering,What should I learn after Airflow ?,lngtmp,corporatededmeat,,https://www.reddit.com/r/dataengineering/comments/lngtmp/what_should_i_learn_after_airflow/,1.0,9.0,0.0,25874.0,"Hi All ,

&amp;#x200B;

I have learnt how to work with Airflow and BigQuery , what should I be learning next I want to start  apply for data engineering jobs in next 5 month  and aspire to bag one at the end of 6th Month."
5164,2021-02-19 17:14:53,1613747693.0,dataengineering,"How possible is it for a beginner to establish pipelines, data warehouse, and visualization solution as a team of 1?",lni7ed,RichHomieCole,,https://www.reddit.com/r/dataengineering/comments/lni7ed/how_possible_is_it_for_a_beginner_to_establish/,1.0,11.0,0.0,25881.0,"Mods delete if not allowed please.

**Background**: Recent grad, solo data analyst for a small enterprise holding 3 distinct companies (think banking, construction, IT) with \~ 25 MM in top line revenue. I work directly for the Owner/CEO.. No senior analyst for guidance.

**Objective**:  Establish a data warehouse pulling data from all 3 companies &amp; their various CRM's and software under one roof from which it can be analyzed, visualized using something like Tableau (nothing too intense, just monitoring trends mostly. Would not call it big data. I'd estimated under 1TB without digging too far under the hood)

I've been given a deadline of a month to figure out how much outsourcing of knowledge is needed and i'm feeling a bit overwhelmed. On one hand, I want to impress my employer, but on the other, I have 0 experience in an operation like this and I don't want to fumble the ball. I am willing to do what it takes to study/read/research, but I am intimidated by the thought of architecting a project like this alone.

Have any of you taken on a similar project? I've started reading the data warehouse toolkit, but my gut tells me this project is much more than a 1 man undertaking. Am I making this out to be more difficult than it needs to be?"
5165,2021-02-19 17:27:37,1613748457.0,dataengineering,"If you want to learn how to work with pandas and S3, this might be an interesting read.",lnii7j,shawemuc,,https://www.reddit.com/r/dataengineering/comments/lnii7j/if_you_want_to_learn_how_to_work_with_pandas_and/,1.0,0.0,0.0,25882.0,
5166,2021-02-19 17:55:00,1613750100.0,dataengineering,"We Don’t Need Data Scientists, We Need Data Engineers - KDnuggets",lnj4lh,Pitiful-Cupcake-2991,,https://www.reddit.com/r/dataengineering/comments/lnj4lh/we_dont_need_data_scientists_we_need_data/,1.0,44.0,0.0,25884.0,
5167,2021-02-19 17:56:55,1613750215.0,dataengineering,Career advice - data engineering in public sector?,lnj6kb,ThrwawayDEadvice,,https://www.reddit.com/r/dataengineering/comments/lnj6kb/career_advice_data_engineering_in_public_sector/,1.0,4.0,0.0,25884.0,"Hi all,

\*\*sorry for the long post\*\*

I have been working in the public sector in Canada as a data analyst - the work is about 99% SQL/R data manipulation &amp; visualization, and about 1% ""advanced"" analytics (clustering, modeling, etc.). I have a BSc in Econ. Currently learning Python (&amp; Vim, Unix on OpenBSD, not really related to DE I know), but it's not yet available for me to use. I am in a second degree in CS program but couldn't manage part time study while working (and moving..) so had to pause it after my intro programming course, and I am agonizing over whether to drop it or make the jump and study full time in September 2021. 

My thinking was that focusing on developing a data engineering skillset would be a reasonable way to move forward given my current skillset and education, but I've found that the opportunity to do so in the public sector is limited (case in point: can't use Python yet, nor other modern data engineering tools such as spark etc). For various personal reasons I would prefer to remain in the public sector. 

Unfortunately, there is a tendency to get pigeon-holed based on educational background in public sector orgs from what I can tell. I've looked at positions in other orgs and it seems like it is always black and white, you're either a software dev with a CS degree doing web development etc., completely disconnected from the data analysis side, or you're completely on the data analysis side with a social science/traditional stats degree background. 

Basically, the data engineering role does not seem to exist in most public sector orgs, and with my current background I'd have to focus more on the data science/statistics side of things to advance. I would rather not do this, partly because I'm not really interested, and also because I feel that even if I was, my credibility without a graduate degree for doing this type of work would be low.  

I really enjoy the software side of things, programming etc. and feel like I'd be much happier doing either traditional software dev or data engineering, but the opportunity cost of a very good salary, cost of tuition, and lost time with family etc. is so large at this point that it seems like a very risky move to do a CS degree full time. 

Hoping someone can give me some inspiration/career advice, particularly if you've done data engineering work in the public sector/in Canada. My thoughts have been totally consumed by this problem for months.."
5168,2021-02-19 18:08:56,1613750936.0,dataengineering,Wallpaper,lnjgww,Altruistic-Canary-50,,https://www.reddit.com/r/dataengineering/comments/lnjgww/wallpaper/,1.0,0.0,0.0,25884.0,
5169,2021-02-19 19:19:03,1613755143.0,dataengineering,Free webinar on Monday: Airflow DAG authoring best practices,lnl7qe,rywalker,,https://www.reddit.com/r/dataengineering/comments/lnl7qe/free_webinar_on_monday_airflow_dag_authoring_best/,1.0,0.0,0.0,25890.0,
5170,2021-02-19 20:53:58,1613760838.0,dataengineering,PropTech Challenge Data Science Competition | $5k Cash Prize | Submissions due Mar 26,lnnfpc,PropTech_Challenge,,https://www.reddit.com/r/dataengineering/comments/lnnfpc/proptech_challenge_data_science_competition_5k/,1.0,4.0,0.0,25901.0,"Hey everyone,

Happy Friday! Hope you're all hanging in. We are looking for advice on how best to promote a data science competition we're running right now over at: [https://www.proptechchallenge.com/nyserda-tenant-energy-data](https://www.proptechchallenge.com/nyserda-tenant-energy-data)

As background, large NYC office buildings saw their occupancy rates drop by 90% on average last year due to COVID-19, but their energy consumption only dropped by 30%. While lease obligations and healthcare protocols contributed, there is still surprising room for previously unknown vampiric loads within these buildings. We now refer to this circumstance as **the Great Energy Disconnect**. 

Rather than leave it to the usual suspects (NYC building owners, managers, and their consultants), **the PropTech Challenge aims to democratize access** to the Great Energy Disconnect. Our website has **over 2.5 years of real-world data** from a Midtown Manhattan office building and the headquarters of a publicly traded tenant available for download. 

We are challenging data engineers and modeling enthusiasts to use our test set to **predict actual electricity consumption** in this headquarters on 8/31/2020 (the day after the test set ends). **Submissions are due by March 26, 2021** via upload on our website. The most accurate, eligible predictions will win **$5k cash**. 

Our test set has been downloaded **over 75 times by teams in 35 cities and towns on 5 continents** so far. We'd greatly appreciate your advice and assistance doubling these figures before our deadline! Solving the Great Energy Disconnect is crucial if New York is to achieve its climate leadership goals. Please join the fight!

Thank you in advance!"
5171,2021-02-19 21:03:15,1613761395.0,dataengineering,Create full-fledged APIs for static datasets without writing a single line of code,lnnnri,houqp,,https://www.reddit.com/r/dataengineering/comments/lnnnri/create_fullfledged_apis_for_static_datasets/,1.0,0.0,0.0,25902.0,
5172,2021-02-19 23:53:24,1613771604.0,dataengineering,Does it make sense to add Azure to this mix?,lnrdg9,pasticciociccio,,https://www.reddit.com/r/dataengineering/comments/lnrdg9/does_it_make_sense_to_add_azure_to_this_mix/,1.0,0.0,0.0,25916.0,
5173,2021-02-20 01:57:47,1613779067.0,dataengineering,how to update on conflict while pushing multiple rows of csv into a table on postgresql using sqlalchemy,lnu1sr,YaswanthBangaru,,https://www.reddit.com/r/dataengineering/comments/lnu1sr/how_to_update_on_conflict_while_pushing_multiple/,1.0,5.0,0.0,25922.0,"Hey folks, I am facing an error while pushing a bunch of csv rows in the  form of a dictionary into the postgresql using sqlalchemy, the issue is  that there are duplicates in the data and I would like to update it but  I can't figure that out while executing a dictionary instead of a  single row.

The code is somewhat like the follows:

    csv_rows=[]
    for row in rows_of_csv:
        values = {all dict key value pairs according to the column names in the table}
        csv_rows.append(values)
    
     stmt = insert(table) 
    result_proxy = connection.execute(stmt, csv_rows)

 ""And then an error is being thrown that there is a conflict with the id, of course  there is a conflict, however I want to update the row on a conflict, how  do I do that in this multiple csv rows setting?"" Would highly  appreciate if someone could through some light on it, thank you"
5174,2021-02-20 02:16:00,1613780160.0,dataengineering,JuNiOR DaTA EngINeeR,lnufos,runawayanimated,,https://www.reddit.com/r/dataengineering/comments/lnufos/junior_data_engineer/,1.0,12.0,0.0,25924.0,"Have a look at this Junior Data Engineer position with Huawei and the requirements. ""Open to internship too"". 

These aren't even the ""nice to have's"":

* Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline.
* Experience in Software Development, Data Engineering, Data Science, or related field with a track record of manipulating, processing, and extracting value from large datasets including visual data (image or video).
* Hands on experience with Deep Learning models with PyTorch
* Proficient programming skill in Python.
* Experience and understanding in Software Engineering and OOP.
* Strong customer focus, ownership, urgency, and drive.
* Excellent communication skills
* Effective analytical, troubleshooting, and problem-solving skills.
* Energetic, self-motivated and execution oriented individuals, passionate in building best in class products.
* Fluent in both English and Mandarin

[https://www.linkedin.com/jobs/view/2419666203/](https://www.linkedin.com/jobs/view/2419666203/)

Ridiculous."
5175,2021-02-20 04:08:24,1613786904.0,dataengineering,Scraping with selenium + docker,lnwmof,digichap28,,https://www.reddit.com/r/dataengineering/comments/lnwmof/scraping_with_selenium_docker/,1.0,4.0,0.0,25929.0,"Hey I’m currently trying to setup an airflow image with google chrome + chromedriver to be able to run some scrapers. I was able to install all the things needed and also was able to run a basic test.

On the basic test (python), I set the following chrome options :

* —headless
* —no-sandbox
* —disable-dev-shm-usage

And run the code...

driver.get(‘https://www.google.com’)
print(“title : %s” %driver.title)
driver.close()

All that works, but when I try to actually run a real case scenario in which i need to open a webpage which prompts a windows auth, and has lots of JavaScript. The —headless option isn’t useful because apparently I need the auth pop up appear as well as the objects I need to screenshot.

The point is, I don’t want to add the headless option. But if I do, the script fails.

I know my script works because I have tried it outside of a container and also with a multi container app which has the airflow containers (scheduler + web server running with the localExecutor), Postgres and a selenium container (using the image standalone-chrome:3.141). On the latter setup I used the remote driver pointing to the selenium container on port 4444, without the adding the headless option.

I would like to get rid of that selenium container as i only use it a few times during the day, and instead using the webdriver from my airflow worker.

I hope someone can help me"
5176,2021-02-20 05:13:35,1613790815.0,dataengineering,Service Delivery at FANG?,lnxtvz,Limp-Ad-7289,,https://www.reddit.com/r/dataengineering/comments/lnxtvz/service_delivery_at_fang/,1.0,0.0,0.0,25938.0,"Hi everyone,

my background is in industry, focusing on industrial automation to make products and machines. I code, design and technically manage projects. Over the years I've seen the demand for data rising, and opted for a MS of DS. I am happy with it, and learned a ton, and am looking for a ""transition"" role to help me bridge my current background, over to DS.

With that in mind, I am working on an offer to work at FANG as a contractor (pay is solid, more than I make currently), and I'd stay at the same rank (manager), but I'd be managing a team that handles data pipelines at FANG. The official title is ""Service Delivery Manager"", and I feel pretty ok about it....I know that DS vs DE can be problematic, but I do also think that working with a team managing big data pipelines (more like DE), can be really great for my career, and my overall understanding of E2E data projects. It may not be the title of DS, but frankly from the offers I've seen....this Service manager role makes more, and I wonder if it's a good idea to ""downgrade"" myself from a manager (whcih i am now), to an individual contributor role....not sure if that's wise for me either considering I've ""earned"" management over the years.

Any thoughts on that? Concerns? Something I may have overlooked? I am leaning towards it, but appreciate critical thought here.

&amp;#x200B;

Thanks!"
5177,2021-02-20 05:30:58,1613791858.0,dataengineering,"Im currently a Sr. Data Analyst and a recruiter reached out to me about interviewing for a Data Engineering position. Not really looking to change jobs at the moment, but is it worth it to take the interview?",lny52g,jerseyse410,,https://www.reddit.com/r/dataengineering/comments/lny52g/im_currently_a_sr_data_analyst_and_a_recruiter/,1.0,9.0,0.0,25940.0,"I've been leaning on moving into Data Engineering for a while. Been an Sr. Analyst for a few years and im more interested in the SQL, Python, DAX, ETL pipeline work more so than the statistical AI/ML of Data Science. Im not really looking to move jobs at the movement, but im curious if its worth it to take the interview?"
5178,2021-02-20 05:32:48,1613791968.0,dataengineering,How to run Spark applications on AWS Fargate,lny6cy,5hahinism,,https://www.reddit.com/r/dataengineering/comments/lny6cy/how_to_run_spark_applications_on_aws_fargate/,1.0,0.0,0.0,25940.0,
5179,2021-02-20 06:53:51,1613796831.0,dataengineering,Good to start with Flink than Spark,lnzll6,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/lnzll6/good_to_start_with_flink_than_spark/,1.0,6.0,0.0,25945.0,"Hello everyone, starting to learn data engineer. I'm overwhelmed with lots of tutorials on which one to follow and which one to ignore. It frustrates me since I'm learning along with my work, My work is different from what I'm learning. So there no defined roadmap I have to say I have completed data engineering and I can go and apply for jobs. 

1. I'm very basic in terms of code,sql,infra(aws) etc. Need some guidance else I feel I'm losing focus.
2. Would that be ok to focus only on Flink rather than Spark to consider future.
3. or Only on Streaming side should i direct my focus on."
5180,2021-02-20 07:03:50,1613797430.0,dataengineering,What's the difference between a Data Engineer and a Cloud Engineer?,lnzrmp,HandsomeBambino,,https://www.reddit.com/r/dataengineering/comments/lnzrmp/whats_the_difference_between_a_data_engineer_and/,1.0,13.0,0.0,25947.0,"Hi all, forgive me if this is such a noob question but coming from a totally different industry (non-tech) and now beginning to develop an interest in Data Engineering, I'm curious to know - what's the difference between a data engineer and a cloud engineer in terms of job roles and skills sets?

Google didn't provide me much details at this time so I thought I might be able to  get clearer insights and direct info here. Thanks for any helpful responses!"
5181,2021-02-20 22:21:39,1613852499.0,dataengineering,Airflow multiple predecessors,lofrzh,kmarq,,https://www.reddit.com/r/dataengineering/comments/lofrzh/airflow_multiple_predecessors/,1.0,3.0,0.0,25994.0,"In our analytics environment we often have tables combining the results of several previous tables. The issue is that these predecessor tables often have very different schedules (Weekly-Monday, Weekly-Wed, 2nd working day of week, Monthly, etc). Does anyone have a good strategy to trigger a DAG that is dependent on multiple upstream DAGs that don't have a common schedule? How do you handle scenarios like this in airflow?"
5182,2021-02-20 23:02:50,1613854970.0,dataengineering,Undergrad data engineering intern expectations?,lognjm,veeeerain,,https://www.reddit.com/r/dataengineering/comments/lognjm/undergrad_data_engineering_intern_expectations/,1.0,9.0,0.0,25995.0,"Hello, this may be a question that is too general because it can vary from company to company, but I was wondering what the general criteria or expectations are from an undergraduate students  applying for data engineering internships. I know this can vary from place to place but if there are any hiring managers out there what do you typically look for a in a undergraduate candidate ? Basic software skills? ETL experience? Data modeling knowledge? SQL + Python knowledge? A lot of the concepts aren’t taught in schools so I’m curious as to what the baseline evaluation is for undergrad students applying for summer internships for data engineering.I’d assume a lot of learning on the job with various tools too so it’s not like we would need to know how to airflow coming in right?"
5183,2021-02-21 00:11:00,1613859060.0,dataengineering,Post Bacc CS or MS in IT Management with data engineering focus?,loi0y6,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/loi0y6/post_bacc_cs_or_ms_in_it_management_with_data/,1.0,3.0,0.0,26000.0,"Just as the title implies, I'm looking into additional schooling. I have an undergrad in accounting but have worked with databases (SQL Server, Snowflake, Postgres, Vertica, 1010Data) for the past 7 years. My company is evolving my BI position into more of a database developer/data engineer, so I am curious which would look better when I leave this company?  


The Post Bacc is Oregon State's CS degree, which has Python, C, and a slew of other CS courses. It doesn't have much in the form of data engineering, except for one db course, a cloud dev course, and that's about it. It's really geared for you to become a SWE.

The MSITM from UT Dallas allows you to take an OOP class in Python, and 6-8 other courses focused on data management, business data warehousing, etc. I fear I would lose out to CS grad though.

Thoughts?"
5184,2021-02-21 00:17:05,1613859425.0,dataengineering,"What is this profession, really?",loi58l,anon006622,,https://www.reddit.com/r/dataengineering/comments/loi58l/what_is_this_profession_really/,1.0,57.0,0.0,26000.0,"I work at a hedge fund, a big one. If you know any large hedge fund names, my employer is probably one of them.

I've been in the finance/investing/hedge fund industry since the mid '00s and have had a variety of titles: software engineer, data scientist, data engineer (and Senior varieties of these). I've managed small teams, directed loads of interns, and have built lots of stuff that made my businesses better and more profitable.

Currently I'm a Senior Quant Data Engineer. Most of what I do is move data around (writing stuff to load from some streaming API or FTP usually), organize it (stuff it into databases, or other storage), enrich it by adding mappings to other data sets, create APIs (usually with some sort of value-add) for it and work with the Data Scientists to help them use the data to make models.

Invariably the models built by the Data Scientists are slow to train, slow to inference (slow to run in general), and when they have to make it run live and be dependable enough to produce results day after day, so they can move on to the next model, they often fail to do so without help.

So I often get involved with them and show them how to only get the data they really need (e.g. stuff like don't get all columns when you only need 2, don't copy dataframes unnecessarily), how to make their stuff run better, etc... Oh and teach them how to parallelize their work. This one is pretty common....

Is this the way data engineering is in most fields?"
5185,2021-02-21 00:48:03,1613861283.0,dataengineering,Csv files from ftp server to mysql database,loir9c,ketaeishi,,https://www.reddit.com/r/dataengineering/comments/loir9c/csv_files_from_ftp_server_to_mysql_database/,1.0,2.0,0.0,26003.0,"I receive daily solar installations CSV files on an FTP server. I would like to put this data on a MYSQL database. At the moment I am using python to concatenate the csvs by installs into a pandas dataframe, then I insert the dataframe into MYSQL.

For the moment I do not manage the files which arrive on the FTP server to put them directly in the database.

How can I handle this? Do you have any sample python code that I can use to do this ? Is there a better method than using python?

Thanks all"
5186,2021-02-21 03:09:14,1613869754.0,dataengineering,Which Data Warehouse Toolkit (DWT) Chapters Should I read for FB DE Interview,lolice,trevcatdangerous,,https://www.reddit.com/r/dataengineering/comments/lolice/which_data_warehouse_toolkit_dwt_chapters_should/,1.0,2.0,0.0,26013.0,"I have a DE interview coming up w/FB &amp; am wondering which chapters of the DWT might best prepare me for the design, modeling, etc. conversation.  I have read the [primer](https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/) of *how* to best read the text - just looking for some insight &amp; advice from those who have been through the onsite &amp; which chapters you're glad you'd read &amp; which you might wish you would have. TIA."
5187,2021-02-21 06:01:38,1613880098.0,dataengineering,AWS Glue VS EMR for ETL,loohq7,blue_sky_time,,https://www.reddit.com/r/dataengineering/comments/loohq7/aws_glue_vs_emr_for_etl/,1.0,4.0,0.0,26020.0,"I currently use EMR now to perform ETL for my company.  We are considering switching to AWS's Glue service.  It seems that the pricing is higher (\~2x more expensive than EMR) and some posts said it is actually slower runtime than EMR.  Does anyone have any first hand experience?  We do a lot of big production ETL jobs, so cost is a concern.

Alternatively, is there another service I could be using?"
5188,2021-02-21 11:06:15,1613898375.0,dataengineering,Free Course with certification,lot8dl,Free_Course,,https://www.reddit.com/r/dataengineering/comments/lot8dl/free_course_with_certification/,1.0,0.0,0.0,26033.0,
5189,2021-02-21 12:17:13,1613902633.0,dataengineering,Interview Question,lou8mo,spopgg,,https://www.reddit.com/r/dataengineering/comments/lou8mo/interview_question/,1.0,4.0,0.0,26034.0,"Your current ETL process relies on CSV files uploaded by your clients to a server and from there ingested by your Database (MS SQL Server) via a set of SSIS packages.

A new client needs to be on-boarded and he is forcing a direct access to his data lake (BigQuery), moreover he is sending additional data (10 TXT compressed files, space formatted) via email and containing additional data that are not in the data lake.

How would you proceed to integrate the new client needs with your existing ETL flow?"
5190,2021-02-21 15:18:55,1613913535.0,dataengineering,Looking for data engineers,lowxhd,spark58510,,https://www.reddit.com/r/dataengineering/comments/lowxhd/looking_for_data_engineers/,1.0,16.0,0.0,26043.0,"Company I work for is looking to hire two Data Engineers. We are a medium sized healthcare provider in Virginia.

Just to get it out of the way:
- Salary range is likely somewhere in the 80-120 range depending on experience. Not positive
- remote option available (east coast hours)
- I believe that we are entertaining sponsorship for the right candidate

Must haves:
- SQL: we are a heavy SQL/ELT shop: CTE, merge, stored procedures, UDFs
- Experience in a DE team, working in agile groups
- experience working with ETL tools and building pipelines (Talend, Informatica, Datastage - Talend preferred)

Nice-to-haves:
- experience with Snowflake and/or Netezza 
- experience in AWS wrt data (EMR, Spark, Lambda)
- Python highly desirable
- Java/scala

DM me for more details if you are seriously interested"
5191,2021-02-21 17:07:13,1613920033.0,dataengineering,"The 30th edition of @data_weekly focus on @UberEng schema-agnostic log analytics platform, @Google opensource model search system, @Intuit Data Mesh strategy, @salesforce secure data intelligence platform, @netflix composable data pipeline",loyvm8,vananth22,,https://www.reddit.com/r/dataengineering/comments/loyvm8/the_30th_edition_of_data_weekly_focus_on_ubereng/,1.0,0.0,0.0,26045.0,
5192,2021-02-21 17:41:24,1613922084.0,dataengineering,What is important for a DE to know about shell scripting in Windows (Powershell)?,lozl0u,darvey_specter,,https://www.reddit.com/r/dataengineering/comments/lozl0u/what_is_important_for_a_de_to_know_about_shell/,1.0,3.0,0.0,26048.0,Noob here. I'm learning using YouTube tutorials and what would you folks say are the key concepts to have nailed when it comes to shell scripting?
5193,2021-02-21 20:22:44,1613931764.0,dataengineering,Interview Question 2,lp3429,spopgg,,https://www.reddit.com/r/dataengineering/comments/lp3429/interview_question_2/,1.0,1.0,0.0,26055.0,"Your company provides some client-facing data quality checks (e.g. KPI verification) through some Grafana dashboards.

A new client asks for more analysis freedom and pushes the implementation of an OLAP cube consumed by and Excel spreadsheet.

How would you proceed with the implementation of this new “data analytics application”? Which are the challenges you can think of?"
5194,2021-02-21 22:50:03,1613940603.0,dataengineering,"What technologies/frameworks do you see getting more popular, and which do you feel will become less used in the next few years?",lp6aom,jana_50n,,https://www.reddit.com/r/dataengineering/comments/lp6aom/what_technologiesframeworks_do_you_see_getting/,1.0,40.0,0.0,26063.0,"For example, Snowflake, BigQuery, Hadoop"
5195,2021-02-21 23:32:19,1613943139.0,dataengineering,Platform for data-ops(ish) workflow?,lp76eu,Jwelch25,,https://www.reddit.com/r/dataengineering/comments/lp76eu/platform_for_dataopsish_workflow/,1.0,7.0,0.0,26065.0,"I recently did [a post on what our data-ops(ish) pipeline ](https://www.reddit.com/r/dataengineering/comments/ln4fi7/dataopsish_design_and_workflow_walkthrough/?utm_medium=android_app&amp;utm_source=share) pipeline looks kike. Based on the positive feedback and general interest, as well as me thinking about this for a while:

If this type of workflow were available in a unified platform, would that be of interest to people?"
5196,2021-02-21 23:54:57,1613944497.0,dataengineering,Advanced Data Science SQL Interview Question [Amazon] (window functions &amp; aliasing),lp7mub,rimon34,,https://www.reddit.com/r/dataengineering/comments/lp7mub/advanced_data_science_sql_interview_question/,1.0,0.0,0.0,26066.0,
5197,2021-02-22 00:25:34,1613946334.0,dataengineering,Interview Question 3,lp8ads,spopgg,,https://www.reddit.com/r/dataengineering/comments/lp8ads/interview_question_3/,1.0,4.0,0.0,26069.0,Your lead data engineer is on holiday while the data scientist team requests to change the indexes on a key table in the data model. How would you proceed? Would you arrange an action plan with your colleagues and the data scientists? Will you wait for your team leader to come back from holiday? Support your answer.
5198,2021-02-22 05:11:50,1613963510.0,dataengineering,Call for papers on open source analytic databases-Percona Live,lpdp1r,dbcicero,,https://www.reddit.com/r/dataengineering/comments/lpdp1r/call_for_papers_on_open_source_analytic/,1.0,0.0,0.0,26086.0,"Hi open source fans,   
I'm helping to manage the Analytics track at the next Percona Live conference May 12-13.  We're all about open source. We would like to invite users and contributors to submit talk proposals on their favorite open source analytic databases.  More details here:  [https://altinity.com/blog/call-for-papers-on-analytics-at-percona-live-online-2021](https://altinity.com/blog/call-for-papers-on-analytics-at-percona-live-online-2021).     
Thanks! There are some great open source analytic projects out there. We hope to see interesting talks from your respective communities.  

Robert Hodges, Altinity"
5199,2021-02-22 10:45:18,1613983518.0,dataengineering,From Senior BI to Azure DE,lpj9cl,lady5ybil,,https://www.reddit.com/r/dataengineering/comments/lpj9cl/from_senior_bi_to_azure_de/,1.0,6.0,0.0,26100.0,"I have an opportunity to transition from a senior BI role (solid SQL/Power BI skills + JavaScript/C# development) to a data engineering role. This would be in the healthcare space in an Azure/Microsoft environment and there’s heavy use of ADF/Databricks and looking towards cloud data warehousing.
Should I venture down this path with no Python/big data experience? How can I best prepare myself to succeed in this role? The primary tasks would be to build data pipelines, store, optimize and model data in a manner fit for data science consumption, as well as enable business users to use said data in a performative manner in Excel/Power BI. 
In summary, should I take a stab at this (I really want to but also want to be realistic, I am eager to learn but feeling overwhelmed by how much other DEs on here seem to know from a scripting perspective).  What should I focus on, if so?"
5200,2021-02-22 17:23:41,1614007421.0,dataengineering,Data Engineering Case Study Interviews,lpqpkl,dead-on-arrival-,,https://www.reddit.com/r/dataengineering/comments/lpqpkl/data_engineering_case_study_interviews/,1.0,8.0,0.0,26110.0,"I am actively interviewing for different DE opportunities and noticed that several companies have a Case Study Inteview section within their processes. 

Is anyone aware of any specific material to help prepare for these DE case studies for interviews? I have experience with my organization's stack but I doubt that will be enough for holistic discussions around all the components that fall into expectations for a DE case study."
5201,2021-02-22 17:47:15,1614008835.0,dataengineering,Should I accept this job,lprjrj,JustJasper,,https://www.reddit.com/r/dataengineering/comments/lprjrj/should_i_accept_this_job/,1.0,5.0,0.0,26112.0,"I am a GIS analyst/developer in a municipality , and have been bored lately, after working there for 3 years. Now there is an open position at a local firm as data engineer. It sounded wonderful, but the catch is that it is a position as sole data engineer, where the job is to setup a data warehouse for their BI dashboards. 

Data Engineering sounds fun, and I have a bunch of experience with SQL, Python and Data Analysis. But I am afraid how much of a warehouse setup they really need. The company has different subsidiaries, including private health care clinics and a hospital for mentally disabled person. They have roughly 300 employees and use a bunch of it systems, which they want to collect data from. How much overkill is a warehouse setup on Azure, airflow, Spark? It seems like a lot to manage for a company this size. The job also includes some PowerBI work, in addition to centralizing and cleaning data.

On a final note I am afraid of not having any collegeas to discuss technical issues with. But I would have a lot of freedom on designing a data warehouse and would get a 15-20% salary bump.

Any thoughts would be appreciated 😀"
5202,2021-02-22 18:02:17,1614009737.0,dataengineering,How can improve my python skills?,lps0xe,joeen10,,https://www.reddit.com/r/dataengineering/comments/lps0xe/how_can_improve_my_python_skills/,1.0,6.0,0.0,26113.0,"Hi all,

I'm struggling with writing python scripts that interact between my database and other platforms (APIs, Google sheets, FTPs). 

I understand perfectly what has to be done, but when coding it could take me lots of time to even write one line. It's mega frustrating!! Also, no one at work is available for help.

How can I learn about better structure of scripts for DE? And how do you stay efficient when writing code?"
5203,2021-02-22 18:06:58,1614010018.0,dataengineering,Data Analysis: The Decisive Factor of Percentage in Discount,lps5cr,Palaksharma22,,https://www.reddit.com/r/dataengineering/comments/lps5cr/data_analysis_the_decisive_factor_of_percentage/,1.0,0.0,0.0,26114.0,
5204,2021-02-22 19:35:39,1614015339.0,dataengineering,Data Observability: How to Ensure Data Quality at Scale,lpuhaf,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/lpuhaf/data_observability_how_to_ensure_data_quality_at/,1.0,0.0,0.0,26117.0,
5205,2021-02-22 21:57:38,1614023858.0,dataengineering,How has COVID-19 impacted data science?,lpy5l6,abe_dearmer,,https://www.reddit.com/r/dataengineering/comments/lpy5l6/how_has_covid19_impacted_data_science/,1.0,1.0,0.0,26127.0,"I wrote a post recently (ish) about [the impact that Covid-19 is having on data science](https://www.xplenty.com/blog/covid-19-and-data-science/?utm_source=content&amp;utm_medium=referral&amp;utm_campaign=covid-19-and-data-science/) overall. 

Two effects of Covid on data stand out:

1. Predictive data models becoming obsolete due to 2020’s unpredictability
2. Demand for data analytics has risen, while spending, in general, has been curbed

What are you seeing impact data science due to Covid-19? How is your job being impacted?"
5206,2021-02-23 04:19:23,1614046763.0,dataengineering,getting started with Apache Superset,lq6oaq,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/lq6oaq/getting_started_with_apache_superset/,1.0,9.0,0.0,26134.0,"Hi everyone, A short while ago, Apache Superset graduated from an Apache incubating project to a top-level project. Having seen thousands of dollars being spent on vendor BI tools I wanted to explore how Apache Superset can be used to offset this cost. Here is a quick write up, to try out Apache Superset locally [https://www.startdataengineering.com/post/apache-superset-tutorial/](https://www.startdataengineering.com/post/apache-superset-tutorial/) 

Any feedback, war stories, comments appreciated."
5207,2021-02-23 06:36:00,1614054960.0,dataengineering,Apache Beam Reading CSVs,lq9bbj,BigBooledHead,,https://www.reddit.com/r/dataengineering/comments/lq9bbj/apache_beam_reading_csvs/,1.0,0.0,0.0,26143.0,"For the life of me, I can't seem to figure out what is the proper way to read csvs with apache beam is. 

Everything online is all over the place, my end goal is to read a csv, grab the headers and sanitize the them to be within bigquery's column name parameters, then I was going to process the data ( all of it and not just the top 200 rows ) to generate the schema. 

I was attempting to utilize: [https://github.com/pabloem/beam\_utils](https://github.com/pabloem/beam_utils) where I modified the code to sanitize the headers. 

&amp;#x200B;

        def sanitize_name(self, value):
                new_value = re.sub('[^a-zA-Z0-9_]', '_', value[:127])
            return new_value
    
        def read_records(self, file_name, range_tracker):
            # If a multi-file pattern was specified as a source then make sure the
            # start/end offsets use the default values for reading the entire file.
            headers = None
            self._file = self.open_file(file_name)
            rder = csv.reader(_Fileobj2Iterator(self._file), delimiter=self.delimiter)
    
            for i, rec in enumerate(rder):
                if (self.header or self.dictionary_output) and i == 0:
                    headers = []
                    for column in rec:
                        headers.append(self.sanitize_name(column))
                    continue
    
                if self.dictionary_output:
                    res = {header: val for header, val in zip(headers, rec)}
                else:
                    res = rec
                yield res
    
    class _Fileobj2Iterator(object):
    
        def __init__(self, obj):
            self._obj = obj
    
        def __iter__(self):
            return self
    
        def next(self):
            line = self._obj.readline()
            if line is None or line == '':
                raise StopIteration
    
            return line

However, whenever I run that I get an error at the ""return line"" in class \_Fileobj... part :

 **TypeError**: argument 1 must be an iterator 

Would someone be able to assist, as all these different methods have been confusing me at this point. 

&amp;#x200B;

I saw that in the apache beam test cases:

&amp;#x200B;

      def test_csv_file_source(self):
        content = 'name,year,place\ngoogle,1999,CA\nspotify,2006,sweden'
        rows = [r.split(',') for r in content.split('\n')]
    
    
        dir = '%s/' % self._new_tempdir()
        self._create_temp_file(dir=dir, content=content)
    
    
        def get_csv_reader(readable_file):
          if sys.version_info &gt;= (3, 0):
            return csv.reader(io.TextIOWrapper(readable_file.open()))
          else:
            return csv.reader(readable_file.open())
    
    
        with TestPipeline() as p:
          content_pc = (p
                        | beam.Create([dir])
                        | fileio.MatchAll()
                        | fileio.ReadMatches()
                        | beam.FlatMap(get_csv_reader))
    
    
          assert_that(content_pc, equal_to(rows))

However, I am unclear if one this would keep the headers if let's say I changed csv.reader to csv.DictReader, that would be sufficient for me as well. However, I would like to sanitize the column names one time, versus doing it every line. 

Research:

[https://github.com/apache/beam/blob/b85795adbd22d8b5cf9ebc684ce43e172a789587/sdks/python/apache\_beam/io/fileio\_test.py#L128-L148](https://github.com/apache/beam/blob/b85795adbd22d8b5cf9ebc684ce43e172a789587/sdks/python/apache_beam/io/fileio_test.py#L128-L148)

[https://stackoverflow.com/questions/59994645/python-apache-beam-how-to-parse-text-file-to-csv](https://stackoverflow.com/questions/59994645/python-apache-beam-how-to-parse-text-file-to-csv)

[https://stackoverflow.com/questions/41170997/how-to-convert-csv-into-a-dictionary-in-apache-beam-dataflow](https://stackoverflow.com/questions/41170997/how-to-convert-csv-into-a-dictionary-in-apache-beam-dataflow)

[https://stackoverflow.com/questions/63239859/read-a-csv-file-clean-it-then-write-out-the-result-as-a-csv-using-apache-beam](https://stackoverflow.com/questions/63239859/read-a-csv-file-clean-it-then-write-out-the-result-as-a-csv-using-apache-beam)

[https://stackoverflow.com/questions/59861800/unable-to-use-csvfilesource-from-beam-utils-sources-in-apache-beam-program-on-cl](https://stackoverflow.com/questions/59861800/unable-to-use-csvfilesource-from-beam-utils-sources-in-apache-beam-program-on-cl)"
5208,2021-02-23 07:00:37,1614056437.0,dataengineering,"Career next steps, looking for thoughts and advice",lq9s7n,Mountain_Reading5628,,https://www.reddit.com/r/dataengineering/comments/lq9s7n/career_next_steps_looking_for_thoughts_and_advice/,2.0,8.0,0.0,26266.0,"TLDR; currently in a senior DE role, money is good not great and company uses old tech, but job is stable, great benefits, and relatively easy. Do I coast at my current job or try to pursue a higher paying/more rewarding job at a FANG/leading edge tech company?

The longer version...

Senior DE, 6 YOE, making between 105-120k depending on bonuses. In my current role I am honestly not doing much hands on development work. I am basically a tech lead for a small scrum team, working with our product team on our workstream, working with architects on designs and solutions, and making sure the team has everything they need to keep the trains moving. We use mostly traditional ETL tools like Datstage/Informatica, and have just recently started talking about cloud solutions. 

I admittedly have a bit of imposter syndrome. I wouldn’t consider what I do data engineering. I feel like an ETL developer with a data engineer label. I’d love to get involved in more “data engineering” type work, but there are not many opportunities where I am now. 

All that being said, the job has some good perks. Benefits are great, work life balance is good, people at work are awesome. I could just coast where I am and make a comfortable living without trying all that hard. I also have opportunities to advance if I wanted to put extra work in. 

I also have a nagging desire to see if I’d be able to get a job at a FANG type company. Sure, the higher salary would be nice, but the challenge of it also sounds appealing. I assume it would be a higher pressure environment, but I’d be learning a ton and working with people who are passionate about the field. I am fully aware it would take a good amount of work to prepare for the interviews to even have a shot at being considered. 

Has anyone else gone through this? Has anyone left a relatively easy/relaxed job for a FANG type job and regretted it? Or was it worth it?

I totally understand the answer will be different for each individual, but at this point I don’t know what I don’t know, so I am wondering what experiences others have had and looking for any advice at all. 

Thanks!"
5209,2021-02-23 07:04:16,1614056656.0,dataengineering,Nature and DE,lq9urr,AphoticSeagull,,https://www.reddit.com/r/dataengineering/comments/lq9urr/nature_and_de/,2.0,0.0,0.0,26266.0,"Some structures and patterns in nature pop up in applicable areas. ROYGBIV pattern in art is pleasing to the eye. The strength of the triangle with structural engineering. What, if anything, in the natural world can we apply to data engineering?"
5210,2021-02-23 07:30:36,1614058236.0,dataengineering,Is there any outlet for finding ways to apply data engineering to make the world a better place ?,lqachs,be_nice_if_u_can,,https://www.reddit.com/r/dataengineering/comments/lqachs/is_there_any_outlet_for_finding_ways_to_apply/,9.0,12.0,0.0,26267.0,"Maybe a website where you can donate work to help a cause or a list of companies that are agreed to be doing “the right thing” that have open jobs ?

It would be awesome to apply this specialized skilled to make the world a better place ( maybe to help environmentalism, mental health efforts , crime ect ) and I think would help link a motivation - as sometimes this work can be so behind the scene and narrow people could have a way to link that to the greater world."
5211,2021-02-23 13:50:34,1614081034.0,dataengineering,[Question] Databricks spark certification,lqgm01,RstarPhoneix,,https://www.reddit.com/r/dataengineering/comments/lqgm01/question_databricks_spark_certification/,5.0,2.0,0.0,26279.0,"Hi guys , Is there any way to get 100% off voucher  or a good discount voucher for Databricks spark  certification? Currently the certification is very costly (200$). Any help is appreciated."
5212,2021-02-23 18:07:24,1614096444.0,dataengineering,"My ""software"" company doesn't know what data engineering is - is this normal or a warning sign?",lqlwug,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/lqlwug/my_software_company_doesnt_know_what_data/,4.0,20.0,0.0,26286.0,"Currently work in analytics as our sole data engineer (title is different, don't get me started)... spoke to a leader on the dev team who manages our AWS engineers and he doesn't know what a data engineer is... he thought it had to do with implementing new customers.

Is this normal? Or a warning sign? This is a technology company."
5213,2021-02-23 18:28:25,1614097705.0,dataengineering,2 YOE. Can get interviews but struggling to pass technical interview and virtual on-site,lqmfas,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/lqmfas/2_yoe_can_get_interviews_but_struggling_to_pass/,46.0,31.0,0.0,26286.0,"Hi guys. I have 2 years of experience as a DE. My experience primarily involves Hadoop/hive/hdfs, python, spark/pyspark and batch jobs. My 2 years is with a non-tech company but very large employer.


**1. initial phone screen**

In the past 2 months, I applied to around 80 listings and got email responses to schedule an initial phone screen from about 17 of the 80 (~20%). I have no issue with this step of applying and getting calls back (even from big name companies: Uber, Amazon, Goldman).

Within these 17 phone screens, every single one wanted to move forward. The next step for majority of the 17 companies was a 1 hour technical phone interview (python data structure and algos)


**2. 1 hour technical interview**

this comprises of leetcode style data structure and algo questions. Some companies ask more SQL while others ask more, or only, python (DS/algo). But from my experience, most focus more on python DS/algo (the SQL is usually assessed during the next step aka virtual on-site). 

But regarding the 1 hour technical interview step, most companies only asked 2 questions (med/hard LC on string/array/dictionary manipulation). This part I can pass with ~70% confidence).


**3. virtual on-site (part where I struggle)**

The final step is generally a virtual on-site (4-6 hours depending on company usually mid/big companies do this. Smaller companies do whatever they want). But this virtual on-site step usually tests you in the following areas:

- ETL
- data modeling
- behavioral (many questions such as: tell me a time when...?)

My question is: for someone with 2 years of experience, what the hell are they generally looking for in the ETL and dimensional modeling rounds? 

Are these rounds focusing more on my system design? I genuinely don’t even know if traditional SWE system design interviews are the same thing as distributed system design. Are the two synonymous (system design vs. distributed system design)? 

Regarding the ETL round, what I’ve heard is that it’s to assess the following:

- Design source schema (so does that mean design relational tables and their schemas?)
- design target scheme (does this mean design data warehouse tables using dimensional modeling such as facts and dimensions?)
- writing sql to transform and load data
- batch and real-time designs
- python &amp; sql

If that’s all for just the ETL round, wtf is tested during the data modeling round?! 


Sorry this is a huge post. I am just overwhelmed. Feel like there’s too much to know for DE interviews. 

tldr; on virtual on-sites for DE interviews, there are generally an ETL round and data modeling round. How do I study for each round when I don’t even exactly know what what I should be expected to be tested on?! Feeling so overwhelmed especially when I spend countless hours studying LC data structures and algo questions just to pass the 1 hour technical interview."
5214,2021-02-23 21:35:19,1614108919.0,dataengineering,DE Career Advice: Beginner,lqqxe1,hassyhulbert,,https://www.reddit.com/r/dataengineering/comments/lqqxe1/de_career_advice_beginner/,6.0,24.0,0.0,26291.0,"&amp;#x200B;

**I am looking for advice on next steps on getting to a point where I am qualified to apply for DE jobs based on my current skills and approach to making a career pivot. Which curriculums, projects, or methods could I leverage to get up to speed?**

I have 3 years of experience in data analytics consulting, two for a boutique firm (technical) and one at an enterprise AI software company (non-technical). I have pretty strong business acumen and understanding business context + value for product / engineering tasks, which I have been told is a plus for entering the sector. 

My technical chops are *good*, but nowhere to the level of being able to jump into a FT job. I am comfortable in python for basic data analysis / data transformation / predictive modeling but I am not at a point in which I feel like I can be creative and organically write python ETL programs, if that makes sense. Alot of my experience came from stackoverflow and tweaking with other people's code to get my outcomes. I am pretty comfortable in SQL, but mostly for querying tasks, and I am currently teaching myself Spark.

In the past, I have done quasi DE tasks without any oversight or knowledge of best practices. At my last job, I built a prototype data pipeline for an ecommerce client using Fivetran (ingestion+ scheduling), AWS (storage), Snowflake, Alteryx, and Tableau which I really enjoyed. The company went under so the real-world practice got axed overnight, along with all of my progress. 

Currently, I am exploring DE tracks from elearning providers like Galvanize, General Assembly, Flatiron, Etc. as well as individual skill courses for intermediate Python, Spark, advanced SQL, and computing in general through MIT online and LinkedIn learning. I am doing all of this while working as a contract analytics consultant where I write basic SQL and do data analysis with pandas.

Thanks"
5215,2021-02-23 21:45:25,1614109525.0,dataengineering,"AWS Glue, DBT, and Snowflake for Data Lineage",lqr5vd,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/lqr5vd/aws_glue_dbt_and_snowflake_for_data_lineage/,1.0,0.0,0.0,26291.0,We have the batch process using AWS Glue and Step functions and the target is on Snowflake. How do we enable Data Lineage in the process. We are planning to use AWS Glue + DBT + Snowflake . Is that a possible solution without spending more time rewiring or more cost.
5216,2021-02-23 21:52:24,1614109944.0,dataengineering,Quick survey on the use of open-source technologies in your data stack,lqrbmd,buntro,,https://www.reddit.com/r/dataengineering/comments/lqrbmd/quick_survey_on_the_use_of_opensource/,10.0,19.0,0.0,26291.0,
5217,2021-02-23 21:55:25,1614110125.0,dataengineering,Pyspark and Python --Job Help,lqre9e,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/lqre9e/pyspark_and_python_job_help/,1.0,0.0,0.0,26291.0,"Hello Everyone, 

have applied for the internal job movement and the requirement almost needs of python and pyspark basic level with at least mini project should be done on my portfolio. I'm just on the basic level with a very basic understanding of the python concepts. I have YouTubed pyspark tutorials. Most of them are saying Python is a pre-requisites and some of them saying it is not. I have nearly one month to complete the project and move to the intermediate level. Assuming I have 5 hours per day can be spent on the learning. Since the time is very limited. Can someone suggest to me the guided tutorials move forward with the learning and achieve the goal?

Search link: [https://www.youtube.com/results?search\_query=pyspark+for+beginners](https://www.youtube.com/results?search_query=pyspark+for+beginners) 

Udemy Course:[https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/](https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/)  [https://www.udemy.com/course/cca-175-spark-and-hadoop-developer-python-pyspark/](https://www.udemy.com/course/cca-175-spark-and-hadoop-developer-python-pyspark/)

The scope of the project is to use AWS Glue, Step functions, and load the transformed data to the target cloud db."
5218,2021-02-23 22:04:34,1614110674.0,dataengineering,What are the MOST pervasive problems you are dealing with in your day-to-day job?,lqrm4f,sk2977,,https://www.reddit.com/r/dataengineering/comments/lqrm4f/what_are_the_most_pervasive_problems_you_are/,2.0,10.0,0.0,26292.0,
5219,2021-02-23 22:46:52,1614113212.0,dataengineering,[Hiring] Sr Data Engineer in SF/LA,lqslra,coyne_operated,,https://www.reddit.com/r/dataengineering/comments/lqslra/hiring_sr_data_engineer_in_sfla/,1.0,0.0,0.0,26295.0,"I manage a small team of Data Engineers at GoodRx and we are currently hiring one senior engineer to join our team of 5.  


Looking for someone with strong SQL and AWS serverless experience (specifically Athena, Lambda Event Bridge).  


I am the hiring manager, feel free to message me with any questions.  


[https://jobs.lever.co/goodrx/a631d409-0f51-475c-8b1f-e8607849544e](https://jobs.lever.co/goodrx/a631d409-0f51-475c-8b1f-e8607849544e)"
5220,2021-02-23 23:35:14,1614116114.0,dataengineering,Advice: DA ---&gt; DE and improving programming skills,lqtpty,HelpMeExitVim,,https://www.reddit.com/r/dataengineering/comments/lqtpty/advice_da_de_and_improving_programming_skills/,1.0,4.0,0.0,26298.0,"I'm a DE with ~1yr of experience and I'm also my team's DevOps engineer (small company, many hats). I was hired as a data analyst doing mostly SQL queries and schema, and then transferred to DE internally. 

However, I don't really feel like I'm a ""strong"" programmer and that seems important for DE. I can get the work done, but I'm still at the stage of programming where I need to look at our current codebase and take examples and work from those. I am pretty much useless if you give me a blank file and tell me to start programming. When I'm working in SQL or bash, I just know what to do and how to approach something, but it's a real challenge when I'm working in our server-side language (go)

 I've tried to work with some leetcode problems but even then it just feels like I'm *missing* something foundational or really important: like, I can google how to read input from `stdin` for an interview, but am missing what seems like pretty basic things: what's a pointer, what's buffered i/o vs unbuffered and when to use...that kind of stuff. 

As my career progresses, I feel like this is a real weak point but I'm not sure what to do about it. Should I try to teach myself to code in my off hours? My boss keeps trying to tell me in 1-1s that I don't really need these skills for a career, but I thought DEs were supposed to have strong programming skills? I feel like I have big strengths in SQL, linux, data-intensive application design &amp; architecture, and since my company has had me as a hybrid DevOps engineer I'm also picking up valuable skills in Docker and k8s / administrating distributed systems and some DBA, as well as learning some networking and security. I do maintain our CICD pipelines (mobile app &amp; GKE). What's the best way to catch up? It doesn't seem like my company is super interested in putting me on projects that would give me a chance to learn by doing (and the times they have put me on those projects, they've also called me away for urgent issues like the test cluster going down, or someone needing an urgent data request and there's no one else who can do it, etc). 

I'd really appreciate any advice or insight. I did about a month ago go through an interview process and was given an offer for another DE role...so maybe I'm overthinking it? But would really, really appreciate any advice."
5221,2021-02-24 00:52:13,1614120733.0,dataengineering,Data engineering technologies,lqvg7r,Bothurin,,https://www.reddit.com/r/dataengineering/comments/lqvg7r/data_engineering_technologies/,2.0,10.0,0.0,26301.0,"Hi all, I've been offered a full year data engineering internship as a second year student and as I currently only know basic database fundamentals and object oriented programming I'm a bit overwhelmed by the list of competencies that the company said would be good for the position: 'Snowflake, Redshift, Big Query, Matillion (or similar ELT tooling), Serverless, Spark, Python, and R. Terraform and Docker. SQL (Snowflake, MS SQL Server, Postgres), Columnar, Graph, Key-Value, and Document, NoSQL database (Cassandra, Neo4J, and/or Elasticsearch). ELT/ETL tools like NiFi, Airflow, Matillion, SSIS, and SnapLogic. BI platforms like Tableau, Looker, Sisense, Power BI, and Qlik', and others'

Because all these names seem very foreign to me and some of them seem to be doing the same thing, I wanted to ask if someone could point out which of these are the most fundamental and important seeing as I have 5 months to prepare and will be working in Azure and doing standard data engineer tasks in a large company."
5222,2021-02-24 01:31:01,1614123061.0,dataengineering,Data Engineering at Facebook,lqwguv,DeepGizzard,,https://www.reddit.com/r/dataengineering/comments/lqwguv/data_engineering_at_facebook/,17.0,19.0,0.0,26302.0,"I'm currently interviewing for a data engineering role at Facebook and have some questions for anyone who has worked there.

Based on the interview thus far, it seems like what Facebook calls a Data Engineer my current company would call a Data Analyst or a Business Intelligence Analyst. The emphasis of the interviewing process is SQL, Python, some data viz, and ""product sense"", which is defined as having sense for the dimensions and metrics that are relevant to a given product.

To me, that's not really the type of Data Engineering I'm interested in. I spend a lot of my time working with Docker, Kubernetes, Spark, Airflow, Terraform, and to a lesser extent CICD processes. I'm wondering if at Facebook these activities are included under the Data Engineering umbrella or if they would would be handled by DevOps or a Data Platform team.

I plan to ask this question directly in my next interview but for anyone who has worked there - what's Data Engineering like at Facebook? How would you define it?"
5223,2021-02-24 02:54:25,1614128065.0,dataengineering,Python developer looking to get into Data Engg.,lqytxa,lordkeith,,https://www.reddit.com/r/dataengineering/comments/lqytxa/python_developer_looking_to_get_into_data_engg/,1.0,6.0,0.0,26306.0,"Hi,

I'm a Python developer and I'm looking to get into a data engineering and I'm wondering what my next steps could be. Below is a summary of my background.

Work experience:


Currently, I'm doing a new graduate rotational program out of college where you rotate between teams for 8 months for a total of 3 terms (2 years in total). My previous rotation involved me developing a process that automated the ingestion work. It consisted of 3 python applications and a Spark application (developed in Scala). Therefore, I got significant experience with Spark and improved my Python skills considerably. 

My current rotation is in a data science team which assists buyers in our company (we're a retailer) in their buying process with the help of machine learning. I'm still new to this team, so still learning their process. What I do know is that they'll be migrating their workflow to be hosted on Azure soon, so I'll get some experience with Azure for sure out of this. 



Education:


I started off with a Business Degree (hated it). Then I did a diploma in college which focused heavily on linux admin and networking. I got interested in programming/development during this and took as many programming related classes as well including data modelling/design. 

Currently, I'm doing a part time post-grad certificate at University Of toronto in Data Science (https://learn.utoronto.ca/programs-courses/certificates/data-science). I'll be done with this by September. 

I also plan on getting a couple of databricks certifications (developer and engineer) in the next few months. 



The following are my weaknesses when it comes to DE:
- No working experience with NoSQL. Only whatever has been taught in my course.
- No CS or SE educational background.
- No knowledge of Java. 
- No knowledge of a queuing system like Kafka.
- Limited cloud knowledge. (though hopefully I'll have some of this in the next few months)


Now, my question is that is it possible for me to get into DE or are my skills too lacking. If so, what should my next steps be? Any help is appreciated."
5224,2021-02-24 06:17:53,1614140273.0,dataengineering,load json data into bigquery,lr3gf8,[deleted],,https://www.reddit.com/r/dataengineering/comments/lr3gf8/load_json_data_into_bigquery/,1.0,2.0,0.0,26317.0,
5225,2021-02-24 06:32:16,1614141136.0,dataengineering,Why do we really need spark?,lr3q6b,jduran9987,,https://www.reddit.com/r/dataengineering/comments/lr3q6b/why_do_we_really_need_spark/,50.0,101.0,0.0,26317.0,"I am currently learning about spark to hopefully use it soon in my job as our data is growing at a very high rate.

As of now, we do most of our computing in Snowflake.  Now I do not know much about distributed computing or where/how spark really shines, but I do wonder why couldn't I just increase my cluster size in Snowflake to improve performance instead of running a spark job over a cluster?"
5226,2021-02-24 11:17:53,1614158273.0,dataengineering,Scaling out Airflow on Celery,lr8glr,[deleted],,https://www.reddit.com/r/dataengineering/comments/lr8glr/scaling_out_airflow_on_celery/,1.0,0.0,0.0,26326.0,
5227,2021-02-24 11:40:03,1614159603.0,dataengineering,Scaling out Airflow With Celery,lr8tje,Botmon_DaDorkNight,,https://www.reddit.com/r/dataengineering/comments/lr8tje/scaling_out_airflow_with_celery/,3.0,2.0,0.0,26326.0,"Hi,
So we are using Airflow for our job orchestration. Right now we are using Airflow on one EC2 instance only.
Now we are looking to expand our structure and I was looking into [this](https://www.accionlabs.com/how-to-setup-airflow-multinode-cluster-with-celery-rabbitmq) article on how to scale out my airflow with celery and workers.

We have a repo with lots of Python scripts and a `DAGS_FOLDER` in it. The `DAGS_FOLDER` uses scripts from the repo using `PythonOperator`, some of the dags have tasks that are dynamically created.
Some of the Python scripts use local directories which are there on our EC2.

I just had some questions and I was hoping if any of you is experienced with doing this can help me

1. Would we have to deploy our latest code/repository on each worker? If yes, how can that be done or how are you doing it in your project?
2. Any change in a directory that Python scripts use or in `local_settings.py` (which is a file having all environment-related settings and is in `.gitignore`) will need to be manually done on each worked?
3. Any change in the `airflow.cfg` will need to be done on all workers?"
5228,2021-02-24 14:25:35,1614169535.0,dataengineering,What would an awesome educational curriculum on data engineering look like?,lrbms9,life_efficient,,https://www.reddit.com/r/dataengineering/comments/lrbms9/what_would_an_awesome_educational_curriculum_on/,4.0,14.0,0.0,26332.0,"What modules would it contain? In what order? Why? What would be your preference between competing tools? What parts would be detailed and which would be surface level?

Would also love to know what your background experience is for context? Industry you work in and roles you've had?

Thought this would be super interesting to me and a lot of others. Thanks!"
5229,2021-02-24 14:52:28,1614171148.0,dataengineering,[Azure] How To Connect Azure Data Factory To On Premise Runtime,lrc5sh,timfcrn,,https://www.reddit.com/r/dataengineering/comments/lrc5sh/azure_how_to_connect_azure_data_factory_to_on/,1.0,0.0,0.0,26332.0,
5230,2021-02-24 17:39:14,1614181154.0,dataengineering,🔴LIVE SOON [11 AM EST]: 𝐀𝐩𝐚𝐜𝐡𝐞 𝐊𝐚𝐟𝐤𝐚 𝐯𝐬 𝐀𝐩𝐚𝐜𝐡𝐞 𝐏𝐮𝐥𝐬𝐚𝐫 - which one to choose?,lrfpu7,BigData-Boutique,,https://www.reddit.com/r/dataengineering/comments/lrfpu7/live_soon_11_am_est_𝐀𝐩𝐚𝐜𝐡𝐞_𝐊𝐚𝐟𝐤𝐚_𝐯𝐬_𝐀𝐩𝐚𝐜𝐡𝐞_𝐏𝐮𝐥𝐬𝐚𝐫/,0.0,0.0,0.0,26336.0,
5231,2021-02-24 18:36:09,1614184569.0,dataengineering,[Survey] Who is responsible to set up a data pipeline infrastructure in your org?,lrh0cd,seanbayarea,,https://www.reddit.com/r/dataengineering/comments/lrh0cd/survey_who_is_responsible_to_set_up_a_data/,3.0,16.0,0.0,26336.0,"Can you share who (which team) in your org is responsible to setup the data pipeline infrastructure?

For example, if the company wanted to setup an ETL (from a list of data source) + snowflake + looker pipeline for data analysts team to work on, is this tasked to the data engineering team or some other (such as infrastructure) team in your org?"
5232,2021-02-24 18:51:27,1614185487.0,dataengineering,WYSIWYG and Restrictive ETL Frameworks,lrhdp3,NotActual,,https://www.reddit.com/r/dataengineering/comments/lrhdp3/wysiwyg_and_restrictive_etl_frameworks/,4.0,12.0,0.0,26337.0,"Throughout my career, I keep seeing companies start with fairly open ETL processes that work well, only to have the drive to standardize make many edge cases nigh impossible.

I'd have less of an issue if it weren't my job to make those edge cases work, but when you get forced to use a framework that doesnt account for your use case, with no flexibility to your assigned timeline and the framework's dev team saying, ""We'll consider it for the next sprint,"" *every sprint*, you end up having to try to ""trick"" the framework into doing what you want, which only works about half the time.

I know I'm being vague here, and that's an unfortunate necessity. Has anyone else dealt with this successfully?"
5233,2021-02-24 18:56:19,1614185779.0,dataengineering,Spark scala v/s pyspark,lrhi0g,crazybunny1212,,https://www.reddit.com/r/dataengineering/comments/lrhi0g/spark_scala_vs_pyspark/,13.0,30.0,0.0,26338.0,"This is more of a scala v/s python question but I believe I can get better understanding from spark developers here.

I come from a java/scala background and have been working in spark based projects for around 2 years. I am now getting an opportunity to work on another spark project, but they use pyspark instead.

I have worked in Python for small, personal data science projects ( jupyter notebooks, pandas, small scripts etc.). Python was very easy to start with, but I feel uncomfortable in working in Python as I rely on typesystem and dynamic typing frustrates me. However the project I am being offered seems interesting and challenging.

Anybody here who has worked on both spark scala and python projects with a similar background as mine? How important is language selection when it comes to working on spark? What is the difference? Did you enjoy working in pyspark or you missed working in scala? Is this a minor thing and it doesn't matter and am I overthinking? 

Also big thing how big is the difference between spark and pyspark?

Thank you!"
5234,2021-02-24 19:26:57,1614187617.0,dataengineering,Airflow DAG is skipping a day,lri9fv,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/lri9fv/airflow_dag_is_skipping_a_day/,3.0,18.0,0.0,26338.0,"Hello jr data engineer here:

&amp;#x200B;

I have a DAG with the following default arguments:

    default_args = {
    'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2021, 2, 12), 'retries': 3, 'provide_context': True, 'on_failure_callback': task_fail_slack_alert }

my first task is an ExternalTaskSensor which is checking if another DAG's task runs successfully.

    wait_for_youtube_dag = ExternalTaskSensor(
    task_id='waiting_for_successful_YOUTUBE', external_dag_id='youtube_daily', external_task_id='youtube_upload', start_date=datetime(2021, 2, 12), mode='reschedule' )

The first day I had it run successfully it looks like this but...

&amp;#x200B;

https://preview.redd.it/hh2l16qkmgj61.png?width=614&amp;format=png&amp;auto=webp&amp;s=2a213ea4270dc1e5a88de75ef6887e984785cb1e

The next day it should have had a run of 2/19, instead I have 2/20 (see below)... why?

&amp;#x200B;

https://preview.redd.it/jib754qrmgj61.png?width=728&amp;format=png&amp;auto=webp&amp;s=cd646eada6fd99c9d9611cd4e0e5dac72fd875c4

The following day that is in reschedule has a run date of 2/22, why does it keep skipping a day? For more context todays date is 2/24"
5235,2021-02-24 20:20:23,1614190823.0,dataengineering,"If you could rebuild your company's data infrastructure from scratch, what tools would/wouldn't make the cut?",lrjkli,LemurPrime,,https://www.reddit.com/r/dataengineering/comments/lrjkli/if_you_could_rebuild_your_companys_data/,9.0,14.0,0.0,26339.0,It's time for Access to go the way of Old Yeller...
5236,2021-02-24 21:16:37,1614194197.0,dataengineering,"Dir, Data Management - Dallas area",lrkxft,txtechzit,,https://www.reddit.com/r/dataengineering/comments/lrkxft/dir_data_management_dallas_area/,5.0,0.0,0.0,26340.0,"My company is hiring for a Director, Data Management  in the Dallas area....

[https://www.linkedin.com/posts/activity-6770419785231663104-TzSJ](https://www.linkedin.com/posts/activity-6770419785231663104-TzSJ)"
5237,2021-02-24 22:30:50,1614198650.0,dataengineering,The Evolution of Data Catalogs: The Data Discovery Platform,lrmo6a,[deleted],,https://www.reddit.com/r/dataengineering/comments/lrmo6a/the_evolution_of_data_catalogs_the_data_discovery/,1.0,0.0,0.0,26341.0,
5238,2021-02-24 22:32:27,1614198747.0,dataengineering,The Evolution of Data Catalogs: The Data Discovery Platform,lrmpkr,nickdni,,https://www.reddit.com/r/dataengineering/comments/lrmpkr/the_evolution_of_data_catalogs_the_data_discovery/,22.0,24.0,0.0,26341.0,
5239,2021-02-25 00:12:06,1614204726.0,dataengineering,Suggestions for a student (practice material),lrohg0,zaakmu,,https://www.reddit.com/r/dataengineering/comments/lrohg0/suggestions_for_a_student_practice_material/,6.0,0.0,0.0,26343.0,"So i am going to do my Master’s in Data Analytics and I want to become a Data Engineer. 
I was wondering if there are sites like leetcode or Algoexpert where someone like me can do some training or practice before my master’s during this summer. 
Leetcode and Algoexpert are more directed towards individual studying software engineering. 
I was wondering if there are any similar sites. I dont want to do online courses just want to study some material from youtube and practice 
on it.
I know a bit about kaggle but dont know where to start.
Any suggestion are welcomed !
Thanks"
5240,2021-02-25 01:35:35,1614209735.0,dataengineering,Airflow TaskFlow API Question,lrqfiv,OkieDaddy,,https://www.reddit.com/r/dataengineering/comments/lrqfiv/airflow_taskflow_api_question/,2.0,0.0,0.0,26345.0,"Is there a way to customize the name of the tasks created in the Taskflow API method? I have the below dag, as an example, and I have it successfully generating out two downstream tasks from the getSessionId task,  However, they are named getAPIObject, and getAPIObject\_\_1. Is there a way to change the task name within the function body to GetAPIObject\_{{objectName}} or something similar? I will be using this api dag to get \~50 different objects from the API, so I'd like them to be more well named, so that when a failure occurs, rather than getAPIObject\_\_35 failed, you'd see getAPIObject\_Customers failed, etc.

    from airflow.operators.python import PythonOperator
    from airflow.models import DAG, Variable
    from airflow.decorators import dag, task
    from airflow.utils.dates import days_ago
    from scripts.my_api_client import MyAPIClient
    
    default_args={
       'owner': 'airflow',
       'start_date':days_ago(31),
       'schedule_interval':'@daily'
    }
    
    @dag(default_args=default_args)
    def api_extract():
    
       @task
       def getSessionId():
          settings = Variable.get('api_connection', deserialize_json=True)
          apiClient = MyAPIClient(settings[username], settings[password])
          apiClient.getSessionId()
          return apiClient.sessionId
    
       @task
       def getAPIObject(objectName, sessionId, dateFrom, dateTo):
          settings = Variable.get('api_connection', deserialize_json=True)
          apiClient = MyAPIClient(settings[username], settings[password])
          apiClient.getAPIObject(objectName=objectName, dateFrom=dateFrom, dateTo=dateTo)
    
       sessionId = getSessionId()
       objectlist = ['Customers','Orders']
       for object in objectlist: 
          getAPIObject(object, sessionId,""{{ prev_ds }}"", ""{{ ds }}"")
    
    api_extract = api_extract()"
5241,2021-02-25 03:48:39,1614217719.0,dataengineering,Should you do all of your data engineering with SQL instead of spark?,lru5fr,fake_actor,,https://www.reddit.com/r/dataengineering/comments/lru5fr/should_you_do_all_of_your_data_engineering_with/,18.0,36.0,0.0,26351.0,"It seems like SQL is associated with data sciences and ad hoc analytics whereas spark is geared towards engineering. But, SQL seems to be more common knowledge than spark and setting up a database like BigQuery is really easy.

So why use spark at all? Just do all of your data processing with SQL.

I have some thoughts on this, but I'm curious to hear other people's perspectives.

&amp;#x200B;

edit: When I say SQL, I mean a database like BigQuery or Snowflake and when I say spark, I mean not using spark sql. "
5242,2021-02-25 05:35:32,1614224132.0,dataengineering,i would love to see you guys roadmaps to dataengineering,lrx5yj,Gusntos,,https://www.reddit.com/r/dataengineering/comments/lrx5yj/i_would_love_to_see_you_guys_roadmaps_to/,53.0,31.0,0.0,26362.0,
5243,2021-02-25 06:26:56,1614227216.0,dataengineering,"Building Data Lakes in AWS with S3, Lambda, Glue, and Athena from Weather Data",lry3hu,amcquistan,,https://www.reddit.com/r/dataengineering/comments/lry3hu/building_data_lakes_in_aws_with_s3_lambda_glue/,7.0,0.0,0.0,26362.0,
5244,2021-02-25 08:38:14,1614235094.0,dataengineering,There's too much here about career paths so here's my blog post on NRT with Delta Lake,ls07su,GleamTheCube,,https://www.reddit.com/r/dataengineering/comments/ls07su/theres_too_much_here_about_career_paths_so_heres/,13.0,0.0,0.0,26363.0,"I figured I share my second blog post on the subject of Near Real Time capture using Delta Lake since there's way too much conversation about what trainings to take or what software you need to know to be a DE. 

Background: I work in a shop with very inexperienced sql developers and need to provide easy to use metadata driven frameworks and notebooks that are easy to clone and modify as they ramp up on OOP and cloud data warehousing concepts. Most of my blog posts are about how we tackle problems every organization faces when using the Azure data stack for BI. 

This blog post is about how we implemented a NRT process to capture data to our middle (silver) zone in our data lake and also used the same metadata tables for overnight batch loading our first (bronze) zone. The development is done in Databricks notebooks that are ran on a three minute tumbling window against an interactive cluster. The outcome of the notebooks is a schema evolving delta table which is used by either other ETL processes to create a dimensional model or directly queried via SQL Analytics for NRT reporting. 

https://corgisandcode.com/2021/02/25/near-real-time-ingestion-with-databricks-delta/"
5245,2021-02-25 08:38:42,1614235122.0,dataengineering,Video tutorial on how to add a regression line to a plot,ls0827,JoachimSchork,,https://www.reddit.com/r/dataengineering/comments/ls0827/video_tutorial_on_how_to_add_a_regression_line_to/,1.0,0.0,0.0,26363.0,
5246,2021-02-25 15:43:23,1614260603.0,dataengineering,Should I pursue a DS Masters ?,ls6v8w,_xplicit,,https://www.reddit.com/r/dataengineering/comments/ls6v8w/should_i_pursue_a_ds_masters/,2.0,6.0,0.0,26377.0,"Hi all,

Hoping to get your opinion or advice.

I am trying to break into a data engineer career and was wondering if diving into a data science masters will help at all or would I be better off self-teaching myself the gaps? 

I believe I have a solid to advance programming concept and a good foundation on SQL and relational database theory.

So, appreciate any advice or perspective. 

Thanks all!"
5247,2021-02-25 15:55:50,1614261350.0,dataengineering,Data Engineering In Insurance,ls73p3,dataengineering_tw,,https://www.reddit.com/r/dataengineering/comments/ls73p3/data_engineering_in_insurance/,2.0,53.0,0.0,26377.0,"Does anybody have experience as a data engineer at a major insurance (F500) company? If so, have you enjoyed it?  I am going to be starting my first DE job at an insurance company soon and while I can find lots of posts about being a data scientist in insurance, I can find little to none about being a data engineer in insurance, and I would find anybody's experiences being shared as helpful.

&amp;#x200B;

Thanks!"
5248,2021-02-25 18:58:38,1614272318.0,dataengineering,Event Driven Databricks ETL with Azure Data Factory,lsb5wa,philmarius,,https://www.reddit.com/r/dataengineering/comments/lsb5wa/event_driven_databricks_etl_with_azure_data/,20.0,16.0,0.0,26383.0,
5249,2021-02-25 22:40:06,1614285606.0,dataengineering,Certifications worth for Data Engineer,lsgb5r,vsmatcha,,https://www.reddit.com/r/dataengineering/comments/lsgb5r/certifications_worth_for_data_engineer/,44.0,77.0,0.0,26390.0,What are some really worth certifications for a Data Engineer (including Cloud) ?
5250,2021-02-25 22:51:59,1614286319.0,dataengineering,How do i start?,lsgksa,[deleted],,https://www.reddit.com/r/dataengineering/comments/lsgksa/how_do_i_start/,0.0,0.0,0.0,26391.0,
5251,2021-02-25 23:30:00,1614288600.0,dataengineering,In-memory systems - advantages/disadvantages?,lshfct,skingkong,,https://www.reddit.com/r/dataengineering/comments/lshfct/inmemory_systems_advantagesdisadvantages/,5.0,2.0,0.0,26391.0,"We have a use case requiring high performance analytics (sub-second, highly concurrent workloads) on \~2TB of data, with streaming data coming in about \~10M events per day... Exploring in-memory systems and trying to weigh the tradeoffs when evaluating various systems. I'm particularly concerned about limiting amount of data in memory - we're not wanting to do this... I know memory is getting cheaper these days, but it feels like our hardware needs could increase rapidly and we need a lot of our data hot for decisioning... so archiving to cold storage (while it will be necessary at some point) is something we are trying to push the limits on not doing frequently. curious on people's experiences with various systems and advantages/disadvantages given our use case. Thx in advance!"
5252,2021-02-26 03:06:01,1614301561.0,dataengineering,Need advice with personal project on archiving financial options data in a data warehouse.,lsm0kf,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/lsm0kf/need_advice_with_personal_project_on_archiving/,6.0,14.0,0.0,26396.0,"I have a personal project that i am starting to store and archive end of day options data using a python api (using FastApi) into a cloud datawarehouse (will probably use a relational db as the ‘warehouse’ instead an actual DWH due to the small size of data, but i will still follow dimensional modelling and DWH principles). 

My current idea of the architecture is:

Python FastApi (a framework similar to flask) is deployed as serverless AWS lambda function.  An extract function is scheduled to run every day after the market closes. It will parse all the necessary data and output it as a blob or csv file to an S3 bucket which i would use as the data lake for raw files. Then once this process is done, another scheduled task will call another function from the api which will load that days raw data file into a relational db (not redshift or any real distributed DWH because again i am not collecting that much data). 

Need advice on if this would accomplish my goal of archiving daily options data. Would it be bad practice to skip the generation of raw data files and storing them in the s3 data lake? And instead make the extract function load and archive the parsed data directly into the DWH? Thanks for any help"
5253,2021-02-26 03:15:32,1614302132.0,dataengineering,How long does it typical take for a person to become an entry-level data engineer?,lsm79d,Phoenix_Rise_UP,,https://www.reddit.com/r/dataengineering/comments/lsm79d/how_long_does_it_typical_take_for_a_person_to/,47.0,42.0,0.0,26397.0,"I was looking at road maps that have been posted on here today. There's so much you need to learn as a data engineer. How many months or years does it take to feel like a decent data engineer? I feel like I have to study everything at once. The last thing I want to do is burn myself out from studying too much, too fast."
5254,2021-02-26 05:03:31,1614308611.0,dataengineering,Data Engineering Capstone Project Suggestions?,lsobro,Cultured_dude,,https://www.reddit.com/r/dataengineering/comments/lsobro/data_engineering_capstone_project_suggestions/,1.0,4.0,0.0,26400.0,"I'm enrolled in Udacity's data engineering nanodegree. I need to design a capstone project.

Does anyone have suggestions for a project and large datasets?  I have a couple of ideas. I thought I ask Reddit given it is a source of vibrancy. I would like to coalesce three or more datasets to create a schematized data lake.

I will use AWS and plan on using the following tools/software S3, Elastic MapReduce, Spark, Airflow, and Redshift.

This is my first data engineering pet project making it my first opportunity to learn new tools/applications. Please share additional recommendations."
5255,2021-02-26 09:37:48,1614325068.0,dataengineering,Has anyone tried or has thoughts on AWS Lake Formation?,lssn9d,Rey661199,,https://www.reddit.com/r/dataengineering/comments/lssn9d/has_anyone_tried_or_has_thoughts_on_aws_lake/,3.0,2.0,0.0,26403.0,"Wondering if anyone tried or has thoughts on AWS’ new service [AWS Lake Formation](https://aws.amazon.com/lake-formation/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc)

My CTO is asking me to look i to it. It seems cool and all that, but I have the feeling that having an opinionated data lake defies the purpose of having a data lake in the first place. Happy to hear your thoughts."
5256,2021-02-26 13:38:48,1614339528.0,dataengineering,Top Data Science Coding Interview Question - Palindrome Algorithm,lsvyxg,[deleted],,https://www.reddit.com/r/dataengineering/comments/lsvyxg/top_data_science_coding_interview_question/,0.0,0.0,0.0,26408.0,
5257,2021-02-26 15:25:51,1614345951.0,dataengineering,Using Delta Tables in Azure Synapse Dedicated/Serverless Databases,lsxn0x,KGoodlip,,https://www.reddit.com/r/dataengineering/comments/lsxn0x/using_delta_tables_in_azure_synapse/,1.0,2.0,0.0,26409.0,"I am currently employed as a Junior Data Developer and recently saw a post saying that Azure Synapse can now create SQL tables from Delta tables. I tried creating an SQL table from a Delta table inside a Delta lake Storage V2, but the table is being populated with extra redundant data (all the data from all snapshots in the folder) when using 'PARQUET' as a file format and wildcard to read the files.I tried creating an external file format for my table but Synapse doesn't accept 'DELTA' as a datatype. I used 'PARQUET' as a file format and used VACUUM on my Delta table to keep only the latest snapshot of it.  Whenever I set the path to a specific file or once there was only a single snappy.parquet file in the Delta table, data was printed properly.

Basically is there any way to create a Synapse Table/External Table that get it's data from a Delta table?If not is there any way to stop Azure Deltalake from creating a new snapshot every time new data is written/updated/deleted?

Script used:

    IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = SynapseParquetFormat') 
    CREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] 
    WITH ( FORMAT_TYPE = PARQUET)
    GO
    
    IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'ExtSource') 
    CREATE EXTERNAL DATA SOURCE [ExtSource] 
    WITH (
        LOCATION   = '*', 
    )
    GO
    
    CREATE EXTERNAL TABLE dbo.Ext_Table (
    	[CostCentre] varchar(8000),
    	[CostCentre_MemberId] int
    )
    WITH (
        LOCATION = 'dimensions/Dim_Example/*.snappy.parquet',
        -- WILDCARD IF THERE IS ONLY ONE FILE OR LATEST FILE MUST BE SPECIFIED
        DATA_SOURCE = [ExtSource],
        FILE_FORMAT = [SynapseParquetFormat]
    )
    GO

Thanks in advance"
5258,2021-02-26 20:10:06,1614363006.0,dataengineering,"In the real world, does Databricks Spark replace ETL tools like Talend?",lt3vgw,ito_integration,,https://www.reddit.com/r/dataengineering/comments/lt3vgw/in_the_real_world_does_databricks_spark_replace/,0.0,14.0,0.0,26416.0," I'm about to graduate into the data science world and I'm doing a lot of reading and practice trying to learn exciting the tools (like Databricks Spark) ... the hardest thing for me is to understand is how databricks spark plays with ETL/ELT tools.

I know there are also a lot of cloud ETL/ELT solutions like Talend, Fivetran, Stitch, Matilion, etc. My question is, in the future will Databricks spark make these tools all less relevant because Spark will take care of the ETL/ELT itself? Or do things not work like this in the real world and in a few years the Talends and Fivetrans of the world will be just as relevant as they are now. Thanks!

[View Poll](https://www.reddit.com/poll/lt3vgw)"
5259,2021-02-26 20:24:11,1614363851.0,dataengineering,What are you looking for in a new job after having a few years experience and being in high demand?,lt475a,hairbear1234,,https://www.reddit.com/r/dataengineering/comments/lt475a/what_are_you_looking_for_in_a_new_job_after/,63.0,92.0,0.0,26415.0,"I am a data engineer with 2 years experience at a big company and a master's degree. After finishing my degree, I was unemployed, pretty much was open to any work, submitting 100s of applications, and talked to every recruiter that slipped into my inbox. 

Now after a few years of experience, having a stable FT job, and entering a hot market, it is weird getting call backs a few days after submitting an application, being able to turn down interest or interviews, or evening naming a higher salary requirement. 

Overall, I'm seeing this passive job search as having the opportunity to choose and think about what I'd like my next career step to be. Obviously this is an extremely lucky spot to be in. But having all the choice does make it a bit more difficult to navigate -- especially when sifting through FT, contract-to-hire, contractor positions, as well as start-ups, FAANGS and other tech companies, and big companies just starting there data science programs.  

I'd like to have a ready set of questions to ask recruiters and interviewers to quickly categorize an opportunity as something I'd like to pursue, or something I pass on. 

What are some of the things you look for besides the standard salary, benefits, team dynamics, and current projects questions? What are some red flags?"
5260,2021-02-26 21:24:13,1614367453.0,dataengineering,Is learning Azure Data Factory &amp; Synapse Analytics worth?,lt5kk7,innotek_JpT,,https://www.reddit.com/r/dataengineering/comments/lt5kk7/is_learning_azure_data_factory_synapse_analytics/,2.0,4.0,0.0,26418.0,"Hi all,  


I saw some job ads which requests Azure Data Factory experience. As I see Azure Data  Factory and Azure Synapse Analytics are user friendly tools where developers can create ETL pipelines by drag&amp;drop. I am considering Azure Data Factory and Synapse Analytics as SSIS/Informatica cloud. (We will not use streaming and big data features of the Synapse Analytics) What are you thinking about these services?  May experience in these tools be helpful at finding a data engineering job?"
5261,2021-02-26 23:33:49,1614375229.0,dataengineering,"What is the tole of a ""data engineering manager""?",lt8fgt,ikwuz,,https://www.reddit.com/r/dataengineering/comments/lt8fgt/what_is_the_tole_of_a_data_engineering_manager/,1.0,2.0,0.0,26418.0,"I am a senior software engineer and thinking of going in a different direction.

I wanna go towards a less technical role and thought of looking for managerial positions. A data engineering manager job caught my interest and it aounds fun. They probably wont hire me due to lack of relevant experience but I wanted to ask anyway. 

My only experience with data science is building a software from the ground up that collects and stores data in a file. I didn't do anything to analyze the data.

I also have knowledge of sql.

I have not done anything with machine learning, data warehousing or business intelligence development.

It seems fun for me to do all sorts of analysis on large amounts of data. 

1.Do you think I can get selected for this kind of role?

2.how will this impact my salary? I am currently doing quite ok.p"
5262,2021-02-26 23:40:13,1614375613.0,dataengineering,Apache Airflow vs. Astronomer.io,lt8k8t,kentmaxwell,,https://www.reddit.com/r/dataengineering/comments/lt8k8t/apache_airflow_vs_astronomerio/,8.0,5.0,0.0,26418.0,"Hello.

I am looking for some community feedback about Astronomer.io. 

**Airflow vs Astronomer**

What is the driving force behind using Astronomer.io versus just using Airflow? I understand that Astronomer provides support for an open-source solution, but is that the main benefit? Does it make Airflow easier for individuals that do not know Python or Airflow? 

**On-Prem Data Sources**

With Apache Airflow, I can run it on containers in my on-prem infrastructure. By doing so I am able to establish data flows from on-prem data sources to my cloud sources. No problem.

If I use Astronomer am I able to do the same? Is there any benefit that Astronomer.io provides me in this context?

**Cost**

Also, it is hard for me to understand what it costs to utilize Astronomer. Can anyone offer some understand of what their service costs and how the cost scales based on usage? 

I really do appreciate any feedback on these points!

Thanks"
5263,2021-02-27 00:00:14,1614376814.0,dataengineering,Question for Experienced DEs,lt8zkv,Suitable-Chemistry-9,,https://www.reddit.com/r/dataengineering/comments/lt8zkv/question_for_experienced_des/,1.0,10.0,0.0,26419.0,"How many recruiters cold message you/email you a week?

How do you filter through them?

It sounds so cocky but I don’t have time for a “15 minute conversation” for every recruiter that messages me. I wish they’d give compensation range right off the back.

I don’t have an insane DE background, but I do have a very polished LinkedIn profile so I don’t know if this is the case for all DEs with my level of experience."
5264,2021-02-27 00:40:33,1614379233.0,dataengineering,How to manage position visibility to maintain company value?,lt9ua1,Tender_Figs,,https://www.reddit.com/r/dataengineering/comments/lt9ua1/how_to_manage_position_visibility_to_maintain/,7.0,30.0,0.0,26419.0,"I'm a ""data engineer"" who manages a data analyst, and we split our workload between backend/db duties and dashboarding/reporting into our two roles. I managed all scripting, ELT, dev work, load tables, dims, facts, etc., and they manage requirements gathering, dashboarding, reporting, etc...

I report to our CFO and he constantly wants me to effectively justify our existence. It's getting rather tiresome because I feel like I could be spending that time enhancing our EDW.

Is this normal?"
5265,2021-02-27 02:09:07,1614384547.0,dataengineering,Loading delimited data into Kafka - quick &amp; dirty (but effective),ltbkqm,rmoff,,https://www.reddit.com/r/dataengineering/comments/ltbkqm/loading_delimited_data_into_kafka_quick_dirty_but/,2.0,0.0,0.0,26421.0,
5266,2021-02-27 02:29:56,1614385796.0,dataengineering,Would I like Data Engineering based on my current job description?,ltbyal,GAD_analytics,,https://www.reddit.com/r/dataengineering/comments/ltbyal/would_i_like_data_engineering_based_on_my_current/,3.0,2.0,0.0,26421.0,"I work as a web analyst at a marketing agency where I provide reporting on Google Analytics and ad campaigns using Google Data Studio dashboards. 

Due to complex reporting requests, and our lack of tools to aggregate all of our data sources into a holistic view, creating these dashboards isn't as simple as connecting to the built-in data connectors in Data Studio and dragging and dropping in charts. Thus, I'm frequently using Google Sheets to do this by using Supermetrics to connect to and pull data from the various sources, and then creating formulas to stitch everything together into a separate Sheet, and then finally feeding that one consolidated Sheet into the dashboard. Based on my handiness with Excel I've been the go-to person for building these ""back-ends"" that other team members can then use to make their reporting easier. 

I quite enjoy this process of understanding the data sources and building processes to stitch it all together into a ""master database"" so it can then be easily visualized and analyzed. 

After starting to learn some Python and SQL I can already see how archaic this process is by doing everything in Google Sheets, and am currently looking into BigQuery to see how I can make this more effective. 

I realize what I do is very basic and not at all what a Data Engineer would be doing - but if I enjoy this kind of stuff, would I possibly like learning and being a Data Engineer? I thought I wanted to be a Data Analyst, but I'm enjoying the behind the scenes work much more than visualization and analysis."
5267,2021-02-27 03:53:34,1614390814.0,dataengineering,Scaling Reporting at Reddit,ltdgnb,drecklia,,https://www.reddit.com/r/dataengineering/comments/ltdgnb/scaling_reporting_at_reddit/,3.0,0.0,0.0,26422.0,
5268,2021-02-27 04:04:50,1614391490.0,dataengineering,How old are you guys and how many YOE do you guys have in this field?,ltdodl,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/ltdodl/how_old_are_you_guys_and_how_many_yoe_do_you_guys/,3.0,13.0,0.0,26423.0,I feel very fortunate to have 2 YOE in this field and i’m just 25. Just curious as to how old you guys are and how many YOE you have?
5269,2021-02-27 05:10:04,1614395404.0,dataengineering,Would you consider data engineering to typically fall under cost center and/or end user support?,lteulo,lessonslearnedaboutr,,https://www.reddit.com/r/dataengineering/comments/lteulo/would_you_consider_data_engineering_to_typically/,6.0,0.0,0.0,26426.0,"I know that I never want to work in an end user support role ever again after my current job. I’m also realizing some of my misery is that I’m labeled a cost center, as is my whole team and department. I’ve never had a technology job where I wasn’t a cost center. I’m trying to direct my efforts through my career with some targets, and wonder if being a data engineer carries a high chance of landing a cost center end user support label compared to otherwise?"
5270,2021-02-27 07:06:40,1614402400.0,dataengineering,"How to learn distributed system, spark, Scala etc",ltgupv,Psychological_Leg493,,https://www.reddit.com/r/dataengineering/comments/ltgupv/how_to_learn_distributed_system_spark_scala_etc/,8.0,10.0,0.0,26431.0,"Hey All,

I'm from a non-big data background and have worked all my life mostly on sql server and oracle. From last three years I've been working on AWS, using python for all the etl work. I think overall I'm pretty good with python, sql and etl/database concepts.

However, ive no knowledge of hadoop ecosystem, I recently had few onsite interviews where I got rejected because of my lack of knowledge in distributed systems. Can someone please help me with how I can pick up these skills on the side in order to clear these interview bars.

One other thing I've noticed with my recent interviews is that lot of companies expect you to know spark for big data engineer roles, which I again don't have. Any guidance on how I can learn that would be super helpful.

Ideally I'd like to spin up clusters in aws and learn there is thats a possibility.

Thank you"
5271,2021-02-27 17:24:16,1614439456.0,dataengineering,Get Experience in Data Engineering,ltqbox,innotek_JpT,,https://www.reddit.com/r/dataengineering/comments/ltqbox/get_experience_in_data_engineering/,37.0,18.0,0.0,26443.0,"Hi all,  


I have some experience in Apache Spark(Scala), Airflow and Python. I am trying to improve myself in data engineering but I can not pass the interviews. Interviewers usually ask qestions about the dataset size that I work with. They also ask performance related questions.   


In my current role, I am developing Python and Airflow. The company is planning to migrate Airflow to Azure Synapse Analytics. We have structured data in the Azure SQL Data Warehouse. We do not have any streaming projects. We also do not have any data set on Azure Data Lake so we do not have unstructured data.  


How can I get enough experience for passing data engineering interviews without an access to a big data set?   
Thanks!"
5272,2021-02-27 19:35:08,1614447308.0,dataengineering,Keep yourself up to date in data engineering,ltt1ax,innotek_JpT,,https://www.reddit.com/r/dataengineering/comments/ltt1ax/keep_yourself_up_to_date_in_data_engineering/,14.0,7.0,0.0,26448.0,"Hi everybody,  


Interviewers may ask about the technologies that you used in the past.   
Data engineering technologies are changing so rapidly. because of that answering interview questions proeprly may be difficult. For instance, in one interview I told about my experience in Hive. I said that "" Hive does not support ACID transactions so I prefer using DataBricks Delta Lake instead of Hive"". Interviewer said that Hive supports ACID now since the version ...   


Similar cases may happen when the interviewer ask: ""what is the difference between the technology A and technology B""  In order to reply this kind of questions correctly, how I can I keep myself up to date in the technologies that I developed/used in the past?"
5273,2021-02-27 20:11:16,1614449476.0,dataengineering,How do you actually do testing as a data engineer/BI developer?,ltttvd,BoofThatShit914,,https://www.reddit.com/r/dataengineering/comments/ltttvd/how_do_you_actually_do_testing_as_a_data/,10.0,8.0,0.0,26448.0,"I've always heard from software engineers on forums/reddit that you should put your ETL pipelines into modules and write testing/validation scripts. What exactly does that look like? 

For me, as a data warehousing/BI developer, ""testing"" has always been a pretty vague concept. Usually I just write some basic ad-hoc SELECT statements against my DW after I run my pipeline, and if it seems to match the source data, I check it off and move on. Maybe I save those scripts into my source control repository, but that's about it.

What else should I be doing?"
5274,2021-02-27 21:02:14,1614452534.0,dataengineering,Does anyone have experience in DE freelancing?,ltuw7h,francesco1093,,https://www.reddit.com/r/dataengineering/comments/ltuw7h/does_anyone_have_experience_in_de_freelancing/,3.0,13.0,0.0,26446.0,"Hi all, I am currently a consultant in DE (Azure, SSIS, SQL Server), doing also some DataScience (Python).

Do you know what are the prospects for a freelancing career in this field? Or can you suggest close fields to move to for someone interested in freelancing in a few years (maybe DS, DA or IT strategy)?"
5275,2021-02-27 21:04:21,1614452661.0,dataengineering,Does nobody use GCP?,ltuxti,fake_actor,,https://www.reddit.com/r/dataengineering/comments/ltuxti/does_nobody_use_gcp/,8.0,17.0,0.0,26446.0,"My current company uses GCP, but I have yet to hear of another company using it. I've been looking for jobs and it seems most descriptions list AWS and a few have Azure. This is a little disappointing because GCP seems more friendly to developers than AWS (not sure about Azure because I haven't used it).

Anyone else use GCP at their company?"
5276,2021-02-27 21:14:00,1614453240.0,dataengineering,Where to store derived data that is expensive to compute and changes often.,ltv4oe,spartan-laser,,https://www.reddit.com/r/dataengineering/comments/ltv4oe/where_to_store_derived_data_that_is_expensive_to/,1.0,1.0,0.0,26446.0,"Hello everyone I am a web developer trying my hardest to tackle a DE problem I am out of my depths on and I was hoping this community could help :)

**Context**:

I have derived data (user similarity and item similarity scores) that are expensive to compute and can change whenever a user likes something (very often). I'm still trying to figure out the right frequency/trigger to reprocess all data. This data will be used to make recommendations to the user later on. This sounds like a data pipeline, but I don't really know what that is.

**Questions:**

1. What resources will help me tackle this problem?
2. Where should I store the derived data?
3. I was going to compute the derived data with a task-queue-worker system. Is there a better pattern for computing expensive results that changes often?

**Here are a few solutions I have considered for the storage of derived data:**

* I could store the derived in my database, but this violates a key rule of database design.
* Maybe I could cache it in a Redis cluster? That sounds hard to get right and deploy...
* A third-party solution like databricks or AWS? I'm really unfamiliar with their offerings in this area
* A totally new database?

Many thanks in advance to those who are willing to help."
5277,2021-02-27 22:51:34,1614459094.0,dataengineering,Got a new job as a DE and transitioning from Quality Assurance,ltx1lo,Liverpool1900,,https://www.reddit.com/r/dataengineering/comments/ltx1lo/got_a_new_job_as_a_de_and_transitioning_from/,2.0,0.0,0.0,26448.0,"Hello everyone,

I recently got a job offer for a DE position and am super excited for it. I start in a week and my company has told me this is going to be the stack I will be working with. I am looking for any advice, tips in general that you all can share with me as I am humbled and honoured by looking at the posts in this subreddit. In addition, can you all guide me to some great learning videos or resources online for the following tools which I will use? I am having a difficult time finding resources or knowing if the resource is worth my limited time investment at the moment due to juggling my current job. I really appreciate all the inputs you guys give me and thank you all once again.

* **Analytics Database:** Vertica 10
* **Secondary Analytics Database :** PostgreSQL 12
* **ETL Tool:** Talend Big Data 6.3.1.
* **Scripting:** Python"
5278,2021-02-27 23:58:34,1614463114.0,dataengineering,The cloud cost of production ETL?,ltybpb,blue_sky_time,,https://www.reddit.com/r/dataengineering/comments/ltybpb/the_cloud_cost_of_production_etl/,7.0,2.0,0.0,26451.0,"Hi folks, I'm trying to do some market research to understand if the cost of production ETL jobs (e.g. with Spark/EMR/Glue run daily) is a significant part of your cloud bill?

If you could cut that bill by 50% with no changes, would that be financially relevant for you?"
5279,2021-02-28 00:39:12,1614465552.0,dataengineering,Introduction to Delta Lake on Apache Spark ... for Data Engineers,ltz41z,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/ltz41z/introduction_to_delta_lake_on_apache_spark_for/,36.0,2.0,0.0,26451.0,
5280,2021-02-28 02:40:37,1614472837.0,dataengineering,New data engineer here. Need some guidance.,lu1c0c,isleepbad,,https://www.reddit.com/r/dataengineering/comments/lu1c0c/new_data_engineer_here_need_some_guidance/,0.0,3.0,0.0,26458.0,"So I just pivoted into the wonderful world of data engineering. I'm coming from an aerospace engineering background and my skillset is based on the fact that I did some db management in SQL, ml projects in python and have some cloud stack experience.


I've seen a lot of good advice on here about projects to do to gain some experience and I'm doing those in my spare time before I start. 


But my question here is what kind of questions do I ask to get up to speed with what's happening in the company? What do I look for? I'm a big picture kind of guy so I like to understand what's happening before I do anything i.e. mess shit up.

Thanks in advance."
5281,2021-02-28 02:48:44,1614473324.0,dataengineering,How many rows of data should be used for a beginner project?,lu1h6s,throwback772,,https://www.reddit.com/r/dataengineering/comments/lu1h6s/how_many_rows_of_data_should_be_used_for_a/,0.0,5.0,0.0,26458.0,"Would +140,000 be sufficient?"
5282,2021-02-28 05:27:09,1614482829.0,dataengineering,"On-Call, is this something data engineers tend to be involved in?",lu4cfn,lessonslearnedaboutr,,https://www.reddit.com/r/dataengineering/comments/lu4cfn/oncall_is_this_something_data_engineers_tend_to/,2.0,9.0,0.0,26463.0,Do you all rotate on-call like general IT would?
5283,2021-03-02 17:38:00,1614699480.0,dataengineering,What vendor-neutral SQL clients are you all using?,lw4s36,Ooberdan,,https://www.reddit.com/r/dataengineering/comments/lw4s36/what_vendorneutral_sql_clients_are_you_all_using/,1.0,6.0,0.0,26562.0,"I've got a background in traditional data warehousing and moved towards more engineering duties the last few years. I've always used Oracle's SQL Developer, which is fine for Oracle, but not so much anything else. Now I'm interfacing with other data sources, I'm looking for something a bit more universal. I'm keen to hear what /r/dataengineering uses.

Thanks in advance!"
5284,2021-03-02 17:55:42,1614700542.0,dataengineering,Getting a FAANG DE interview,lw57co,Svidrigailovvv,,https://www.reddit.com/r/dataengineering/comments/lw57co/getting_a_faang_de_interview/,1.0,0.0,0.0,26564.0,"Hi,

I have about 2 years of experience working with data, building pipelines; using scheduling tools etc.

Current workplace is in Financial Services (not fintech) and the work has become a bit boring, so I’ve started practicing interview questions on Leetcode etc.

Problem is, every time I apply to any of the companies doing very interesting DE, I never get a response. Never.

Just wondering if there are any specific tips on how to go about these applications.

I’m in the UK btw.

Any help is appreciated. Cheers!"
5285,2021-03-02 18:08:31,1614701311.0,dataengineering,Saving 50kb csv’s daily. Want to archive data in DWH for analytical purposes in the future. Should i use a DWH?,lw5izm,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/lw5izm/saving_50kb_csvs_daily_want_to_archive_data_in/,1.0,6.0,0.0,26564.0,"Im saving around 50kb of data daily as flat files locally (csv). I eventually want to load them all into a cloud (specifically cloud) DWH (probably redshift, snowflake or bigquery) to ultimately do analysis on it. But would it even make sense to use a cloud DWH for personal use for data this small? An alternative approach would just be to use a normal relational cloud db. But my concern is the daily size of the data in the future for might increase a lot based on my personal needs and im concered the migration from relational dwh to a traditional DWH might take more effort than pre planning now. 

What are the cost differences in terms of a relational cloud db for this size vs archiving in a traditional DWH. Advice appreciated thanks"
5286,2021-03-02 18:31:25,1614702685.0,dataengineering,Help with setting up apache kafka/druid via docker-compose,lw63l1,jc-de,,https://www.reddit.com/r/dataengineering/comments/lw63l1/help_with_setting_up_apache_kafkadruid_via/,1.0,0.0,0.0,26566.0,"Hi all, I am trying to connect apache kafka --&gt; druid via docker-compose. All of the services get up and running except one and I am getting this error on middlemanager which exits with a code 1: 

    sed: /jvm.config: No such file or directory

my docker-compose file: 

    services:
        zookeeper
        kafka
        
        # druid services
        coordinator
        broker 
        historical
        middlemanager:
            image: apache/druid:0.20.0
            container_name: middlemanager
            volumes:
              - ./storage:/opt/data
              - ./supervisor/:/supervisor
              #- middle_var:/opt/druid/var
            depends_on:
              - zookeeper
              - postgres
              - coordinator
            ports:
              - ""8091:8091""
            command:
              - middlemanager
            env_file: 
              - environment
        router

I am just not sure how to debug at this point; the error clearly says the folder doesn't exist but I have no idea whether I should be mounting something and where it should go? there also isn't anything regarding middlemanager in the environment config file...so I am lost. 

Jeez, apache druid has not been nice to set up. 

Thanks for your help!"
5287,2021-03-02 20:41:02,1614710462.0,dataengineering,The New Rules of Data Quality,lw99rp,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/lw99rp/the_new_rules_of_data_quality/,1.0,8.0,0.0,26574.0,"Should data engineers be doing more than unit testing when it comes to data quality? 

[https://towardsdatascience.com/the-new-rules-of-data-quality-5e4fdecb9618?source=friends\_link&amp;sk=2c4e339d7c8ee57767da734ff073fd51](https://towardsdatascience.com/the-new-rules-of-data-quality-5e4fdecb9618?source=friends_link&amp;sk=2c4e339d7c8ee57767da734ff073fd51)"
5288,2021-03-02 21:28:00,1614713280.0,dataengineering,Common-sense data infrastructure for many small projects?,lwad05,lemmerlac,,https://www.reddit.com/r/dataengineering/comments/lwad05/commonsense_data_infrastructure_for_many_small/,1.0,1.0,0.0,26576.0,"I'm the first Data Engineer hire at an innovation shop focused around public health. My background in engineering practices is pretty shallow, so I'm really grateful for places like this where I can learn more about what experienced folks do in places where the data culture is more mature. So, thanks!

I've been here about 9 months now, and they're looking at making a second hire - one with about the same level of experience I had when I started. I'm thinking it's a good time to think about adopting some more modern practices. But I want to make sure I make suggestions that would actually improve things rather than simply add unneeded complexity.

At any given time, there tends to be around 4-7 different projects going, all with different stacks depending on which team set things up. Here are a couple of examples:

**Project 1**
Size of Data: ~30mil records

Ingestion: AWS Lambda/Batch jobs grabbing data from ~25 sources and storing in S3
Storage: MongoDB Atlas
Transformation: Python scripts running on AWS Lambda/Batch
Analysis: Wholly custom visualization frontend

**Project 2**
Size of Data: ~5mil records

Ingestion: User transactions stored in MongoDB / csvs loaded to S3
Storage: MongoDB Atlas for transactions, Snowflake for analysis
Transformation: Python, SQL, cli tools running on AWS Batch
Analysis: Tableau

You get the idea. Lots of AWS services, lots of Postgres, lots of Mongo, a lot of time spent debugging custom ETL scripts. I think one project has much larger data size but that's an outlier.

So, my question: given that ingestion is pretty complex, but none of this is huge data, what practices actually make sense? Snowflake/Redshift seems like overkill for data of this size, for instance.

My reading keeps taking me to the same set of tools and practices centered around bigger data, but nothing quite for tackling lots of little projects like this in such a way that it makes it a better experience for the dev team.

Any thoughts from wiser heads hugely appreciated!"
5289,2021-03-02 21:51:41,1614714701.0,dataengineering,Looking for a detailed online course to learn DataBricks + PySpark,lwavw1,Jigsaw1609,,https://www.reddit.com/r/dataengineering/comments/lwavw1/looking_for_a_detailed_online_course_to_learn/,1.0,5.0,0.0,26578.0,"I have been working in Data Warehousing and ETL since a long time using tools like Informatica and SQL. Few months back, our company added DataBricks to the ecosystem and we were asked to explore and build pipelines using a combination of PySpark and SQL.

I did several online courses to learn the technology, and am able to perform day to day activities using that knowledge and searching on google. However most of the courses are pretty basic, and try to cover a lot of stuff like Data Science, ML and R in a short time which are not relevant to my current role. I have access to LinkedIn learning and Microsoft trainings through my company, however can someone guide me towards some more online courses (e.g. Udemy or youtube videos) which concentrate on practical usage of PySpark with DataBricks for Data Engineers (not Data Scientist) and not superficial and theoretical knowledge."
5290,2021-03-02 21:59:45,1614715185.0,dataengineering,What is your salary and where are you from?,lwb26q,The_Alpacas,,https://www.reddit.com/r/dataengineering/comments/lwb26q/what_is_your_salary_and_where_are_you_from/,1.0,280.0,0.0,26581.0,"I’m from San Fran area, I get paid 77,000 base with about 11k in bonuses/benefits. 

Would love to contrast with other data engineers to figure out a median/average salary."
5291,2021-03-02 22:59:21,1614718761.0,dataengineering,Refactoring Legacy Code,lwcd8w,metrd,,https://www.reddit.com/r/dataengineering/comments/lwcd8w/refactoring_legacy_code/,1.0,3.0,0.0,26587.0,"Hey I am a lone DE and I recently inherited more than 10,000 lines of undocumented R code that runs pipelines for a multi-tenant system. For the most part, it is the way it is because of monkey patching and hard coding my boss has been doing since he joined the company as a consequence of unreliable data sources. The SE team doesn't take responsibility for the mess they write to the database and it's affecting workflows since onboarding new tenants is as manual as it gets and having to account for the SE tech debt in pipeline code makes things very unpredictable. Each tenant has their somewhat custom pieces of code that filter in a specific tenant's data and tracks certain KPIs but their calculation is not consistent across the board. 

As it stands I've been tasked with refactoring it but I am having trouble figuring out how to handle it. I have basic knowledge of R at best and feel most comfortable with Python and SQL. Some advice on ways to approach this would be appreciated."
5292,2021-03-02 23:05:01,1614719101.0,dataengineering,Any list of URLs or repo for best ETL batch architecture,lwchu1,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/lwchu1/any_list_of_urls_or_repo_for_best_etl_batch/,1.0,6.0,0.0,26588.0,"Guys do we have any list of solution architecture diagram for aws in ETL

Currently we use step functions and glue . However to achieve near time .we’ll be running batch process 10 times a day In different intervals. Approx 2 -3 hours of run time per load.  How can we improve this flow a lot.

Also what could be the best alternative for landing zone we decommissioning old legacy servers to bring the files to s3 . Shall I use control towers or aws sftp solution."
5293,2021-03-02 23:14:52,1614719692.0,dataengineering,Pipeline with only azure synapse available and small data.,lwcptk,expatwithajetpack,,https://www.reddit.com/r/dataengineering/comments/lwcptk/pipeline_with_only_azure_synapse_available_and/,1.0,1.0,0.0,26591.0,"Currently stuck with azure synapse: offering a low-code solution to ETL processes with spark. 

Look it’s great and probably good for large scale solutions, but we have tiny, medical data that changes from time to time (due to clinician failures) 

I suggested opting out of synapse and simply using azure functions with a blob trigger (when the file is added, run through a series of severless functions, store necessary data in SQL) 

I don’t know enough about DE or azure to think if this is a good solution, and I’m the only one on the team responsible for this. 

I desperately need advice."
5294,2021-03-02 23:24:24,1614720264.0,dataengineering,Transitioning from DS to DE - Best Courses/Resources available,lwcx2m,lmarcondes95,,https://www.reddit.com/r/dataengineering/comments/lwcx2m/transitioning_from_ds_to_de_best_coursesresources/,1.0,6.0,0.0,26592.0,"I'm an Industrial Engineering graduate that fell in love with tech and programming and I'm currently transitioning from a data science to a data engineering position in my company. I'm looking for the best resources to help me in this transition, so far I've looked at a couple post grad courses, one from Purdue University in Simplilearn and the Nanodegree from Udacity (even though a lot of you guys said it's not worth it). I'm also really considering just subscribing to O'Reilly and taking courses there as they are needed, as I've always liked the quality of the content on the platform. Really need some content focusing on AWS technologies, DWH and Data Lakes. Any help and/or advice would be much appreciated!!!

&amp;#x200B;

I've got a pretty good experience with Python and SQL, and some experience with back-end and front-end JS"
5295,2021-03-02 23:34:51,1614720891.0,dataengineering,Data Quality Management in three steps,lwd4w9,RaikoL,,https://www.reddit.com/r/dataengineering/comments/lwd4w9/data_quality_management_in_three_steps/,1.0,0.0,0.0,26593.0,
5296,2021-03-03 00:19:51,1614723591.0,dataengineering,"Clickhouse as an alternative to ElasticSearch and MySQL, for log storage and analysis, in 2021",lwe4eg,dbcicero,,https://www.reddit.com/r/dataengineering/comments/lwe4eg/clickhouse_as_an_alternative_to_elasticsearch_and/,1.0,1.0,0.0,26594.0,
5297,2021-03-03 01:25:03,1614727503.0,dataengineering,Airflow error: 'NoneType' object has no attribute 'dag_id',lwfhca,kristiclimbs,,https://www.reddit.com/r/dataengineering/comments/lwfhca/airflow_error_nonetype_object_has_no_attribute/,1.0,0.0,0.0,26595.0,"Hello!

I created 4 subdags that all look similar to the below:

&amp;#x200B;

    DAG_NAME = 'my_dag'
    
dag = DAG(DAG_NAME,
 default_args=default_args,
 schedule_interval='0 */5 * * *',
 max_active_runs=1,
 catchup=False
 )
    
    def my_function(parent, child, start_date, schedule_interval):
    subdag = DAG(f'{parent}.{child}', schedule_interval=schedule_interval,start_date=start_date)
    
    DummyOperator(""blah blah blah"")
    
    
    my_task_name = SubDagOperator(
 subdag=my_function(
        DAG_NAME, 'my_function', default_args['start_date'], dag.schedule_interval),
 task_id='my_function'
)
    
    my_task_name

But I'm seeing this error: 

`'NoneType' object has no attribute 'dag_id'`

coming from this line:

`task_id='my_function'`

any suggestions are always welcome!"
5298,2021-03-03 06:01:33,1614744093.0,dataengineering,Advice for a statistics masters student trying to get into data engineering,lwkny5,secant78,,https://www.reddit.com/r/dataengineering/comments/lwkny5/advice_for_a_statistics_masters_student_trying_to/,2.0,0.0,0.0,26709.0,"I am currently getting a masters in statistics but feel like the developer side of data is much more suited for me. I struggle more with learning statistical theory compared to my classmates but I am better at the coding side. In an ML class I took recently, I was the one who wrote most of the code and enjoyed the coding process much more compared to the rest of my group (not trying to humble brag).

So far, I have a few projects listed on my resume that detail my experience in coding ML algorithms in Python, which does show my competence in coding in general, but it is not directly applicable experience to a data engineering job. What kinds of courses/certifications should I be getting, as well as projects I could do to show that I am qualified for a junior data engineering position?"
5299,2021-03-03 08:06:15,1614751575.0,dataengineering,Facebook Data Engineer Onsite - How did you prepare and what would you have done differently?,lwmt4b,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/lwmt4b/facebook_data_engineer_onsite_how_did_you_prepare/,1.0,0.0,0.0,26715.0,"This might be a long shot but does anyone who has gone through the on-site loop want to break down how the interviews went and what the most efficient way to prepare for it was/is?

Unlike the technical phone screen there is a lot more ambiguity around this even after getting the prep package in terms of how to prepare. It's also testing a much broader set of topics - Product sense, ETL, Data Modeling, SQL, Python, Ownership.

There will be 3 technical rounds -  2 ETL rounds. 1 data modeling.

I'm mostly confused about the ETL rounds and how best to prepare. It doesn't sound like there will be any algorithmic leetcode style questions, and the python will be relevant to the ETL process. So I don't think grinding leetcode will help. There will also be one streaming question.

Any suggestions from someone who did this recently will help.

&amp;#x200B;

Thanks in advance."
5300,2021-03-03 09:45:50,1614757550.0,dataengineering,Data Engineer Course for complete beginners?,lwocgi,Shiroelf,,https://www.reddit.com/r/dataengineering/comments/lwocgi/data_engineer_course_for_complete_beginners/,19.0,17.0,0.0,26719.0,"Hi, sorry if this question has been asked many times but I can't find any resources for me. I want to find some courses that teach data engineering not data science like they teach stuff about data warehouse, ETL, and stuff. I have to try Datacamp when it gives 3 months free but I feel the context is too light and Codecademy emphasizes more on the Data Scientist part. Does anyone know a good course that helps me to the point that I can make a project on my own?"
5301,2021-03-03 09:47:08,1614757628.0,dataengineering,If you had an extra 10k/year to throw at certifications or other extra education what would you spend it on?,lwod34,TheCauthon,,https://www.reddit.com/r/dataengineering/comments/lwod34/if_you_had_an_extra_10kyear_to_throw_at/,6.0,24.0,0.0,26719.0,"I work for a company that lets us expense 10k on extra education.

What would you spend it on in the DE space that is worth the time and money?"
5302,2021-03-03 10:09:47,1614758987.0,dataengineering,Cleaner analytics workflows with Avo,lwop9z,data_pointer,,https://www.reddit.com/r/dataengineering/comments/lwop9z/cleaner_analytics_workflows_with_avo/,0.0,0.0,0.0,26719.0,"We created [Avo](https://www.avo.app) for makers who were tired of spending more time managing their data than building their products. Avo works with your self-serve analytics tools to offer a guided collaboration layer for Product Managers, Data Practitioners and Engineers. Check out our latest release and get instant feedback on your tracking plan: [https://www.avo.app/blog/launching-now-error-proof-your-analytics-with-avo-for-pms](https://www.avo.app/blog/launching-now-error-proof-your-analytics-with-avo-for-pms)

We're live on Product Hunt today as well. We'll be answering questions all day there [https://www.producthunt.com/posts/avo-for-pms](https://www.producthunt.com/posts/avo-for-pms) and we're streaming live at 10am PT on Twitch to show you how! Design better data, error proof your tracking and sync it all with your existing tools (Mixpanel, Amplitude, Segment and more!)

We’d be super stoked if you’d check us out today on Product Hunt and join us on Twitch!"
5303,2021-03-03 12:02:40,1614765760.0,dataengineering,Data Quality Testing: Ways to Test Data Validity and Accuracy,lwqbvt,ydr-,,https://www.reddit.com/r/dataengineering/comments/lwqbvt/data_quality_testing_ways_to_test_data_validity/,44.0,14.0,0.0,26722.0,
5304,2021-03-03 14:38:45,1614775125.0,dataengineering,How do you use Apache Airflow?,lwsxff,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/lwsxff/how_do_you_use_apache_airflow/,5.0,12.0,0.0,26724.0,"At my last two jobs we've used Apache Airflow for orchestration and dependency management of pipelines, but we've always offloaded the actual compute/work to other systems like Kubernetes or EMR etc etc. Curious how this looks for others? Do most people actually run loads on Airflow workers? We find our datasets are just too big, usually requires Spark etc."
5305,2021-03-03 15:36:36,1614778596.0,dataengineering,How many abbreviations exist within your data models?,lwu166,importpandaaspd,,https://www.reddit.com/r/dataengineering/comments/lwu166/how_many_abbreviations_exist_within_your_data/,1.0,6.0,0.0,26726.0,"What's a reasonable amount of abbreviations for column names? Whether it be at the physical or logical layer?

With modern technology allowing for longer character limits, I like the idea of self documented column names with as little abbreviations as possible. What have you done in the past and how has it worked for your organization?"
5306,2021-03-03 16:03:26,1614780206.0,dataengineering,Convert PostgreSQL query to BigQuery Standard SQL,lwulyz,emnak,,https://www.reddit.com/r/dataengineering/comments/lwulyz/convert_postgresql_query_to_bigquery_standard_sql/,12.0,2.0,0.0,26728.0,"I recently had to **migrate a PostgreSQL Database to Google's Cloud Data Warehouse Solution BigQuery**. So, I had a bunch of PostgreSQL queries to convert using **BigQuery Standard SQL**. It was not as easy as I thought it would be.

Here is an [article](https://www.sicara.ai/blog/convert-postgresql-bigquery-standard-sql) I just published with some of the tips I learned."
5307,2021-03-03 17:56:48,1614787008.0,dataengineering,How Oracle manages Airflow in research and production and highlights on Airflow 2.0 - live webinar event,lwx5hk,databandai,,https://www.reddit.com/r/dataengineering/comments/lwx5hk/how_oracle_manages_airflow_in_research_and/,0.0,0.0,0.0,26732.0,
5308,2021-03-03 17:57:21,1614787041.0,dataengineering,How Oracle manages Airflow in research and production and highlights on Airflow 2.0 - live webinar event,lwx5y2,databandai,,https://www.reddit.com/r/dataengineering/comments/lwx5y2/how_oracle_manages_airflow_in_research_and/,0.0,0.0,0.0,26732.0,
5309,2021-03-03 19:17:52,1614791872.0,dataengineering,Scientific research databases and analysis,lwz44r,DarioSidd,,https://www.reddit.com/r/dataengineering/comments/lwz44r/scientific_research_databases_and_analysis/,1.0,4.0,0.0,26733.0,"Hello, I am scientific researcher. 
I know python, numPy and sql basics.
I have data from pre and post transplantations, approximately 200 patients. Blood tests, different types of scales... I want to make database and then analyse it. 
Which languages do i need? Is there any guide how to start this project? Is there any rules?
My goal is to study working on databases and data analysis and get more info from my results.
Thank you in advance!"
5310,2021-03-03 19:38:21,1614793101.0,dataengineering,Looking for blogger to write Airflow vs Data Pipelines comparison,lwzldy,foundergiant,,https://www.reddit.com/r/dataengineering/comments/lwzldy/looking_for_blogger_to_write_airflow_vs_data/,4.0,6.0,0.0,26732.0,I'm looking for someone experienced with Airflow to compare it to Data Pipelines ([https://datapipelines.com/](https://datapipelines.com/)) and write a blog post highlighting differences and similarities. Ideally you will already have a few tech blog posts published somewhere. DM or comment to discuss details.
5311,2021-03-03 23:09:42,1614805782.0,dataengineering,"Which degree would be more helpful in landing a DE position, just by the title?",lx4fi1,[deleted],,https://www.reddit.com/r/dataengineering/comments/lx4fi1/which_degree_would_be_more_helpful_in_landing_a/,0.0,0.0,0.0,26739.0,"[deleted]

[View Poll](https://www.reddit.com/poll/lx4fi1)"
5312,2021-03-03 23:24:52,1614806692.0,dataengineering,Project-based self-directed learning is exhausting but rewarding!,lx4rb9,Phoenix_Rise_UP,,https://www.reddit.com/r/dataengineering/comments/lx4rb9/projectbased_selfdirected_learning_is_exhausting/,51.0,36.0,0.0,26739.0,"Today, I wanted to do something different in my learning journey. After yesterday's research and sifting through my old notes, I decided on what type of project I wanted to do. 

I love listening to a lot of music, which is why I wanted to build a project related to it.  I want to use Spotify's API to collect my Spotify account data so that I extract data from my Spotify account. My goal is to create a table that tracks what music I liked listening to in real-time. Being a newbie, it was daunting researching the proper tools I needed for this project. However, it was a good learning experience. I learned that it's important to understand what tools are essential for your goals rather than what tools are trending. Also, I'm learning the importance of slowing down and reading documentations. 

I feel a little bit more confident in my understanding of data engineering. If there are any tips I should know while doing my project, please let me know!"
5313,2021-03-03 23:42:16,1614807736.0,dataengineering,What do I need for a fast distributed pyspark analysis?,lx54hp,alecrimi,,https://www.reddit.com/r/dataengineering/comments/lx54hp/what_do_i_need_for_a_fast_distributed_pyspark/,3.0,20.0,0.0,26739.0,"Assuming I want to set up a structure with distributed images and analyzing them with PySpark, what are the steps I have to follow to set up first an infrastructure?

Is this realistic:

1- buy few computers (to make a cluster)

2. Set up HDFS

3. I am ready to use Pyspark?"
5314,2021-03-04 03:12:39,1614820359.0,dataengineering,How to get real time hands on experience on data engineering projects ?,lx94xb,Iffy-diffy,,https://www.reddit.com/r/dataengineering/comments/lx94xb/how_to_get_real_time_hands_on_experience_on_data/,5.0,2.0,0.0,26745.0,"Hi guys ,sorry if this question has been asked before ,but I wanted to know how to best get hands on projects . I’m learning DE and planning to certify for google data engineer .post that I want to get hands on some projects .any suggestions ? Appreciate it"
5315,2021-03-04 03:50:20,1614822620.0,dataengineering,Entering the industry with only GCP (Google Cloud) Professional Data Engineer Certification Exam?,lx9tcm,Dapper_Distribution8,,https://www.reddit.com/r/dataengineering/comments/lx9tcm/entering_the_industry_with_only_gcp_google_cloud/,7.0,8.0,0.0,26744.0,Has anyone been able to enter the industry with only the GCP Professional Data Engineer certification? I'm coming from a totally unrelated industry and hoping this certification can land me any type of job in data engineering. Hopefully I'm not completely wasting my time with this certification. I have no CS background.
5316,2021-03-04 06:32:45,1614832365.0,dataengineering,"What does ""logging sources"" mean in the context of ETL/ELT?",lxcnwf,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/lxcnwf/what_does_logging_sources_mean_in_the_context_of/,1.0,4.0,0.0,26754.0,"I've seen this term in data engineering job/interview descriptions and I'm not sure I understand what it means. When I think of logging in the context of ETL, I think of a log of all actions/failures that the ETL program performs. Not sure how that relates to ""logging sources""

Here is an example of it used in a sentence:

&gt;Demonstrate understanding of SQL functions used to load/transform data from logging sources into target in an efficient, scalable manner. 

Also:

&gt;Create logging designs of how you think data should be captured."
5317,2021-03-04 08:47:55,1614840475.0,dataengineering,Why a block size should be configured with a multiple of 512 KB?,lxenyd,theOrignalNiazi,,https://www.reddit.com/r/dataengineering/comments/lxenyd/why_a_block_size_should_be_configured_with_a/,0.0,4.0,0.0,26752.0,I was taking an online lecture and my teacher gave us an assignment and this question was in it. I would appreciate it if it could be answered.
5318,2021-03-04 09:17:59,1614842279.0,dataengineering,Streaming ETL patterns preferably in python,lxf3at,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/lxf3at/streaming_etl_patterns_preferably_in_python/,2.0,6.0,0.0,26754.0,"I'm prepping for an interview and I know one round has to do with streaming ETL (the other is batch).

I'm not provided a lot of information and I don't deal with streaming/real-time data where I work. Everything is batch. Is there a resource I can use that explains the patterns?

I'm going to be expected to code (without execution) in python. There will be no expectation to use a streaming engine or importing specific libraries. It should basically be done in vanilla python.

So, I'm assuming I'll be provided some sample data and a scenario and asked to build a realtime etl pipeline. 

I'd like to understand how to think of the pattern and the data structures I'll need. Will the data be coming in as tuples that I'll need to append to a list and process where the one element of the tuple is the eventide and the other is the timestamp?

Will I need to create a data structure like a Queue to process it? I.e. FIFO.. so the oldest events get processed first and discarded?

Any resources that can help me prep and conceptualize this in an ETL framework would help me."
5319,2021-03-04 11:37:41,1614850661.0,dataengineering,Video introduction to the join functions of the dplyr package in R programming,lxgw70,JoachimSchork,,https://www.reddit.com/r/dataengineering/comments/lxgw70/video_introduction_to_the_join_functions_of_the/,0.0,3.0,0.0,26759.0,
5320,2021-03-04 12:32:33,1614853953.0,dataengineering,IKEA Interviewing Process,lxhmiz,HiIAmAlbino,,https://www.reddit.com/r/dataengineering/comments/lxhmiz/ikea_interviewing_process/,24.0,17.0,0.0,26761.0,"Hi everyone,

I'm being considered for a position as data engineer for IKEA in Malmo, Sweden, and couldn't find much info online on how's the process, what the interviews are like and if there's any code challenges. I'm really interested in the position and would appreciate your help.

Do you guys have info that can help? Thanks!"
5321,2021-03-04 14:08:48,1614859728.0,dataengineering,Code your first DAG in Apache Airflow for Beginners,lxj2bt,marclamberti,,https://www.reddit.com/r/dataengineering/comments/lxj2bt/code_your_first_dag_in_apache_airflow_for/,18.0,6.0,0.0,26763.0,
5322,2021-03-04 14:37:41,1614861461.0,dataengineering,Data Mesh – Rethinking Enterprise Data Architecture,lxjjlt,VibhutiSingh,,https://www.reddit.com/r/dataengineering/comments/lxjjlt/data_mesh_rethinking_enterprise_data_architecture/,8.0,17.0,0.0,26763.0,"Ask anyone what’s hot these days in data industry, chances are “Data Mesh” will come up first in their minds💭. Here's the all-in-one resource to better understand the concept👽  [https://www.cuelogic.com/blog/data-mesh](https://www.cuelogic.com/blog/data-mesh)"
5323,2021-03-04 14:53:37,1614862417.0,dataengineering,"Ask Ben Wilson - the author of ""Machine Learning Engineering in Action""",lxjtbq,stolzen,,https://www.reddit.com/r/dataengineering/comments/lxjtbq/ask_ben_wilson_the_author_of_machine_learning/,6.0,0.0,0.0,26762.0,"We've already asked Ben:

* Cases when ML wasn't required
* Understand the requirements and defining the scope for an ML project
* Difference between a prototype and an MVP
* Biggest mistakes when moving from a prototype to MVP
* Key skills and core responsibilities of ML engineers

More info here: [https://datatalks.club/books/20210301-ml-engineering.html](https://datatalks.club/books/20210301-ml-engineering.html)"
5324,2021-03-04 17:13:06,1614870786.0,dataengineering,"5th Swiss Streaming Meetup is coming - 1st April, 17:30-18:40",lxmp12,[deleted],,https://www.reddit.com/r/dataengineering/comments/lxmp12/5th_swiss_streaming_meetup_is_coming_1st_april/,1.0,0.0,0.0,26766.0,
5325,2021-03-04 17:22:17,1614871337.0,dataengineering,"5th Swiss Streaming Meetup is coming - 1st April, 17:30-18:40",lxmwpf,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/lxmwpf/5th_swiss_streaming_meetup_is_coming_1st_april/,3.0,0.0,0.0,26766.0,"Hey guys  We are happy to invite you to the next Swiss Streaming Meetup to join the talks of our speakers⁠:  
⁠  
\- Ant Kutschera from the Swiss Mobiliar about “Using KTables to track the State of a Billing Process” (in German)⁠  
\- Nicolas Frankel from Hazelcast about “A Change-Data-Capture use-case: designing an evergreen cache” (in English)⁠  
⁠  
The meetup will be free, more information on the agenda, speeches and registration here.  
[https://www.meetup.com/de-DE/Messaging-Streaming-Switzerland/events/276688300/](https://www.meetup.com/de-DE/Messaging-Streaming-Switzerland/events/276688300/)"
5326,2021-03-04 17:36:32,1614872192.0,dataengineering,Simplifying SQL Analytics with UDFs in Ververica Platform,lxn8h5,Marksfik,,https://www.reddit.com/r/dataengineering/comments/lxn8h5/simplifying_sql_analytics_with_udfs_in_ververica/,1.0,0.0,0.0,26766.0,
5327,2021-03-04 17:50:28,1614873028.0,dataengineering,Google Cloud or AWS,lxnkhn,dramaqueen2408,,https://www.reddit.com/r/dataengineering/comments/lxnkhn/google_cloud_or_aws/,2.0,18.0,0.0,26766.0,"Hello,

We need to store our data and video, and as we are a pretty new start-up project, I assume we wont need more than 10GB for the next 3-6 months - that will be the testing phase. However it could escalate very fast within a year after we complete, polish our service to send out to the clients. 

We also want to use the data to input to ML and AI for our future project in about 2 years time.

We are considering which to purchase Google Cloud or AWS. I contacted Google Cloud but they is not really helpful in providing pricing information and convince me much. So Im just trying to gather more information, it would be brilliant if anyone can help.

Many thanks!"
5328,2021-03-04 18:00:32,1614873632.0,dataengineering,"Airbyte - an open-source EL(T) platform that helps you replicate your data in your warehouses, lakes and databases.",lxnty2,binaryfor,,https://www.reddit.com/r/dataengineering/comments/lxnty2/airbyte_an_opensource_elt_platform_that_helps_you/,4.0,11.0,0.0,26766.0,
5329,2021-03-04 18:18:00,1614874680.0,dataengineering,"Beyond ""Hello World"": Zero-Downtime Deployments with Hazelcast on Kubernetes",lxobja,nfrankel,,https://www.reddit.com/r/dataengineering/comments/lxobja/beyond_hello_world_zerodowntime_deployments_with/,2.0,0.0,0.0,26766.0,
5330,2021-03-04 18:42:12,1614876132.0,dataengineering,Udacity Data Streaming Details,lxoy83,pebrn,,https://www.reddit.com/r/dataengineering/comments/lxoy83/udacity_data_streaming_details/,1.0,1.0,0.0,26766.0,"Hi there, I am currently doing the Data Eng nanodegree from Udacity and am at the Cloud Data WH project which I assume it is just slightly less than half-way through the whole degree. It took me about a month to get here with about 20 h/wk put in and I'm finding it OK so far. I've bought the 5 months plan for this one.

Currently Udacity has big discounts on and I can start the Data Streaming nanodegree at a fraction of its normal price. They're saying that this one is more advanced than the DE one and am wondering if it's too much to do both in parallel while working full time. I've never worked with kafka or spark before but I am comfortable coding.

Is there anyone that took both of these and if so can you please share what you think of the workload as a comparison?

Also, does the DE nanodegree require considerably more effort during the second half compared to the first?"
5331,2021-03-04 18:42:15,1614876135.0,dataengineering,Why Production Machine Learning Fails — And How to Fix It,lxoy9v,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/lxoy9v/why_production_machine_learning_fails_and_how_to/,7.0,0.0,0.0,26766.0,
5332,2021-03-04 19:13:17,1614877997.0,dataengineering,How to Migrate Your Data Lake to AWS,lxpp7s,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/lxpp7s/how_to_migrate_your_data_lake_to_aws/,1.0,0.0,0.0,26768.0,"This live, free [@streamsets](https://twitter.com/streamsets/) \+ [@awscloud](https://twitter.com/awscloud/) webinar — ""**How to Migrate Your Data Lake to AWS**” — starts in less than an hour at 10:00AM PT!

[https://go.streamsets.com/migrate-data-lake-to-aws.html?\_ga=2.211663772.845538255.1614627533-523515328.1598821021&amp;\_gac=1.124363256.1612312838.CjwKCAiAjeSABhAPEiwAqfxURW0wV7kMNNfvzh5k3yPwVflKXiSBnx4C2h1XPrWLDQqEe5xY\_RNd7BoCKEMQAvD\_BwE](https://go.streamsets.com/migrate-data-lake-to-aws.html?_ga=2.211663772.845538255.1614627533-523515328.1598821021&amp;_gac=1.124363256.1612312838.CjwKCAiAjeSABhAPEiwAqfxURW0wV7kMNNfvzh5k3yPwVflKXiSBnx4C2h1XPrWLDQqEe5xY_RNd7BoCKEMQAvD_BwE)"
5333,2021-03-04 19:40:40,1614879640.0,dataengineering,Data Engineering at Wikimedia,lxqeia,xlpz,,https://www.reddit.com/r/dataengineering/comments/lxqeia/data_engineering_at_wikimedia/,59.0,5.0,0.0,26769.0,
5334,2021-03-04 21:35:27,1614886527.0,dataengineering,How to extract data from SaaS platform? (Real time and batch),lxt4ok,mango_sorbet13,,https://www.reddit.com/r/dataengineering/comments/lxt4ok/how_to_extract_data_from_saas_platform_real_time/,1.0,10.0,0.0,26776.0,"Hello everyone,

I work in a data team of 2 at a small start up. I handle the more infrastructure and programming related tasks in the team and my colleague handles more of the analysis and business interfacing part. Between the two of us, we've built the analytics infrastructure of our company completely from scratch, using Fivetran, DBT, Snowflake and Looker. 

Lately, our company has been getting SaaS tools which don't have pre built integrations with Fivetran,  which has required me to build custom data extraction scripts specifically for those tools. I've done this in Python, and my method has been as follows: 

*  Use the requests library to make get requests to the respective APIs of the platforms
* Convert the JSON response from those requests to a structured format using pandas
* Uploading the output as csv to S3 using the boto3 library
* Configure Fivetran to load the data on these s3 buckets to Snowflake

After the last step, these data sources become a part of our regular analytics modelling and testing flow using DBT and LookML. 

The python scripts are run on a schedule either hourly or daily using cron. Furthermore, in order to avoid long run times for the scripts, at each extraction I only extract data points created from up until 7 days ago and merge it with the data that already resides on s3. The reason I chose this strategy, and not simply extracting data points whose ids are greater than the max id I have on s3 is that the state of the events tend to change from day to day, but tends to settle after max 7 days. 

I have a task coming up where I have been asked to deliver some of these data points extracted via custom python scripts to dashboard in real time, or at a latency of maximum 5 minutes. 

Now, I am looking for advice on the following questions:   
\- How can I deliver real time data from these SaaS tools to Snowflake? Once these data points reach Snowflake in real time, how can I deliver it from Snowflake to some dashboarding/reporting tool (Looker or Sheets) in real time?   
\- What other methods for data extraction can I use for these tools that don't have integrations with Fivetran? This doesn't necessarily have to be real time extraction. I am looking to improve my general vocabulary on how to build data pipelines.  
\- Is there anything dumb I am doing in the processes I have outlined? I am a novice data engineer and I have no one to guide or mentor me in my company. I've learned everything I know so far on my own.   Any advice would be deeply appreciated."
5335,2021-03-04 22:18:29,1614889109.0,dataengineering,ClickHouse March 10 Meetup -- Data contracts and an intro to Cube.js with ClickHouse,lxu56y,dbcicero,,https://www.reddit.com/r/dataengineering/comments/lxu56y/clickhouse_march_10_meetup_data_contracts_and_an/,1.0,0.0,0.0,26776.0,"Hi!  

There's a March 10th ClickHouse SF Bay Area meetup coming up fast. You are all invited to attend and learn more about ClickHouse data warehouse and some of the cool things people do with it.  This month's meetup has talks on data cleaning &amp; contracts by Abe Gong as well as an introduction to Cube.js by Igor Lukanin. Check out the [meetup invitation here](https://www.meetup.com/San-Francisco-Bay-Area-ClickHouse-Meetup/events/276499022/). 

It's no longer the Middle Ages, so we will of course be [live streaming to Youtube](https://youtu.be/fVpkcOadCg4) as well. :)

Robert Hodges (Meetup Organizer)"
5336,2021-03-04 23:01:36,1614891696.0,dataengineering,I got ya homie!,lxvbnn,nsfw_celbs,,https://www.reddit.com/r/dataengineering/comments/lxvbnn/i_got_ya_homie/,1.0,0.0,0.0,26778.0,
5337,2021-03-04 23:28:07,1614893287.0,dataengineering,Data Discovery Platform w/ Emphasis around Analytic Events,lxvwrv,Cloakie,,https://www.reddit.com/r/dataengineering/comments/lxvwrv/data_discovery_platform_w_emphasis_around/,1.0,0.0,0.0,26777.0,"I'm on the lookout for a data discovery platform like [https://www.amundsen.io](https://www.amundsen.io) or [https://github.com/linkedin/datahub](https://github.com/linkedin/datahub) that could allow my team to help democratize data across my company. However, it seems to me (at least from the initial reading I've done) that these tools are centralized around documentation for specific tables/columns in warehouses. A big need in the company is to keep track of all of the different analytic events (and their parameters) that we send to platforms like Google Firebase, Amplitude, etc. I'd prefer to document these events as individual entities rather than as a single giant field on an ""event\_name"" column text box in one of these tools... Does anyone have any ideas on how to best keep track of various analytic events and their documentation in a structured way?"
5338,2021-03-04 23:42:28,1614894148.0,dataengineering,How to prepare for DE onsite interview ETL round?,lxw7mh,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/lxw7mh/how_to_prepare_for_de_onsite_interview_etl_round/,1.0,1.0,0.0,26777.0,"I was told to be prepared for very in-depth technical discussions on my previous ETL experience and previous scheduling experience and also building a data pipeline in granular detail. 

The company has a strong emphasis on airflow but idk if that is something i should expect when discussing building a data pipeline. 

Kind of nervous as i don’t really know what to expect. They are hiring for a mid level DE. I have 2 YOE as a DE. How do you guys suggest i prepare based on this very limited amount of given info? Thanks. Im nervous because my distributed system design is not my strong suit."
5339,2021-03-04 23:54:26,1614894866.0,dataengineering,Quick profiling of data in Apache Kafka using kafkacat and visidata,lxwgvr,rmoff,,https://www.reddit.com/r/dataengineering/comments/lxwgvr/quick_profiling_of_data_in_apache_kafka_using/,2.0,0.0,0.0,26777.0,
5340,2021-03-05 00:18:23,1614896303.0,dataengineering,Excel: A novel ETL tool,lxwzuw,reddit_hates_me_91,,https://www.reddit.com/r/dataengineering/comments/lxwzuw/excel_a_novel_etl_tool/,6.0,28.0,0.0,26777.0,
5341,2021-03-05 04:48:32,1614912512.0,dataengineering,PySpark question: RDD Api vs SparkSql Api?,ly26tv,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/ly26tv/pyspark_question_rdd_api_vs_sparksql_api/,3.0,2.0,0.0,26784.0,I have a exercise where i can only use sparksql api (and not rdd api) can someone give me an example and also the difference?
5342,2021-03-05 09:08:54,1614928134.0,dataengineering,Do you guys also get contacted directly by recruiters for DE roles?,ly6frl,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/ly6frl/do_you_guys_also_get_contacted_directly_by/,47.0,39.0,0.0,26795.0,"I’m bombarded by linkedin inmails and direct emails from recruiters from all types of companies (from shitty recruiting agencies to recruiters from Facebook, Amazon, Uber, etc) only for DE roles. My linked in profile is a 7.5/10 at best. I have 2 YOE but with a mediocre tier but very large employer. most of these cold reach outs from recruiters is something like “im impressed by your experience blahaaaaaaabalalaalalalaa interview when?”.

I don’t have my resume online. I went to a average school. Currently on the job hunt, but im literally having to decline interviews because i get so many requests. I guess this is similar to what it feels like to he a half decently attractive girl on a dating app lol. Made me wonder, am i actually doing something right? Or is there THAT much demand for data engineers? Im thinking its the latter. What are your guys’ experience?"
5343,2021-03-07 16:17:13,1615126633.0,dataengineering,Wallpapers,lzriom,David851922,,https://www.reddit.com/r/dataengineering/comments/lzriom/wallpapers/,1.0,0.0,0.0,26847.0,
5344,2021-03-07 17:06:09,1615129569.0,dataengineering,Analytical Challenges in Big Data,lzsdm6,luminoumen,,https://www.reddit.com/r/dataengineering/comments/lzsdm6/analytical_challenges_in_big_data/,1.0,0.0,0.0,26854.0,
5345,2021-03-08 04:39:02,1615171142.0,dataengineering,How did you get into data engineering?,m05vt2,analystlady,,https://www.reddit.com/r/dataengineering/comments/m05vt2/how_did_you_get_into_data_engineering/,1.0,16.0,0.0,26882.0,"Over the last few months, I've been focused on getting the skills to become an entry-level Data Analyst. I never considered Data engineering because it seemed quite intimidating. A few days ago, I found a Data Engineering Trainee program and just applied for it to give it a shot. It's with a consulting group that would train you and then tries to place you at an organization and you are locked with them for two years. I do not mind this at all because it would allow me to get actual work experience so I can apply for permanent residence in the country I live in. If I get into this program, it will not be exactly how I planned my career, but it will help me stay in the country I want to live in. 

I know my motives may not be the best, but this is what I really need at the moment.  At this point, this is all wishful thinking, but it made me wonder how people got into this field."
5346,2021-03-08 18:06:53,1615219613.0,dataengineering,Advice for data engineering,m0iyc5,claufbane,,https://www.reddit.com/r/dataengineering/comments/m0iyc5/advice_for_data_engineering/,1.0,0.0,0.0,26905.0,"I have a math stats background and recently got a job as a data engineer and data scientist. I am quite comfortable with the data science side of things however, I am not sure about the data engineering. Anyone else with a similar backgound? Any advice or resources to learn data engineering?"
5347,2021-03-08 18:10:39,1615219839.0,dataengineering,The Top 20 Most Commonly Used Data Engineering Tools,m0j1gd,secodaHQ,,https://www.reddit.com/r/dataengineering/comments/m0j1gd/the_top_20_most_commonly_used_data_engineering/,1.0,10.0,0.0,26905.0,"Some of you might find this article interesting. The most commonly used data engineering tools at mid-sized tech companies based on research from over 150 interviews with data engineers:

[https://www.secoda.co/blog/the-top-20-most-commonly-used-data-engineering-tools](https://www.secoda.co/blog/the-top-20-most-commonly-used-data-engineering-tools)"
5348,2021-03-08 18:57:17,1615222637.0,dataengineering,That one weird DE thing!,m0k4of,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/m0k4of/that_one_weird_de_thing/,1.0,14.0,0.0,26904.0,"As we work on data engineering projects, no matter how big or small, there’s always that one weird (technical) thing you run into or have to deal with. For example, dealing with different date formats between various platforms, databases, sources and destinations. Is there something similar that stands out for you that you’re able to share?"
5349,2021-03-08 18:59:15,1615222755.0,dataengineering,Azure SQL CI/CD,m0k677,Jeremyvce,,https://www.reddit.com/r/dataengineering/comments/m0k677/azure_sql_cicd/,1.0,3.0,0.0,26904.0,What are your favorite tools/techniques you have used in the past or are currently using to implement CI/CD with Azure SQL?
5350,2021-03-08 19:59:11,1615226351.0,dataengineering,What did companies use before Airflow?,m0lml7,hyperandaman,,https://www.reddit.com/r/dataengineering/comments/m0lml7/what_did_companies_use_before_airflow/,1.0,14.0,0.0,26907.0,"I always like to know what problems new tools solve. I am sure Airflow solves problems that existed before but was manageable through other tools and services.

What were the tools and services that Airflow is now replacing

What’s so great about Airflow?

What are some disadvantages of using Airflow?"
5351,2021-03-08 20:03:17,1615226597.0,dataengineering,PropTech Challenge Data Science Competition Update | $7.5k Cash Prize | Submissions due Mar 26,m0lq6h,PropTech_Challenge,,https://www.reddit.com/r/dataengineering/comments/m0lq6h/proptech_challenge_data_science_competition/,1.0,0.0,0.0,26907.0,"Happy Monyay everyone!

Thank you so much for raising awareness about [our data science competition](https://www.proptechchallenge.com/)! We need all the help we can get solving [the Great Energy Disconnect](https://www.urbangreencouncil.org/content/events/great-energy-disconnect) to fight Climate Change.

We are thrilled that our real-world NYC test set has been downloaded over 100 times by participants on six continents!

[PropTech Challenge test set downloads](https://preview.redd.it/7lzo0zd9gul61.png?width=468&amp;format=png&amp;auto=webp&amp;s=606c0a475b4c2fcf21d84bc10772de05d6c1d0ae)

Based on the increased interest, we have **added $2,500** in cash prizes! Our **top 3 eligible submissions will receive:**

1. **$5,000**
2. **$2,000**
3. **$500**

We're down to our final weeks of competition, **submissions are due Friday 3/26.** Please get involved by [downloading our data](https://www.proptechchallenge.com/nyserda-tenant-energy-data) and spreading the word today!

Thanks,

PropTech Challenge"
5352,2021-03-08 20:24:05,1615227845.0,dataengineering,Ideas for Data Management Software in a Large Logistics Company,m0m8ed,Net_Limp,,https://www.reddit.com/r/dataengineering/comments/m0m8ed/ideas_for_data_management_software_in_a_large/,1.0,14.0,0.0,26908.0," 

Hey everyone,

I recently started a new job as a Data Management Analyst at a large Healthcare Logistics and Distribution company. My boss asked me last week if I have any ideas on how to improve the business process of creating new items in the company system, and any other inputs I have. She says that younger minds may have a fresh outlook on the business. I want to write her a report on what can improve the business efficiency in item creation.

Personally, the Data Management software in this company is old and unpractical. They use KLD, which is an ancient relic of the 1980s.

**My question for you is; do you know an easy to use, accessible and more automated item creation and management software that I can propose to my boss?** This software needs to be for a large data management, we're talking over thousands of items in the system.

Personally, I come from a Bachelor in Statistics and Economics, so I have experience with Python, R and SQL in business. I wanted to know other people's input.

Thank you for taking the time!"
5353,2021-03-08 20:45:47,1615229147.0,dataengineering,In Celebration of International Women’s Day!,m0mr2u,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/m0mr2u/in_celebration_of_international_womens_day/,1.0,0.0,0.0,26909.0,Data Engineers: Powerful Allies in Achieving an Equal Future: [https://streamsets.com/blog/data-engineers-powerful-allies-in-achieving-an-equal-future/](https://streamsets.com/blog/data-engineers-powerful-allies-in-achieving-an-equal-future/)
5354,2021-03-08 20:52:39,1615229559.0,dataengineering,Ask Martin Kleppmann - the author of Designing Data-Intensive Applications,m0mx2g,stolzen,,https://www.reddit.com/r/dataengineering/comments/m0mx2g/ask_martin_kleppmann_the_author_of_designing/,1.0,15.0,0.0,26909.0,"Martin is answering our questions! We've already asked:

* Should a start-up immediately plan applications for data-intense use cases?
* Should we read the book if the cloud providers already implement everything?
* Are relational databases suitable for banking systems?

And other things.

Join us and ask Marting questions! More info: [https://datatalks.club/books/20210308-designing-data-intensive-applications.html](https://datatalks.club/books/20210308-designing-data-intensive-applications.html)"
5355,2021-03-09 08:52:32,1615272752.0,dataengineering,Is big data engineering dead?,m10xn5,Limp-Ad-7289,,https://www.reddit.com/r/dataengineering/comments/m10xn5/is_big_data_engineering_dead/,1.0,9.0,0.0,26957.0,"has the big been axed, and now just DE? I think that's my hiccup is understanding this aspect of the data industry."
5356,2021-03-09 15:33:41,1615296821.0,dataengineering,Implementing CI/CD for Data using Pre-merge Hooks,m16h07,ydr-,,https://www.reddit.com/r/dataengineering/comments/m16h07/implementing_cicd_for_data_using_premerge_hooks/,1.0,0.0,0.0,26967.0,
5357,2021-03-09 15:55:53,1615298153.0,dataengineering,NEW PODCAST about Apache Kafka use-cases by Confluent und SPOUD,m16vfz,spoudagoora,,https://www.reddit.com/r/dataengineering/comments/m16vfz/new_podcast_about_apache_kafka_usecases_by/,1.0,0.0,0.0,26968.0,"[https://developer.confluent.io/podcast/the-human-side-of-apache-kafka-and-microservices-ft-spoud?utm\_source=twitter&amp;utm\_medium=organicsocial&amp;utm\_campaign=tm.devx\_ch.sa-the-human-side-of-apache-kafka-and-microservices\_content.use-cases](https://developer.confluent.io/podcast/the-human-side-of-apache-kafka-and-microservices-ft-spoud?utm_source=twitter&amp;utm_medium=organicsocial&amp;utm_campaign=tm.devx_ch.sa-the-human-side-of-apache-kafka-and-microservices_content.use-cases) 

**Summary:**

\- 4 Kafka use cases around microservices, event processing, event sourcing/the data lake, and integration architecture and their challenges.  
\- trivial problems that arise when integrating Kafka into the enterprise, because it’s not just about technology but also people and how they react to a new technology that they are not yet familiar with."
5358,2021-03-09 19:28:16,1615310896.0,dataengineering,Apache Spark 3.1 Release: Spark on Kubernetes is now Generally Available - Take a deep dive into the new features!,m1bgto,JY-DataMechanics,,https://www.reddit.com/r/dataengineering/comments/m1bgto/apache_spark_31_release_spark_on_kubernetes_is/,1.0,0.0,0.0,26981.0,
5359,2021-03-09 19:33:01,1615311181.0,dataengineering,Astronomer Apache airflow certificate,m1bkp8,powok,,https://www.reddit.com/r/dataengineering/comments/m1bkp8/astronomer_apache_airflow_certificate/,1.0,0.0,0.0,26982.0,"Hi guys , does any have any voucher for the newly released Apache airflow certificate ?"
5360,2021-03-09 20:14:03,1615313643.0,dataengineering,CICD for DE,m1ci61,powok,,https://www.reddit.com/r/dataengineering/comments/m1ci61/cicd_for_de/,1.0,9.0,0.0,26983.0,"Hi Team ,

We just started with aws EMR and airflow .We are now looking at implementing a CiCD pipeline for production release of spark jobs,etc .

Is there a good guide for me to follow to get started with this process , any ideas or videos would be much appreciated .

Regards,
Your dear data engineer who wants to implement CICD pipeline."
5361,2021-03-09 20:18:46,1615313926.0,dataengineering,Data ingestion to S3,m1cm3u,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/m1cm3u/data_ingestion_to_s3/,1.0,17.0,0.0,26983.0,"Help 

Hello everyone 

 We are closing data centers or retiring storage arrays. We need to bring data to s3 . Security team is not allowing to connect to data-lake from onprem environment to move the data to s3 . What are the options we have without compromising on security with solid plan on how to move the data. It’s just plain data movement only. Optionally: convert data from fixed width to delimited format during the movement to s3 

I can think of data sync installed . Can’t be approved legacy servers are very old and it runs on aix 
Please help on any solid guide or link to refer"
5362,2021-03-09 21:10:02,1615317002.0,dataengineering,FB DE Technical Interviews - Finished Questions and Results?,m1dpz2,clueless3867,,https://www.reddit.com/r/dataengineering/comments/m1dpz2/fb_de_technical_interviews_finished_questions_and/,1.0,4.0,0.0,26985.0,"Hey everyone!

I've heard there's 5 coding and 5 sql questions in the Facebook technical interview for Data Engineers. For those of you that have done a technical interview with Facebook: How many of the coding/SQL questions did you finish? Were you invited to an on site?"
5363,2021-03-09 21:15:43,1615317343.0,dataengineering,"Airflow system requirements (ram,cpu)",m1duo6,digichap28,,https://www.reddit.com/r/dataengineering/comments/m1duo6/airflow_system_requirements_ramcpu/,1.0,7.0,0.0,26985.0,"Hey there! I have been testing airflow without thinking too much about system requirements.
What are the minimum requirements (ram, cpu) for each component ?

Let’s say we deploy the scheduler, webserver and worker (without running anything) in 3 separate containers. If deployed with Celery, another container with redis."
5364,2021-03-09 21:47:56,1615319276.0,dataengineering,Subsetting and data masking in SQL Server,m1ek5t,Awkward-Truth-7464,,https://www.reddit.com/r/dataengineering/comments/m1ek5t/subsetting_and_data_masking_in_sql_server/,1.0,2.0,0.0,26988.0,"Hi! I was asked to create a CI/CD process within a company and they want to be able to control the amount of data ""extracted"" from production to the QA environment (only entities related to the test cases and just a coherent subset, not ALL data from the tables. AND they want to mask PII data (already catalogued). Is there a Tool or something that can manage this automatically? I did this before but in a more manual way, understanding model, rules, relationships, entities impacted per app and creating complex extraction scripts. This was at least 10 years ago so I imagine there should be something out there to help this kind of ""automated test environments"". and configurable. Thank you? I did my googling and found this: [https://curiositysoftware.ie/solutions/](https://curiositysoftware.ie/solutions/) Is it any good?"
5365,2021-03-09 22:07:34,1615320454.0,dataengineering,"Vincent Lepage, CTO of Sarus Technologies, on Confidential Data Mesh",m1f0dp,MichaelPhelan,,https://www.reddit.com/r/dataengineering/comments/m1f0dp/vincent_lepage_cto_of_sarus_technologies_on/,1.0,0.0,0.0,26992.0,
5366,2021-03-10 00:44:46,1615329886.0,dataengineering,What should I expect from a short (30 min) technical interview?,m1ii9q,engthrowaway8305,,https://www.reddit.com/r/dataengineering/comments/m1ii9q/what_should_i_expect_from_a_short_30_min/,1.0,12.0,0.0,26998.0,"I have an interview tomorrow for a data engineering position at a biotech company, and the recruiter (who did my HR interview) referred me to the interviewer (hiring manager/would-be boss) who ""will have more technical questions"" me. I'm fairly inexperienced having only done ETL for my research, an internship, and a side project along with having only used SQL in limited settings to do fairly simple queries and inserts. My primary programming language is Python though (I can do a few mediums and most easy leetcodes) but I'm not sure what to expect from the interview.

They seem interested in me because I'm an engineering student with knowledge about the process/industry who can also do some SQL/Python more than I am a SQL/Python wizard who can dip their feet into the industry.

The interview is only 30 minutes long and they didn't explicitly call it a ""technical"" or ""coding"" interview so what do you think I should expect? Any tips for it? Anything really 'extra' I could do to blow them away/make myself stand out?

Thank you!"
5367,2021-03-10 01:43:08,1615333388.0,dataengineering,Data Engineer Intern Technical Interview,m1jr81,accountingmajor7758,,https://www.reddit.com/r/dataengineering/comments/m1jr81/data_engineer_intern_technical_interview/,1.0,16.0,0.0,27002.0,"Hi Everyone!

I have an upcoming technical interview for a data engineering intern role, and I'm wondering what types of questions to expect (LC med, hard) and what concepts to be aware of. I was told Python &amp; SQL as well as ETL concepts. The teams tech stack is Python, SQL , Tableau, and Snowflake! Thank you!"
5368,2021-03-10 06:15:14,1615349714.0,dataengineering,Would I be able to land a Job as a DE in Canada?,m1p3eh,Simonaque,,https://www.reddit.com/r/dataengineering/comments/m1p3eh/would_i_be_able_to_land_a_job_as_a_de_in_canada/,1.0,11.0,0.0,27012.0,"I live in a large Canadian City(not Toronto or Vancouver) and I work as a Data Analyst. I want to jump ship to Data Engineering, but I'm not sure if I'm properly qualified. I already possess the following

Credentials:
- Social Science Undergraduate degree (McMaster U)
- Data Science Diploma from a Bootcamp (BrainStation)
- University Diploma in Business Administration, Specialization in Project Management (McMaster U)
- Cloud Practitioner AWS Certificate 
- University Certificate in Big Data Programming &amp; Architecture (McMaster U) (Completing soon)

Work Experience:
Two years experience as a Data Analyst, two years a Business Analyst(SQL monkey)

Other:
I have a few projects from the Udacity DE nano degree(I torrented it so no certification) on my GitHub, but I didn't mark them as such, just as my own projects."
5369,2021-03-10 14:59:37,1615381177.0,dataengineering,Data Science Core Skills – What Matters the Most in 2021,m1wxaa,Shradha_Singh,,https://www.reddit.com/r/dataengineering/comments/m1wxaa/data_science_core_skills_what_matters_the_most_in/,1.0,0.0,0.0,27025.0,
5370,2021-03-10 15:27:26,1615382846.0,dataengineering,Is data engineering by nature somehow unfit for remote working?,m1xfr1,Irajk,,https://www.reddit.com/r/dataengineering/comments/m1xfr1/is_data_engineering_by_nature_somehow_unfit_for/,1.0,21.0,0.0,27025.0,"I have a feeling that since we, data engineers, work with important and protected data of the companies, it might not be the best field to work remotely and employer might wish to have you in office or at least in the same country.

With new changes in working environment I'm thinking about living in a warm country and work remotely. How much will it be practical? and which skills might be more demanded for remote data engineering positions?  
Thank you."
5371,2021-03-10 16:04:23,1615385063.0,dataengineering,The Most Popular Databases – 2006/2021,m1y5wt,cuffia_azzurra_2,,https://www.reddit.com/r/dataengineering/comments/m1y5wt/the_most_popular_databases_20062021/,1.0,0.0,0.0,27025.0,
5372,2021-03-10 16:26:15,1615386375.0,dataengineering,How to model claims?,m1ymq3,werteen1,,https://www.reddit.com/r/dataengineering/comments/m1ymq3/how_to_model_claims/,1.0,5.0,0.0,27027.0,"I'm helping my team organize all our claims data. We receive monthly claims files in excel files. I'm currently finishing the a script that will process, cleanse, and standardize the files but I'm not sure how to store the data so we can see change in the claims from historical view. Someone mentioned I might need to create an OLAP cube but this isn't something I'm familiar with. Can this be done in regular SQL tables? Does anyone have some pointers or know where I can learn more for modeling claims data?"
5373,2021-03-10 16:48:14,1615387694.0,dataengineering,"As someone with no work experience in data engineering, how should I construct my resumé in order to attract the attention of hiring managers?",m1z3w2,Phoenix_Rise_UP,,https://www.reddit.com/r/dataengineering/comments/m1z3w2/as_someone_with_no_work_experience_in_data/,1.0,11.0,0.0,27029.0,"I made a career change (mental health to technology) last year, so I have no experience working within the field. How should I market myself to hiring managers?"
5374,2021-03-10 17:45:13,1615391113.0,dataengineering,Events Documentation tool,m20ehb,gil_bi,,https://www.reddit.com/r/dataengineering/comments/m20ehb/events_documentation_tool/,1.0,0.0,0.0,27030.0,"Hey,

I'm looking for an events documentation tool (preferably open source) to keep track of all the data events that designed in my company. The events usually are constructed by a set of common attributes and one semi-structed for specific attributes.

Right now I'm using a shared spreadsheet and it is somewhat uncomfortable and not really scalable"
5375,2021-03-10 17:49:46,1615391386.0,dataengineering,Is NoSQL irrelevant for data engineering? (article),m20ib5,Dashbird,,https://www.reddit.com/r/dataengineering/comments/m20ib5/is_nosql_irrelevant_for_data_engineering_article/,1.0,0.0,0.0,27032.0,"In this article, we’ll investigate use cases for which data engineers may need to interact with NoSQL data stores. 

Read more: [https://dashbird.io/blog/nosql-database-data-engineering/](https://dashbird.io/blog/nosql-database-data-engineering/?fbclid=IwAR0z7aA57uMFy2LQa3b74IaI1-DWmTVrlcc8P2AeIS1I8mvmvzmNiI3vLg0)"
5376,2021-03-10 17:57:40,1615391860.0,dataengineering,Redata - Open-Source platform that helps you monitor data-quality in all your tables.,m20ozf,mateusz_klimek,,https://www.reddit.com/r/dataengineering/comments/m20ozf/redata_opensource_platform_that_helps_you_monitor/,1.0,22.0,0.0,27032.0,
5377,2021-03-10 18:54:10,1615395250.0,dataengineering,Top 7 Big Data Trends to Dominate 2021,m22337,saik2363,,https://www.reddit.com/r/dataengineering/comments/m22337/top_7_big_data_trends_to_dominate_2021/,1.0,0.0,0.0,27034.0,
5378,2021-03-10 19:01:14,1615395674.0,dataengineering,I got ya homie!,m22a3y,nsfw_celbs,,https://www.reddit.com/r/dataengineering/comments/m22a3y/i_got_ya_homie/,1.0,0.0,0.0,27035.0,
5379,2021-03-10 19:39:49,1615397989.0,dataengineering,Role of big data engineer professionals in 2021,m23c9n,Palaksharma22,,https://www.reddit.com/r/dataengineering/comments/m23c9n/role_of_big_data_engineer_professionals_in_2021/,1.0,0.0,0.0,27036.0,
5380,2021-03-10 21:19:35,1615403975.0,dataengineering,Need feedback on a project,m25y5r,Kyruji,,https://www.reddit.com/r/dataengineering/comments/m25y5r/need_feedback_on_a_project/,1.0,5.0,0.0,27040.0,"I am working on my first data engineering project that reads data from 3 different map service APIs and writes that raw API data to a Kafka broker with 3 topics, one for each of those APIs.

Here is what I want to achieve

1. Fetch API data as is and write it to Kafka
2. Use Spark to clean and transform, then write it to a database
3. Query DB / visualize data / make a recommendation engine

I have completed \[1\] of the 3 above. I want some feedback before moving forward and implementing this whole idea. Should I clean data before writing to Kafka (in that case, will I really need Kafka?). Should I choose a graph based database? Since I want to be able to derive meanings between two points."
5381,2021-03-10 23:20:21,1615411221.0,dataengineering,What Questions to Ask?,m28x6q,BigBooledHead,,https://www.reddit.com/r/dataengineering/comments/m28x6q/what_questions_to_ask/,1.0,9.0,0.0,27056.0,"So, being new to all of this still, I feel as if I don't ask the right questions around the data we ingest. 

Our basic process looks like this:

1. Speak with the vendor regarding the data that will be received ( columns ), formatting of the exports, cadence, etc.
2. Setup pipeline that takes this data, does some basic transformation like formatting column names, dates, emails, and then loading the data raw into Bigquery ( CSV Batch example), then loads the data according to the type of append, it appends, if overwrite it overwrites, etc. 
3. Done - that is the final table

&amp;#x200B;

However, outside of receiving a data dictionary from the vendor, what should I be asking them in regards to the data? Or when we get the data, what should I be asking myself or the stakeholders about this data. What should I ask myself in regards to cleansing this data, or structuring it? 

&amp;#x200B;

Any input is greatly appreciated."
5382,2021-03-10 23:37:32,1615412252.0,dataengineering,How would you prepare for technical questions dealing with large and/or streaming data in SQL and python,m29eal,nowrongturns,,https://www.reddit.com/r/dataengineering/comments/m29eal/how_would_you_prepare_for_technical_questions/,1.0,19.0,0.0,27056.0,"If you are asked to prepare for a DE interview. 

1. The only info you are given that sql and python will be tested. 
2. You should know your basic python data structures and concepts like loops very well. Sql is ANSI standard. 
3. There will be no reference to specific libraries like pandas/numpy, or frameworks, or engines like spark, Kafka etc. 
4. You will be asked to manipulate data sets in either sql and/or python. Think fo the T in ETL.
5. You should consider dealing with massive amounts of data in how you answer these questions. You should consider how you would answer the questions if all the data isn't available to you. For example you might be asked to update aggregations as new data comes in.

How would you guys prepare for something like this? I've been doing a combination of leetcode and just making sure I know how to manipulate basic data structures well in python. But I'm not sure on how to practice the ""dealing with a lot of data esp. when you can't process all the data at once"" portion.

Conceptually, if asked to compute an aggregate like a sum I would just calculate the initial sum. And when new data came in I would sum that up and add it to that initial sum. So in python something like set x = initial sum. And then identify new data based on checking to see if a timestamp &gt; last\_timestamp. Summing that data up if it's in a list or something. and then adding that sum to x.

If it's a relational table basically do the samething but with sql. Check to see whether there is new data available in the table using a timestamp parameter. Then aggregating the new sum and then adding it to the last sum I had computed.

Things like averages would be similar but I would also have to track number of items. Like if for the last 4 items the price of each was $1. The average be would be 1. I would persist that sum (4) and the count of items (4). Then, if a new item ($2) show up I would increase the sum ($6)  and the count of items (5) and re-compute the average ($1.2).

I just don't know how to practice more complex scenarios or if I'm even thinking about these things the right way."
5383,2021-03-11 00:48:27,1615416507.0,dataengineering,The Ultimate Data Observability Checklist,m2b83z,Top-Substance2185,,https://www.reddit.com/r/dataengineering/comments/m2b83z/the_ultimate_data_observability_checklist/,1.0,0.0,0.0,27057.0,
5384,2021-03-11 03:29:34,1615426174.0,dataengineering,which python package to implement oauth 2.0 with flask?,m2emje,Ok-Message1053,,https://www.reddit.com/r/dataengineering/comments/m2emje/which_python_package_to_implement_oauth_20_with/,1.0,1.0,0.0,27058.0,"i'm developing a flask app and need to implement oauth 2.0.  which package would you recommend i look into?  i've seen authlib, flask-oauthlib, flask-dance... not sure where's the best place to start. thanks"
5385,2021-03-11 07:49:10,1615441750.0,dataengineering,Best ETL flow implementation in aws,m2j1l9,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/m2j1l9/best_etl_flow_implementation_in_aws/,1.0,24.0,0.0,27064.0,Can you guys link me to best ETL flow for server-less arch and Server architecture
5386,2021-03-11 09:40:01,1615448401.0,dataengineering,Need feedback on our product which can be useful for DE,m2ksot,prakashgupta01,,https://www.reddit.com/r/dataengineering/comments/m2ksot/need_feedback_on_our_product_which_can_be_useful/,1.0,5.0,0.0,27065.0,"Hi Folks

I am the founder of [Clouderizer](https://clouderizer.com/). We recently launched support for [deploying Jupyter Notebooks as serverless functions](https://clouderizer.com/jupyter-notebook-deployment/), with just a single CLI command. Once deployed, the serverless function can be invoked using REST endpoints. Every invocation is archived and all executed notebooks can be viewed or downloaded, with all output cells and visualisations intact. 

One of the use cases that we aimed to address with this was Data Engineering ETLs. Netflix, [famously](https://pyvideo.org/pydata-la-2019/data-and-etl-with-notebooks-in-papermill.html) has been a promoter of this use of notebooks. Converting the notebooks as an API endpoint, can help put them in the existing pipeline and gives lots of flexibility. They can be scheduled for periodic invocation or invoked from CI/CD pipelines.

I would like to know what you guys think about this feature and do you foresee this useful in your work?"
5387,2021-03-11 12:36:28,1615458988.0,dataengineering,How to Learn Python for Data Science,m2nczp,_sumit_rana,,https://www.reddit.com/r/dataengineering/comments/m2nczp/how_to_learn_python_for_data_science/,1.0,0.0,0.0,27070.0,
5388,2021-03-11 13:21:20,1615461680.0,dataengineering,"Help with 504 Deadline error when working with Dataflow, Apache Beam and Python",m2nzu7,saltysoul__,,https://www.reddit.com/r/dataengineering/comments/m2nzu7/help_with_504_deadline_error_when_working_with/,1.0,0.0,0.0,27071.0,"Hello! I am creating an Apache Beam app on Python that runs on Google's Dataflow. My app ingests data from a Spanner table of around \~60m rows.

There's an issue where the workers timeout several times when the data is being ingested causing the job to fail. I am trying to tweak the timeout limits in Python, however I have not found a proper way to do this.

I have specified the details and code in the following Stack Overflow link: [https://stackoverflow.com/questions/66553275/i-get-504-deadline-exceeded-in-apache-beam-using-readfromspanner](https://stackoverflow.com/questions/66553275/i-get-504-deadline-exceeded-in-apache-beam-using-readfromspanner)

I am reaching out here as well in case anyone has solved this in the past.

&amp;#x200B;

Thank you!"
5389,2021-03-11 13:53:10,1615463590.0,dataengineering,The dangers of untrusted Spark SQL input in a shared environment,m2og50,foundergiant,,https://www.reddit.com/r/dataengineering/comments/m2og50/the_dangers_of_untrusted_spark_sql_input_in_a/,1.0,0.0,0.0,27072.0,
5390,2021-03-11 15:01:54,1615467714.0,dataengineering,Rant - Crappy interview experience,m2pkog,anAnonymousWolverine,,https://www.reddit.com/r/dataengineering/comments/m2pkog/rant_crappy_interview_experience/,1.0,36.0,0.0,27075.0,"Just wanted to vent a bit my frustrations.

Had an interview yesterday for a data engineer. I had specifically asked the recruiter if there would be a coding assesment and, if so, which tools should I prepare for.

She flat out replied that there would be no coding assessment so naturally did not prepare for one. Lo and behold, I join the interview and there's an SQL coding assesment. Not like a simple join type question (to make sure I actually know the basics) but an assessment where he changes the requirements halfway through.

So I bombed it, mostly because I haven't really done SQL at my job for the last six months (been mostly working on setting up a data pipeline infrastructure).

I guess it doesn't really matter because I was mostly just curious about the position but it kinda affected my confidence a bit.

End rant! Has something like this happened to anyone else before?"
5391,2021-03-11 16:34:02,1615473242.0,dataengineering,Introducing a framework to build a documentation-first culture in your data team,m2rfvg,knlph,,https://www.reddit.com/r/dataengineering/comments/m2rfvg/introducing_a_framework_to_build_a/,1.0,7.0,0.0,27077.0,"All of us face challenges around documentation, but it's rarely talked about. While companies do come up with their internal tools and framework to build a stronger data culture, the cofounder of Atlan (a modern data catalog solution) shares the framework that helped Atlan build a documentation-first culture.

I'd love to hear from you about the steps you have taken to encourage a culture of documentation in your data team? What has helped your team really bring it into practice?

[https://towardsdatascience.com/data-documentation-woes-heres-a-framework-6aba8f20626c](https://towardsdatascience.com/data-documentation-woes-heres-a-framework-6aba8f20626c)"
5392,2021-03-11 16:50:22,1615474222.0,dataengineering,DBT integration with different tools?,m2ru2u,dubidabidubidu,,https://www.reddit.com/r/dataengineering/comments/m2ru2u/dbt_integration_with_different_tools/,1.0,3.0,0.0,27078.0,"Hi all,

I am starting to explore solutions for data transformation, and one of the requisites is that our ML training pipelines (that run on kubeflow pipelines) are automatically started once the necessary tables have been materialized.

Looking at the [docs](https://docs.getdbt.com/docs/building-a-dbt-project/hooks-operations), I can see that it has support for sql hooks, but I was looking for some webhook support or to trigger an http call. Are there any workarounds for that, or tools that have this implemented? The only thing I can think of is a watcher service that would poll changes on the database and trigger the training pipelines accordingly."
5393,2021-03-11 18:58:32,1615481912.0,dataengineering,"Have till end of May, GCP DE Certification?",m2uwjp,seriousplatyboi,,https://www.reddit.com/r/dataengineering/comments/m2uwjp/have_till_end_of_may_gcp_de_certification/,1.0,6.0,0.0,27089.0,"I have till the end of May to get my GCP data engineering certification. The only knowledge I have on data engineering is from a class I did in my master's last year, but that was mainly just theory. I have not applied data engineering concepts before. 

Is it possible to get a sense of GC tools and interview for a DE gig and do well enough by end of May? Thanks."
5394,2021-03-11 19:43:21,1615484601.0,dataengineering,I got ya homie!,m2w12z,nsfw_celbs,,https://www.reddit.com/r/dataengineering/comments/m2w12z/i_got_ya_homie/,1.0,0.0,0.0,27090.0,
5395,2021-03-11 20:03:01,1615485781.0,dataengineering,Rust for DE?,m2wj1k,Albertulysses,,https://www.reddit.com/r/dataengineering/comments/m2wj1k/rust_for_de/,1.0,18.0,0.0,27093.0,"I'm curious to know what the landscape of Rust in the DE field is? I know Java, Scala and Python are the primary DE languages right now but are any of you using Rust or plan to use Rust? I'm just curious to hear some stories for the community."
5396,2021-03-11 20:38:57,1615487937.0,dataengineering,Importing MySQL Data into Delta Lake,m2xdpx,houqp,,https://www.reddit.com/r/dataengineering/comments/m2xdpx/importing_mysql_data_into_delta_lake/,1.0,0.0,0.0,27092.0,
5397,2021-03-12 02:31:25,1615509085.0,dataengineering,Currently working in reporting/analytics - I'd like to move to data engineering ASAP. What steps do I need to take? What can I do to make my resume competitive?,m35220,DEThrowaway_21,,https://www.reddit.com/r/dataengineering/comments/m35220/currently_working_in_reportinganalytics_id_like/,1.0,11.0,0.0,27111.0,"Hi all,

I currently work in analytics, and after 10+ years in this field, I'd really like to move on.  I've been at my current company for 5+ years, I'm not learning anything new, and it's starting to affect my performance.  I don't really want to just move to another analytics position.  It also seems like I've reached the top of the analytics pay scale without moving into management (no pls).  I've been considering software engineering and data engineering, and either would be more interesting and challenging than what I'm doing now.   That being said, it seems my current job is more tangentially related to data engineering so that might be an easier pivot/move.

My current job involves mostly pulling and staging data for reporting using SAS, staging data for Tableau dashboards, creating Tableau dashboards, and running reports.  What things can I learn after work that can help me pivot to data engineering?

I've taken 6 comp sci classes, currently enrolled in OMSCS at Georgia Tech, but don't really have much in the way of projects or anything on my resume.  I've put together two very low, low, low level projects.  That being said I have decent working knowledge of SQL (using in SAS at work), Python, and Java. 

I've applied to a few data engineering positions, but I don't think my resume is very appealing as it stands...

Has anyone else taken this route?  Any advice on things I can learn after work?  Udemy courses?  YouTube channels?  Reading?  

Any help is appreciated!"
5398,2021-03-12 02:33:50,1615509230.0,dataengineering,ETL vs ELT: Key Differences and Latest Trends,m3540d,audiologician,,https://www.reddit.com/r/dataengineering/comments/m3540d/etl_vs_elt_key_differences_and_latest_trends/,1.0,9.0,0.0,27111.0,
5399,2021-03-12 03:13:16,1615511596.0,dataengineering,Data lineage,m35wfc,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/m35wfc/data_lineage/,1.0,1.0,0.0,27111.0,"We’re working on a Etl solution using glue, step functions and lambda . How do I make a data lineage solution here. If it’s airflow we could use dbt tool . How do I able to do it in scenario. 

Flow . S3-&gt;Lambda-&gt;meta table-&gt;Lambda-&gt;step functions-&gt;Glue -&gt; Snowflake database. Where exactly I can insert data lineage solution here."
5400,2021-03-12 04:54:12,1615517652.0,dataengineering,Airflow?,m37w0t,Excelisorr,,https://www.reddit.com/r/dataengineering/comments/m37w0t/airflow/,1.0,18.0,0.0,27116.0,"I have a several python scripts that I want to run at midnight that scrape websites and generate Cvs files. I have lot of issues of cron not successfully launching and running the scripts. I have started to move over to airflow but when the scripts run none of the files are saved on local drive. I am thinking I setup airflow wrong. Anyone have a good tutorial or example GitHub they can share?

Thanks"
5401,2021-03-12 08:56:51,1615532211.0,dataengineering,Looking for any advice or critique on my resume!,m3c4dd,-hacker,,https://www.reddit.com/r/dataengineering/comments/m3c4dd/looking_for_any_advice_or_critique_on_my_resume/,1.0,14.0,0.0,27124.0,
5402,2021-03-12 09:57:43,1615535863.0,dataengineering,How will solving a problem to find longest subsequence (maintaining order) between 2 strings prove that I am a good data engineer ?,m3cxbm,ezio20,,https://www.reddit.com/r/dataengineering/comments/m3cxbm/how_will_solving_a_problem_to_find_longest/,1.0,10.0,0.0,27127.0,I feel the process is becoming one sided.. These questions should be asked for Software Engineering roles(if at all).
5403,2021-03-12 11:31:01,1615541461.0,dataengineering,Is it realistic to move from SAS Statistical programming to data engineering with Spark/Scala.,m3e4nc,Born-Comment3359,,https://www.reddit.com/r/dataengineering/comments/m3e4nc/is_it_realistic_to_move_from_sas_statistical/,1.0,3.0,0.0,27130.0,"I have an experience with SAS Statistical programming, and I want to move to data engineering. I am a  quick learner and can learn Spark/Scala. Is it realistic to find a Spark/Scala data engineering job without any experience? I mean should I give it a try (learning Spark) or it will be a waste of my time because I can't find a Spark job without data engineering experience anyway.

And then, if I find a Spark job, what salary should I expect on average?"
5404,2021-03-12 12:50:37,1615546237.0,dataengineering,Data Engineering,m3f4v9,taliun,,https://www.reddit.com/r/dataengineering/comments/m3f4v9/data_engineering/,1.0,0.0,0.0,27131.0,
5405,2021-03-12 14:14:29,1615551269.0,dataengineering,Matillion and Snowflake Data Warehouse Tutorial,m3gbru,Silentspue,,https://www.reddit.com/r/dataengineering/comments/m3gbru/matillion_and_snowflake_data_warehouse_tutorial/,1.0,0.0,0.0,27135.0,
5406,2021-03-12 15:34:41,1615556081.0,dataengineering,What are your thoughts/experiences with building vector databases for AI/MLOps?,m3hndj,namenotpicked,,https://www.reddit.com/r/dataengineering/comments/m3hndj/what_are_your_thoughtsexperiences_with_building/,1.0,9.0,0.0,27139.0,
5407,2021-03-12 19:32:18,1615570338.0,dataengineering,The Most Popular Databases – 2006/2021,m3mkm9,cuffia_azzurra_2,,https://www.reddit.com/r/dataengineering/comments/m3mkm9/the_most_popular_databases_20062021/,1.0,4.0,0.0,27151.0,
5408,2021-03-12 19:40:54,1615570854.0,dataengineering,Example Driven High Level Overview of Spark with Python,m3mrbq,amcquistan,,https://www.reddit.com/r/dataengineering/comments/m3mrbq/example_driven_high_level_overview_of_spark_with/,1.0,6.0,0.0,27150.0,
5409,2021-03-12 19:48:15,1615571295.0,dataengineering,I have created a new metadata first approach to data infrastructure. And I also created Apache Hive and was a former Facebook Data Architect. AMA!,m3mx01,raghumurthy,,https://www.reddit.com/r/dataengineering/comments/m3mx01/i_have_created_a_new_metadata_first_approach_to/,1.0,3.0,0.0,27150.0,
5410,2021-03-12 21:02:43,1615575763.0,dataengineering,The perfect data hand-off: from Raw to Machine Learniing,m3oj1e,therealiamontheinet,,https://www.reddit.com/r/dataengineering/comments/m3oj1e/the_perfect_data_handoff_from_raw_to_machine/,1.0,0.0,0.0,27151.0,[https://streamsets.com/blog/how-to-load-data-into-google-bigquery-on-dataproc-and-automl/](https://streamsets.com/blog/how-to-load-data-into-google-bigquery-on-dataproc-and-automl/)
5411,2021-03-12 22:50:19,1615582219.0,dataengineering,Customer Facing API (Cloud/Azure) Advice/Discussion,m3qtkw,MrFlamingQueen,,https://www.reddit.com/r/dataengineering/comments/m3qtkw/customer_facing_api_cloudazure_advicediscussion/,1.0,2.0,0.0,27151.0,"I spent a few days searching on this and I wanted to hopefully get some answers since my searches are kinda dry.

I'm a new grad hired to work as a junior data scientist, but my company has a lot of data engineering troubles.

&amp;#x200B;

**Introduction**

We're trying to create a customer facing API from our cloud based storage (Azure); however, I'm finding it difficult to figure out how exactly this API needs to be structured.

&amp;#x200B;

**Requirements**

Some of the requirements are:

* queryable around time
* secure authentication, only give customers access to their data
* JSON as the output
* Simple. Refrain from going beyond stating parameters (No SQL queries).

&amp;#x200B;

**Data Sources**

For our data source:

* Looking at 5-10 GB/day (could grow in the future)
* I've cleaned and aggregated the data for the API use. Intermediate steps are stored in Azure Data Lake Gen2 with your standard zones

&amp;#x200B;

**Initial Thoughts**

In my experience, I just work with the data directly using Azure Machine Learning, but customers probably won't be writing Python code.

Initially, I was looking at GraphQL on a Azure Function directly to the data lake (ADLS Gen2). If we can use RBAC to authenticate, we can use ACL's on the directory structure. I'm a little uneasy on how exactly the time filtering will workout (if we have to read directory -&gt; then filter or if we use further date partitions beyond year, month, day, hour). This would be the first GraphQL project at my company, so I'm a bit hesitant with introducing it and being stuck with managing it.

Some other options use Azure SQL Server, HDInsight, Data Explorer. For the sake of brevity, I won't go super deep into those options.

&amp;#x200B;

**Request For Help :(**

Overall, I'm just really confused about how I'm supposed to interact with these technologies to provide an API. I feel like a lot of the things I'm reading are more marketing materials than technical documentation or speak about data lakes in an esoteric manner (""With your data lake, you can serve data and it's gonna be great!"").

I feel like I'm either overcomplicating this or there's just some piece of knowledge I'm missing to make the connection, so if you have any thoughts or additional questions, please leave a reply!

Thanks in advance!"
5412,2021-03-13 01:38:06,1615592286.0,dataengineering,How to reduce the latency of the web app which uses postgres tables?,m3u7xh,YaswanthBangaru,,https://www.reddit.com/r/dataengineering/comments/m3u7xh/how_to_reduce_the_latency_of_the_web_app_which/,1.0,6.0,0.0,27155.0,"I am generating dash plotly apps by fetching data from postgresql, it works fine, however, it shows a latency of upto 5 seconds before actually showing the plots, because I believe that's because of the connection to postgres and fetching data and doing tiny amount of preprocessing, what are some ways I could reduce it?

Also, my app is in such a way that whenever someone opens the link, it fetches the latest data from postgres tables, how could I change this to (I don't know the right work tbh) in a way that, the app stores the latest data in the temp memory by itself and is independent of someone opening the link? I am trying to deploy it on aws elasticbeanstalk by the way if that matters."
5413,2021-03-13 02:18:20,1615594700.0,dataengineering,ELT with MySQL?,m3uyl3,bdawgfresh,,https://www.reddit.com/r/dataengineering/comments/m3uyl3/elt_with_mysql/,1.0,13.0,0.0,27155.0,"Interested in getting some opinions or experiences. I currently work as a BI engineer doing fairly standard management style cyber compliance reporting.

Our team’s responsibilities include extracting, modeling data, and building dashboards.

The tools we have at our disposal are a MySQL database, python, tableau, and a modestly powered Linux host.

I want to implement an ELT architecture for our team to reap the benefits it offers over traditional ETL. We have some sql alchemy and pandas already in use. Tools like DBT and Singer don’t seem to support MySQL.

Theoretically, how could a combination of python tools and MySQL be used to accomplish this?

Thanks in advance for your thoughts!"
5414,2021-03-13 04:57:32,1615604252.0,dataengineering,Looking for Design Pattern store entities in multiple dimensions,m3xm8b,wildstumbler,,https://www.reddit.com/r/dataengineering/comments/m3xm8b/looking_for_design_pattern_store_entities_in/,1.0,11.0,0.0,27160.0,"Hi,

I don't know if this technology or design pattern or solutions for it exists. I was thinking it might be possible to store attributes of entities in a (relational) database in multiple versions of certain dimensions.

For example:

If you had the dimension 'time' for an entity \`User\` which contains an attribute \`name\` as a string and \`age\` as integer, changes to the name attribute could be stored as multiple versions in the dimension of time for that one entity.

|dimension\_time|user\_id|name|age|
|:-|:-|:-|:-|
|2021-03-10 10:02|1|John|30|
|2021-03-12 12:34|1|Johnny|30|

Also, some logic would have to be applied to declare the entity with the highest date in the time dimension as 'active', which would (probably) require some processing on application layer at retrieval from the database, I assume?

The downside of the above example is obviously that the unchanged attributes, in this age \`age\`, become redundant. Normalization could be be applied, but I think it would require a dimension of, in this case, 'time' for every entity to which it applies.

Other examples of dimensions could be:

* 'language', for storing attributes of an entity in multiple languages.

I think in some sense it could be seen as git-tree like structure, with forks and merges, if I understand correctly.

I hope my question is at least somewhat understandable. I do not know the relevant terms so throwing any possibly relevant terms or principles/concepts would be greatly appreciated!

Thanks"
5415,2021-03-13 07:12:00,1615612320.0,dataengineering,How smart is Spark?,m3zpep,dataengineerdude,,https://www.reddit.com/r/dataengineering/comments/m3zpep/how_smart_is_spark/,1.0,6.0,0.0,27162.0,"If I have a pipeline that reads parquets, goes through a long complicated set of transformations, is Spark always smart enough to only read the columns used through out the pipeline. Or do I need to put a select ontop of the parquet read immediately for only those columns needed?"
5416,2021-03-13 07:30:30,1615613430.0,dataengineering,Online Learning Tracks?,m3zz60,vladproex,,https://www.reddit.com/r/dataengineering/comments/m3zz60/online_learning_tracks/,1.0,8.0,0.0,27162.0,"Any experience with online learning tracks or bootcamps for data engineering? I'm interested in a long term learning track with a certification, but I'm not high on budget. 

Currently looking at Datacamp and Udacity."
5417,2021-03-13 14:18:24,1615637904.0,dataengineering,Streaming Annotated Monthly – March 2021,m457gq,antonmry,,https://www.reddit.com/r/dataengineering/comments/m457gq/streaming_annotated_monthly_march_2021/,1.0,0.0,0.0,27175.0,
5418,2021-03-13 20:18:52,1615659532.0,dataengineering,Is there a shortage of Spark/Scala developers? What is the average salary?,m4bxxc,Born-Comment3359,,https://www.reddit.com/r/dataengineering/comments/m4bxxc/is_there_a_shortage_of_sparkscala_developers_what/,1.0,4.0,0.0,27195.0,Will it be hard for a beginner to get a Spark/Scala job? What salay can a beginner expect?
5419,2021-03-13 20:52:38,1615661558.0,dataengineering,how to get into data engineering ?,m4cmnu,aingeliic,,https://www.reddit.com/r/dataengineering/comments/m4cmnu/how_to_get_into_data_engineering/,1.0,35.0,0.0,27200.0,"hi everyone, I’m a highschool senior starting university this fall. The school I plan on going to offers coop, so I’d be able to start interning right after my first year in uni. 

I’ve taken an interest in the data engineering route and was wondering how I could prepare myself before applying for internships. I’m a total noob and just started learning sql (I saw online that this was a good first step). 

Are there any resources you would recommend looking into? any beginner projects you’d recommend doing so I could prepare myself? or tips/advice in general?

thanks for your response in advance, i appreciate it !"
5420,2021-03-13 21:18:51,1615663131.0,dataengineering,Data Engineering compensation,m4d65w,United-Marionberry38,,https://www.reddit.com/r/dataengineering/comments/m4d65w/data_engineering_compensation/,1.0,11.0,0.0,27200.0,"Why Data Engineer compensation is less than Software Engineers?  As the companies turning into more data oriented, we need more experienced Data Engineers to handle the company data intelligently ensuring security, legal compliance, easily accessible to stake holders etc. The current Data engineers not only excel in building data models, writing sql queries they also have to write code to process big data, integrate 3 rd party data using APIs unlike traditional data integration tools. They set the foundation for Data analysts and Data scientists and Product. It is in fact, a Software Engineering role with additional skills of Data engineering."
5421,2021-03-13 22:18:29,1615666709.0,dataengineering,Data engineer/analyst position opening up. How do I prepare ?,m4efrb,ProUnicornz,,https://www.reddit.com/r/dataengineering/comments/m4efrb/data_engineeranalyst_position_opening_up_how_do_i/,1.0,2.0,0.0,27201.0,"Hey Guys,

I'm currently employed as a Sysadmin at a Radio Station. With a coming restructure of our digital strategy, there is an data engineer/analyst opening up. I've been interested in this field for about a year and I build some funprojects to get into the basics of the trade.I have already told HR that I'm interested in the position and got positiv feedback. I will definitely get a chance to work it.Even though this will be learning by doing thing, which is cool with the Company, I want to come in as prepared as possible and show them that I can step up my role and perform good in this position.So now comes the question: How do I prepare for this.My current experiences are:

* networking
* linux server administration
* relational databases and sql basics
* python,bash,powershell
* (win server administration) - which is not particular use full i guess ?

What are some courses/books/programs/certifications I can do in preparation for this Job.We are using google analytics, so I think the google data engineer exam and google cloud architect certs are interesting to me right?

Can you guys recommend anything to prepare for these certifications or anything else to prepare me especially for the analyst part?

Thanks !"
5422,2021-03-13 23:42:31,1615671751.0,dataengineering,Cost of data engineering?,m4g69o,fake_actor,,https://www.reddit.com/r/dataengineering/comments/m4g69o/cost_of_data_engineering/,1.0,7.0,0.0,27206.0,"Working with large datasets can get expensive. Both running pipelines on the data and even just storing the data once you get to TB and PB scale. 

Do you consider cost when designing solutions for data engineering?

Does your company care about costs?

Do you feel limited in what you can do due to cost?"
5423,2021-03-13 23:53:02,1615672382.0,dataengineering,Tips on creating a data dictionary,m4geet,secodaHQ,,https://www.reddit.com/r/dataengineering/comments/m4geet/tips_on_creating_a_data_dictionary/,1.0,1.0,0.0,27206.0,"Our team wrote this article with some tips on creating a data dictionary, it might interest some of you looking to better organize your data

[https://www.secoda.co/blog/how-to-create-a-data-dictionary-a-step-by-step-guide](https://www.secoda.co/blog/how-to-create-a-data-dictionary-a-step-by-step-guide)"
5424,2021-03-14 02:17:04,1615681024.0,dataengineering,Is software engineering experience/degree required to break into data engineering?,m4jb02,Sure-Fox9929,,https://www.reddit.com/r/dataengineering/comments/m4jb02/is_software_engineering_experiencedegree_required/,2.0,20.0,0.0,27208.0,"Hi all!

I'm a data analyst (some experience in Python and SQL) who is interested in becoming a data engineer. From my perspective, it seems that those with backgrounds/degrees in software engineering have an easier time than data analysts in transitioning to data engineering.

In your experience, can a data analyst reasonably transition to being a data engineer? Does it require getting a degree/credential in software engineering first, or are there other routes that a data analyst should take?

Many thanks for your perspectives!"
5425,2021-03-14 06:05:02,1615694702.0,dataengineering,What data intensive project have you developed outside of work?,m4ngs9,Bulky_Aardvark_1335,,https://www.reddit.com/r/dataengineering/comments/m4ngs9/what_data_intensive_project_have_you_developed/,1.0,4.0,0.0,27210.0,"I am currently working as a data engineer, but recently I've reached a level of proficiency on the system I work on such that my ability to learn more on the job has stalled. In trying to further my development, I've done reading outside of work, namely ""Designing Data-Intensive Applications"" which i really enjoyed and inspired what I would like to do next: build a data intensive application. Building up my general web dev skills in the process also would be cool. 

&amp;#x200B;

So, before I put serious thought into what I want this project to be, I was interested to hear other people in the community talk about projects they have done that they are proud of and feel they have learned a lot from."
5426,2021-03-14 08:50:44,1615704644.0,dataengineering,25 y/o: Went from $45k to $145k TC in 2 years as a Data Engineer AMA,m4q21s,1337codethrow,,https://www.reddit.com/r/dataengineering/comments/m4q21s/25_yo_went_from_45k_to_145k_tc_in_2_years_as_a/,1.0,88.0,0.0,27215.0,"Began my first DE role 2 years ago started off $45k and then got a raise to $65k. I just accepted my 2nd job as a DE for a mid-level role for $145k TC. Honestly feels surreal... 

I spent about 30-50 hours a week studying/applying/interviewing for 3-4 months to land 2 offers out of about 90 applications. AMA"
5427,2021-03-14 09:13:38,1615706018.0,dataengineering,Scope/Complexity Inflation?,m4qd8q,m_mk1213,,https://www.reddit.com/r/dataengineering/comments/m4qd8q/scopecomplexity_inflation/,1.0,0.0,0.0,27215.0,"Does anyone else get the sense that a lot of jargon and new tools or applications make the areas of data engineering and other domains within data science appear more complex than they actually are? Making it more opaque for outsiders. DS itself is a fairly new term and there have been new products that AWS/Google/MS are offering but that don't necessarily make these tasks more complex than they previously were. What are your thoughts?

One of the implications could be - Let's say I am building a data science team at a mid-size startup (say 100+ people) , do I need to have a specialized data engineering or can I task those performing business intelligence to be responsible for the end to end process. I don't mean to imply that Data Engineers are redundant- In a large size corporation, It could make sense to have them operating in silos."
5428,2021-03-14 09:34:06,1615707246.0,dataengineering,Extract delta from flat file,m4qmz0,priyasweety1,,https://www.reddit.com/r/dataengineering/comments/m4qmz0/extract_delta_from_flat_file/,1.0,6.0,0.0,27216.0," We are working on a project where we receive the file daily file of 90GB approx which consists of \~60M rows. The problem is source team is sending the full file every day (90GB) which is inclusive of the history of unchanged data as well as new data.

&amp;#x200B;

History data:

1 ABC 2020

2 DEF 1950

3 FEG 2010

4 REF 2011

&amp;#x200B;

New file:

1 ABC 2021 --&gt; Update

3 FEG 2010 --&gt; No change

4 REF 2011 --&gt; No change 

5 Yed 2016 --&gt; New insert

note: number 2 has been deleted . 

Now it is our task to split only the New inserts, Deletes, and Updates to separate files and load. How could we do this efficiently in python or directly do this in snowflake if the time and cost are considered less and best practice to do. The final result should be in a flat file with only delta/incremental in that file by eliminating the unchanged records."
5429,2021-03-14 13:56:40,1615723000.0,dataengineering,"The 33rd edition @data_weekly focus on Michael Stonebraker’s Top 10 Big Data Blunders, Stanford University’s AI index report 2021, Maxime’s The future of the Business Intelligence is open source, Mehdi’s data engineering skills report, Apache Airflow survey 2020,",m4tyos,vananth22,,https://www.reddit.com/r/dataengineering/comments/m4tyos/the_33rd_edition_data_weekly_focus_on_michael/,1.0,0.0,0.0,27232.0,
5430,2021-03-14 16:47:18,1615733238.0,dataengineering,The minimum salary I should expect for a Spark DE role.,m4wd4a,Born-Comment3359,,https://www.reddit.com/r/dataengineering/comments/m4wd4a/the_minimum_salary_i_should_expect_for_a_spark_de/,1.0,14.0,0.0,27235.0,"I know Spark with python, but unfortunately have no experience with DE, but I'd like to break into DE. What salary should I expect as a starter? Do I have any chances to be accepted for a middle Spark developer position if I ace the interview?"
5431,2021-03-14 16:51:52,1615733512.0,dataengineering,We are strong together,m4wg0s,Sarifslv,,https://www.reddit.com/r/dataengineering/comments/m4wg0s/we_are_strong_together/,1.0,0.0,0.0,27235.0,
5432,2021-03-14 16:55:34,1615733734.0,dataengineering,Operational Challenges in Big Data,m4wifn,luminoumen,,https://www.reddit.com/r/dataengineering/comments/m4wifn/operational_challenges_in_big_data/,1.0,0.0,0.0,27235.0,
5433,2021-03-14 17:11:38,1615734698.0,dataengineering,Wich OS for a Data Engineer,m4wts7,_Marwan02,,https://www.reddit.com/r/dataengineering/comments/m4wts7/wich_os_for_a_data_engineer/,1.0,5.0,0.0,27236.0,"Hello,

As a Data Engineerr, would you recommand to buy a Mac or a Windows LapTop  ?"
5434,2021-03-14 17:20:10,1615735210.0,dataengineering,ETL developer to Data Engineer,m4wzn2,ahyamon,,https://www.reddit.com/r/dataengineering/comments/m4wzn2/etl_developer_to_data_engineer/,1.0,4.0,0.0,27237.0,"Hi, I recently started as an ETL developer at a service based company. I have a bachelor's degree and was looking for advice on whether I should do a master's degree for better prospects or if there were resources/ roadmap available for me to become a data engineer/ data scientist. Thank you!"
5435,2021-03-14 18:29:48,1615739388.0,dataengineering,"Went From 90k to 140k by Managing Phone Screens Really Well (originally posted in r/cscareerquestions, but thought it would be might be more relevant here in retrospect)",m4ycom,SuhDudeGoBlue,,https://www.reddit.com/r/dataengineering/comments/m4ycom/went_from_90k_to_140k_by_managing_phone_screens/,1.0,0.0,0.0,27243.0,
5436,2021-03-14 20:52:26,1615747946.0,dataengineering,Career change to data engineering,m51cbx,xwenren,,https://www.reddit.com/r/dataengineering/comments/m51cbx/career_change_to_data_engineering/,1.0,0.0,0.0,27248.0,"Hey guys, 

I actually just graduated from my masters program in statistics and started my job this week as a reporting analyst(python, sql). Mostly just accepted this job since it’s risky to not have a job during the pandemic. 

I did mostly data science and ml work during my internship, but had a few etl tasks and schemes design tasks, and enjoyed these much more than the ds types of work.

I am wondering about how realistic it would it be for me to transition to a data engineering position in the future given my current profile, and what would be a good way to do it. I have a few classmates that managed to secure entry level de positions at faang companies, but these people had been grinding leetcode for a while. Will I get pigeonholed by my current profile on my resume? On LinkedIn, I see a lot of people start in de after graduating, but not a lot transition to it."
5437,2021-03-14 21:04:18,1615748658.0,dataengineering,Career change to data engineering,m51lve,Crafty-Prior-9785,,https://www.reddit.com/r/dataengineering/comments/m51lve/career_change_to_data_engineering/,1.0,2.0,0.0,27249.0,"Hey guys, 

I actually just graduated from my masters program in statistics and started my job this week as a reporting analyst(python, sql). Mostly just accepted this job since it’s risky to not have a job during the pandemic. Have another offer for a bi analyst position right now that I think I’m going to turn down.

I did mostly data science and ml work during my internship, but had a few etl tasks and schemes design tasks, and enjoyed these much more than the ds types of work.

I am wondering about how realistic it would it be for me to transition to a data engineering position in the future given my current profile, and what would be a good way to do it. I have a few classmates that managed to secure entry level de positions at faang companies, but these people had been grinding leetcode for a while. Will I get pigeonholed by my current profile on my resume? On LinkedIn, I see a lot of people start in de after graduating, but not a lot transition to it. Will people question my motivations if I apply in 1-2 years from now after learning things on my own, or applying concepts to work."
5438,2021-03-14 21:51:57,1615751517.0,dataengineering,Introducing Kamu - World's first global collaborative data pipeline,m52nmb,sergiimk,,https://www.reddit.com/r/dataengineering/comments/m52nmb/introducing_kamu_worlds_first_global/,1.0,23.0,0.0,27249.0,
5439,2021-03-14 22:59:27,1615755567.0,dataengineering,Coding test - Text file processing. What should I expect in the interview?,m543qg,Initial_Squirrel2693,,https://www.reddit.com/r/dataengineering/comments/m543qg/coding_test_text_file_processing_what_should_i/,1.0,7.0,0.0,27254.0,"Hi, 

I have my first coding round test at a company for a Data engineer role. They have sent me a text file “Alice in wonderland” story and the questions will be to process the text file in python. I have never given such a test before and hence wanted to understand what kind of file/text processing should i expect. I understand that this depends on the company but if this process is normal, what should one prepare for? I looked at Glassdoor and researched a little more on Google about what to expect but couldn’t find anything. 

Thanks in advance!!"
5440,2021-03-15 01:30:50,1615764650.0,dataengineering,How do you schedule your Python jobs?,m57a43,Lolmanza7,,https://www.reddit.com/r/dataengineering/comments/m57a43/how_do_you_schedule_your_python_jobs/,1.0,9.0,0.0,27264.0,"Hi 

The company I work for is basically a Microsoft shop (we use SSIS, SSAS, SQL Server, Powershell).

We schedule out jobs using SQL Server Agent.

I started writing my ETL jobs in Python and am trying to schedule python scripts using SQL Server Agent.

What do you guys use?"
5441,2021-03-15 03:53:24,1615773204.0,dataengineering,Data extraction (incremental - file formats - data lakes ),m59z2u,digichap28,,https://www.reddit.com/r/dataengineering/comments/m59z2u/data_extraction_incremental_file_formats_data/,1.0,2.0,0.0,27268.0,"Hey there! I have some questions related to data extraction which I have been thinking about for a while.

I worked some time ago extracting data from txt files. The company I was working for was receiving these files from several providers. These files had a standard format with a header and trailer, and the data followed a | separator approach. The files were loaded to a staging table where we applied some transformations, before loading them to a core table. Basically, if there was an updated row (based on updateDte field), then we updated the core and later inserted new records. This was basically following an incremental load approach using only SQL server.

For the last couple of years, I have been doing the same but with a BI tool called Qlikview (qliksense also) which isn’t actually mentioned that much in online posts, forums and conferences in the US. The ones you can hear about most of the times are Tableau and PowerBi. Anyway, I was following the same approach for incremental loads using QVD files (I think a QVD file could be similar to parquet but instead a patented one from Qlik tech). 

As I’m trying to move away from any proprietary format or system, I started to study and practice with airflow, and all the possible mature open source tools available out there to process the data.

My questions are; how do you do the data extraction from structured sources ? Which tools do you use ? How do you store the data (Avro, parquet, csv, etc) ?
How do you handle incremental loads with, for example, parquet? 

I recently watched some videos and I heard about a data “snapshot” and partitions approach. Does it mean all tables are stored in parquet, or any other format as they are in a data lake, every time the extraction process runs?

I’m a little bit lost with this, and would love to clear up my mind. 

Thanks for all your help!"
5442,2021-03-15 09:04:15,1615791855.0,dataengineering,What online courses should I take to learn data engineering?,m5eyqw,richardcho2,,https://www.reddit.com/r/dataengineering/comments/m5eyqw/what_online_courses_should_i_take_to_learn_data/,1.0,1.0,0.0,27285.0,"Hello, I am working as a junior data analyst who has concerns with the career.

I have degrees of B.S and M.S in Statistics, which means that I have not taken Computer science courses such as data structure and algorithm. Of course, I have some knowledge but definitely, I am not an expert.

The problem is, I sometimes don't understand what data engineers say when I work with them. I just, literally, don't understand. Shame to say but I didn't know what ETL means at first.

My question is, how can I study data engineering? I don't expect that I could switch my career to data engineer but I really need some minimum knowledge for better communication. I need to know what exactly data engineering is, what skills are necessary, how the process goes on, etc. I have googled some online courses such as datacamp, but I am not sure if they are really helpful. Are there any recommended courses for beginners? 

Thanks in advance."
5443,2021-03-15 09:29:45,1615793385.0,dataengineering,Data Pipelines With DBT (Data Build Tool) in Azure,m5fb3t,valdasm,,https://www.reddit.com/r/dataengineering/comments/m5fb3t/data_pipelines_with_dbt_data_build_tool_in_azure/,1.0,8.0,0.0,27286.0,
5444,2021-03-15 11:57:07,1615802227.0,dataengineering,Best etl data flow creator?,m5h9zj,Comfortable_Ad_7070,,https://www.reddit.com/r/dataengineering/comments/m5h9zj/best_etl_data_flow_creator/,1.0,3.0,0.0,27292.0,"Pls give me suggestions for etl data flow creator, DOMO is quite best option, something workflow maker suggestions?"
5445,2021-03-15 13:09:24,1615806564.0,dataengineering,Ververica Platform 2.4: Complete support for Flink SQL and improved resource utilization,m5ic39,Marksfik,,https://www.reddit.com/r/dataengineering/comments/m5ic39/ververica_platform_24_complete_support_for_flink/,1.0,0.0,0.0,27296.0,
5446,2021-03-15 16:39:18,1615819158.0,dataengineering,I start my DE job in two weeks. I could really use some sage advice,m5m3hz,omiobabbinocarokann,,https://www.reddit.com/r/dataengineering/comments/m5m3hz/i_start_my_de_job_in_two_weeks_i_could_really_use/,1.0,6.0,0.0,27305.0,"Hello r/dataengineering,

I’ve been provided an amazing opportunity to work in data engineering, despite the fact that most of my work up to this point has been business analysis. I start in two weeks, and I really want to succeed in my new role. According to the job description, I’ll be building and maintaining data pipelines and will be involved in collection, mapping across many sources, storage, and processing. Between the description and the interview, I’ve also picked up a few languages and programs I’ll be exposed to: SQL, Python, Spark, IBM Db2, Tableau, and more.

I have SQL and Python experience (admittedly lower-intermediate at best), and I’m currently working through online coursework and books on both Tableau and Spark. While I can appreciate that the company is investing in me as a long-term resource and trusts me to develop over the coming months and years, I really want to do everything I can to succeed and begin contributing as soon as possible. What advice would you have for me, given my current position? What can I do to make sure I perform well in this role?"
5447,2021-03-15 18:11:26,1615824686.0,dataengineering,"Stuck on a normalization task for multiple large datasets, really don't know where to start and i feel like i'm going in circles with it, can someone advise on what i should do.",m5o67v,WhiteWulph,,https://www.reddit.com/r/dataengineering/comments/m5o67v/stuck_on_a_normalization_task_for_multiple_large/,1.0,3.0,0.0,27310.0,"I have about 8 tables that require to be normalised.   


I've ""normalised"" one table which is now split into 3 tables but its all manual inserts.   


I dont get how to add new records to the foreign tables, automatically and then the wider issue of having to do this for another 7 tables mean that i'm going to end up splitting 8 tables into near 30 tables which to me seems like over kill (correct me if i'm wrong). I'm still relatively new to DE so its hard for me to grasp this concept.   


Can you give me any examples or advice on how you've scheduled it all to always split and append new records into normalised tables."
5448,2021-03-15 18:19:59,1615825199.0,dataengineering,Database equivalent of Unix shell,m5od9i,balkon16,,https://www.reddit.com/r/dataengineering/comments/m5od9i/database_equivalent_of_unix_shell/,1.0,3.0,0.0,27310.0,"I'm looking for a topic for my master thesis. I've read a book recommended by many ""Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems"" by Martin Kleppmann in a hope that I may get inspired by reading it. The last chapter of the book is dedicated to yet-to-be-solved problems. One of them is designing an unbundled-database equivalent of the Unix shell. This is what the author has to say on the topic (p. 525):

&gt;The tools for composing data systems are getting better, but I think one major part is missing: we don’t yet have the unbundled-database equivalent of the Unix shell (i.e., a high-level language for composing storage and processing systems in a simple and declarative way). For example, I would love it if we could simply declare \`mysql | elasticsearch\`, by analogy to Unix pipes \[22\], which would be the unbundled equivalent of \`CREATE INDEX\`: it would take all the documents in a MySQL database and index them in an Elasticsearch cluster. It would then continually capture all the changes made to the database and automatically apply them to the search index, without us having to write custom application code. This kind of integration should be possible with almost any kind of storage or indexing system.

The only research that I've found is that concerning timely dataflow and differential dataflow. I wonder whether there is more research that I'm missing or maybe there isn't any because the problem presented is unimportant or has been solved a long time ago. I'd appreciate your thoughts on the idea of building such a system: is it needed?"
5449,2021-03-15 18:35:28,1615826128.0,dataengineering,Kafka Connect JDBC Sink deep-dive: Working with Primary Keys,m5oq3v,rmoff,,https://www.reddit.com/r/dataengineering/comments/m5oq3v/kafka_connect_jdbc_sink_deepdive_working_with/,1.0,0.0,0.0,27310.0,
5450,2021-03-15 20:15:56,1615832156.0,dataengineering,Ask Alex Petrov - the author of Database Internals,m5r1wx,stolzen,,https://www.reddit.com/r/dataengineering/comments/m5r1wx/ask_alex_petrov_the_author_of_database_internals/,1.0,1.0,0.0,27315.0,"We've already asked Alex:

* How graph databases can be distributed
* How BigQuery manages to process lots of information so fast
* Byzantine Consensus

And other things!

More information here: [https://datatalks.club/books/20210315-database-internals.html](https://datatalks.club/books/20210315-database-internals.html)"
5451,2021-03-15 20:22:57,1615832577.0,dataengineering,Capturing Every Change From Shopify’s Sharded Monolith,m5r7tv,nfrankel,,https://www.reddit.com/r/dataengineering/comments/m5r7tv/capturing_every_change_from_shopifys_sharded/,1.0,0.0,0.0,27315.0,
5452,2021-03-15 21:00:20,1615834820.0,dataengineering,Data Engineering tracks/courses to pursue,m5s2rm,morgoth07,,https://www.reddit.com/r/dataengineering/comments/m5s2rm/data_engineering_trackscourses_to_pursue/,1.0,3.0,0.0,27314.0,"Hello there,

I am a beginner at this field (DE) and currently in 4th year of CS. I just finished the IBM Data Engineering Specialization on coursera but I feel like it wasn't really of much help apart from just revising some basic concepts. 

Which courses should I take now to improve my skills and is there a site I can pick up projects to practice my skills? Also I want to learn aws related to DE as well. Which websites career path should I go with? Is there some website that teaches from A to Z or is there so ordered course format that I should follow?

I am just really confused, any help will be greatly appreciated."
5453,2021-03-16 04:26:42,1615861602.0,dataengineering,data engineering interview with reddit!,m5yufj,ryeryebread,,https://www.reddit.com/r/dataengineering/comments/m5yufj/data_engineering_interview_with_reddit/,1.0,17.0,0.0,27333.0,"Hi! I have a data engineering interview with reddit, and I've only had 2-3 DE interviews. The technical interview will be half case and half python/sql problems. What can I expect from a DE interview from the front page of the internet?"
5454,2021-03-16 05:32:44,1615865564.0,dataengineering,Data engineering culture at New York Time,m6018y,neoneo112,,https://www.reddit.com/r/dataengineering/comments/m6018y/data_engineering_culture_at_new_york_time/,1.0,0.0,0.0,27336.0,Does anyone have any insights in the engineering culture at the New York Times?
5455,2021-03-16 07:02:25,1615870945.0,dataengineering,Storage for Incremental Load from REST API in Azure,m61h75,lady5ybil,,https://www.reddit.com/r/dataengineering/comments/m61h75/storage_for_incremental_load_from_rest_api_in/,1.0,0.0,0.0,27337.0,"I have a scenario that involves an initial bulk load from a REST API followed by an incremental load at close intervals to upsert new/modified records. We are talking service desk data with over a hundred million records and thousands added everyday with constant updates. The ETL tool of choice is Azure Data Factory with a lookupactivity to retrieve records based on last modified time stamp. However, what is the most efficient or sensible storage design? 
For example: data lake, delta lake, azure sql, azure synapse DW (has to be in the azure eco system) given that we want to minimize any necessary transformations and keep the raw data in a valid state (no duplicates, upserts, near real time).
Would this require a relational data store or is there any feasibility within the data lake storage to account for incremental loads balancing storage and performance?"
5456,2021-03-16 07:59:18,1615874358.0,dataengineering,Microsoft data engineer phone interview tips,m62au0,archa290,,https://www.reddit.com/r/dataengineering/comments/m62au0/microsoft_data_engineer_phone_interview_tips/,1.0,0.0,0.0,27340.0,"Hey y'all! This is my first post in reddit and I need help!
I am a new grad searching for jobs in data domain with previous data engineer and BI experience. 
I have a phone interview scheduled with Microsoft in a couple of days and need inputs as to what they generally ask in this interview.
I've seen a lot of posts that helped me along thus far and any valuable inputs for this will definitely be of great help! 
Thanks in advance :)"
5457,2021-03-16 09:00:44,1615878044.0,dataengineering,Newbie DE: is a postgraduate course on Deep Learning worth it?,m63525,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/m63525/newbie_de_is_a_postgraduate_course_on_deep/,1.0,0.0,0.0,27343.0,"I have done a postgraduate course on Data Engineering last year, and am going to start working as a DE soon. Obviously I'm new to this so will take a little bit of time to get myself up and running. Now, I have enrolled in another postgraduate course on Deep Learning (Computer Vision, NLP, ML, Robotics etc).

Would it be worth it for the future opportunities to carry on and complete this Deep Learning course as a DE? My main focus is DE (a DE who also knows ML/DL)."
5458,2021-03-16 12:59:50,1615892390.0,dataengineering,Books on modern data warehousing?,m66g7z,stigmatic666,,https://www.reddit.com/r/dataengineering/comments/m66g7z/books_on_modern_data_warehousing/,1.0,13.0,0.0,27350.0,Most popular and recommended data warehousing books are written in early 2000 or before. Are there any books on modern data warehousing?
5459,2021-03-16 14:41:26,1615898486.0,dataengineering,Cost-efficient way to schedule lightweight jobs in AWS?,m685z0,__hey_there,,https://www.reddit.com/r/dataengineering/comments/m685z0/costefficient_way_to_schedule_lightweight_jobs_in/,1.0,0.0,0.0,27354.0,"Hi there,   
I have a few Python repos with jobs that I need to run on a schedule (let's say daily). I can use dockerized or non-dockerized way to run them.

I need to spend as little money on their scheduler as possible (so let's say managed Airflow is not an option). 

What's do you think about the following approach: 

1) Cloud formation scheduled trigger triggers    
2) Lambda function submits fagate task   
3) Fargete task automatically shuts down when it's done   


I guess this will allow me to have no 24/7 running resources, which is the most cost-efficient thing I can think of. 

Would you suggest it or something else?"
5460,2021-03-16 14:48:53,1615898933.0,dataengineering,How likely you would get an SQL course for free from a data analytics perspective?,m68aqi,enuintor,,https://www.reddit.com/r/dataengineering/comments/m68aqi/how_likely_you_would_get_an_sql_course_for_free/,1.0,0.0,0.0,27354.0,"I'm making a course for my youtube channel focussed on data analytics and wanted to know how many people are willing to get that course on youtube for free.   
If you want to get a course as soon as I release it on youtube you can subscribe to my channel as well.  
[One Developer Pirate - YouTube](https://www.youtube.com/channel/UCfvrpZcHfBl-AJvZnHyhBPw)

[View Poll](https://www.reddit.com/poll/m68aqi)"
5461,2021-03-16 16:08:04,1615903684.0,dataengineering,Does anyone have experience with Deloitte?,m69v75,-hacker,,https://www.reddit.com/r/dataengineering/comments/m69v75/does_anyone_have_experience_with_deloitte/,1.0,7.0,0.0,27357.0,"Hello Everyone, 

I am currently a Business Intelligence Analyst. I have been graduated and in this role coming up on 18 months. 

I am trying to make the transition to Data Engineering, and I have an opportunity with Deloitte. I am not sure what the industry stigma is with this type of consulting work.

Does anyone have experience working with one of these firms, if so how did it go? Also is that a good stepping stone to get some actual real world experience? Do other companies look down on this type of work when you are trying to position to a more permanent role (Asking because I know there can be some weird cultural things in the industry, does it look good on a resume)?

Sorry for such stream of consciousness type post. 

Thanks in advance,"
5462,2021-03-16 17:51:00,1615909860.0,dataengineering,How would you QA data before/after a migration?,m6c4kx,romanX7,,https://www.reddit.com/r/dataengineering/comments/m6c4kx/how_would_you_qa_data_beforeafter_a_migration/,1.0,23.0,0.0,27360.0,"My team is about to migrate our data warehouse from AWS to GCP. We already have a good plan around how we will execute the migration. However, there has not been much discussion around QA/testing the data after execution. In my mind I just figured we would do simple statistical summary comparisons (mean, max, min, counts, etc) along with basically building duplicate dashboards, one using the old data source and the other using the new and eyeballing it...

Has anybody created a test plan for data migration before? I'd love to hear some suggestions."
5463,2021-03-16 17:51:50,1615909910.0,dataengineering,DE with ML/DL qualifications?,m6c58s,AMGraduate564,,https://www.reddit.com/r/dataengineering/comments/m6c58s/de_with_mldl_qualifications/,1.0,3.0,0.0,27360.0,"I have done a postgraduate course on Data Engineering last year, and am going to start working as a DE soon. Obviously I'm new to this so will take a little bit of time to get myself up and running. Now, I have enrolled in another postgraduate course on Machine Learning (ML) and Deep Learning (DL).

Would it be worth it for the future opportunities to carry on and complete this ML/DL course? My main focus is to be a DE (a DE who also knows ML/DL)."
5464,2021-03-16 17:59:50,1615910390.0,dataengineering,Apache Flink Community Survey (seeking input),m6cboy,databACE,,https://www.reddit.com/r/dataengineering/comments/m6cboy/apache_flink_community_survey_seeking_input/,1.0,1.0,0.0,27359.0,
5465,2021-03-16 18:11:47,1615911107.0,dataengineering,Automated Data Quality Testing At Scale With SQL And Machine Learning,m6clvd,monacodev,,https://www.reddit.com/r/dataengineering/comments/m6clvd/automated_data_quality_testing_at_scale_with_sql/,1.0,0.0,0.0,27359.0,
5466,2021-03-16 19:46:08,1615916768.0,dataengineering,Strategies for better data pipeline building,m6etji,databandai,,https://www.reddit.com/r/dataengineering/comments/m6etji/strategies_for_better_data_pipeline_building/,1.0,0.0,0.0,27365.0,[https://databand.ai/blog/10-advanced-data-pipeline-strategies-for-data-engineers/](https://databand.ai/blog/10-advanced-data-pipeline-strategies-for-data-engineers/)
5467,2021-03-16 20:40:34,1615920034.0,dataengineering,Apache Spark configuration optimization,m6g2jj,luminoumen,,https://www.reddit.com/r/dataengineering/comments/m6g2jj/apache_spark_configuration_optimization/,1.0,0.0,0.0,27368.0,[http://spark-configuration.luminousmen.com/](http://spark-configuration.luminousmen.com/)
5468,2021-03-16 21:20:28,1615922428.0,dataengineering,"As a data leader, how do you reinforce the importance of documentation for your teams?",m6h0hd,knlph,,https://www.reddit.com/r/dataengineering/comments/m6h0hd/as_a_data_leader_how_do_you_reinforce_the/,1.0,1.0,0.0,27371.0,"All of us face challenges around documentation, but it's rarely talked about. While companies do come up with their internal tools and framework to build a stronger data culture, the cofounder of Atlan (a modern data catalog solution) shares the framework that helped Atlan build a documentation-first culture.

I'd love to hear from you about the steps you have taken to encourage a culture of documentation in your data team? What has helped your team really bring it into practice?

[https://towardsdatascience.com/data-documentation-woes-heres-a-framework-6aba8f20626c](https://towardsdatascience.com/data-documentation-woes-heres-a-framework-6aba8f20626c)"
5469,2021-03-16 21:48:16,1615924096.0,dataengineering,I hate my job.,m6hnek,Not-NedFlanders,,https://www.reddit.com/r/dataengineering/comments/m6hnek/i_hate_my_job/,1.0,10.0,0.0,27372.0,"I hate my job, but I love working with data. This is my second job as a DE and it’s been miserable the entire time, which is in stark contrast to my first DE job.

I’m in a government contracting position so there’s a ton of oversight on DB permissions and whatnot. Intellisense was taken away a while back, but after a good fight, we got it back. 

Today I got word that we’re going to encrypt all objects on our servers which means we won’t be able to see, script, alter or create stored procedures or views.

I don’t understand how I’m supposed to reasonably do my job without asking for a personal DBA to sit with me and open what I need on the day to day. 

There are a lot of other things wrong with this job but today it just drained me. I don’t feel like I can effectively do my job. 

Part of the reason we’re stripping access to SP is because we have to “protect” business logic....but I’m the one who’s writing that business logic in the first place. I don’t get it. 

I love being a data engineer. I love this career path, but I don’t love this job right now."
5470,2021-03-16 22:45:26,1615927526.0,dataengineering,Managing S3 Data Store Partitions with AWS Glue Crawlers and Glue Partitions API using Boto3 SDK,m6iycp,amcquistan,,https://www.reddit.com/r/dataengineering/comments/m6iycp/managing_s3_data_store_partitions_with_aws_glue/,1.0,0.0,0.0,27376.0,
5471,2021-03-17 00:23:02,1615933382.0,dataengineering,Answers from Martin Kleppmann's AMA,m6l3sy,stolzen,,https://www.reddit.com/r/dataengineering/comments/m6l3sy/answers_from_martin_kleppmanns_ama/,1.0,3.0,0.0,27379.0,"Martin was doing an AMA in the [DataTalks.Club](https://DataTalks.Club) last week. You asked me to put the questions and Martin's answers somewhere online. 

I finally finished writing a script for extracting that from Slack. So here they are now: [https://datatalks.club/books/20210308-designing-data-intensive-applications.html](https://datatalks.club/books/20210308-designing-data-intensive-applications.html)"
5472,2021-03-17 02:02:48,1615939368.0,dataengineering,"Setting up a dbt data-ops workflow, using dbt cloud and snowflake",m6n56p,joseph_machado,,https://www.reddit.com/r/dataengineering/comments/m6n56p/setting_up_a_dbt_dataops_workflow_using_dbt_cloud/,1.0,0.0,0.0,27384.0,"Hi everyone,  If you are setting up a transformation layer with dbt, then dbt cloud is a very convenient way to get started. In this post I go over how to 

1. setup a local dev environment with dbt
2. Setup warehouse(Snowflake) permissions
3. Setup CI to run on pull requests and
4. Schedule jobs from dbt cloud.

[https://www.startdataengineering.com/post/cicd-dbt/](https://www.startdataengineering.com/post/cicd-dbt/)

   Hope this helps someone. Any feedback is appreciated."
5473,2021-03-17 05:05:47,1615950347.0,dataengineering,DE interview for GoldMan Sachs,m6qoli,sahilgupta201191,,https://www.reddit.com/r/dataengineering/comments/m6qoli/de_interview_for_goldman_sachs/,1.0,0.0,0.0,27395.0,"Hi 
I have Coderpad round for Goldman Sachs for DE role in few days. They mentioned it will be more of Data structures and Algorithms. 
Did anyone has experience with this ? Any sample questions and difficulty level ? 
Any help is greatly appreciated. 
Thanks."
5474,2021-03-17 10:29:14,1615969754.0,dataengineering,$100 Paid Research,m6vp1l,SnooDoughnuts2516,,https://www.reddit.com/r/dataengineering/comments/m6vp1l/100_paid_research/,1.0,1.0,0.0,27404.0,"Take part in this paid research! We are looking for data experts who are familiar with Google BigQuery. We are looking for experts working in the field of Data Analysis, Migration and Data Integration. We focus on people who have working experience with Google BigQuery. Please do not participate if you are not a Google BigQuery user. https://app.respondent.io/respondents/projects/view/6051b5d91d21f700126af3b0/we-are-looking-for-data-experts-who-are-familiar-with-google-bigquery.-take-our-survey!?referralCode=irinaudrescu-e421bcda0c1a"
5475,2021-03-17 12:25:08,1615976708.0,dataengineering,Best laptop for data engineer,m6xc7w,Key_Base8254,,https://www.reddit.com/r/dataengineering/comments/m6xc7w/best_laptop_for_data_engineer/,1.0,5.0,0.0,27404.0,"Gaming laptop like   Lenovo Legion 5 - 2XID | Ryzen 7 4800H | 16GB | SSD 512GB | GTX1650Ti 4GB or 

Acer nitro 5 2021 11th Gen Intel® Core™ i5-11300H @ 3.10GHz, NVIDIA® GeForce® GTX 1650 with 4GB of dedicated GDDR 6 1x8GB DDRIV, upgradeable up to 32GB, 512 GB SSD.

Btw i used for machine kearning and deep learning also.

Thank you"
5476,2021-03-17 14:15:27,1615983327.0,dataengineering,I wrote a guide on querying S3 object stores with Presto or Trino,m6z2o4,njanakiev,,https://www.reddit.com/r/dataengineering/comments/m6z2o4/i_wrote_a_guide_on_querying_s3_object_stores_with/,1.0,0.0,0.0,27410.0,
5477,2021-03-17 15:07:32,1615986452.0,dataengineering,New data analyst tasked with major overhaul needing guidance!,m6zzld,closetedswiftie,,https://www.reddit.com/r/dataengineering/comments/m6zzld/new_data_analyst_tasked_with_major_overhaul/,1.0,8.0,0.0,27412.0,"TL;DR new data analyst tasked to overhaul entire data analytics process needs guidance  

I’m relatively new at data analytics and I have been mostly self taught. I recently accepted a job as a data analyst at a startup. This is my first official job as a data person!   

My new team is not technical at all but had been able to develop several Google Sheet dashboards with data being pulled from Snowflake warehouse. In the process, they created such a complicated backend process. And our dashboards regularly display wrong numbers. They use a data connector to pull data from Snowflake warehouse, and use vlookup to pull the numbers they need. To me, this seems elementary and can cause a lot of potential human errors.   

Well, I was tasked to do an complete overhaul of our dashboards. I want to move our dashboards to Excel, since it has more powerful functionality and I figure I can do pivot tables among other things to pull and combine data more systematically and precisely.  I love working with data but I have no experience with data warehousing, querying, etc. I feel like I’m in over my head.   

I need some guidance and direction on how to essentially create an entire data analytics process. From connecting to data warehouse, querying, analyzing the data, to displaying the data!  

Any book, article, or guide is greatly appreciated!!! Thank you"
5478,2021-03-17 15:11:54,1615986714.0,dataengineering,WHAT IS DATA PIPELINE? | BEST TOOLS FOR OPERATIONS WITH DATA PIPELINES,m702q5,Sasha-Jelvix,,https://www.reddit.com/r/dataengineering/comments/m702q5/what_is_data_pipeline_best_tools_for_operations/,1.0,2.0,0.0,27413.0,"What is Meant by a [Data Pipeline](https://www.youtube.com/watch?v=0qJgz5A3ND8)? It is a series of tools and actions for organizing and transferring the data to different storage and analysis system. It automates the ETL process (extraction, transformation, load).  

*Processing img 1xuack8k8ln61...*

As a data pipeline example, you can collect information about your customers’ devices, location, and session duration and track their purchases and interaction with your brand’s customer service."
5479,2021-03-17 18:11:27,1615997487.0,dataengineering,Building a Cloud Data Platform - Webinar,m73wtw,lensesio,,https://www.reddit.com/r/dataengineering/comments/m73wtw/building_a_cloud_data_platform_webinar/,1.0,0.0,0.0,27421.0,
5480,2021-03-17 19:04:46,1616000686.0,dataengineering,Getting into Data Engineering,m754fv,chillypanda71,,https://www.reddit.com/r/dataengineering/comments/m754fv/getting_into_data_engineering/,1.0,0.0,0.0,27426.0,"I like to do freelance work on the side of my day job as a data analyst for a defense contractor. I mostly do the freelance work with excel as I’m really good with excel functions, vba, pivot tables, etc. I am always looking to make an extra buck but there is not much demand out there. I mostly work on Upwork and have heard about fiverr. 

I am really interested in getting a data engineering role in my company. But my current background is mostly with Excel, Python, SQL(SAP HANA), Tableau, And SAP business Objects. A lot of these roles are asking in addition to what I know for Kubernetes, presto, kafka, spark, Hadoop, pig, and hive(Varying on the role). I am taking some classes on Udemy but need to practice these kinds of skills. I was wondering if you guys have any experience in these tools and if you would know of any company looking to hire someone or let them shadow to gain this experience. For me learning the skillset is more important than the money because this is what I really want to do and just want the experience.
I’m single, in my late 20s"
